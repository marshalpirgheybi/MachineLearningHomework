{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b46fe41",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\n",
    "<font size=5>\n",
    "    In the Name of God\n",
    "<font/>\n",
    "<br/>\n",
    "<br/>\n",
    "<font>\n",
    "    Sharif University of Technology - Departmenet of Electrical Engineering\n",
    "</font>\n",
    "<br/>\n",
    "<font>\n",
    "    Introducing with Machine Learing - Dr. S. Amini\n",
    "</font>\n",
    "<br/>\n",
    "<br/>\n",
    "Spring 2023\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr/>\n",
    "<div align=center>\n",
    "<font size=6>\n",
    "    Neural Networks Practical Assignment\n",
    "    \n",
    "    Question 1\n",
    "</font>\n",
    "<br/>\t\t\n",
    "<font size=4>\n",
    "<br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0fc13",
   "metadata": {
    "id": "a6c2fd6d"
   },
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "44babb65",
   "metadata": {
    "id": "4e90a030"
   },
   "outputs": [],
   "source": [
    "# Set your student number\n",
    "student_number = 98109815\n",
    "Name = 'Amirmohammad'\n",
    "Last_Name = 'Marshalpirgheybi'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a337a",
   "metadata": {
    "id": "339da203"
   },
   "source": [
    "# Rules\n",
    "- You are not allowed to add or remove cells. You **must use the provided space to write your code**. If you don't follow this rule, **your Practical Assignment won't be graded**.  \n",
    "\n",
    "- Collaboration and using the internet is allowed, but your code **must be written by yourself**. **Copying code** from each other or from available resources will result in a **zero score for the assignment**.\n",
    "\n",
    "- You are not allowed to use `torch.nn`, `torch.optim` and any activation function and loss function implemented in torch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "12b76789",
   "metadata": {
    "id": "881f2e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (1.21.6)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.10.0 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from matplotlib) (1.21.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (4.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: torch==1.13.0 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from torchvision) (1.13.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: torch in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\tarahan\\anaconda3\\envs\\pytorchenv\\lib\\site-packages (from torch) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install torchvision\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886188c7",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "55a0adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Dict\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18510868",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders\n",
    "\n",
    "Here, we download and load the train and test `FashionMNIST` dataset with the desired transforms. Then, we define the dataloaders for `train` and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dc8759e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FashionMNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_set = FashionMNIST(root='.', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8f6763e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = train_set[0][0].shape\n",
    "input_dim = np.prod(image_shape).item()\n",
    "num_classes = len(FashionMNIST.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c695ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, 64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dac6c2",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualize 1 random image from each class\n",
    "\n",
    "- **Hint**:  You can use `plt.subplots` for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e3d6b0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## FILL HERE\n",
    "data = train_set.data\n",
    "targets = train_set.targets\n",
    "plt.figure()\n",
    "for i in range(num_classes):\n",
    "    class_data = data[targets == i,:,:]\n",
    "    random_idx = np.random.choice(class_data.shape[0],1)\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(class_data[random_idx,:,:].reshape(class_data.shape[1],class_data.shape[2]))\n",
    "    title = train_set.classes[i]\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c5aba",
   "metadata": {},
   "source": [
    "## Initializing model's parameters\n",
    "\n",
    "In this part, we create the model and initialize its parameters and store the values of these parameters in the variable `parameters` which is a dictionary including the weigths and biases of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e6d40952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_linear_layer(parameters: dict, shape, device, i=None):\n",
    "    \"\"\"\n",
    "    This function adds parameters of a linear unit of shape `shape` to the `parameters` dictionary.\n",
    "    \"\"\"\n",
    "    n_in, n_out = shape\n",
    "    with torch.no_grad():\n",
    "        w = torch.zeros(*shape, device=device)\n",
    "        # kaiming initialization for ReLU activations:\n",
    "        bound = 1 / np.sqrt(n_in).item()\n",
    "        w.uniform_(-bound, bound)\n",
    "        b = torch.zeros(n_out, device=device)  # no need to (1, n_out). it will broadcast itself.\n",
    "    w.requires_grad = True\n",
    "    b.requires_grad = True\n",
    "    # `i` is used to give numbers to parameter names\n",
    "    parameters.update({f'w{i}': w, f'b{i}': b})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce914706",
   "metadata": {},
   "source": [
    "Now we define our neural network with the given layers and add the weights and biases to the dictionary `parameters`. **You are allowed to modify the values of the layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8f3867d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache() \n",
    "gc.collect()\n",
    "layers = [\n",
    "    (input_dim, 512),\n",
    "    (512, 256),\n",
    "    (256, 128),\n",
    "    (128, 64),\n",
    "    (64, num_classes)\n",
    "]\n",
    "num_layers = len(layers)\n",
    "parameters = {}\n",
    "\n",
    "# setting the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# adding the parameters to the dictionary\n",
    "for i, shape in enumerate(layers):\n",
    "    add_linear_layer(parameters, shape, device, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd2c8e",
   "metadata": {},
   "source": [
    "## Defining the required functions\n",
    "\n",
    "In this section, we should define the required functions. For each of these functions, the inputs and the desired outputs are given and you should write all or part of the function. **You are not allowed to use the activation functions and the loss functions implemented in torch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b413d8",
   "metadata": {},
   "source": [
    "Computing affine and relu outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bebeeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    ## FILL HERE\n",
    "    return x @ w+b\n",
    "\n",
    "def relu(x):\n",
    "    ## FILL HERE\n",
    "    return (torch.abs(x)+x)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9baa5e",
   "metadata": {},
   "source": [
    "Function `model` returns output of the whole model for the input `x` using the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d2562962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x: torch.Tensor, parameters, num_layers=num_layers):\n",
    "    # number of batches\n",
    "    B = x.shape[0]\n",
    "    x = x.view(B, -1)\n",
    "    ## FILL HERE\n",
    "    for i in range(num_layers):\n",
    "        \n",
    "        x = affine_forward(x,parameters[f'w{i}'],parameters[f'b{i}'])\n",
    "        if(i < num_layers-1):\n",
    "            x = relu(x)\n",
    "    output = x\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a9b4c",
   "metadata": {},
   "source": [
    "Implementing cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6959621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(scores, y):\n",
    "    n = len(y)\n",
    "    ## FILL HERE\n",
    "    pred = scores.exp() / (scores.exp().sum(-1)).unsqueeze(-1)\n",
    "    loss = -pred[range(y.shape[0]), y].log().mean()\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a589af",
   "metadata": {},
   "source": [
    "Implementing a function for optimizing paramters and a function to zeroing out their gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3121c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_optimizer(parameters: Dict[str, torch.Tensor], learning_rate=0.001):\n",
    "    torch.cuda.empty_cache()\n",
    "    '''This function gets the parameters and a learning rate. Then updates the parameters using their\n",
    "    gradient (parameter.grad). Finally, you should zero the gradients of the parameters after updating\n",
    "    the parameter value.'''\n",
    "    ## FILL HERE\n",
    "    for p in parameters:\n",
    "        new = parameters[p]\n",
    "        with torch.no_grad():\n",
    "            new -= learning_rate * new.grad\n",
    "        new.grad.zero_()\n",
    "        parameters.update({p: new})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b4cf8",
   "metadata": {},
   "source": [
    "Training functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "76c0f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred: np.ndarray, y_true: np.ndarray):\n",
    "    ## FILL HERE\n",
    "    acc = (y_pred == y_true).sum(axis = 0)/len(y_pred)\n",
    "    return acc\n",
    "\n",
    "def train(train_loader,learning_rate=0.001, epoch=None):\n",
    "    '''This function implements the training loop for a single epoch. For each batch you should do the following:\n",
    "        1- Calculate the output of the model to the given input batch\n",
    "        2- Calculate the loss based on the model output\n",
    "        3- Update the gradients using loss.backward() method\n",
    "        4- Optimize the model parameters using the sgd_optimizer function defined previously\n",
    "        5- Print the train loss (Show the epoch and batch as well)\n",
    "        '''\n",
    "    train_loss = 0\n",
    "    N_train = len(train_loader.dataset)\n",
    "    \n",
    "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
    "    # for calculateing the accuracy later\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        p = model(x, parameters)\n",
    "        del x\n",
    "\n",
    "        ## FILL HERE\n",
    "        loss = cross_entropy_loss(p,y)\n",
    "        loss.backward()\n",
    "        sgd_optimizer(parameters,learning_rate)\n",
    "        \n",
    "        print(f'Loss of train set: {loss} at epoch: {epoch} and batch_num: {i}')\n",
    "        y_pred = p.argmax(dim=-1)\n",
    "        Y.append(y.cpu().numpy())\n",
    "        Y_pred.append(y_pred.cpu().numpy())\n",
    "        train_loss += loss.item()/N_train\n",
    "    Y = np.concatenate(Y)\n",
    "    Y_pred = np.concatenate(Y_pred)\n",
    "    acc = accuracy(Y_pred, Y)\n",
    "    print(f'Accuracy of train set: {acc}')\n",
    "    return train_loss, acc\n",
    "\n",
    "\n",
    "def validate(loader, epoch=None, set_name=None):\n",
    "    '''This function validates the model on the test dataloader. The function goes through each batch and does\n",
    "    the following on each batch:\n",
    "        1- Calculate the model output\n",
    "        2- Calculate the loss using the model output\n",
    "        3- Print the loss for each batch and epoch\n",
    "    \n",
    "    Finally the function calculates the model accuracy.'''\n",
    "    total_loss = 0\n",
    "    N = len(loader.dataset)\n",
    "    \n",
    "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
    "    # for calculateing the accuracy later\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        p = model(x, parameters)\n",
    "\n",
    "        ## FILL HERE\n",
    "        loss = cross_entropy_loss(p,y)\n",
    "        y_pred = p.argmax(dim=-1)\n",
    "        Y.append(y.cpu().numpy())\n",
    "        Y_pred.append(y_pred.cpu().numpy())\n",
    "        print(f'Loss of test set: {loss} at epoch: {epoch} and batch_num: {i}')\n",
    "        total_loss += loss.item()\n",
    "    Y = np.concatenate(Y)\n",
    "    Y_pred = np.concatenate(Y_pred)\n",
    "    total_loss /= N\n",
    "    acc = accuracy(Y_pred, Y)\n",
    "    print(f'Accuracy of {set_name} set: {acc}')\n",
    "\n",
    "    return total_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "87ebb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "28d4eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloaders, num_epochs, learning_rate=0.001, model_name='pytorch_model'):\n",
    "    '''This function trains the model for the number of epochs given and stores, calculates and prints the train\n",
    "    and test losses and accuracies. Finally, it plots the accuracy and loss history for training and test sets'''\n",
    "    train_loader, test_loader = dataloaders\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        ## FILL HERE\n",
    "        ## You should calculate the train and test loss and accuracies for each epoch and add them to\n",
    "        ## the lists `train_losses`, `test_losses`, `train_accuracies` and `test_accuracies`\n",
    "        train_loss, train_acc = train(train_loader, learning_rate, epoch)\n",
    "        test_loss, test_acc = validate(test_loader, epoch, set_name='FashionMNIST')\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "    ## plot the loss history of training and test sets \n",
    "    ## FILL HERE\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(test_losses)\n",
    "    plt.grid()\n",
    "    plt.legend([\"Train\",\"Test\"])\n",
    "    plt.title(\"Loss VS Iteration\")\n",
    "    plt.xlabel(\"#iteration\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    \n",
    "\n",
    "    ## plot the accuracy history of training and test sets\n",
    "    ## FILL HERE\n",
    "    plt.figure()\n",
    "    plt.plot(train_accuracies)\n",
    "    plt.plot(test_accuracies)\n",
    "    plt.grid()\n",
    "    plt.legend([\"Train\",\"Test\"])\n",
    "    plt.title(\"Accuracy VS Iteration\")\n",
    "    plt.xlabel(\"#iteration\")\n",
    "    plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2ec4bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.23684629797935486 at epoch: 0 and batch_num: 0\n",
      "Loss of train set: 0.34154099225997925 at epoch: 0 and batch_num: 1\n",
      "Loss of train set: 0.38155078887939453 at epoch: 0 and batch_num: 2\n",
      "Loss of train set: 0.5982749462127686 at epoch: 0 and batch_num: 3\n",
      "Loss of train set: 0.22734199464321136 at epoch: 0 and batch_num: 4\n",
      "Loss of train set: 0.30521678924560547 at epoch: 0 and batch_num: 5\n",
      "Loss of train set: 0.31141021847724915 at epoch: 0 and batch_num: 6\n",
      "Loss of train set: 0.4261656701564789 at epoch: 0 and batch_num: 7\n",
      "Loss of train set: 0.3480401337146759 at epoch: 0 and batch_num: 8\n",
      "Loss of train set: 0.2765907645225525 at epoch: 0 and batch_num: 9\n",
      "Loss of train set: 0.1956864893436432 at epoch: 0 and batch_num: 10\n",
      "Loss of train set: 0.3707488775253296 at epoch: 0 and batch_num: 11\n",
      "Loss of train set: 0.501805305480957 at epoch: 0 and batch_num: 12\n",
      "Loss of train set: 0.32864975929260254 at epoch: 0 and batch_num: 13\n",
      "Loss of train set: 0.394482284784317 at epoch: 0 and batch_num: 14\n",
      "Loss of train set: 0.5095744729042053 at epoch: 0 and batch_num: 15\n",
      "Loss of train set: 0.26721954345703125 at epoch: 0 and batch_num: 16\n",
      "Loss of train set: 0.32065439224243164 at epoch: 0 and batch_num: 17\n",
      "Loss of train set: 0.5576989054679871 at epoch: 0 and batch_num: 18\n",
      "Loss of train set: 0.27780529856681824 at epoch: 0 and batch_num: 19\n",
      "Loss of train set: 0.6885896921157837 at epoch: 0 and batch_num: 20\n",
      "Loss of train set: 0.26170384883880615 at epoch: 0 and batch_num: 21\n",
      "Loss of train set: 0.33835935592651367 at epoch: 0 and batch_num: 22\n",
      "Loss of train set: 0.35030004382133484 at epoch: 0 and batch_num: 23\n",
      "Loss of train set: 0.518413782119751 at epoch: 0 and batch_num: 24\n",
      "Loss of train set: 0.4944462180137634 at epoch: 0 and batch_num: 25\n",
      "Loss of train set: 0.27050814032554626 at epoch: 0 and batch_num: 26\n",
      "Loss of train set: 0.3398040533065796 at epoch: 0 and batch_num: 27\n",
      "Loss of train set: 0.24836376309394836 at epoch: 0 and batch_num: 28\n",
      "Loss of train set: 0.3811490535736084 at epoch: 0 and batch_num: 29\n",
      "Loss of train set: 0.25091952085494995 at epoch: 0 and batch_num: 30\n",
      "Loss of train set: 0.38696953654289246 at epoch: 0 and batch_num: 31\n",
      "Loss of train set: 0.44005221128463745 at epoch: 0 and batch_num: 32\n",
      "Loss of train set: 0.4653036296367645 at epoch: 0 and batch_num: 33\n",
      "Loss of train set: 0.38645535707473755 at epoch: 0 and batch_num: 34\n",
      "Loss of train set: 0.2565288543701172 at epoch: 0 and batch_num: 35\n",
      "Loss of train set: 0.30337947607040405 at epoch: 0 and batch_num: 36\n",
      "Loss of train set: 0.31138715147972107 at epoch: 0 and batch_num: 37\n",
      "Loss of train set: 0.24266469478607178 at epoch: 0 and batch_num: 38\n",
      "Loss of train set: 0.26224085688591003 at epoch: 0 and batch_num: 39\n",
      "Loss of train set: 0.562812089920044 at epoch: 0 and batch_num: 40\n",
      "Loss of train set: 0.47801482677459717 at epoch: 0 and batch_num: 41\n",
      "Loss of train set: 0.20531690120697021 at epoch: 0 and batch_num: 42\n",
      "Loss of train set: 0.3967423439025879 at epoch: 0 and batch_num: 43\n",
      "Loss of train set: 0.3256222605705261 at epoch: 0 and batch_num: 44\n",
      "Loss of train set: 0.3515082001686096 at epoch: 0 and batch_num: 45\n",
      "Loss of train set: 0.5086158514022827 at epoch: 0 and batch_num: 46\n",
      "Loss of train set: 0.3636843264102936 at epoch: 0 and batch_num: 47\n",
      "Loss of train set: 0.3906748294830322 at epoch: 0 and batch_num: 48\n",
      "Loss of train set: 0.47806882858276367 at epoch: 0 and batch_num: 49\n",
      "Loss of train set: 0.28912487626075745 at epoch: 0 and batch_num: 50\n",
      "Loss of train set: 0.4454931616783142 at epoch: 0 and batch_num: 51\n",
      "Loss of train set: 0.491301566362381 at epoch: 0 and batch_num: 52\n",
      "Loss of train set: 0.27288082242012024 at epoch: 0 and batch_num: 53\n",
      "Loss of train set: 0.23459213972091675 at epoch: 0 and batch_num: 54\n",
      "Loss of train set: 0.35156285762786865 at epoch: 0 and batch_num: 55\n",
      "Loss of train set: 0.2680869698524475 at epoch: 0 and batch_num: 56\n",
      "Loss of train set: 0.4167540371417999 at epoch: 0 and batch_num: 57\n",
      "Loss of train set: 0.24698056280612946 at epoch: 0 and batch_num: 58\n",
      "Loss of train set: 0.1269691288471222 at epoch: 0 and batch_num: 59\n",
      "Loss of train set: 0.1889531910419464 at epoch: 0 and batch_num: 60\n",
      "Loss of train set: 0.3940820097923279 at epoch: 0 and batch_num: 61\n",
      "Loss of train set: 0.13996794819831848 at epoch: 0 and batch_num: 62\n",
      "Loss of train set: 0.22667905688285828 at epoch: 0 and batch_num: 63\n",
      "Loss of train set: 0.17583347856998444 at epoch: 0 and batch_num: 64\n",
      "Loss of train set: 0.3941708207130432 at epoch: 0 and batch_num: 65\n",
      "Loss of train set: 0.37089043855667114 at epoch: 0 and batch_num: 66\n",
      "Loss of train set: 0.6057265996932983 at epoch: 0 and batch_num: 67\n",
      "Loss of train set: 0.3202816843986511 at epoch: 0 and batch_num: 68\n",
      "Loss of train set: 0.35025209188461304 at epoch: 0 and batch_num: 69\n",
      "Loss of train set: 0.30115681886672974 at epoch: 0 and batch_num: 70\n",
      "Loss of train set: 0.4136539697647095 at epoch: 0 and batch_num: 71\n",
      "Loss of train set: 0.33160269260406494 at epoch: 0 and batch_num: 72\n",
      "Loss of train set: 0.5009219646453857 at epoch: 0 and batch_num: 73\n",
      "Loss of train set: 0.2887142300605774 at epoch: 0 and batch_num: 74\n",
      "Loss of train set: 0.43743568658828735 at epoch: 0 and batch_num: 75\n",
      "Loss of train set: 0.33436882495880127 at epoch: 0 and batch_num: 76\n",
      "Loss of train set: 0.36231493949890137 at epoch: 0 and batch_num: 77\n",
      "Loss of train set: 0.2599164843559265 at epoch: 0 and batch_num: 78\n",
      "Loss of train set: 0.245793879032135 at epoch: 0 and batch_num: 79\n",
      "Loss of train set: 0.23054635524749756 at epoch: 0 and batch_num: 80\n",
      "Loss of train set: 0.5090506076812744 at epoch: 0 and batch_num: 81\n",
      "Loss of train set: 0.4138118028640747 at epoch: 0 and batch_num: 82\n",
      "Loss of train set: 0.3126869201660156 at epoch: 0 and batch_num: 83\n",
      "Loss of train set: 0.5229883193969727 at epoch: 0 and batch_num: 84\n",
      "Loss of train set: 0.39791226387023926 at epoch: 0 and batch_num: 85\n",
      "Loss of train set: 0.4229804277420044 at epoch: 0 and batch_num: 86\n",
      "Loss of train set: 0.509381890296936 at epoch: 0 and batch_num: 87\n",
      "Loss of train set: 0.2363714873790741 at epoch: 0 and batch_num: 88\n",
      "Loss of train set: 0.36188656091690063 at epoch: 0 and batch_num: 89\n",
      "Loss of train set: 0.29334282875061035 at epoch: 0 and batch_num: 90\n",
      "Loss of train set: 0.33425474166870117 at epoch: 0 and batch_num: 91\n",
      "Loss of train set: 0.43469148874282837 at epoch: 0 and batch_num: 92\n",
      "Loss of train set: 0.4325900971889496 at epoch: 0 and batch_num: 93\n",
      "Loss of train set: 0.34000152349472046 at epoch: 0 and batch_num: 94\n",
      "Loss of train set: 0.3035535216331482 at epoch: 0 and batch_num: 95\n",
      "Loss of train set: 0.4617326855659485 at epoch: 0 and batch_num: 96\n",
      "Loss of train set: 0.39683327078819275 at epoch: 0 and batch_num: 97\n",
      "Loss of train set: 0.7458177804946899 at epoch: 0 and batch_num: 98\n",
      "Loss of train set: 0.4278002679347992 at epoch: 0 and batch_num: 99\n",
      "Loss of train set: 0.35666540265083313 at epoch: 0 and batch_num: 100\n",
      "Loss of train set: 0.32236501574516296 at epoch: 0 and batch_num: 101\n",
      "Loss of train set: 0.47195571660995483 at epoch: 0 and batch_num: 102\n",
      "Loss of train set: 0.396138072013855 at epoch: 0 and batch_num: 103\n",
      "Loss of train set: 0.1941685974597931 at epoch: 0 and batch_num: 104\n",
      "Loss of train set: 0.41493478417396545 at epoch: 0 and batch_num: 105\n",
      "Loss of train set: 0.21585819125175476 at epoch: 0 and batch_num: 106\n",
      "Loss of train set: 0.2982749342918396 at epoch: 0 and batch_num: 107\n",
      "Loss of train set: 0.40298300981521606 at epoch: 0 and batch_num: 108\n",
      "Loss of train set: 0.5512455701828003 at epoch: 0 and batch_num: 109\n",
      "Loss of train set: 0.28981873393058777 at epoch: 0 and batch_num: 110\n",
      "Loss of train set: 0.4114142060279846 at epoch: 0 and batch_num: 111\n",
      "Loss of train set: 0.22958135604858398 at epoch: 0 and batch_num: 112\n",
      "Loss of train set: 0.4672878682613373 at epoch: 0 and batch_num: 113\n",
      "Loss of train set: 0.46987611055374146 at epoch: 0 and batch_num: 114\n",
      "Loss of train set: 0.40945714712142944 at epoch: 0 and batch_num: 115\n",
      "Loss of train set: 0.331256240606308 at epoch: 0 and batch_num: 116\n",
      "Loss of train set: 0.3676646053791046 at epoch: 0 and batch_num: 117\n",
      "Loss of train set: 0.3342018723487854 at epoch: 0 and batch_num: 118\n",
      "Loss of train set: 0.41411083936691284 at epoch: 0 and batch_num: 119\n",
      "Loss of train set: 0.2620771825313568 at epoch: 0 and batch_num: 120\n",
      "Loss of train set: 0.40572744607925415 at epoch: 0 and batch_num: 121\n",
      "Loss of train set: 0.33536791801452637 at epoch: 0 and batch_num: 122\n",
      "Loss of train set: 0.12088176608085632 at epoch: 0 and batch_num: 123\n",
      "Loss of train set: 0.2939949631690979 at epoch: 0 and batch_num: 124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.32154372334480286 at epoch: 0 and batch_num: 125\n",
      "Loss of train set: 0.33914077281951904 at epoch: 0 and batch_num: 126\n",
      "Loss of train set: 0.5132722854614258 at epoch: 0 and batch_num: 127\n",
      "Loss of train set: 0.25594258308410645 at epoch: 0 and batch_num: 128\n",
      "Loss of train set: 0.5964510440826416 at epoch: 0 and batch_num: 129\n",
      "Loss of train set: 0.3360803425312042 at epoch: 0 and batch_num: 130\n",
      "Loss of train set: 0.5301743745803833 at epoch: 0 and batch_num: 131\n",
      "Loss of train set: 0.525313138961792 at epoch: 0 and batch_num: 132\n",
      "Loss of train set: 0.4906216859817505 at epoch: 0 and batch_num: 133\n",
      "Loss of train set: 0.5831438302993774 at epoch: 0 and batch_num: 134\n",
      "Loss of train set: 0.3179851770401001 at epoch: 0 and batch_num: 135\n",
      "Loss of train set: 0.24308940768241882 at epoch: 0 and batch_num: 136\n",
      "Loss of train set: 0.30702394247055054 at epoch: 0 and batch_num: 137\n",
      "Loss of train set: 0.46001702547073364 at epoch: 0 and batch_num: 138\n",
      "Loss of train set: 0.41512399911880493 at epoch: 0 and batch_num: 139\n",
      "Loss of train set: 0.3590807616710663 at epoch: 0 and batch_num: 140\n",
      "Loss of train set: 0.5411717891693115 at epoch: 0 and batch_num: 141\n",
      "Loss of train set: 0.3547137677669525 at epoch: 0 and batch_num: 142\n",
      "Loss of train set: 0.23910453915596008 at epoch: 0 and batch_num: 143\n",
      "Loss of train set: 0.3730941414833069 at epoch: 0 and batch_num: 144\n",
      "Loss of train set: 0.3954054117202759 at epoch: 0 and batch_num: 145\n",
      "Loss of train set: 0.5349692106246948 at epoch: 0 and batch_num: 146\n",
      "Loss of train set: 0.44047433137893677 at epoch: 0 and batch_num: 147\n",
      "Loss of train set: 0.3319438397884369 at epoch: 0 and batch_num: 148\n",
      "Loss of train set: 0.2289283126592636 at epoch: 0 and batch_num: 149\n",
      "Loss of train set: 0.5251092314720154 at epoch: 0 and batch_num: 150\n",
      "Loss of train set: 0.37261730432510376 at epoch: 0 and batch_num: 151\n",
      "Loss of train set: 0.21084333956241608 at epoch: 0 and batch_num: 152\n",
      "Loss of train set: 0.47144660353660583 at epoch: 0 and batch_num: 153\n",
      "Loss of train set: 0.4313722550868988 at epoch: 0 and batch_num: 154\n",
      "Loss of train set: 0.36469894647598267 at epoch: 0 and batch_num: 155\n",
      "Loss of train set: 0.4652273952960968 at epoch: 0 and batch_num: 156\n",
      "Loss of train set: 0.31353694200515747 at epoch: 0 and batch_num: 157\n",
      "Loss of train set: 0.4077405333518982 at epoch: 0 and batch_num: 158\n",
      "Loss of train set: 0.6683567762374878 at epoch: 0 and batch_num: 159\n",
      "Loss of train set: 0.49983030557632446 at epoch: 0 and batch_num: 160\n",
      "Loss of train set: 0.4944484531879425 at epoch: 0 and batch_num: 161\n",
      "Loss of train set: 0.38067153096199036 at epoch: 0 and batch_num: 162\n",
      "Loss of train set: 0.33462125062942505 at epoch: 0 and batch_num: 163\n",
      "Loss of train set: 0.3855019211769104 at epoch: 0 and batch_num: 164\n",
      "Loss of train set: 0.31732597947120667 at epoch: 0 and batch_num: 165\n",
      "Loss of train set: 0.2571192979812622 at epoch: 0 and batch_num: 166\n",
      "Loss of train set: 0.3908519148826599 at epoch: 0 and batch_num: 167\n",
      "Loss of train set: 0.3934199810028076 at epoch: 0 and batch_num: 168\n",
      "Loss of train set: 0.5363014936447144 at epoch: 0 and batch_num: 169\n",
      "Loss of train set: 0.2615154981613159 at epoch: 0 and batch_num: 170\n",
      "Loss of train set: 0.5456734895706177 at epoch: 0 and batch_num: 171\n",
      "Loss of train set: 0.23246729373931885 at epoch: 0 and batch_num: 172\n",
      "Loss of train set: 0.23635932803153992 at epoch: 0 and batch_num: 173\n",
      "Loss of train set: 0.4147055745124817 at epoch: 0 and batch_num: 174\n",
      "Loss of train set: 0.325508713722229 at epoch: 0 and batch_num: 175\n",
      "Loss of train set: 0.5114948749542236 at epoch: 0 and batch_num: 176\n",
      "Loss of train set: 0.5296527743339539 at epoch: 0 and batch_num: 177\n",
      "Loss of train set: 0.32733476161956787 at epoch: 0 and batch_num: 178\n",
      "Loss of train set: 0.32603609561920166 at epoch: 0 and batch_num: 179\n",
      "Loss of train set: 0.4337124228477478 at epoch: 0 and batch_num: 180\n",
      "Loss of train set: 0.2579706013202667 at epoch: 0 and batch_num: 181\n",
      "Loss of train set: 0.3220289349555969 at epoch: 0 and batch_num: 182\n",
      "Loss of train set: 0.499917209148407 at epoch: 0 and batch_num: 183\n",
      "Loss of train set: 0.4146588146686554 at epoch: 0 and batch_num: 184\n",
      "Loss of train set: 0.5132884979248047 at epoch: 0 and batch_num: 185\n",
      "Loss of train set: 0.3591657280921936 at epoch: 0 and batch_num: 186\n",
      "Loss of train set: 0.48308897018432617 at epoch: 0 and batch_num: 187\n",
      "Loss of train set: 0.6516779065132141 at epoch: 0 and batch_num: 188\n",
      "Loss of train set: 0.3135840594768524 at epoch: 0 and batch_num: 189\n",
      "Loss of train set: 0.44102615118026733 at epoch: 0 and batch_num: 190\n",
      "Loss of train set: 0.40892404317855835 at epoch: 0 and batch_num: 191\n",
      "Loss of train set: 0.20178848505020142 at epoch: 0 and batch_num: 192\n",
      "Loss of train set: 0.27348142862319946 at epoch: 0 and batch_num: 193\n",
      "Loss of train set: 0.3888796269893646 at epoch: 0 and batch_num: 194\n",
      "Loss of train set: 0.30517685413360596 at epoch: 0 and batch_num: 195\n",
      "Loss of train set: 0.40015387535095215 at epoch: 0 and batch_num: 196\n",
      "Loss of train set: 0.5374516248703003 at epoch: 0 and batch_num: 197\n",
      "Loss of train set: 0.4846094250679016 at epoch: 0 and batch_num: 198\n",
      "Loss of train set: 0.48988914489746094 at epoch: 0 and batch_num: 199\n",
      "Loss of train set: 0.31878191232681274 at epoch: 0 and batch_num: 200\n",
      "Loss of train set: 0.40322235226631165 at epoch: 0 and batch_num: 201\n",
      "Loss of train set: 0.30431029200553894 at epoch: 0 and batch_num: 202\n",
      "Loss of train set: 0.4717210829257965 at epoch: 0 and batch_num: 203\n",
      "Loss of train set: 0.3127903342247009 at epoch: 0 and batch_num: 204\n",
      "Loss of train set: 0.3702998161315918 at epoch: 0 and batch_num: 205\n",
      "Loss of train set: 0.4277258515357971 at epoch: 0 and batch_num: 206\n",
      "Loss of train set: 0.2775350511074066 at epoch: 0 and batch_num: 207\n",
      "Loss of train set: 0.5054640173912048 at epoch: 0 and batch_num: 208\n",
      "Loss of train set: 0.26255935430526733 at epoch: 0 and batch_num: 209\n",
      "Loss of train set: 0.3233129382133484 at epoch: 0 and batch_num: 210\n",
      "Loss of train set: 0.42240262031555176 at epoch: 0 and batch_num: 211\n",
      "Loss of train set: 0.27873218059539795 at epoch: 0 and batch_num: 212\n",
      "Loss of train set: 0.5134475231170654 at epoch: 0 and batch_num: 213\n",
      "Loss of train set: 0.342045396566391 at epoch: 0 and batch_num: 214\n",
      "Loss of train set: 0.2721709907054901 at epoch: 0 and batch_num: 215\n",
      "Loss of train set: 0.2647678554058075 at epoch: 0 and batch_num: 216\n",
      "Loss of train set: 0.3717401623725891 at epoch: 0 and batch_num: 217\n",
      "Loss of train set: 0.4093905985355377 at epoch: 0 and batch_num: 218\n",
      "Loss of train set: 0.6963379383087158 at epoch: 0 and batch_num: 219\n",
      "Loss of train set: 0.3429645299911499 at epoch: 0 and batch_num: 220\n",
      "Loss of train set: 0.30204951763153076 at epoch: 0 and batch_num: 221\n",
      "Loss of train set: 0.3715146780014038 at epoch: 0 and batch_num: 222\n",
      "Loss of train set: 0.6180415153503418 at epoch: 0 and batch_num: 223\n",
      "Loss of train set: 0.4024113416671753 at epoch: 0 and batch_num: 224\n",
      "Loss of train set: 0.3895794451236725 at epoch: 0 and batch_num: 225\n",
      "Loss of train set: 0.4881691634654999 at epoch: 0 and batch_num: 226\n",
      "Loss of train set: 0.3166738748550415 at epoch: 0 and batch_num: 227\n",
      "Loss of train set: 0.3745671808719635 at epoch: 0 and batch_num: 228\n",
      "Loss of train set: 0.5760604739189148 at epoch: 0 and batch_num: 229\n",
      "Loss of train set: 0.3089890778064728 at epoch: 0 and batch_num: 230\n",
      "Loss of train set: 0.44100040197372437 at epoch: 0 and batch_num: 231\n",
      "Loss of train set: 0.4921117126941681 at epoch: 0 and batch_num: 232\n",
      "Loss of train set: 0.1922115832567215 at epoch: 0 and batch_num: 233\n",
      "Loss of train set: 0.39916905760765076 at epoch: 0 and batch_num: 234\n",
      "Loss of train set: 0.3775089979171753 at epoch: 0 and batch_num: 235\n",
      "Loss of train set: 0.3120771646499634 at epoch: 0 and batch_num: 236\n",
      "Loss of train set: 0.3755665421485901 at epoch: 0 and batch_num: 237\n",
      "Loss of train set: 0.5056977272033691 at epoch: 0 and batch_num: 238\n",
      "Loss of train set: 0.3920997381210327 at epoch: 0 and batch_num: 239\n",
      "Loss of train set: 0.2883530557155609 at epoch: 0 and batch_num: 240\n",
      "Loss of train set: 0.30124014616012573 at epoch: 0 and batch_num: 241\n",
      "Loss of train set: 0.5544617176055908 at epoch: 0 and batch_num: 242\n",
      "Loss of train set: 0.3210050165653229 at epoch: 0 and batch_num: 243\n",
      "Loss of train set: 0.4076192080974579 at epoch: 0 and batch_num: 244\n",
      "Loss of train set: 0.4479360580444336 at epoch: 0 and batch_num: 245\n",
      "Loss of train set: 0.31261152029037476 at epoch: 0 and batch_num: 246\n",
      "Loss of train set: 0.3532991409301758 at epoch: 0 and batch_num: 247\n",
      "Loss of train set: 0.23946377635002136 at epoch: 0 and batch_num: 248\n",
      "Loss of train set: 0.45982876420021057 at epoch: 0 and batch_num: 249\n",
      "Loss of train set: 0.3915693759918213 at epoch: 0 and batch_num: 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.7013425230979919 at epoch: 0 and batch_num: 251\n",
      "Loss of train set: 0.22862622141838074 at epoch: 0 and batch_num: 252\n",
      "Loss of train set: 0.4040961265563965 at epoch: 0 and batch_num: 253\n",
      "Loss of train set: 0.22281140089035034 at epoch: 0 and batch_num: 254\n",
      "Loss of train set: 0.2657378315925598 at epoch: 0 and batch_num: 255\n",
      "Loss of train set: 0.711071789264679 at epoch: 0 and batch_num: 256\n",
      "Loss of train set: 0.3382020592689514 at epoch: 0 and batch_num: 257\n",
      "Loss of train set: 0.39246630668640137 at epoch: 0 and batch_num: 258\n",
      "Loss of train set: 0.20642030239105225 at epoch: 0 and batch_num: 259\n",
      "Loss of train set: 0.4002939462661743 at epoch: 0 and batch_num: 260\n",
      "Loss of train set: 0.3465844988822937 at epoch: 0 and batch_num: 261\n",
      "Loss of train set: 0.35452738404273987 at epoch: 0 and batch_num: 262\n",
      "Loss of train set: 0.3618354797363281 at epoch: 0 and batch_num: 263\n",
      "Loss of train set: 0.4109650254249573 at epoch: 0 and batch_num: 264\n",
      "Loss of train set: 0.23350156843662262 at epoch: 0 and batch_num: 265\n",
      "Loss of train set: 0.2546961009502411 at epoch: 0 and batch_num: 266\n",
      "Loss of train set: 0.49703601002693176 at epoch: 0 and batch_num: 267\n",
      "Loss of train set: 0.2509424090385437 at epoch: 0 and batch_num: 268\n",
      "Loss of train set: 0.3775904178619385 at epoch: 0 and batch_num: 269\n",
      "Loss of train set: 0.2670404314994812 at epoch: 0 and batch_num: 270\n",
      "Loss of train set: 0.2827613651752472 at epoch: 0 and batch_num: 271\n",
      "Loss of train set: 0.2316332310438156 at epoch: 0 and batch_num: 272\n",
      "Loss of train set: 0.4631260931491852 at epoch: 0 and batch_num: 273\n",
      "Loss of train set: 0.39670515060424805 at epoch: 0 and batch_num: 274\n",
      "Loss of train set: 0.2808458209037781 at epoch: 0 and batch_num: 275\n",
      "Loss of train set: 0.6475998163223267 at epoch: 0 and batch_num: 276\n",
      "Loss of train set: 0.423182874917984 at epoch: 0 and batch_num: 277\n",
      "Loss of train set: 0.2533304691314697 at epoch: 0 and batch_num: 278\n",
      "Loss of train set: 0.3632505238056183 at epoch: 0 and batch_num: 279\n",
      "Loss of train set: 0.3114614188671112 at epoch: 0 and batch_num: 280\n",
      "Loss of train set: 0.289497047662735 at epoch: 0 and batch_num: 281\n",
      "Loss of train set: 0.2729465961456299 at epoch: 0 and batch_num: 282\n",
      "Loss of train set: 0.3191440999507904 at epoch: 0 and batch_num: 283\n",
      "Loss of train set: 0.3005242943763733 at epoch: 0 and batch_num: 284\n",
      "Loss of train set: 0.28462618589401245 at epoch: 0 and batch_num: 285\n",
      "Loss of train set: 0.27712512016296387 at epoch: 0 and batch_num: 286\n",
      "Loss of train set: 0.26148346066474915 at epoch: 0 and batch_num: 287\n",
      "Loss of train set: 0.5158307552337646 at epoch: 0 and batch_num: 288\n",
      "Loss of train set: 0.3481488823890686 at epoch: 0 and batch_num: 289\n",
      "Loss of train set: 0.5626407265663147 at epoch: 0 and batch_num: 290\n",
      "Loss of train set: 0.36038661003112793 at epoch: 0 and batch_num: 291\n",
      "Loss of train set: 0.4036373496055603 at epoch: 0 and batch_num: 292\n",
      "Loss of train set: 0.2894396185874939 at epoch: 0 and batch_num: 293\n",
      "Loss of train set: 0.21036149561405182 at epoch: 0 and batch_num: 294\n",
      "Loss of train set: 0.26313188672065735 at epoch: 0 and batch_num: 295\n",
      "Loss of train set: 0.3072447180747986 at epoch: 0 and batch_num: 296\n",
      "Loss of train set: 0.29373258352279663 at epoch: 0 and batch_num: 297\n",
      "Loss of train set: 0.3537830412387848 at epoch: 0 and batch_num: 298\n",
      "Loss of train set: 0.41036951541900635 at epoch: 0 and batch_num: 299\n",
      "Loss of train set: 0.15506060421466827 at epoch: 0 and batch_num: 300\n",
      "Loss of train set: 0.37146246433258057 at epoch: 0 and batch_num: 301\n",
      "Loss of train set: 0.20727044343948364 at epoch: 0 and batch_num: 302\n",
      "Loss of train set: 0.4364359378814697 at epoch: 0 and batch_num: 303\n",
      "Loss of train set: 0.5026678442955017 at epoch: 0 and batch_num: 304\n",
      "Loss of train set: 0.32164639234542847 at epoch: 0 and batch_num: 305\n",
      "Loss of train set: 0.4832639694213867 at epoch: 0 and batch_num: 306\n",
      "Loss of train set: 0.4079623222351074 at epoch: 0 and batch_num: 307\n",
      "Loss of train set: 0.42407944798469543 at epoch: 0 and batch_num: 308\n",
      "Loss of train set: 0.2987183630466461 at epoch: 0 and batch_num: 309\n",
      "Loss of train set: 0.5084537267684937 at epoch: 0 and batch_num: 310\n",
      "Loss of train set: 0.47116363048553467 at epoch: 0 and batch_num: 311\n",
      "Loss of train set: 0.32201462984085083 at epoch: 0 and batch_num: 312\n",
      "Loss of train set: 0.5223958492279053 at epoch: 0 and batch_num: 313\n",
      "Loss of train set: 0.2498593032360077 at epoch: 0 and batch_num: 314\n",
      "Loss of train set: 0.4788672626018524 at epoch: 0 and batch_num: 315\n",
      "Loss of train set: 0.38529255986213684 at epoch: 0 and batch_num: 316\n",
      "Loss of train set: 0.3813709020614624 at epoch: 0 and batch_num: 317\n",
      "Loss of train set: 0.3963508605957031 at epoch: 0 and batch_num: 318\n",
      "Loss of train set: 0.4181070327758789 at epoch: 0 and batch_num: 319\n",
      "Loss of train set: 0.541137158870697 at epoch: 0 and batch_num: 320\n",
      "Loss of train set: 0.24429035186767578 at epoch: 0 and batch_num: 321\n",
      "Loss of train set: 0.42435967922210693 at epoch: 0 and batch_num: 322\n",
      "Loss of train set: 0.43922165036201477 at epoch: 0 and batch_num: 323\n",
      "Loss of train set: 0.38483357429504395 at epoch: 0 and batch_num: 324\n",
      "Loss of train set: 0.5091559290885925 at epoch: 0 and batch_num: 325\n",
      "Loss of train set: 0.3751465678215027 at epoch: 0 and batch_num: 326\n",
      "Loss of train set: 0.2971850633621216 at epoch: 0 and batch_num: 327\n",
      "Loss of train set: 0.2704688012599945 at epoch: 0 and batch_num: 328\n",
      "Loss of train set: 0.3195890188217163 at epoch: 0 and batch_num: 329\n",
      "Loss of train set: 0.40940213203430176 at epoch: 0 and batch_num: 330\n",
      "Loss of train set: 0.3432849645614624 at epoch: 0 and batch_num: 331\n",
      "Loss of train set: 0.46320146322250366 at epoch: 0 and batch_num: 332\n",
      "Loss of train set: 0.32373738288879395 at epoch: 0 and batch_num: 333\n",
      "Loss of train set: 0.35656166076660156 at epoch: 0 and batch_num: 334\n",
      "Loss of train set: 0.29391881823539734 at epoch: 0 and batch_num: 335\n",
      "Loss of train set: 0.3042639493942261 at epoch: 0 and batch_num: 336\n",
      "Loss of train set: 0.3311285674571991 at epoch: 0 and batch_num: 337\n",
      "Loss of train set: 0.32714444398880005 at epoch: 0 and batch_num: 338\n",
      "Loss of train set: 0.3572683334350586 at epoch: 0 and batch_num: 339\n",
      "Loss of train set: 0.437671959400177 at epoch: 0 and batch_num: 340\n",
      "Loss of train set: 0.5318354964256287 at epoch: 0 and batch_num: 341\n",
      "Loss of train set: 0.33460095524787903 at epoch: 0 and batch_num: 342\n",
      "Loss of train set: 0.31973880529403687 at epoch: 0 and batch_num: 343\n",
      "Loss of train set: 0.425560861825943 at epoch: 0 and batch_num: 344\n",
      "Loss of train set: 0.37532761693000793 at epoch: 0 and batch_num: 345\n",
      "Loss of train set: 0.3402809500694275 at epoch: 0 and batch_num: 346\n",
      "Loss of train set: 0.4513233006000519 at epoch: 0 and batch_num: 347\n",
      "Loss of train set: 0.40660151839256287 at epoch: 0 and batch_num: 348\n",
      "Loss of train set: 0.2740117311477661 at epoch: 0 and batch_num: 349\n",
      "Loss of train set: 0.4633379280567169 at epoch: 0 and batch_num: 350\n",
      "Loss of train set: 0.46812543272972107 at epoch: 0 and batch_num: 351\n",
      "Loss of train set: 0.3823341131210327 at epoch: 0 and batch_num: 352\n",
      "Loss of train set: 0.25452589988708496 at epoch: 0 and batch_num: 353\n",
      "Loss of train set: 0.14150145649909973 at epoch: 0 and batch_num: 354\n",
      "Loss of train set: 0.4347670376300812 at epoch: 0 and batch_num: 355\n",
      "Loss of train set: 0.4842972755432129 at epoch: 0 and batch_num: 356\n",
      "Loss of train set: 0.3025326728820801 at epoch: 0 and batch_num: 357\n",
      "Loss of train set: 0.4289427399635315 at epoch: 0 and batch_num: 358\n",
      "Loss of train set: 0.3160507082939148 at epoch: 0 and batch_num: 359\n",
      "Loss of train set: 0.23829218745231628 at epoch: 0 and batch_num: 360\n",
      "Loss of train set: 0.3836580514907837 at epoch: 0 and batch_num: 361\n",
      "Loss of train set: 0.32085734605789185 at epoch: 0 and batch_num: 362\n",
      "Loss of train set: 0.34035709500312805 at epoch: 0 and batch_num: 363\n",
      "Loss of train set: 0.5144095420837402 at epoch: 0 and batch_num: 364\n",
      "Loss of train set: 0.4887896180152893 at epoch: 0 and batch_num: 365\n",
      "Loss of train set: 0.3229774236679077 at epoch: 0 and batch_num: 366\n",
      "Loss of train set: 0.3003973364830017 at epoch: 0 and batch_num: 367\n",
      "Loss of train set: 0.25256603956222534 at epoch: 0 and batch_num: 368\n",
      "Loss of train set: 0.30814647674560547 at epoch: 0 and batch_num: 369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.4293748736381531 at epoch: 0 and batch_num: 370\n",
      "Loss of train set: 0.30681705474853516 at epoch: 0 and batch_num: 371\n",
      "Loss of train set: 0.3165408968925476 at epoch: 0 and batch_num: 372\n",
      "Loss of train set: 0.2413996160030365 at epoch: 0 and batch_num: 373\n",
      "Loss of train set: 0.5736427903175354 at epoch: 0 and batch_num: 374\n",
      "Loss of train set: 0.41641366481781006 at epoch: 0 and batch_num: 375\n",
      "Loss of train set: 0.43236368894577026 at epoch: 0 and batch_num: 376\n",
      "Loss of train set: 0.3901330828666687 at epoch: 0 and batch_num: 377\n",
      "Loss of train set: 0.4189893901348114 at epoch: 0 and batch_num: 378\n",
      "Loss of train set: 0.45793092250823975 at epoch: 0 and batch_num: 379\n",
      "Loss of train set: 0.2322806864976883 at epoch: 0 and batch_num: 380\n",
      "Loss of train set: 0.356760710477829 at epoch: 0 and batch_num: 381\n",
      "Loss of train set: 0.2587815523147583 at epoch: 0 and batch_num: 382\n",
      "Loss of train set: 0.3672368824481964 at epoch: 0 and batch_num: 383\n",
      "Loss of train set: 0.3007408380508423 at epoch: 0 and batch_num: 384\n",
      "Loss of train set: 0.5419952273368835 at epoch: 0 and batch_num: 385\n",
      "Loss of train set: 0.373752236366272 at epoch: 0 and batch_num: 386\n",
      "Loss of train set: 0.3418239951133728 at epoch: 0 and batch_num: 387\n",
      "Loss of train set: 0.31160593032836914 at epoch: 0 and batch_num: 388\n",
      "Loss of train set: 0.2952482998371124 at epoch: 0 and batch_num: 389\n",
      "Loss of train set: 0.5204653739929199 at epoch: 0 and batch_num: 390\n",
      "Loss of train set: 0.2769954800605774 at epoch: 0 and batch_num: 391\n",
      "Loss of train set: 0.4903656244277954 at epoch: 0 and batch_num: 392\n",
      "Loss of train set: 0.40503180027008057 at epoch: 0 and batch_num: 393\n",
      "Loss of train set: 0.3830913007259369 at epoch: 0 and batch_num: 394\n",
      "Loss of train set: 0.3083231747150421 at epoch: 0 and batch_num: 395\n",
      "Loss of train set: 0.25813397765159607 at epoch: 0 and batch_num: 396\n",
      "Loss of train set: 0.3169970214366913 at epoch: 0 and batch_num: 397\n",
      "Loss of train set: 0.47603437304496765 at epoch: 0 and batch_num: 398\n",
      "Loss of train set: 0.4779912531375885 at epoch: 0 and batch_num: 399\n",
      "Loss of train set: 0.36054152250289917 at epoch: 0 and batch_num: 400\n",
      "Loss of train set: 0.32855862379074097 at epoch: 0 and batch_num: 401\n",
      "Loss of train set: 0.32552701234817505 at epoch: 0 and batch_num: 402\n",
      "Loss of train set: 0.5122094750404358 at epoch: 0 and batch_num: 403\n",
      "Loss of train set: 0.39176031947135925 at epoch: 0 and batch_num: 404\n",
      "Loss of train set: 0.30943214893341064 at epoch: 0 and batch_num: 405\n",
      "Loss of train set: 0.4945448040962219 at epoch: 0 and batch_num: 406\n",
      "Loss of train set: 0.30394798517227173 at epoch: 0 and batch_num: 407\n",
      "Loss of train set: 0.3305612802505493 at epoch: 0 and batch_num: 408\n",
      "Loss of train set: 0.3379816710948944 at epoch: 0 and batch_num: 409\n",
      "Loss of train set: 0.3205508291721344 at epoch: 0 and batch_num: 410\n",
      "Loss of train set: 0.23304526507854462 at epoch: 0 and batch_num: 411\n",
      "Loss of train set: 0.3265269994735718 at epoch: 0 and batch_num: 412\n",
      "Loss of train set: 0.6412244439125061 at epoch: 0 and batch_num: 413\n",
      "Loss of train set: 0.41601815819740295 at epoch: 0 and batch_num: 414\n",
      "Loss of train set: 0.4094651937484741 at epoch: 0 and batch_num: 415\n",
      "Loss of train set: 0.5752785205841064 at epoch: 0 and batch_num: 416\n",
      "Loss of train set: 0.1966075599193573 at epoch: 0 and batch_num: 417\n",
      "Loss of train set: 0.309392511844635 at epoch: 0 and batch_num: 418\n",
      "Loss of train set: 0.2719082832336426 at epoch: 0 and batch_num: 419\n",
      "Loss of train set: 0.4944448173046112 at epoch: 0 and batch_num: 420\n",
      "Loss of train set: 0.6160398721694946 at epoch: 0 and batch_num: 421\n",
      "Loss of train set: 0.3212021589279175 at epoch: 0 and batch_num: 422\n",
      "Loss of train set: 0.5457611083984375 at epoch: 0 and batch_num: 423\n",
      "Loss of train set: 0.29827171564102173 at epoch: 0 and batch_num: 424\n",
      "Loss of train set: 0.6591004729270935 at epoch: 0 and batch_num: 425\n",
      "Loss of train set: 0.42053282260894775 at epoch: 0 and batch_num: 426\n",
      "Loss of train set: 0.3997194766998291 at epoch: 0 and batch_num: 427\n",
      "Loss of train set: 0.13576281070709229 at epoch: 0 and batch_num: 428\n",
      "Loss of train set: 0.3629686236381531 at epoch: 0 and batch_num: 429\n",
      "Loss of train set: 0.4670565128326416 at epoch: 0 and batch_num: 430\n",
      "Loss of train set: 0.35201796889305115 at epoch: 0 and batch_num: 431\n",
      "Loss of train set: 0.2966093420982361 at epoch: 0 and batch_num: 432\n",
      "Loss of train set: 0.411499947309494 at epoch: 0 and batch_num: 433\n",
      "Loss of train set: 0.3749889135360718 at epoch: 0 and batch_num: 434\n",
      "Loss of train set: 0.3998585343360901 at epoch: 0 and batch_num: 435\n",
      "Loss of train set: 0.3027050495147705 at epoch: 0 and batch_num: 436\n",
      "Loss of train set: 0.3521418273448944 at epoch: 0 and batch_num: 437\n",
      "Loss of train set: 0.3766559958457947 at epoch: 0 and batch_num: 438\n",
      "Loss of train set: 0.334184467792511 at epoch: 0 and batch_num: 439\n",
      "Loss of train set: 0.38630950450897217 at epoch: 0 and batch_num: 440\n",
      "Loss of train set: 0.4661562442779541 at epoch: 0 and batch_num: 441\n",
      "Loss of train set: 0.5323959589004517 at epoch: 0 and batch_num: 442\n",
      "Loss of train set: 0.3715282082557678 at epoch: 0 and batch_num: 443\n",
      "Loss of train set: 0.26402124762535095 at epoch: 0 and batch_num: 444\n",
      "Loss of train set: 0.43682247400283813 at epoch: 0 and batch_num: 445\n",
      "Loss of train set: 0.3766619861125946 at epoch: 0 and batch_num: 446\n",
      "Loss of train set: 0.4090244770050049 at epoch: 0 and batch_num: 447\n",
      "Loss of train set: 0.4783555269241333 at epoch: 0 and batch_num: 448\n",
      "Loss of train set: 0.4465174376964569 at epoch: 0 and batch_num: 449\n",
      "Loss of train set: 0.3876107633113861 at epoch: 0 and batch_num: 450\n",
      "Loss of train set: 0.3975799083709717 at epoch: 0 and batch_num: 451\n",
      "Loss of train set: 0.2839323580265045 at epoch: 0 and batch_num: 452\n",
      "Loss of train set: 0.45364734530448914 at epoch: 0 and batch_num: 453\n",
      "Loss of train set: 0.5545915365219116 at epoch: 0 and batch_num: 454\n",
      "Loss of train set: 0.2799936532974243 at epoch: 0 and batch_num: 455\n",
      "Loss of train set: 0.32199808955192566 at epoch: 0 and batch_num: 456\n",
      "Loss of train set: 0.3912279009819031 at epoch: 0 and batch_num: 457\n",
      "Loss of train set: 0.24252018332481384 at epoch: 0 and batch_num: 458\n",
      "Loss of train set: 0.2701157033443451 at epoch: 0 and batch_num: 459\n",
      "Loss of train set: 0.28850865364074707 at epoch: 0 and batch_num: 460\n",
      "Loss of train set: 0.37914517521858215 at epoch: 0 and batch_num: 461\n",
      "Loss of train set: 0.3365664482116699 at epoch: 0 and batch_num: 462\n",
      "Loss of train set: 0.4474089741706848 at epoch: 0 and batch_num: 463\n",
      "Loss of train set: 0.5686018466949463 at epoch: 0 and batch_num: 464\n",
      "Loss of train set: 0.5624061822891235 at epoch: 0 and batch_num: 465\n",
      "Loss of train set: 0.3649393320083618 at epoch: 0 and batch_num: 466\n",
      "Loss of train set: 0.5448291897773743 at epoch: 0 and batch_num: 467\n",
      "Loss of train set: 0.16494068503379822 at epoch: 0 and batch_num: 468\n",
      "Loss of train set: 0.3591742515563965 at epoch: 0 and batch_num: 469\n",
      "Loss of train set: 0.39541003108024597 at epoch: 0 and batch_num: 470\n",
      "Loss of train set: 0.3791044354438782 at epoch: 0 and batch_num: 471\n",
      "Loss of train set: 0.20649749040603638 at epoch: 0 and batch_num: 472\n",
      "Loss of train set: 0.36531609296798706 at epoch: 0 and batch_num: 473\n",
      "Loss of train set: 0.45950061082839966 at epoch: 0 and batch_num: 474\n",
      "Loss of train set: 0.48026975989341736 at epoch: 0 and batch_num: 475\n",
      "Loss of train set: 0.24949626624584198 at epoch: 0 and batch_num: 476\n",
      "Loss of train set: 0.3599225878715515 at epoch: 0 and batch_num: 477\n",
      "Loss of train set: 0.33925116062164307 at epoch: 0 and batch_num: 478\n",
      "Loss of train set: 0.742419421672821 at epoch: 0 and batch_num: 479\n",
      "Loss of train set: 0.2292390614748001 at epoch: 0 and batch_num: 480\n",
      "Loss of train set: 0.44323891401290894 at epoch: 0 and batch_num: 481\n",
      "Loss of train set: 0.5061057209968567 at epoch: 0 and batch_num: 482\n",
      "Loss of train set: 0.32299262285232544 at epoch: 0 and batch_num: 483\n",
      "Loss of train set: 0.3100188672542572 at epoch: 0 and batch_num: 484\n",
      "Loss of train set: 0.4554283022880554 at epoch: 0 and batch_num: 485\n",
      "Loss of train set: 0.23941659927368164 at epoch: 0 and batch_num: 486\n",
      "Loss of train set: 0.3746940791606903 at epoch: 0 and batch_num: 487\n",
      "Loss of train set: 0.4046066701412201 at epoch: 0 and batch_num: 488\n",
      "Loss of train set: 0.37845730781555176 at epoch: 0 and batch_num: 489\n",
      "Loss of train set: 0.360180526971817 at epoch: 0 and batch_num: 490\n",
      "Loss of train set: 0.333919882774353 at epoch: 0 and batch_num: 491\n",
      "Loss of train set: 0.43758073449134827 at epoch: 0 and batch_num: 492\n",
      "Loss of train set: 0.2140854299068451 at epoch: 0 and batch_num: 493\n",
      "Loss of train set: 0.33822378516197205 at epoch: 0 and batch_num: 494\n",
      "Loss of train set: 0.3357190191745758 at epoch: 0 and batch_num: 495\n",
      "Loss of train set: 0.2750086784362793 at epoch: 0 and batch_num: 496\n",
      "Loss of train set: 0.34656473994255066 at epoch: 0 and batch_num: 497\n",
      "Loss of train set: 0.5129786133766174 at epoch: 0 and batch_num: 498\n",
      "Loss of train set: 0.23468199372291565 at epoch: 0 and batch_num: 499\n",
      "Loss of train set: 0.2877275347709656 at epoch: 0 and batch_num: 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.39410775899887085 at epoch: 0 and batch_num: 501\n",
      "Loss of train set: 0.42626428604125977 at epoch: 0 and batch_num: 502\n",
      "Loss of train set: 0.31831395626068115 at epoch: 0 and batch_num: 503\n",
      "Loss of train set: 0.5429269671440125 at epoch: 0 and batch_num: 504\n",
      "Loss of train set: 0.47803419828414917 at epoch: 0 and batch_num: 505\n",
      "Loss of train set: 0.4239692687988281 at epoch: 0 and batch_num: 506\n",
      "Loss of train set: 0.38657742738723755 at epoch: 0 and batch_num: 507\n",
      "Loss of train set: 0.27424895763397217 at epoch: 0 and batch_num: 508\n",
      "Loss of train set: 0.5735201835632324 at epoch: 0 and batch_num: 509\n",
      "Loss of train set: 0.6509040594100952 at epoch: 0 and batch_num: 510\n",
      "Loss of train set: 0.5490021705627441 at epoch: 0 and batch_num: 511\n",
      "Loss of train set: 0.23069101572036743 at epoch: 0 and batch_num: 512\n",
      "Loss of train set: 0.2676221430301666 at epoch: 0 and batch_num: 513\n",
      "Loss of train set: 0.3766956925392151 at epoch: 0 and batch_num: 514\n",
      "Loss of train set: 0.3195558190345764 at epoch: 0 and batch_num: 515\n",
      "Loss of train set: 0.2959706783294678 at epoch: 0 and batch_num: 516\n",
      "Loss of train set: 0.5064859986305237 at epoch: 0 and batch_num: 517\n",
      "Loss of train set: 0.4325014650821686 at epoch: 0 and batch_num: 518\n",
      "Loss of train set: 0.4035089612007141 at epoch: 0 and batch_num: 519\n",
      "Loss of train set: 0.40517181158065796 at epoch: 0 and batch_num: 520\n",
      "Loss of train set: 0.4226166605949402 at epoch: 0 and batch_num: 521\n",
      "Loss of train set: 0.3639291822910309 at epoch: 0 and batch_num: 522\n",
      "Loss of train set: 0.23965616524219513 at epoch: 0 and batch_num: 523\n",
      "Loss of train set: 0.34092608094215393 at epoch: 0 and batch_num: 524\n",
      "Loss of train set: 0.5493295788764954 at epoch: 0 and batch_num: 525\n",
      "Loss of train set: 0.27748385071754456 at epoch: 0 and batch_num: 526\n",
      "Loss of train set: 0.37333548069000244 at epoch: 0 and batch_num: 527\n",
      "Loss of train set: 0.3833114504814148 at epoch: 0 and batch_num: 528\n",
      "Loss of train set: 0.3067435026168823 at epoch: 0 and batch_num: 529\n",
      "Loss of train set: 0.38659945130348206 at epoch: 0 and batch_num: 530\n",
      "Loss of train set: 0.2908821105957031 at epoch: 0 and batch_num: 531\n",
      "Loss of train set: 0.2458932101726532 at epoch: 0 and batch_num: 532\n",
      "Loss of train set: 0.2985353171825409 at epoch: 0 and batch_num: 533\n",
      "Loss of train set: 0.4490821063518524 at epoch: 0 and batch_num: 534\n",
      "Loss of train set: 0.3797527551651001 at epoch: 0 and batch_num: 535\n",
      "Loss of train set: 0.2874058485031128 at epoch: 0 and batch_num: 536\n",
      "Loss of train set: 0.5826032161712646 at epoch: 0 and batch_num: 537\n",
      "Loss of train set: 0.24583107233047485 at epoch: 0 and batch_num: 538\n",
      "Loss of train set: 0.36031195521354675 at epoch: 0 and batch_num: 539\n",
      "Loss of train set: 0.37532567977905273 at epoch: 0 and batch_num: 540\n",
      "Loss of train set: 0.2948482036590576 at epoch: 0 and batch_num: 541\n",
      "Loss of train set: 0.4664742946624756 at epoch: 0 and batch_num: 542\n",
      "Loss of train set: 0.4780489206314087 at epoch: 0 and batch_num: 543\n",
      "Loss of train set: 0.4639016389846802 at epoch: 0 and batch_num: 544\n",
      "Loss of train set: 0.30348730087280273 at epoch: 0 and batch_num: 545\n",
      "Loss of train set: 0.45889684557914734 at epoch: 0 and batch_num: 546\n",
      "Loss of train set: 0.3195025622844696 at epoch: 0 and batch_num: 547\n",
      "Loss of train set: 0.30696678161621094 at epoch: 0 and batch_num: 548\n",
      "Loss of train set: 0.6198514699935913 at epoch: 0 and batch_num: 549\n",
      "Loss of train set: 0.2721029818058014 at epoch: 0 and batch_num: 550\n",
      "Loss of train set: 0.30446064472198486 at epoch: 0 and batch_num: 551\n",
      "Loss of train set: 0.3381854295730591 at epoch: 0 and batch_num: 552\n",
      "Loss of train set: 0.3573877215385437 at epoch: 0 and batch_num: 553\n",
      "Loss of train set: 0.23043391108512878 at epoch: 0 and batch_num: 554\n",
      "Loss of train set: 0.34030795097351074 at epoch: 0 and batch_num: 555\n",
      "Loss of train set: 0.6316934823989868 at epoch: 0 and batch_num: 556\n",
      "Loss of train set: 0.42584219574928284 at epoch: 0 and batch_num: 557\n",
      "Loss of train set: 0.3568218946456909 at epoch: 0 and batch_num: 558\n",
      "Loss of train set: 0.39325955510139465 at epoch: 0 and batch_num: 559\n",
      "Loss of train set: 0.32743319869041443 at epoch: 0 and batch_num: 560\n",
      "Loss of train set: 0.6138644814491272 at epoch: 0 and batch_num: 561\n",
      "Loss of train set: 0.44204461574554443 at epoch: 0 and batch_num: 562\n",
      "Loss of train set: 0.357451856136322 at epoch: 0 and batch_num: 563\n",
      "Loss of train set: 0.31312572956085205 at epoch: 0 and batch_num: 564\n",
      "Loss of train set: 0.3522079586982727 at epoch: 0 and batch_num: 565\n",
      "Loss of train set: 0.514080286026001 at epoch: 0 and batch_num: 566\n",
      "Loss of train set: 0.3702527582645416 at epoch: 0 and batch_num: 567\n",
      "Loss of train set: 0.4301682710647583 at epoch: 0 and batch_num: 568\n",
      "Loss of train set: 0.43521568179130554 at epoch: 0 and batch_num: 569\n",
      "Loss of train set: 0.5303521156311035 at epoch: 0 and batch_num: 570\n",
      "Loss of train set: 0.21244806051254272 at epoch: 0 and batch_num: 571\n",
      "Loss of train set: 0.3180200159549713 at epoch: 0 and batch_num: 572\n",
      "Loss of train set: 0.2556951642036438 at epoch: 0 and batch_num: 573\n",
      "Loss of train set: 0.2883727550506592 at epoch: 0 and batch_num: 574\n",
      "Loss of train set: 0.26644742488861084 at epoch: 0 and batch_num: 575\n",
      "Loss of train set: 0.34424465894699097 at epoch: 0 and batch_num: 576\n",
      "Loss of train set: 0.3346806764602661 at epoch: 0 and batch_num: 577\n",
      "Loss of train set: 0.37993863224983215 at epoch: 0 and batch_num: 578\n",
      "Loss of train set: 0.29160988330841064 at epoch: 0 and batch_num: 579\n",
      "Loss of train set: 0.44660210609436035 at epoch: 0 and batch_num: 580\n",
      "Loss of train set: 0.22402261197566986 at epoch: 0 and batch_num: 581\n",
      "Loss of train set: 0.43921950459480286 at epoch: 0 and batch_num: 582\n",
      "Loss of train set: 0.48624491691589355 at epoch: 0 and batch_num: 583\n",
      "Loss of train set: 0.3685854971408844 at epoch: 0 and batch_num: 584\n",
      "Loss of train set: 0.25619184970855713 at epoch: 0 and batch_num: 585\n",
      "Loss of train set: 0.4291267395019531 at epoch: 0 and batch_num: 586\n",
      "Loss of train set: 0.22515639662742615 at epoch: 0 and batch_num: 587\n",
      "Loss of train set: 0.7146692276000977 at epoch: 0 and batch_num: 588\n",
      "Loss of train set: 0.35625892877578735 at epoch: 0 and batch_num: 589\n",
      "Loss of train set: 0.29902833700180054 at epoch: 0 and batch_num: 590\n",
      "Loss of train set: 0.4702988564968109 at epoch: 0 and batch_num: 591\n",
      "Loss of train set: 0.26553797721862793 at epoch: 0 and batch_num: 592\n",
      "Loss of train set: 0.22677305340766907 at epoch: 0 and batch_num: 593\n",
      "Loss of train set: 0.26326611638069153 at epoch: 0 and batch_num: 594\n",
      "Loss of train set: 0.4773975908756256 at epoch: 0 and batch_num: 595\n",
      "Loss of train set: 0.4116184711456299 at epoch: 0 and batch_num: 596\n",
      "Loss of train set: 0.3127696216106415 at epoch: 0 and batch_num: 597\n",
      "Loss of train set: 0.30183279514312744 at epoch: 0 and batch_num: 598\n",
      "Loss of train set: 0.25988855957984924 at epoch: 0 and batch_num: 599\n",
      "Loss of train set: 0.29895156621932983 at epoch: 0 and batch_num: 600\n",
      "Loss of train set: 0.41101235151290894 at epoch: 0 and batch_num: 601\n",
      "Loss of train set: 0.5255312919616699 at epoch: 0 and batch_num: 602\n",
      "Loss of train set: 0.2972604036331177 at epoch: 0 and batch_num: 603\n",
      "Loss of train set: 0.45788320899009705 at epoch: 0 and batch_num: 604\n",
      "Loss of train set: 0.5583164691925049 at epoch: 0 and batch_num: 605\n",
      "Loss of train set: 0.5214405059814453 at epoch: 0 and batch_num: 606\n",
      "Loss of train set: 0.21137301623821259 at epoch: 0 and batch_num: 607\n",
      "Loss of train set: 0.3453707695007324 at epoch: 0 and batch_num: 608\n",
      "Loss of train set: 0.39065900444984436 at epoch: 0 and batch_num: 609\n",
      "Loss of train set: 0.5042575001716614 at epoch: 0 and batch_num: 610\n",
      "Loss of train set: 0.3234370946884155 at epoch: 0 and batch_num: 611\n",
      "Loss of train set: 0.2805000841617584 at epoch: 0 and batch_num: 612\n",
      "Loss of train set: 0.18766269087791443 at epoch: 0 and batch_num: 613\n",
      "Loss of train set: 0.3679940700531006 at epoch: 0 and batch_num: 614\n",
      "Loss of train set: 0.4253472685813904 at epoch: 0 and batch_num: 615\n",
      "Loss of train set: 0.46041151881217957 at epoch: 0 and batch_num: 616\n",
      "Loss of train set: 0.3518070876598358 at epoch: 0 and batch_num: 617\n",
      "Loss of train set: 0.3431042432785034 at epoch: 0 and batch_num: 618\n",
      "Loss of train set: 0.23565669357776642 at epoch: 0 and batch_num: 619\n",
      "Loss of train set: 0.4302283525466919 at epoch: 0 and batch_num: 620\n",
      "Loss of train set: 0.27193018794059753 at epoch: 0 and batch_num: 621\n",
      "Loss of train set: 0.4245172441005707 at epoch: 0 and batch_num: 622\n",
      "Loss of train set: 0.5579515695571899 at epoch: 0 and batch_num: 623\n",
      "Loss of train set: 0.3233734667301178 at epoch: 0 and batch_num: 624\n",
      "Loss of train set: 0.3331858217716217 at epoch: 0 and batch_num: 625\n",
      "Loss of train set: 0.39414891600608826 at epoch: 0 and batch_num: 626\n",
      "Loss of train set: 0.46458369493484497 at epoch: 0 and batch_num: 627\n",
      "Loss of train set: 0.27160748839378357 at epoch: 0 and batch_num: 628\n",
      "Loss of train set: 0.2556222081184387 at epoch: 0 and batch_num: 629\n",
      "Loss of train set: 0.3683921694755554 at epoch: 0 and batch_num: 630\n",
      "Loss of train set: 0.3394905626773834 at epoch: 0 and batch_num: 631\n",
      "Loss of train set: 0.3580509424209595 at epoch: 0 and batch_num: 632\n",
      "Loss of train set: 0.518844723701477 at epoch: 0 and batch_num: 633\n",
      "Loss of train set: 0.3303246796131134 at epoch: 0 and batch_num: 634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.33376452326774597 at epoch: 0 and batch_num: 635\n",
      "Loss of train set: 0.38349321484565735 at epoch: 0 and batch_num: 636\n",
      "Loss of train set: 0.507037878036499 at epoch: 0 and batch_num: 637\n",
      "Loss of train set: 0.6931674480438232 at epoch: 0 and batch_num: 638\n",
      "Loss of train set: 0.370506227016449 at epoch: 0 and batch_num: 639\n",
      "Loss of train set: 0.28609544038772583 at epoch: 0 and batch_num: 640\n",
      "Loss of train set: 0.3822576105594635 at epoch: 0 and batch_num: 641\n",
      "Loss of train set: 0.19011306762695312 at epoch: 0 and batch_num: 642\n",
      "Loss of train set: 0.23292607069015503 at epoch: 0 and batch_num: 643\n",
      "Loss of train set: 0.3433769941329956 at epoch: 0 and batch_num: 644\n",
      "Loss of train set: 0.2655166983604431 at epoch: 0 and batch_num: 645\n",
      "Loss of train set: 0.5559884309768677 at epoch: 0 and batch_num: 646\n",
      "Loss of train set: 0.28695857524871826 at epoch: 0 and batch_num: 647\n",
      "Loss of train set: 0.37560540437698364 at epoch: 0 and batch_num: 648\n",
      "Loss of train set: 0.43450576066970825 at epoch: 0 and batch_num: 649\n",
      "Loss of train set: 0.3319869041442871 at epoch: 0 and batch_num: 650\n",
      "Loss of train set: 0.42022159695625305 at epoch: 0 and batch_num: 651\n",
      "Loss of train set: 0.4025835394859314 at epoch: 0 and batch_num: 652\n",
      "Loss of train set: 0.3261203169822693 at epoch: 0 and batch_num: 653\n",
      "Loss of train set: 0.6225850582122803 at epoch: 0 and batch_num: 654\n",
      "Loss of train set: 0.4411194920539856 at epoch: 0 and batch_num: 655\n",
      "Loss of train set: 0.28830403089523315 at epoch: 0 and batch_num: 656\n",
      "Loss of train set: 0.5505927801132202 at epoch: 0 and batch_num: 657\n",
      "Loss of train set: 0.40077823400497437 at epoch: 0 and batch_num: 658\n",
      "Loss of train set: 0.3677076995372772 at epoch: 0 and batch_num: 659\n",
      "Loss of train set: 0.26460132002830505 at epoch: 0 and batch_num: 660\n",
      "Loss of train set: 0.3118304908275604 at epoch: 0 and batch_num: 661\n",
      "Loss of train set: 0.4947822690010071 at epoch: 0 and batch_num: 662\n",
      "Loss of train set: 0.3315393030643463 at epoch: 0 and batch_num: 663\n",
      "Loss of train set: 0.2543068528175354 at epoch: 0 and batch_num: 664\n",
      "Loss of train set: 0.4139832854270935 at epoch: 0 and batch_num: 665\n",
      "Loss of train set: 0.5019130706787109 at epoch: 0 and batch_num: 666\n",
      "Loss of train set: 0.37787169218063354 at epoch: 0 and batch_num: 667\n",
      "Loss of train set: 0.4746411442756653 at epoch: 0 and batch_num: 668\n",
      "Loss of train set: 0.5221734046936035 at epoch: 0 and batch_num: 669\n",
      "Loss of train set: 0.39521366357803345 at epoch: 0 and batch_num: 670\n",
      "Loss of train set: 0.4817548394203186 at epoch: 0 and batch_num: 671\n",
      "Loss of train set: 0.2866607904434204 at epoch: 0 and batch_num: 672\n",
      "Loss of train set: 0.47892823815345764 at epoch: 0 and batch_num: 673\n",
      "Loss of train set: 0.3709442615509033 at epoch: 0 and batch_num: 674\n",
      "Loss of train set: 0.2903560400009155 at epoch: 0 and batch_num: 675\n",
      "Loss of train set: 0.2468937635421753 at epoch: 0 and batch_num: 676\n",
      "Loss of train set: 0.32798147201538086 at epoch: 0 and batch_num: 677\n",
      "Loss of train set: 0.3973809480667114 at epoch: 0 and batch_num: 678\n",
      "Loss of train set: 0.4514651596546173 at epoch: 0 and batch_num: 679\n",
      "Loss of train set: 0.36485666036605835 at epoch: 0 and batch_num: 680\n",
      "Loss of train set: 0.5453639030456543 at epoch: 0 and batch_num: 681\n",
      "Loss of train set: 0.4353383183479309 at epoch: 0 and batch_num: 682\n",
      "Loss of train set: 0.17808254063129425 at epoch: 0 and batch_num: 683\n",
      "Loss of train set: 0.6390559673309326 at epoch: 0 and batch_num: 684\n",
      "Loss of train set: 0.4648224115371704 at epoch: 0 and batch_num: 685\n",
      "Loss of train set: 0.3876657485961914 at epoch: 0 and batch_num: 686\n",
      "Loss of train set: 0.4625546336174011 at epoch: 0 and batch_num: 687\n",
      "Loss of train set: 0.1806761622428894 at epoch: 0 and batch_num: 688\n",
      "Loss of train set: 0.3348514437675476 at epoch: 0 and batch_num: 689\n",
      "Loss of train set: 0.4342939853668213 at epoch: 0 and batch_num: 690\n",
      "Loss of train set: 0.4664275050163269 at epoch: 0 and batch_num: 691\n",
      "Loss of train set: 0.2462349236011505 at epoch: 0 and batch_num: 692\n",
      "Loss of train set: 0.3837006688117981 at epoch: 0 and batch_num: 693\n",
      "Loss of train set: 0.653128981590271 at epoch: 0 and batch_num: 694\n",
      "Loss of train set: 0.45486700534820557 at epoch: 0 and batch_num: 695\n",
      "Loss of train set: 0.3715142011642456 at epoch: 0 and batch_num: 696\n",
      "Loss of train set: 0.2108609825372696 at epoch: 0 and batch_num: 697\n",
      "Loss of train set: 0.2920183539390564 at epoch: 0 and batch_num: 698\n",
      "Loss of train set: 0.31325244903564453 at epoch: 0 and batch_num: 699\n",
      "Loss of train set: 0.22513070702552795 at epoch: 0 and batch_num: 700\n",
      "Loss of train set: 0.3675987720489502 at epoch: 0 and batch_num: 701\n",
      "Loss of train set: 0.40344202518463135 at epoch: 0 and batch_num: 702\n",
      "Loss of train set: 0.2389911711215973 at epoch: 0 and batch_num: 703\n",
      "Loss of train set: 0.30637213587760925 at epoch: 0 and batch_num: 704\n",
      "Loss of train set: 0.3204050660133362 at epoch: 0 and batch_num: 705\n",
      "Loss of train set: 0.3669983744621277 at epoch: 0 and batch_num: 706\n",
      "Loss of train set: 0.3865645229816437 at epoch: 0 and batch_num: 707\n",
      "Loss of train set: 0.41075459122657776 at epoch: 0 and batch_num: 708\n",
      "Loss of train set: 0.382504940032959 at epoch: 0 and batch_num: 709\n",
      "Loss of train set: 0.31666022539138794 at epoch: 0 and batch_num: 710\n",
      "Loss of train set: 0.3839495778083801 at epoch: 0 and batch_num: 711\n",
      "Loss of train set: 0.32204973697662354 at epoch: 0 and batch_num: 712\n",
      "Loss of train set: 0.228693887591362 at epoch: 0 and batch_num: 713\n",
      "Loss of train set: 0.25533100962638855 at epoch: 0 and batch_num: 714\n",
      "Loss of train set: 0.3550798296928406 at epoch: 0 and batch_num: 715\n",
      "Loss of train set: 0.500487208366394 at epoch: 0 and batch_num: 716\n",
      "Loss of train set: 0.331520676612854 at epoch: 0 and batch_num: 717\n",
      "Loss of train set: 0.345727801322937 at epoch: 0 and batch_num: 718\n",
      "Loss of train set: 0.34199950098991394 at epoch: 0 and batch_num: 719\n",
      "Loss of train set: 0.21826131641864777 at epoch: 0 and batch_num: 720\n",
      "Loss of train set: 0.655978798866272 at epoch: 0 and batch_num: 721\n",
      "Loss of train set: 0.538916289806366 at epoch: 0 and batch_num: 722\n",
      "Loss of train set: 0.41192299127578735 at epoch: 0 and batch_num: 723\n",
      "Loss of train set: 0.29799744486808777 at epoch: 0 and batch_num: 724\n",
      "Loss of train set: 0.3296160101890564 at epoch: 0 and batch_num: 725\n",
      "Loss of train set: 0.302579790353775 at epoch: 0 and batch_num: 726\n",
      "Loss of train set: 0.4089162349700928 at epoch: 0 and batch_num: 727\n",
      "Loss of train set: 0.24174988269805908 at epoch: 0 and batch_num: 728\n",
      "Loss of train set: 0.36472854018211365 at epoch: 0 and batch_num: 729\n",
      "Loss of train set: 0.3353879451751709 at epoch: 0 and batch_num: 730\n",
      "Loss of train set: 0.4866538643836975 at epoch: 0 and batch_num: 731\n",
      "Loss of train set: 0.36684107780456543 at epoch: 0 and batch_num: 732\n",
      "Loss of train set: 0.5507277250289917 at epoch: 0 and batch_num: 733\n",
      "Loss of train set: 0.3517722189426422 at epoch: 0 and batch_num: 734\n",
      "Loss of train set: 0.24675527215003967 at epoch: 0 and batch_num: 735\n",
      "Loss of train set: 0.43076226115226746 at epoch: 0 and batch_num: 736\n",
      "Loss of train set: 0.31466856598854065 at epoch: 0 and batch_num: 737\n",
      "Loss of train set: 0.3493175208568573 at epoch: 0 and batch_num: 738\n",
      "Loss of train set: 0.3779987096786499 at epoch: 0 and batch_num: 739\n",
      "Loss of train set: 0.2955673336982727 at epoch: 0 and batch_num: 740\n",
      "Loss of train set: 0.40193381905555725 at epoch: 0 and batch_num: 741\n",
      "Loss of train set: 0.49937689304351807 at epoch: 0 and batch_num: 742\n",
      "Loss of train set: 0.34873273968696594 at epoch: 0 and batch_num: 743\n",
      "Loss of train set: 0.3561699688434601 at epoch: 0 and batch_num: 744\n",
      "Loss of train set: 0.32225868105888367 at epoch: 0 and batch_num: 745\n",
      "Loss of train set: 0.36918342113494873 at epoch: 0 and batch_num: 746\n",
      "Loss of train set: 0.5257384777069092 at epoch: 0 and batch_num: 747\n",
      "Loss of train set: 0.22088691592216492 at epoch: 0 and batch_num: 748\n",
      "Loss of train set: 0.32409390807151794 at epoch: 0 and batch_num: 749\n",
      "Loss of train set: 0.4197852611541748 at epoch: 0 and batch_num: 750\n",
      "Loss of train set: 0.4158729910850525 at epoch: 0 and batch_num: 751\n",
      "Loss of train set: 0.45830070972442627 at epoch: 0 and batch_num: 752\n",
      "Loss of train set: 0.5265558362007141 at epoch: 0 and batch_num: 753\n",
      "Loss of train set: 0.41049784421920776 at epoch: 0 and batch_num: 754\n",
      "Loss of train set: 0.2309502363204956 at epoch: 0 and batch_num: 755\n",
      "Loss of train set: 0.44405290484428406 at epoch: 0 and batch_num: 756\n",
      "Loss of train set: 0.5704643726348877 at epoch: 0 and batch_num: 757\n",
      "Loss of train set: 0.4431907534599304 at epoch: 0 and batch_num: 758\n",
      "Loss of train set: 0.3005983233451843 at epoch: 0 and batch_num: 759\n",
      "Loss of train set: 0.3113933205604553 at epoch: 0 and batch_num: 760\n",
      "Loss of train set: 0.503628134727478 at epoch: 0 and batch_num: 761\n",
      "Loss of train set: 0.2622581124305725 at epoch: 0 and batch_num: 762\n",
      "Loss of train set: 0.4797547161579132 at epoch: 0 and batch_num: 763\n",
      "Loss of train set: 0.46682238578796387 at epoch: 0 and batch_num: 764\n",
      "Loss of train set: 0.426769882440567 at epoch: 0 and batch_num: 765\n",
      "Loss of train set: 0.3151523768901825 at epoch: 0 and batch_num: 766\n",
      "Loss of train set: 0.4238935708999634 at epoch: 0 and batch_num: 767\n",
      "Loss of train set: 0.3759124279022217 at epoch: 0 and batch_num: 768\n",
      "Loss of train set: 0.566730260848999 at epoch: 0 and batch_num: 769\n",
      "Loss of train set: 0.3630931079387665 at epoch: 0 and batch_num: 770\n",
      "Loss of train set: 0.43339991569519043 at epoch: 0 and batch_num: 771\n",
      "Loss of train set: 0.3311852216720581 at epoch: 0 and batch_num: 772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3399146795272827 at epoch: 0 and batch_num: 773\n",
      "Loss of train set: 0.3275381028652191 at epoch: 0 and batch_num: 774\n",
      "Loss of train set: 0.41566264629364014 at epoch: 0 and batch_num: 775\n",
      "Loss of train set: 0.29501309990882874 at epoch: 0 and batch_num: 776\n",
      "Loss of train set: 0.46446162462234497 at epoch: 0 and batch_num: 777\n",
      "Loss of train set: 0.30979636311531067 at epoch: 0 and batch_num: 778\n",
      "Loss of train set: 0.2963276505470276 at epoch: 0 and batch_num: 779\n",
      "Loss of train set: 0.2753964364528656 at epoch: 0 and batch_num: 780\n",
      "Loss of train set: 0.7420386075973511 at epoch: 0 and batch_num: 781\n",
      "Loss of train set: 0.22282949090003967 at epoch: 0 and batch_num: 782\n",
      "Loss of train set: 0.34051620960235596 at epoch: 0 and batch_num: 783\n",
      "Loss of train set: 0.4855588972568512 at epoch: 0 and batch_num: 784\n",
      "Loss of train set: 0.41189417243003845 at epoch: 0 and batch_num: 785\n",
      "Loss of train set: 0.36117637157440186 at epoch: 0 and batch_num: 786\n",
      "Loss of train set: 0.3769991397857666 at epoch: 0 and batch_num: 787\n",
      "Loss of train set: 0.5285615921020508 at epoch: 0 and batch_num: 788\n",
      "Loss of train set: 0.4124710261821747 at epoch: 0 and batch_num: 789\n",
      "Loss of train set: 0.3494477868080139 at epoch: 0 and batch_num: 790\n",
      "Loss of train set: 0.4090869724750519 at epoch: 0 and batch_num: 791\n",
      "Loss of train set: 0.21767356991767883 at epoch: 0 and batch_num: 792\n",
      "Loss of train set: 0.30636897683143616 at epoch: 0 and batch_num: 793\n",
      "Loss of train set: 0.4935325086116791 at epoch: 0 and batch_num: 794\n",
      "Loss of train set: 0.2153075635433197 at epoch: 0 and batch_num: 795\n",
      "Loss of train set: 0.42493903636932373 at epoch: 0 and batch_num: 796\n",
      "Loss of train set: 0.3621595501899719 at epoch: 0 and batch_num: 797\n",
      "Loss of train set: 0.3187583386898041 at epoch: 0 and batch_num: 798\n",
      "Loss of train set: 0.24969995021820068 at epoch: 0 and batch_num: 799\n",
      "Loss of train set: 0.5847244262695312 at epoch: 0 and batch_num: 800\n",
      "Loss of train set: 0.31974369287490845 at epoch: 0 and batch_num: 801\n",
      "Loss of train set: 0.6279957294464111 at epoch: 0 and batch_num: 802\n",
      "Loss of train set: 0.4192996025085449 at epoch: 0 and batch_num: 803\n",
      "Loss of train set: 0.3733460605144501 at epoch: 0 and batch_num: 804\n",
      "Loss of train set: 0.4722038507461548 at epoch: 0 and batch_num: 805\n",
      "Loss of train set: 0.4780789613723755 at epoch: 0 and batch_num: 806\n",
      "Loss of train set: 0.5527734756469727 at epoch: 0 and batch_num: 807\n",
      "Loss of train set: 0.4100854992866516 at epoch: 0 and batch_num: 808\n",
      "Loss of train set: 0.48055705428123474 at epoch: 0 and batch_num: 809\n",
      "Loss of train set: 0.33875009417533875 at epoch: 0 and batch_num: 810\n",
      "Loss of train set: 0.3546147346496582 at epoch: 0 and batch_num: 811\n",
      "Loss of train set: 0.35938018560409546 at epoch: 0 and batch_num: 812\n",
      "Loss of train set: 0.6425682306289673 at epoch: 0 and batch_num: 813\n",
      "Loss of train set: 0.21395714581012726 at epoch: 0 and batch_num: 814\n",
      "Loss of train set: 0.39500874280929565 at epoch: 0 and batch_num: 815\n",
      "Loss of train set: 0.21171467006206512 at epoch: 0 and batch_num: 816\n",
      "Loss of train set: 0.3303920030593872 at epoch: 0 and batch_num: 817\n",
      "Loss of train set: 0.367201030254364 at epoch: 0 and batch_num: 818\n",
      "Loss of train set: 0.21433663368225098 at epoch: 0 and batch_num: 819\n",
      "Loss of train set: 0.42598050832748413 at epoch: 0 and batch_num: 820\n",
      "Loss of train set: 0.17763303220272064 at epoch: 0 and batch_num: 821\n",
      "Loss of train set: 0.478439599275589 at epoch: 0 and batch_num: 822\n",
      "Loss of train set: 0.2597622573375702 at epoch: 0 and batch_num: 823\n",
      "Loss of train set: 0.5208419561386108 at epoch: 0 and batch_num: 824\n",
      "Loss of train set: 0.4279428720474243 at epoch: 0 and batch_num: 825\n",
      "Loss of train set: 0.3469698131084442 at epoch: 0 and batch_num: 826\n",
      "Loss of train set: 0.5692235231399536 at epoch: 0 and batch_num: 827\n",
      "Loss of train set: 0.2817822992801666 at epoch: 0 and batch_num: 828\n",
      "Loss of train set: 0.4195975661277771 at epoch: 0 and batch_num: 829\n",
      "Loss of train set: 0.33714431524276733 at epoch: 0 and batch_num: 830\n",
      "Loss of train set: 0.37832653522491455 at epoch: 0 and batch_num: 831\n",
      "Loss of train set: 0.38001924753189087 at epoch: 0 and batch_num: 832\n",
      "Loss of train set: 0.4025265574455261 at epoch: 0 and batch_num: 833\n",
      "Loss of train set: 0.48826995491981506 at epoch: 0 and batch_num: 834\n",
      "Loss of train set: 0.47930872440338135 at epoch: 0 and batch_num: 835\n",
      "Loss of train set: 0.3969331383705139 at epoch: 0 and batch_num: 836\n",
      "Loss of train set: 0.24595248699188232 at epoch: 0 and batch_num: 837\n",
      "Loss of train set: 0.34081512689590454 at epoch: 0 and batch_num: 838\n",
      "Loss of train set: 0.504637598991394 at epoch: 0 and batch_num: 839\n",
      "Loss of train set: 0.4436415433883667 at epoch: 0 and batch_num: 840\n",
      "Loss of train set: 0.4165368676185608 at epoch: 0 and batch_num: 841\n",
      "Loss of train set: 0.28053152561187744 at epoch: 0 and batch_num: 842\n",
      "Loss of train set: 0.38516801595687866 at epoch: 0 and batch_num: 843\n",
      "Loss of train set: 0.3132498264312744 at epoch: 0 and batch_num: 844\n",
      "Loss of train set: 0.20173326134681702 at epoch: 0 and batch_num: 845\n",
      "Loss of train set: 0.290524959564209 at epoch: 0 and batch_num: 846\n",
      "Loss of train set: 0.3566533327102661 at epoch: 0 and batch_num: 847\n",
      "Loss of train set: 0.32359156012535095 at epoch: 0 and batch_num: 848\n",
      "Loss of train set: 0.3010520339012146 at epoch: 0 and batch_num: 849\n",
      "Loss of train set: 0.3437051773071289 at epoch: 0 and batch_num: 850\n",
      "Loss of train set: 0.5643088817596436 at epoch: 0 and batch_num: 851\n",
      "Loss of train set: 0.5013452768325806 at epoch: 0 and batch_num: 852\n",
      "Loss of train set: 0.3562687635421753 at epoch: 0 and batch_num: 853\n",
      "Loss of train set: 0.2922208905220032 at epoch: 0 and batch_num: 854\n",
      "Loss of train set: 0.39699995517730713 at epoch: 0 and batch_num: 855\n",
      "Loss of train set: 0.3545750081539154 at epoch: 0 and batch_num: 856\n",
      "Loss of train set: 0.36609697341918945 at epoch: 0 and batch_num: 857\n",
      "Loss of train set: 0.42743945121765137 at epoch: 0 and batch_num: 858\n",
      "Loss of train set: 0.21528705954551697 at epoch: 0 and batch_num: 859\n",
      "Loss of train set: 0.2787061035633087 at epoch: 0 and batch_num: 860\n",
      "Loss of train set: 0.4034588038921356 at epoch: 0 and batch_num: 861\n",
      "Loss of train set: 0.5959650874137878 at epoch: 0 and batch_num: 862\n",
      "Loss of train set: 0.3332100510597229 at epoch: 0 and batch_num: 863\n",
      "Loss of train set: 0.42171213030815125 at epoch: 0 and batch_num: 864\n",
      "Loss of train set: 0.2446359097957611 at epoch: 0 and batch_num: 865\n",
      "Loss of train set: 0.3858725428581238 at epoch: 0 and batch_num: 866\n",
      "Loss of train set: 0.572739839553833 at epoch: 0 and batch_num: 867\n",
      "Loss of train set: 0.2993292212486267 at epoch: 0 and batch_num: 868\n",
      "Loss of train set: 0.3522534966468811 at epoch: 0 and batch_num: 869\n",
      "Loss of train set: 0.22228257358074188 at epoch: 0 and batch_num: 870\n",
      "Loss of train set: 0.5317277312278748 at epoch: 0 and batch_num: 871\n",
      "Loss of train set: 0.44024091958999634 at epoch: 0 and batch_num: 872\n",
      "Loss of train set: 0.3683055639266968 at epoch: 0 and batch_num: 873\n",
      "Loss of train set: 0.33420318365097046 at epoch: 0 and batch_num: 874\n",
      "Loss of train set: 0.3956051468849182 at epoch: 0 and batch_num: 875\n",
      "Loss of train set: 0.34022510051727295 at epoch: 0 and batch_num: 876\n",
      "Loss of train set: 0.3910589814186096 at epoch: 0 and batch_num: 877\n",
      "Loss of train set: 0.4923056364059448 at epoch: 0 and batch_num: 878\n",
      "Loss of train set: 0.4825717806816101 at epoch: 0 and batch_num: 879\n",
      "Loss of train set: 0.26809582114219666 at epoch: 0 and batch_num: 880\n",
      "Loss of train set: 0.3078330159187317 at epoch: 0 and batch_num: 881\n",
      "Loss of train set: 0.4098593592643738 at epoch: 0 and batch_num: 882\n",
      "Loss of train set: 0.41052132844924927 at epoch: 0 and batch_num: 883\n",
      "Loss of train set: 0.25780197978019714 at epoch: 0 and batch_num: 884\n",
      "Loss of train set: 0.3272746503353119 at epoch: 0 and batch_num: 885\n",
      "Loss of train set: 0.4294796586036682 at epoch: 0 and batch_num: 886\n",
      "Loss of train set: 0.41405755281448364 at epoch: 0 and batch_num: 887\n",
      "Loss of train set: 0.3263397514820099 at epoch: 0 and batch_num: 888\n",
      "Loss of train set: 0.33931952714920044 at epoch: 0 and batch_num: 889\n",
      "Loss of train set: 0.2349778115749359 at epoch: 0 and batch_num: 890\n",
      "Loss of train set: 0.32889410853385925 at epoch: 0 and batch_num: 891\n",
      "Loss of train set: 0.26870816946029663 at epoch: 0 and batch_num: 892\n",
      "Loss of train set: 0.34154126048088074 at epoch: 0 and batch_num: 893\n",
      "Loss of train set: 0.5350269079208374 at epoch: 0 and batch_num: 894\n",
      "Loss of train set: 0.33161136507987976 at epoch: 0 and batch_num: 895\n",
      "Loss of train set: 0.3051314353942871 at epoch: 0 and batch_num: 896\n",
      "Loss of train set: 0.4046957790851593 at epoch: 0 and batch_num: 897\n",
      "Loss of train set: 0.41808056831359863 at epoch: 0 and batch_num: 898\n",
      "Loss of train set: 0.3649062514305115 at epoch: 0 and batch_num: 899\n",
      "Loss of train set: 0.297993004322052 at epoch: 0 and batch_num: 900\n",
      "Loss of train set: 0.44657835364341736 at epoch: 0 and batch_num: 901\n",
      "Loss of train set: 0.5890638828277588 at epoch: 0 and batch_num: 902\n",
      "Loss of train set: 0.5564221143722534 at epoch: 0 and batch_num: 903\n",
      "Loss of train set: 0.6340509653091431 at epoch: 0 and batch_num: 904\n",
      "Loss of train set: 0.604270339012146 at epoch: 0 and batch_num: 905\n",
      "Loss of train set: 0.4523293077945709 at epoch: 0 and batch_num: 906\n",
      "Loss of train set: 0.44702664017677307 at epoch: 0 and batch_num: 907\n",
      "Loss of train set: 0.23945681750774384 at epoch: 0 and batch_num: 908\n",
      "Loss of train set: 0.38623422384262085 at epoch: 0 and batch_num: 909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.415225625038147 at epoch: 0 and batch_num: 910\n",
      "Loss of train set: 0.37174880504608154 at epoch: 0 and batch_num: 911\n",
      "Loss of train set: 0.3559023439884186 at epoch: 0 and batch_num: 912\n",
      "Loss of train set: 0.35702306032180786 at epoch: 0 and batch_num: 913\n",
      "Loss of train set: 0.35807785391807556 at epoch: 0 and batch_num: 914\n",
      "Loss of train set: 0.4248388111591339 at epoch: 0 and batch_num: 915\n",
      "Loss of train set: 0.34579408168792725 at epoch: 0 and batch_num: 916\n",
      "Loss of train set: 0.43540486693382263 at epoch: 0 and batch_num: 917\n",
      "Loss of train set: 0.33084195852279663 at epoch: 0 and batch_num: 918\n",
      "Loss of train set: 0.3291042745113373 at epoch: 0 and batch_num: 919\n",
      "Loss of train set: 0.3111221492290497 at epoch: 0 and batch_num: 920\n",
      "Loss of train set: 0.28372102975845337 at epoch: 0 and batch_num: 921\n",
      "Loss of train set: 0.36876025795936584 at epoch: 0 and batch_num: 922\n",
      "Loss of train set: 0.6022666692733765 at epoch: 0 and batch_num: 923\n",
      "Loss of train set: 0.18513408303260803 at epoch: 0 and batch_num: 924\n",
      "Loss of train set: 0.28038913011550903 at epoch: 0 and batch_num: 925\n",
      "Loss of train set: 0.4039132595062256 at epoch: 0 and batch_num: 926\n",
      "Loss of train set: 0.2875068783760071 at epoch: 0 and batch_num: 927\n",
      "Loss of train set: 0.46115031838417053 at epoch: 0 and batch_num: 928\n",
      "Loss of train set: 0.3382311463356018 at epoch: 0 and batch_num: 929\n",
      "Loss of train set: 0.29227393865585327 at epoch: 0 and batch_num: 930\n",
      "Loss of train set: 0.33641454577445984 at epoch: 0 and batch_num: 931\n",
      "Loss of train set: 0.24122531712055206 at epoch: 0 and batch_num: 932\n",
      "Loss of train set: 0.36713603138923645 at epoch: 0 and batch_num: 933\n",
      "Loss of train set: 0.3216273784637451 at epoch: 0 and batch_num: 934\n",
      "Loss of train set: 0.30824071168899536 at epoch: 0 and batch_num: 935\n",
      "Loss of train set: 0.4283764958381653 at epoch: 0 and batch_num: 936\n",
      "Loss of train set: 0.5271669030189514 at epoch: 0 and batch_num: 937\n",
      "Accuracy of train set: 0.8658833333333333\n",
      "Loss of test set: 0.5179848670959473 at epoch: 0 and batch_num: 0\n",
      "Loss of test set: 0.47135835886001587 at epoch: 0 and batch_num: 1\n",
      "Loss of test set: 0.4288548529148102 at epoch: 0 and batch_num: 2\n",
      "Loss of test set: 0.45293548703193665 at epoch: 0 and batch_num: 3\n",
      "Loss of test set: 0.4481259286403656 at epoch: 0 and batch_num: 4\n",
      "Loss of test set: 0.5793436765670776 at epoch: 0 and batch_num: 5\n",
      "Loss of test set: 0.46759045124053955 at epoch: 0 and batch_num: 6\n",
      "Loss of test set: 0.3735462427139282 at epoch: 0 and batch_num: 7\n",
      "Loss of test set: 0.6533371210098267 at epoch: 0 and batch_num: 8\n",
      "Loss of test set: 0.6106768846511841 at epoch: 0 and batch_num: 9\n",
      "Loss of test set: 0.45604372024536133 at epoch: 0 and batch_num: 10\n",
      "Loss of test set: 0.5444111824035645 at epoch: 0 and batch_num: 11\n",
      "Loss of test set: 0.28437554836273193 at epoch: 0 and batch_num: 12\n",
      "Loss of test set: 0.36864787340164185 at epoch: 0 and batch_num: 13\n",
      "Loss of test set: 0.4202294945716858 at epoch: 0 and batch_num: 14\n",
      "Loss of test set: 0.566076934337616 at epoch: 0 and batch_num: 15\n",
      "Loss of test set: 0.4472183585166931 at epoch: 0 and batch_num: 16\n",
      "Loss of test set: 0.6235836744308472 at epoch: 0 and batch_num: 17\n",
      "Loss of test set: 0.4840173125267029 at epoch: 0 and batch_num: 18\n",
      "Loss of test set: 0.3436352610588074 at epoch: 0 and batch_num: 19\n",
      "Loss of test set: 0.4440600275993347 at epoch: 0 and batch_num: 20\n",
      "Loss of test set: 0.3737957775592804 at epoch: 0 and batch_num: 21\n",
      "Loss of test set: 0.5113328695297241 at epoch: 0 and batch_num: 22\n",
      "Loss of test set: 0.5090312361717224 at epoch: 0 and batch_num: 23\n",
      "Loss of test set: 0.6035313010215759 at epoch: 0 and batch_num: 24\n",
      "Loss of test set: 0.20050394535064697 at epoch: 0 and batch_num: 25\n",
      "Loss of test set: 0.3650476038455963 at epoch: 0 and batch_num: 26\n",
      "Loss of test set: 0.5641502141952515 at epoch: 0 and batch_num: 27\n",
      "Loss of test set: 0.551267683506012 at epoch: 0 and batch_num: 28\n",
      "Loss of test set: 0.4300385117530823 at epoch: 0 and batch_num: 29\n",
      "Loss of test set: 0.4560588300228119 at epoch: 0 and batch_num: 30\n",
      "Loss of test set: 0.4225831925868988 at epoch: 0 and batch_num: 31\n",
      "Loss of test set: 0.4584171772003174 at epoch: 0 and batch_num: 32\n",
      "Loss of test set: 0.3548870086669922 at epoch: 0 and batch_num: 33\n",
      "Loss of test set: 0.4898294508457184 at epoch: 0 and batch_num: 34\n",
      "Loss of test set: 0.5945907831192017 at epoch: 0 and batch_num: 35\n",
      "Loss of test set: 0.3263322412967682 at epoch: 0 and batch_num: 36\n",
      "Loss of test set: 0.376955509185791 at epoch: 0 and batch_num: 37\n",
      "Loss of test set: 0.42751720547676086 at epoch: 0 and batch_num: 38\n",
      "Loss of test set: 0.5783184766769409 at epoch: 0 and batch_num: 39\n",
      "Loss of test set: 0.3394365906715393 at epoch: 0 and batch_num: 40\n",
      "Loss of test set: 0.5834865570068359 at epoch: 0 and batch_num: 41\n",
      "Loss of test set: 0.4606245756149292 at epoch: 0 and batch_num: 42\n",
      "Loss of test set: 0.5147783756256104 at epoch: 0 and batch_num: 43\n",
      "Loss of test set: 0.5926995277404785 at epoch: 0 and batch_num: 44\n",
      "Loss of test set: 0.6833863258361816 at epoch: 0 and batch_num: 45\n",
      "Loss of test set: 0.4834696054458618 at epoch: 0 and batch_num: 46\n",
      "Loss of test set: 0.5518322587013245 at epoch: 0 and batch_num: 47\n",
      "Loss of test set: 0.46458929777145386 at epoch: 0 and batch_num: 48\n",
      "Loss of test set: 0.4919584393501282 at epoch: 0 and batch_num: 49\n",
      "Loss of test set: 0.27703964710235596 at epoch: 0 and batch_num: 50\n",
      "Loss of test set: 0.3967393636703491 at epoch: 0 and batch_num: 51\n",
      "Loss of test set: 0.36264416575431824 at epoch: 0 and batch_num: 52\n",
      "Loss of test set: 0.3902721405029297 at epoch: 0 and batch_num: 53\n",
      "Loss of test set: 0.40546393394470215 at epoch: 0 and batch_num: 54\n",
      "Loss of test set: 0.524969756603241 at epoch: 0 and batch_num: 55\n",
      "Loss of test set: 0.3470374345779419 at epoch: 0 and batch_num: 56\n",
      "Loss of test set: 0.5922967195510864 at epoch: 0 and batch_num: 57\n",
      "Loss of test set: 0.4092226028442383 at epoch: 0 and batch_num: 58\n",
      "Loss of test set: 0.49544042348861694 at epoch: 0 and batch_num: 59\n",
      "Loss of test set: 0.34036558866500854 at epoch: 0 and batch_num: 60\n",
      "Loss of test set: 0.5768628120422363 at epoch: 0 and batch_num: 61\n",
      "Loss of test set: 0.6387804746627808 at epoch: 0 and batch_num: 62\n",
      "Loss of test set: 0.4421243667602539 at epoch: 0 and batch_num: 63\n",
      "Loss of test set: 0.5222134590148926 at epoch: 0 and batch_num: 64\n",
      "Loss of test set: 0.39100873470306396 at epoch: 0 and batch_num: 65\n",
      "Loss of test set: 0.33775320649147034 at epoch: 0 and batch_num: 66\n",
      "Loss of test set: 0.4063970446586609 at epoch: 0 and batch_num: 67\n",
      "Loss of test set: 0.42970412969589233 at epoch: 0 and batch_num: 68\n",
      "Loss of test set: 0.3157683312892914 at epoch: 0 and batch_num: 69\n",
      "Loss of test set: 0.28994858264923096 at epoch: 0 and batch_num: 70\n",
      "Loss of test set: 0.24777567386627197 at epoch: 0 and batch_num: 71\n",
      "Loss of test set: 0.34461960196495056 at epoch: 0 and batch_num: 72\n",
      "Loss of test set: 0.24133306741714478 at epoch: 0 and batch_num: 73\n",
      "Loss of test set: 0.5921095609664917 at epoch: 0 and batch_num: 74\n",
      "Loss of test set: 0.6617163419723511 at epoch: 0 and batch_num: 75\n",
      "Loss of test set: 0.26595184206962585 at epoch: 0 and batch_num: 76\n",
      "Loss of test set: 0.4320671260356903 at epoch: 0 and batch_num: 77\n",
      "Loss of test set: 0.46139565110206604 at epoch: 0 and batch_num: 78\n",
      "Loss of test set: 0.4704529941082001 at epoch: 0 and batch_num: 79\n",
      "Loss of test set: 0.549209713935852 at epoch: 0 and batch_num: 80\n",
      "Loss of test set: 0.7735404968261719 at epoch: 0 and batch_num: 81\n",
      "Loss of test set: 0.5245027542114258 at epoch: 0 and batch_num: 82\n",
      "Loss of test set: 0.5010101795196533 at epoch: 0 and batch_num: 83\n",
      "Loss of test set: 0.28986841440200806 at epoch: 0 and batch_num: 84\n",
      "Loss of test set: 0.6207326054573059 at epoch: 0 and batch_num: 85\n",
      "Loss of test set: 0.358708918094635 at epoch: 0 and batch_num: 86\n",
      "Loss of test set: 0.421302855014801 at epoch: 0 and batch_num: 87\n",
      "Loss of test set: 0.7406128644943237 at epoch: 0 and batch_num: 88\n",
      "Loss of test set: 0.512389063835144 at epoch: 0 and batch_num: 89\n",
      "Loss of test set: 0.3257395029067993 at epoch: 0 and batch_num: 90\n",
      "Loss of test set: 0.4455970525741577 at epoch: 0 and batch_num: 91\n",
      "Loss of test set: 0.2659405767917633 at epoch: 0 and batch_num: 92\n",
      "Loss of test set: 0.2949252426624298 at epoch: 0 and batch_num: 93\n",
      "Loss of test set: 0.5859847068786621 at epoch: 0 and batch_num: 94\n",
      "Loss of test set: 0.5021394491195679 at epoch: 0 and batch_num: 95\n",
      "Loss of test set: 0.465053528547287 at epoch: 0 and batch_num: 96\n",
      "Loss of test set: 0.433891236782074 at epoch: 0 and batch_num: 97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.46167612075805664 at epoch: 0 and batch_num: 98\n",
      "Loss of test set: 0.4520862102508545 at epoch: 0 and batch_num: 99\n",
      "Loss of test set: 0.3219566345214844 at epoch: 0 and batch_num: 100\n",
      "Loss of test set: 0.5382290482521057 at epoch: 0 and batch_num: 101\n",
      "Loss of test set: 0.5521970391273499 at epoch: 0 and batch_num: 102\n",
      "Loss of test set: 0.4601646363735199 at epoch: 0 and batch_num: 103\n",
      "Loss of test set: 0.25799795985221863 at epoch: 0 and batch_num: 104\n",
      "Loss of test set: 0.45640501379966736 at epoch: 0 and batch_num: 105\n",
      "Loss of test set: 0.25931137800216675 at epoch: 0 and batch_num: 106\n",
      "Loss of test set: 0.4268757104873657 at epoch: 0 and batch_num: 107\n",
      "Loss of test set: 0.33202579617500305 at epoch: 0 and batch_num: 108\n",
      "Loss of test set: 0.5083356499671936 at epoch: 0 and batch_num: 109\n",
      "Loss of test set: 0.7721631526947021 at epoch: 0 and batch_num: 110\n",
      "Loss of test set: 0.4207928776741028 at epoch: 0 and batch_num: 111\n",
      "Loss of test set: 0.4700452387332916 at epoch: 0 and batch_num: 112\n",
      "Loss of test set: 0.21307295560836792 at epoch: 0 and batch_num: 113\n",
      "Loss of test set: 0.5323463678359985 at epoch: 0 and batch_num: 114\n",
      "Loss of test set: 0.35423707962036133 at epoch: 0 and batch_num: 115\n",
      "Loss of test set: 0.5816238522529602 at epoch: 0 and batch_num: 116\n",
      "Loss of test set: 0.5496928691864014 at epoch: 0 and batch_num: 117\n",
      "Loss of test set: 0.5863807797431946 at epoch: 0 and batch_num: 118\n",
      "Loss of test set: 0.47911757230758667 at epoch: 0 and batch_num: 119\n",
      "Loss of test set: 0.3092300295829773 at epoch: 0 and batch_num: 120\n",
      "Loss of test set: 0.44137662649154663 at epoch: 0 and batch_num: 121\n",
      "Loss of test set: 0.35840851068496704 at epoch: 0 and batch_num: 122\n",
      "Loss of test set: 0.19118110835552216 at epoch: 0 and batch_num: 123\n",
      "Loss of test set: 0.44983601570129395 at epoch: 0 and batch_num: 124\n",
      "Loss of test set: 0.3610242009162903 at epoch: 0 and batch_num: 125\n",
      "Loss of test set: 0.4053635895252228 at epoch: 0 and batch_num: 126\n",
      "Loss of test set: 0.5779447555541992 at epoch: 0 and batch_num: 127\n",
      "Loss of test set: 0.5251818895339966 at epoch: 0 and batch_num: 128\n",
      "Loss of test set: 0.5216408371925354 at epoch: 0 and batch_num: 129\n",
      "Loss of test set: 0.35323870182037354 at epoch: 0 and batch_num: 130\n",
      "Loss of test set: 0.5184644460678101 at epoch: 0 and batch_num: 131\n",
      "Loss of test set: 0.508668065071106 at epoch: 0 and batch_num: 132\n",
      "Loss of test set: 0.5091315507888794 at epoch: 0 and batch_num: 133\n",
      "Loss of test set: 0.30376869440078735 at epoch: 0 and batch_num: 134\n",
      "Loss of test set: 0.2776598334312439 at epoch: 0 and batch_num: 135\n",
      "Loss of test set: 0.3950175642967224 at epoch: 0 and batch_num: 136\n",
      "Loss of test set: 0.5356405377388 at epoch: 0 and batch_num: 137\n",
      "Loss of test set: 0.4256574511528015 at epoch: 0 and batch_num: 138\n",
      "Loss of test set: 0.31738966703414917 at epoch: 0 and batch_num: 139\n",
      "Loss of test set: 0.5767838954925537 at epoch: 0 and batch_num: 140\n",
      "Loss of test set: 0.46846869587898254 at epoch: 0 and batch_num: 141\n",
      "Loss of test set: 0.5550010204315186 at epoch: 0 and batch_num: 142\n",
      "Loss of test set: 0.606869101524353 at epoch: 0 and batch_num: 143\n",
      "Loss of test set: 0.3586437702178955 at epoch: 0 and batch_num: 144\n",
      "Loss of test set: 0.27899789810180664 at epoch: 0 and batch_num: 145\n",
      "Loss of test set: 0.26411303877830505 at epoch: 0 and batch_num: 146\n",
      "Loss of test set: 0.31465524435043335 at epoch: 0 and batch_num: 147\n",
      "Loss of test set: 0.42166972160339355 at epoch: 0 and batch_num: 148\n",
      "Loss of test set: 0.4628080129623413 at epoch: 0 and batch_num: 149\n",
      "Loss of test set: 0.48672735691070557 at epoch: 0 and batch_num: 150\n",
      "Loss of test set: 0.466766893863678 at epoch: 0 and batch_num: 151\n",
      "Loss of test set: 0.6051924228668213 at epoch: 0 and batch_num: 152\n",
      "Loss of test set: 0.4829500913619995 at epoch: 0 and batch_num: 153\n",
      "Loss of test set: 0.2980596423149109 at epoch: 0 and batch_num: 154\n",
      "Loss of test set: 0.5955818891525269 at epoch: 0 and batch_num: 155\n",
      "Loss of test set: 0.5256291627883911 at epoch: 0 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8413\n",
      "Loss of train set: 0.2792755663394928 at epoch: 1 and batch_num: 0\n",
      "Loss of train set: 0.6416999697685242 at epoch: 1 and batch_num: 1\n",
      "Loss of train set: 0.3816365599632263 at epoch: 1 and batch_num: 2\n",
      "Loss of train set: 0.4540625810623169 at epoch: 1 and batch_num: 3\n",
      "Loss of train set: 0.3056681156158447 at epoch: 1 and batch_num: 4\n",
      "Loss of train set: 0.3978448808193207 at epoch: 1 and batch_num: 5\n",
      "Loss of train set: 0.35825228691101074 at epoch: 1 and batch_num: 6\n",
      "Loss of train set: 0.21998156607151031 at epoch: 1 and batch_num: 7\n",
      "Loss of train set: 0.30274778604507446 at epoch: 1 and batch_num: 8\n",
      "Loss of train set: 0.39452487230300903 at epoch: 1 and batch_num: 9\n",
      "Loss of train set: 0.47129392623901367 at epoch: 1 and batch_num: 10\n",
      "Loss of train set: 0.5146673321723938 at epoch: 1 and batch_num: 11\n",
      "Loss of train set: 0.3383498191833496 at epoch: 1 and batch_num: 12\n",
      "Loss of train set: 0.38433337211608887 at epoch: 1 and batch_num: 13\n",
      "Loss of train set: 0.2914150357246399 at epoch: 1 and batch_num: 14\n",
      "Loss of train set: 0.3504028916358948 at epoch: 1 and batch_num: 15\n",
      "Loss of train set: 0.3880220949649811 at epoch: 1 and batch_num: 16\n",
      "Loss of train set: 0.3694334924221039 at epoch: 1 and batch_num: 17\n",
      "Loss of train set: 0.558617115020752 at epoch: 1 and batch_num: 18\n",
      "Loss of train set: 0.34954673051834106 at epoch: 1 and batch_num: 19\n",
      "Loss of train set: 0.3594408333301544 at epoch: 1 and batch_num: 20\n",
      "Loss of train set: 0.38211628794670105 at epoch: 1 and batch_num: 21\n",
      "Loss of train set: 0.3953229784965515 at epoch: 1 and batch_num: 22\n",
      "Loss of train set: 0.27452898025512695 at epoch: 1 and batch_num: 23\n",
      "Loss of train set: 0.2493005096912384 at epoch: 1 and batch_num: 24\n",
      "Loss of train set: 0.3088451027870178 at epoch: 1 and batch_num: 25\n",
      "Loss of train set: 0.2769579589366913 at epoch: 1 and batch_num: 26\n",
      "Loss of train set: 0.6910660266876221 at epoch: 1 and batch_num: 27\n",
      "Loss of train set: 0.32029688358306885 at epoch: 1 and batch_num: 28\n",
      "Loss of train set: 0.21688160300254822 at epoch: 1 and batch_num: 29\n",
      "Loss of train set: 0.3798910975456238 at epoch: 1 and batch_num: 30\n",
      "Loss of train set: 0.35553455352783203 at epoch: 1 and batch_num: 31\n",
      "Loss of train set: 0.24937057495117188 at epoch: 1 and batch_num: 32\n",
      "Loss of train set: 0.3385224938392639 at epoch: 1 and batch_num: 33\n",
      "Loss of train set: 0.4094354212284088 at epoch: 1 and batch_num: 34\n",
      "Loss of train set: 0.3390114903450012 at epoch: 1 and batch_num: 35\n",
      "Loss of train set: 0.30067020654678345 at epoch: 1 and batch_num: 36\n",
      "Loss of train set: 0.52110755443573 at epoch: 1 and batch_num: 37\n",
      "Loss of train set: 0.28406697511672974 at epoch: 1 and batch_num: 38\n",
      "Loss of train set: 0.38619568943977356 at epoch: 1 and batch_num: 39\n",
      "Loss of train set: 0.40361908078193665 at epoch: 1 and batch_num: 40\n",
      "Loss of train set: 0.3147686719894409 at epoch: 1 and batch_num: 41\n",
      "Loss of train set: 0.29826557636260986 at epoch: 1 and batch_num: 42\n",
      "Loss of train set: 0.35978254675865173 at epoch: 1 and batch_num: 43\n",
      "Loss of train set: 0.31888845562934875 at epoch: 1 and batch_num: 44\n",
      "Loss of train set: 0.28752297163009644 at epoch: 1 and batch_num: 45\n",
      "Loss of train set: 0.3703169822692871 at epoch: 1 and batch_num: 46\n",
      "Loss of train set: 0.30173274874687195 at epoch: 1 and batch_num: 47\n",
      "Loss of train set: 0.5977818965911865 at epoch: 1 and batch_num: 48\n",
      "Loss of train set: 0.3833174705505371 at epoch: 1 and batch_num: 49\n",
      "Loss of train set: 0.4957948923110962 at epoch: 1 and batch_num: 50\n",
      "Loss of train set: 0.35452568531036377 at epoch: 1 and batch_num: 51\n",
      "Loss of train set: 0.44171538949012756 at epoch: 1 and batch_num: 52\n",
      "Loss of train set: 0.287313848733902 at epoch: 1 and batch_num: 53\n",
      "Loss of train set: 0.4777615964412689 at epoch: 1 and batch_num: 54\n",
      "Loss of train set: 0.19066306948661804 at epoch: 1 and batch_num: 55\n",
      "Loss of train set: 0.421883225440979 at epoch: 1 and batch_num: 56\n",
      "Loss of train set: 0.3981786072254181 at epoch: 1 and batch_num: 57\n",
      "Loss of train set: 0.36196041107177734 at epoch: 1 and batch_num: 58\n",
      "Loss of train set: 0.3379606008529663 at epoch: 1 and batch_num: 59\n",
      "Loss of train set: 0.3547838628292084 at epoch: 1 and batch_num: 60\n",
      "Loss of train set: 0.545823335647583 at epoch: 1 and batch_num: 61\n",
      "Loss of train set: 0.4770868718624115 at epoch: 1 and batch_num: 62\n",
      "Loss of train set: 0.42113029956817627 at epoch: 1 and batch_num: 63\n",
      "Loss of train set: 0.5561779141426086 at epoch: 1 and batch_num: 64\n",
      "Loss of train set: 0.4963975250720978 at epoch: 1 and batch_num: 65\n",
      "Loss of train set: 0.5432567596435547 at epoch: 1 and batch_num: 66\n",
      "Loss of train set: 0.2332933396100998 at epoch: 1 and batch_num: 67\n",
      "Loss of train set: 0.4590083360671997 at epoch: 1 and batch_num: 68\n",
      "Loss of train set: 0.3814719319343567 at epoch: 1 and batch_num: 69\n",
      "Loss of train set: 0.3836868405342102 at epoch: 1 and batch_num: 70\n",
      "Loss of train set: 0.3336203396320343 at epoch: 1 and batch_num: 71\n",
      "Loss of train set: 0.4340076148509979 at epoch: 1 and batch_num: 72\n",
      "Loss of train set: 0.2796313166618347 at epoch: 1 and batch_num: 73\n",
      "Loss of train set: 0.32256874442100525 at epoch: 1 and batch_num: 74\n",
      "Loss of train set: 0.35515081882476807 at epoch: 1 and batch_num: 75\n",
      "Loss of train set: 0.4194399118423462 at epoch: 1 and batch_num: 76\n",
      "Loss of train set: 0.31469085812568665 at epoch: 1 and batch_num: 77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.350083589553833 at epoch: 1 and batch_num: 78\n",
      "Loss of train set: 0.47792738676071167 at epoch: 1 and batch_num: 79\n",
      "Loss of train set: 0.3940805196762085 at epoch: 1 and batch_num: 80\n",
      "Loss of train set: 0.29568904638290405 at epoch: 1 and batch_num: 81\n",
      "Loss of train set: 0.45258066058158875 at epoch: 1 and batch_num: 82\n",
      "Loss of train set: 0.2434501051902771 at epoch: 1 and batch_num: 83\n",
      "Loss of train set: 0.41878658533096313 at epoch: 1 and batch_num: 84\n",
      "Loss of train set: 0.3370389938354492 at epoch: 1 and batch_num: 85\n",
      "Loss of train set: 0.36273080110549927 at epoch: 1 and batch_num: 86\n",
      "Loss of train set: 0.4451390504837036 at epoch: 1 and batch_num: 87\n",
      "Loss of train set: 0.42934465408325195 at epoch: 1 and batch_num: 88\n",
      "Loss of train set: 0.46310022473335266 at epoch: 1 and batch_num: 89\n",
      "Loss of train set: 0.31179800629615784 at epoch: 1 and batch_num: 90\n",
      "Loss of train set: 0.36664095520973206 at epoch: 1 and batch_num: 91\n",
      "Loss of train set: 0.3133951425552368 at epoch: 1 and batch_num: 92\n",
      "Loss of train set: 0.3838455080986023 at epoch: 1 and batch_num: 93\n",
      "Loss of train set: 0.38083088397979736 at epoch: 1 and batch_num: 94\n",
      "Loss of train set: 0.3391270041465759 at epoch: 1 and batch_num: 95\n",
      "Loss of train set: 0.41080743074417114 at epoch: 1 and batch_num: 96\n",
      "Loss of train set: 0.21677270531654358 at epoch: 1 and batch_num: 97\n",
      "Loss of train set: 0.39162617921829224 at epoch: 1 and batch_num: 98\n",
      "Loss of train set: 0.27093207836151123 at epoch: 1 and batch_num: 99\n",
      "Loss of train set: 0.449342280626297 at epoch: 1 and batch_num: 100\n",
      "Loss of train set: 0.21213094890117645 at epoch: 1 and batch_num: 101\n",
      "Loss of train set: 0.2499077022075653 at epoch: 1 and batch_num: 102\n",
      "Loss of train set: 0.2603543996810913 at epoch: 1 and batch_num: 103\n",
      "Loss of train set: 0.3668738007545471 at epoch: 1 and batch_num: 104\n",
      "Loss of train set: 0.23117557168006897 at epoch: 1 and batch_num: 105\n",
      "Loss of train set: 0.37552475929260254 at epoch: 1 and batch_num: 106\n",
      "Loss of train set: 0.27834582328796387 at epoch: 1 and batch_num: 107\n",
      "Loss of train set: 0.34054553508758545 at epoch: 1 and batch_num: 108\n",
      "Loss of train set: 0.36167338490486145 at epoch: 1 and batch_num: 109\n",
      "Loss of train set: 0.3918236792087555 at epoch: 1 and batch_num: 110\n",
      "Loss of train set: 0.4437984824180603 at epoch: 1 and batch_num: 111\n",
      "Loss of train set: 0.49782031774520874 at epoch: 1 and batch_num: 112\n",
      "Loss of train set: 0.4985145926475525 at epoch: 1 and batch_num: 113\n",
      "Loss of train set: 0.46431392431259155 at epoch: 1 and batch_num: 114\n",
      "Loss of train set: 0.46699243783950806 at epoch: 1 and batch_num: 115\n",
      "Loss of train set: 0.5042153596878052 at epoch: 1 and batch_num: 116\n",
      "Loss of train set: 0.2758702337741852 at epoch: 1 and batch_num: 117\n",
      "Loss of train set: 0.2585151195526123 at epoch: 1 and batch_num: 118\n",
      "Loss of train set: 0.5381597280502319 at epoch: 1 and batch_num: 119\n",
      "Loss of train set: 0.4874541759490967 at epoch: 1 and batch_num: 120\n",
      "Loss of train set: 0.5796557664871216 at epoch: 1 and batch_num: 121\n",
      "Loss of train set: 0.45005008578300476 at epoch: 1 and batch_num: 122\n",
      "Loss of train set: 0.4105513393878937 at epoch: 1 and batch_num: 123\n",
      "Loss of train set: 0.3986988663673401 at epoch: 1 and batch_num: 124\n",
      "Loss of train set: 0.2729954719543457 at epoch: 1 and batch_num: 125\n",
      "Loss of train set: 0.353788286447525 at epoch: 1 and batch_num: 126\n",
      "Loss of train set: 0.5136263370513916 at epoch: 1 and batch_num: 127\n",
      "Loss of train set: 0.3077967166900635 at epoch: 1 and batch_num: 128\n",
      "Loss of train set: 0.33539673686027527 at epoch: 1 and batch_num: 129\n",
      "Loss of train set: 0.2919900417327881 at epoch: 1 and batch_num: 130\n",
      "Loss of train set: 0.49895885586738586 at epoch: 1 and batch_num: 131\n",
      "Loss of train set: 0.3290301561355591 at epoch: 1 and batch_num: 132\n",
      "Loss of train set: 0.38396990299224854 at epoch: 1 and batch_num: 133\n",
      "Loss of train set: 0.3260679244995117 at epoch: 1 and batch_num: 134\n",
      "Loss of train set: 0.36723604798316956 at epoch: 1 and batch_num: 135\n",
      "Loss of train set: 0.18537132441997528 at epoch: 1 and batch_num: 136\n",
      "Loss of train set: 0.31155115365982056 at epoch: 1 and batch_num: 137\n",
      "Loss of train set: 0.36069533228874207 at epoch: 1 and batch_num: 138\n",
      "Loss of train set: 0.38633137941360474 at epoch: 1 and batch_num: 139\n",
      "Loss of train set: 0.504357099533081 at epoch: 1 and batch_num: 140\n",
      "Loss of train set: 0.37901026010513306 at epoch: 1 and batch_num: 141\n",
      "Loss of train set: 0.4259384274482727 at epoch: 1 and batch_num: 142\n",
      "Loss of train set: 0.3309115171432495 at epoch: 1 and batch_num: 143\n",
      "Loss of train set: 0.2864701449871063 at epoch: 1 and batch_num: 144\n",
      "Loss of train set: 0.25796326994895935 at epoch: 1 and batch_num: 145\n",
      "Loss of train set: 0.207350492477417 at epoch: 1 and batch_num: 146\n",
      "Loss of train set: 0.452470064163208 at epoch: 1 and batch_num: 147\n",
      "Loss of train set: 0.38782352209091187 at epoch: 1 and batch_num: 148\n",
      "Loss of train set: 0.3956855237483978 at epoch: 1 and batch_num: 149\n",
      "Loss of train set: 0.31316524744033813 at epoch: 1 and batch_num: 150\n",
      "Loss of train set: 0.33767756819725037 at epoch: 1 and batch_num: 151\n",
      "Loss of train set: 0.36096128821372986 at epoch: 1 and batch_num: 152\n",
      "Loss of train set: 0.3742659389972687 at epoch: 1 and batch_num: 153\n",
      "Loss of train set: 0.4682820439338684 at epoch: 1 and batch_num: 154\n",
      "Loss of train set: 0.2883644104003906 at epoch: 1 and batch_num: 155\n",
      "Loss of train set: 0.35363274812698364 at epoch: 1 and batch_num: 156\n",
      "Loss of train set: 0.36805444955825806 at epoch: 1 and batch_num: 157\n",
      "Loss of train set: 0.3834497928619385 at epoch: 1 and batch_num: 158\n",
      "Loss of train set: 0.4126709997653961 at epoch: 1 and batch_num: 159\n",
      "Loss of train set: 0.3519582450389862 at epoch: 1 and batch_num: 160\n",
      "Loss of train set: 0.3017997443675995 at epoch: 1 and batch_num: 161\n",
      "Loss of train set: 0.27170053124427795 at epoch: 1 and batch_num: 162\n",
      "Loss of train set: 0.4260037839412689 at epoch: 1 and batch_num: 163\n",
      "Loss of train set: 0.34498244524002075 at epoch: 1 and batch_num: 164\n",
      "Loss of train set: 0.2577879726886749 at epoch: 1 and batch_num: 165\n",
      "Loss of train set: 0.3159295320510864 at epoch: 1 and batch_num: 166\n",
      "Loss of train set: 0.3161778450012207 at epoch: 1 and batch_num: 167\n",
      "Loss of train set: 0.46325159072875977 at epoch: 1 and batch_num: 168\n",
      "Loss of train set: 0.2957819402217865 at epoch: 1 and batch_num: 169\n",
      "Loss of train set: 0.3202511668205261 at epoch: 1 and batch_num: 170\n",
      "Loss of train set: 0.38773059844970703 at epoch: 1 and batch_num: 171\n",
      "Loss of train set: 0.3067929148674011 at epoch: 1 and batch_num: 172\n",
      "Loss of train set: 0.35868796706199646 at epoch: 1 and batch_num: 173\n",
      "Loss of train set: 0.23895525932312012 at epoch: 1 and batch_num: 174\n",
      "Loss of train set: 0.23122847080230713 at epoch: 1 and batch_num: 175\n",
      "Loss of train set: 0.3709346055984497 at epoch: 1 and batch_num: 176\n",
      "Loss of train set: 0.2903812527656555 at epoch: 1 and batch_num: 177\n",
      "Loss of train set: 0.2913745939731598 at epoch: 1 and batch_num: 178\n",
      "Loss of train set: 0.4367009401321411 at epoch: 1 and batch_num: 179\n",
      "Loss of train set: 0.6550650596618652 at epoch: 1 and batch_num: 180\n",
      "Loss of train set: 0.39080971479415894 at epoch: 1 and batch_num: 181\n",
      "Loss of train set: 0.36698418855667114 at epoch: 1 and batch_num: 182\n",
      "Loss of train set: 0.5836842060089111 at epoch: 1 and batch_num: 183\n",
      "Loss of train set: 0.28451600670814514 at epoch: 1 and batch_num: 184\n",
      "Loss of train set: 0.2994306683540344 at epoch: 1 and batch_num: 185\n",
      "Loss of train set: 0.5254442691802979 at epoch: 1 and batch_num: 186\n",
      "Loss of train set: 0.3388890027999878 at epoch: 1 and batch_num: 187\n",
      "Loss of train set: 0.5456627011299133 at epoch: 1 and batch_num: 188\n",
      "Loss of train set: 0.38116639852523804 at epoch: 1 and batch_num: 189\n",
      "Loss of train set: 0.28390488028526306 at epoch: 1 and batch_num: 190\n",
      "Loss of train set: 0.17286115884780884 at epoch: 1 and batch_num: 191\n",
      "Loss of train set: 0.44991356134414673 at epoch: 1 and batch_num: 192\n",
      "Loss of train set: 0.39343732595443726 at epoch: 1 and batch_num: 193\n",
      "Loss of train set: 0.32916244864463806 at epoch: 1 and batch_num: 194\n",
      "Loss of train set: 0.39468294382095337 at epoch: 1 and batch_num: 195\n",
      "Loss of train set: 0.30728647112846375 at epoch: 1 and batch_num: 196\n",
      "Loss of train set: 0.3232675790786743 at epoch: 1 and batch_num: 197\n",
      "Loss of train set: 0.5477184057235718 at epoch: 1 and batch_num: 198\n",
      "Loss of train set: 0.5089808106422424 at epoch: 1 and batch_num: 199\n",
      "Loss of train set: 0.4552633464336395 at epoch: 1 and batch_num: 200\n",
      "Loss of train set: 0.41064590215682983 at epoch: 1 and batch_num: 201\n",
      "Loss of train set: 0.3631555438041687 at epoch: 1 and batch_num: 202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2194800078868866 at epoch: 1 and batch_num: 203\n",
      "Loss of train set: 0.3928673267364502 at epoch: 1 and batch_num: 204\n",
      "Loss of train set: 0.41501450538635254 at epoch: 1 and batch_num: 205\n",
      "Loss of train set: 0.2601366639137268 at epoch: 1 and batch_num: 206\n",
      "Loss of train set: 0.4095039963722229 at epoch: 1 and batch_num: 207\n",
      "Loss of train set: 0.40781229734420776 at epoch: 1 and batch_num: 208\n",
      "Loss of train set: 0.3235400915145874 at epoch: 1 and batch_num: 209\n",
      "Loss of train set: 0.40283650159835815 at epoch: 1 and batch_num: 210\n",
      "Loss of train set: 0.28695571422576904 at epoch: 1 and batch_num: 211\n",
      "Loss of train set: 0.3593689203262329 at epoch: 1 and batch_num: 212\n",
      "Loss of train set: 0.37625330686569214 at epoch: 1 and batch_num: 213\n",
      "Loss of train set: 0.35065963864326477 at epoch: 1 and batch_num: 214\n",
      "Loss of train set: 0.26796969771385193 at epoch: 1 and batch_num: 215\n",
      "Loss of train set: 0.4872586727142334 at epoch: 1 and batch_num: 216\n",
      "Loss of train set: 0.33476120233535767 at epoch: 1 and batch_num: 217\n",
      "Loss of train set: 0.3395133316516876 at epoch: 1 and batch_num: 218\n",
      "Loss of train set: 0.43633294105529785 at epoch: 1 and batch_num: 219\n",
      "Loss of train set: 0.5268245339393616 at epoch: 1 and batch_num: 220\n",
      "Loss of train set: 0.23069950938224792 at epoch: 1 and batch_num: 221\n",
      "Loss of train set: 0.2887301445007324 at epoch: 1 and batch_num: 222\n",
      "Loss of train set: 0.27042049169540405 at epoch: 1 and batch_num: 223\n",
      "Loss of train set: 0.27842891216278076 at epoch: 1 and batch_num: 224\n",
      "Loss of train set: 0.40926262736320496 at epoch: 1 and batch_num: 225\n",
      "Loss of train set: 0.32541894912719727 at epoch: 1 and batch_num: 226\n",
      "Loss of train set: 0.5635918378829956 at epoch: 1 and batch_num: 227\n",
      "Loss of train set: 0.3388059139251709 at epoch: 1 and batch_num: 228\n",
      "Loss of train set: 0.24283775687217712 at epoch: 1 and batch_num: 229\n",
      "Loss of train set: 0.4108462929725647 at epoch: 1 and batch_num: 230\n",
      "Loss of train set: 0.4715842008590698 at epoch: 1 and batch_num: 231\n",
      "Loss of train set: 0.30835264921188354 at epoch: 1 and batch_num: 232\n",
      "Loss of train set: 0.40969550609588623 at epoch: 1 and batch_num: 233\n",
      "Loss of train set: 0.3637368083000183 at epoch: 1 and batch_num: 234\n",
      "Loss of train set: 0.3533132076263428 at epoch: 1 and batch_num: 235\n",
      "Loss of train set: 0.6213918924331665 at epoch: 1 and batch_num: 236\n",
      "Loss of train set: 0.3230472803115845 at epoch: 1 and batch_num: 237\n",
      "Loss of train set: 0.3146251142024994 at epoch: 1 and batch_num: 238\n",
      "Loss of train set: 0.5242040157318115 at epoch: 1 and batch_num: 239\n",
      "Loss of train set: 0.4752264618873596 at epoch: 1 and batch_num: 240\n",
      "Loss of train set: 0.4228796064853668 at epoch: 1 and batch_num: 241\n",
      "Loss of train set: 0.3751518428325653 at epoch: 1 and batch_num: 242\n",
      "Loss of train set: 0.2864363491535187 at epoch: 1 and batch_num: 243\n",
      "Loss of train set: 0.18838384747505188 at epoch: 1 and batch_num: 244\n",
      "Loss of train set: 0.4244031310081482 at epoch: 1 and batch_num: 245\n",
      "Loss of train set: 0.36141735315322876 at epoch: 1 and batch_num: 246\n",
      "Loss of train set: 0.3034129738807678 at epoch: 1 and batch_num: 247\n",
      "Loss of train set: 0.44913554191589355 at epoch: 1 and batch_num: 248\n",
      "Loss of train set: 0.4682905673980713 at epoch: 1 and batch_num: 249\n",
      "Loss of train set: 0.21344152092933655 at epoch: 1 and batch_num: 250\n",
      "Loss of train set: 0.378741055727005 at epoch: 1 and batch_num: 251\n",
      "Loss of train set: 0.3276063799858093 at epoch: 1 and batch_num: 252\n",
      "Loss of train set: 0.41656962037086487 at epoch: 1 and batch_num: 253\n",
      "Loss of train set: 0.45900508761405945 at epoch: 1 and batch_num: 254\n",
      "Loss of train set: 0.21939772367477417 at epoch: 1 and batch_num: 255\n",
      "Loss of train set: 0.4148319959640503 at epoch: 1 and batch_num: 256\n",
      "Loss of train set: 0.4262179136276245 at epoch: 1 and batch_num: 257\n",
      "Loss of train set: 0.2759401202201843 at epoch: 1 and batch_num: 258\n",
      "Loss of train set: 0.5343911647796631 at epoch: 1 and batch_num: 259\n",
      "Loss of train set: 0.31980353593826294 at epoch: 1 and batch_num: 260\n",
      "Loss of train set: 0.3554646074771881 at epoch: 1 and batch_num: 261\n",
      "Loss of train set: 0.34284496307373047 at epoch: 1 and batch_num: 262\n",
      "Loss of train set: 0.47119978070259094 at epoch: 1 and batch_num: 263\n",
      "Loss of train set: 0.34531575441360474 at epoch: 1 and batch_num: 264\n",
      "Loss of train set: 0.3859279155731201 at epoch: 1 and batch_num: 265\n",
      "Loss of train set: 0.3680810034275055 at epoch: 1 and batch_num: 266\n",
      "Loss of train set: 0.580478310585022 at epoch: 1 and batch_num: 267\n",
      "Loss of train set: 0.49637025594711304 at epoch: 1 and batch_num: 268\n",
      "Loss of train set: 0.42282843589782715 at epoch: 1 and batch_num: 269\n",
      "Loss of train set: 0.3342663049697876 at epoch: 1 and batch_num: 270\n",
      "Loss of train set: 0.28002142906188965 at epoch: 1 and batch_num: 271\n",
      "Loss of train set: 0.39071038365364075 at epoch: 1 and batch_num: 272\n",
      "Loss of train set: 0.458025723695755 at epoch: 1 and batch_num: 273\n",
      "Loss of train set: 0.37784430384635925 at epoch: 1 and batch_num: 274\n",
      "Loss of train set: 0.3487563133239746 at epoch: 1 and batch_num: 275\n",
      "Loss of train set: 0.2253686487674713 at epoch: 1 and batch_num: 276\n",
      "Loss of train set: 0.5370169878005981 at epoch: 1 and batch_num: 277\n",
      "Loss of train set: 0.4885692894458771 at epoch: 1 and batch_num: 278\n",
      "Loss of train set: 0.3834676146507263 at epoch: 1 and batch_num: 279\n",
      "Loss of train set: 0.3940059542655945 at epoch: 1 and batch_num: 280\n",
      "Loss of train set: 0.3909568190574646 at epoch: 1 and batch_num: 281\n",
      "Loss of train set: 0.3116469383239746 at epoch: 1 and batch_num: 282\n",
      "Loss of train set: 0.40721917152404785 at epoch: 1 and batch_num: 283\n",
      "Loss of train set: 0.252962589263916 at epoch: 1 and batch_num: 284\n",
      "Loss of train set: 0.3204289674758911 at epoch: 1 and batch_num: 285\n",
      "Loss of train set: 0.35070013999938965 at epoch: 1 and batch_num: 286\n",
      "Loss of train set: 0.34750497341156006 at epoch: 1 and batch_num: 287\n",
      "Loss of train set: 0.33337172865867615 at epoch: 1 and batch_num: 288\n",
      "Loss of train set: 0.31980714201927185 at epoch: 1 and batch_num: 289\n",
      "Loss of train set: 0.3649396598339081 at epoch: 1 and batch_num: 290\n",
      "Loss of train set: 0.36075589060783386 at epoch: 1 and batch_num: 291\n",
      "Loss of train set: 0.3830550014972687 at epoch: 1 and batch_num: 292\n",
      "Loss of train set: 0.385661244392395 at epoch: 1 and batch_num: 293\n",
      "Loss of train set: 0.31964221596717834 at epoch: 1 and batch_num: 294\n",
      "Loss of train set: 0.41392678022384644 at epoch: 1 and batch_num: 295\n",
      "Loss of train set: 0.3996484875679016 at epoch: 1 and batch_num: 296\n",
      "Loss of train set: 0.44071173667907715 at epoch: 1 and batch_num: 297\n",
      "Loss of train set: 0.419025719165802 at epoch: 1 and batch_num: 298\n",
      "Loss of train set: 0.4106683135032654 at epoch: 1 and batch_num: 299\n",
      "Loss of train set: 0.5156459808349609 at epoch: 1 and batch_num: 300\n",
      "Loss of train set: 0.27243393659591675 at epoch: 1 and batch_num: 301\n",
      "Loss of train set: 0.3322361707687378 at epoch: 1 and batch_num: 302\n",
      "Loss of train set: 0.41161927580833435 at epoch: 1 and batch_num: 303\n",
      "Loss of train set: 0.41092434525489807 at epoch: 1 and batch_num: 304\n",
      "Loss of train set: 0.29290902614593506 at epoch: 1 and batch_num: 305\n",
      "Loss of train set: 0.3304023742675781 at epoch: 1 and batch_num: 306\n",
      "Loss of train set: 0.49050474166870117 at epoch: 1 and batch_num: 307\n",
      "Loss of train set: 0.38845041394233704 at epoch: 1 and batch_num: 308\n",
      "Loss of train set: 0.14784571528434753 at epoch: 1 and batch_num: 309\n",
      "Loss of train set: 0.43573224544525146 at epoch: 1 and batch_num: 310\n",
      "Loss of train set: 0.3097950220108032 at epoch: 1 and batch_num: 311\n",
      "Loss of train set: 0.26573190093040466 at epoch: 1 and batch_num: 312\n",
      "Loss of train set: 0.263681024312973 at epoch: 1 and batch_num: 313\n",
      "Loss of train set: 0.18543976545333862 at epoch: 1 and batch_num: 314\n",
      "Loss of train set: 0.5125550031661987 at epoch: 1 and batch_num: 315\n",
      "Loss of train set: 0.3113386631011963 at epoch: 1 and batch_num: 316\n",
      "Loss of train set: 0.2947543263435364 at epoch: 1 and batch_num: 317\n",
      "Loss of train set: 0.40902867913246155 at epoch: 1 and batch_num: 318\n",
      "Loss of train set: 0.44047099351882935 at epoch: 1 and batch_num: 319\n",
      "Loss of train set: 0.4271860122680664 at epoch: 1 and batch_num: 320\n",
      "Loss of train set: 0.6148543357849121 at epoch: 1 and batch_num: 321\n",
      "Loss of train set: 0.24625040590763092 at epoch: 1 and batch_num: 322\n",
      "Loss of train set: 0.3295210599899292 at epoch: 1 and batch_num: 323\n",
      "Loss of train set: 0.40974855422973633 at epoch: 1 and batch_num: 324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2844749689102173 at epoch: 1 and batch_num: 325\n",
      "Loss of train set: 0.5336766242980957 at epoch: 1 and batch_num: 326\n",
      "Loss of train set: 0.36674216389656067 at epoch: 1 and batch_num: 327\n",
      "Loss of train set: 0.6216031908988953 at epoch: 1 and batch_num: 328\n",
      "Loss of train set: 0.4845908284187317 at epoch: 1 and batch_num: 329\n",
      "Loss of train set: 0.36342427134513855 at epoch: 1 and batch_num: 330\n",
      "Loss of train set: 0.5148757100105286 at epoch: 1 and batch_num: 331\n",
      "Loss of train set: 0.2349933534860611 at epoch: 1 and batch_num: 332\n",
      "Loss of train set: 0.23643049597740173 at epoch: 1 and batch_num: 333\n",
      "Loss of train set: 0.35072797536849976 at epoch: 1 and batch_num: 334\n",
      "Loss of train set: 0.4258663058280945 at epoch: 1 and batch_num: 335\n",
      "Loss of train set: 0.3106231391429901 at epoch: 1 and batch_num: 336\n",
      "Loss of train set: 0.3365819454193115 at epoch: 1 and batch_num: 337\n",
      "Loss of train set: 0.23307140171527863 at epoch: 1 and batch_num: 338\n",
      "Loss of train set: 0.3333017826080322 at epoch: 1 and batch_num: 339\n",
      "Loss of train set: 0.4331457018852234 at epoch: 1 and batch_num: 340\n",
      "Loss of train set: 0.5168668031692505 at epoch: 1 and batch_num: 341\n",
      "Loss of train set: 0.28014764189720154 at epoch: 1 and batch_num: 342\n",
      "Loss of train set: 0.42942720651626587 at epoch: 1 and batch_num: 343\n",
      "Loss of train set: 0.38363197445869446 at epoch: 1 and batch_num: 344\n",
      "Loss of train set: 0.42037880420684814 at epoch: 1 and batch_num: 345\n",
      "Loss of train set: 0.38027188181877136 at epoch: 1 and batch_num: 346\n",
      "Loss of train set: 0.5008375644683838 at epoch: 1 and batch_num: 347\n",
      "Loss of train set: 0.3176427185535431 at epoch: 1 and batch_num: 348\n",
      "Loss of train set: 0.31431126594543457 at epoch: 1 and batch_num: 349\n",
      "Loss of train set: 0.37732189893722534 at epoch: 1 and batch_num: 350\n",
      "Loss of train set: 0.24708619713783264 at epoch: 1 and batch_num: 351\n",
      "Loss of train set: 0.2678467929363251 at epoch: 1 and batch_num: 352\n",
      "Loss of train set: 0.48529741168022156 at epoch: 1 and batch_num: 353\n",
      "Loss of train set: 0.3315281867980957 at epoch: 1 and batch_num: 354\n",
      "Loss of train set: 0.33914828300476074 at epoch: 1 and batch_num: 355\n",
      "Loss of train set: 0.3599487245082855 at epoch: 1 and batch_num: 356\n",
      "Loss of train set: 0.3332480788230896 at epoch: 1 and batch_num: 357\n",
      "Loss of train set: 0.32078316807746887 at epoch: 1 and batch_num: 358\n",
      "Loss of train set: 0.4496356248855591 at epoch: 1 and batch_num: 359\n",
      "Loss of train set: 0.34414464235305786 at epoch: 1 and batch_num: 360\n",
      "Loss of train set: 0.3990514576435089 at epoch: 1 and batch_num: 361\n",
      "Loss of train set: 0.3632258176803589 at epoch: 1 and batch_num: 362\n",
      "Loss of train set: 0.3252713084220886 at epoch: 1 and batch_num: 363\n",
      "Loss of train set: 0.3124784231185913 at epoch: 1 and batch_num: 364\n",
      "Loss of train set: 0.3127061128616333 at epoch: 1 and batch_num: 365\n",
      "Loss of train set: 0.47704797983169556 at epoch: 1 and batch_num: 366\n",
      "Loss of train set: 0.44151538610458374 at epoch: 1 and batch_num: 367\n",
      "Loss of train set: 0.32389774918556213 at epoch: 1 and batch_num: 368\n",
      "Loss of train set: 0.3875257670879364 at epoch: 1 and batch_num: 369\n",
      "Loss of train set: 0.42304933071136475 at epoch: 1 and batch_num: 370\n",
      "Loss of train set: 0.45520612597465515 at epoch: 1 and batch_num: 371\n",
      "Loss of train set: 0.2948968708515167 at epoch: 1 and batch_num: 372\n",
      "Loss of train set: 0.3051871657371521 at epoch: 1 and batch_num: 373\n",
      "Loss of train set: 0.4590262770652771 at epoch: 1 and batch_num: 374\n",
      "Loss of train set: 0.26264697313308716 at epoch: 1 and batch_num: 375\n",
      "Loss of train set: 0.23111438751220703 at epoch: 1 and batch_num: 376\n",
      "Loss of train set: 0.36326801776885986 at epoch: 1 and batch_num: 377\n",
      "Loss of train set: 0.3695151209831238 at epoch: 1 and batch_num: 378\n",
      "Loss of train set: 0.3101907968521118 at epoch: 1 and batch_num: 379\n",
      "Loss of train set: 0.2884947657585144 at epoch: 1 and batch_num: 380\n",
      "Loss of train set: 0.4040248990058899 at epoch: 1 and batch_num: 381\n",
      "Loss of train set: 0.5923037528991699 at epoch: 1 and batch_num: 382\n",
      "Loss of train set: 0.565360426902771 at epoch: 1 and batch_num: 383\n",
      "Loss of train set: 0.3569648265838623 at epoch: 1 and batch_num: 384\n",
      "Loss of train set: 0.28802362084388733 at epoch: 1 and batch_num: 385\n",
      "Loss of train set: 0.23724962770938873 at epoch: 1 and batch_num: 386\n",
      "Loss of train set: 0.22712577879428864 at epoch: 1 and batch_num: 387\n",
      "Loss of train set: 0.2941785454750061 at epoch: 1 and batch_num: 388\n",
      "Loss of train set: 0.38968372344970703 at epoch: 1 and batch_num: 389\n",
      "Loss of train set: 0.22983962297439575 at epoch: 1 and batch_num: 390\n",
      "Loss of train set: 0.33300140500068665 at epoch: 1 and batch_num: 391\n",
      "Loss of train set: 0.5043946504592896 at epoch: 1 and batch_num: 392\n",
      "Loss of train set: 0.4033516049385071 at epoch: 1 and batch_num: 393\n",
      "Loss of train set: 0.5095186233520508 at epoch: 1 and batch_num: 394\n",
      "Loss of train set: 0.5603681206703186 at epoch: 1 and batch_num: 395\n",
      "Loss of train set: 0.3189506232738495 at epoch: 1 and batch_num: 396\n",
      "Loss of train set: 0.3686301112174988 at epoch: 1 and batch_num: 397\n",
      "Loss of train set: 0.4114544987678528 at epoch: 1 and batch_num: 398\n",
      "Loss of train set: 0.30699944496154785 at epoch: 1 and batch_num: 399\n",
      "Loss of train set: 0.35410076379776 at epoch: 1 and batch_num: 400\n",
      "Loss of train set: 0.49520477652549744 at epoch: 1 and batch_num: 401\n",
      "Loss of train set: 0.36262810230255127 at epoch: 1 and batch_num: 402\n",
      "Loss of train set: 0.1591378152370453 at epoch: 1 and batch_num: 403\n",
      "Loss of train set: 0.3957361578941345 at epoch: 1 and batch_num: 404\n",
      "Loss of train set: 0.3410748839378357 at epoch: 1 and batch_num: 405\n",
      "Loss of train set: 0.4423207640647888 at epoch: 1 and batch_num: 406\n",
      "Loss of train set: 0.43815910816192627 at epoch: 1 and batch_num: 407\n",
      "Loss of train set: 0.43044817447662354 at epoch: 1 and batch_num: 408\n",
      "Loss of train set: 0.49234771728515625 at epoch: 1 and batch_num: 409\n",
      "Loss of train set: 0.3304702639579773 at epoch: 1 and batch_num: 410\n",
      "Loss of train set: 0.37786179780960083 at epoch: 1 and batch_num: 411\n",
      "Loss of train set: 0.2621605396270752 at epoch: 1 and batch_num: 412\n",
      "Loss of train set: 0.2927648425102234 at epoch: 1 and batch_num: 413\n",
      "Loss of train set: 0.27231326699256897 at epoch: 1 and batch_num: 414\n",
      "Loss of train set: 0.4938945174217224 at epoch: 1 and batch_num: 415\n",
      "Loss of train set: 0.4105154275894165 at epoch: 1 and batch_num: 416\n",
      "Loss of train set: 0.21366214752197266 at epoch: 1 and batch_num: 417\n",
      "Loss of train set: 0.25218361616134644 at epoch: 1 and batch_num: 418\n",
      "Loss of train set: 0.35215380787849426 at epoch: 1 and batch_num: 419\n",
      "Loss of train set: 0.3719158470630646 at epoch: 1 and batch_num: 420\n",
      "Loss of train set: 0.5657545328140259 at epoch: 1 and batch_num: 421\n",
      "Loss of train set: 0.2650606632232666 at epoch: 1 and batch_num: 422\n",
      "Loss of train set: 0.3213995099067688 at epoch: 1 and batch_num: 423\n",
      "Loss of train set: 0.34699931740760803 at epoch: 1 and batch_num: 424\n",
      "Loss of train set: 0.22216778993606567 at epoch: 1 and batch_num: 425\n",
      "Loss of train set: 0.3966561257839203 at epoch: 1 and batch_num: 426\n",
      "Loss of train set: 0.4793057143688202 at epoch: 1 and batch_num: 427\n",
      "Loss of train set: 0.5097469687461853 at epoch: 1 and batch_num: 428\n",
      "Loss of train set: 0.3663620948791504 at epoch: 1 and batch_num: 429\n",
      "Loss of train set: 0.45723533630371094 at epoch: 1 and batch_num: 430\n",
      "Loss of train set: 0.2760331630706787 at epoch: 1 and batch_num: 431\n",
      "Loss of train set: 0.30476438999176025 at epoch: 1 and batch_num: 432\n",
      "Loss of train set: 0.46562692523002625 at epoch: 1 and batch_num: 433\n",
      "Loss of train set: 0.28590208292007446 at epoch: 1 and batch_num: 434\n",
      "Loss of train set: 0.44769424200057983 at epoch: 1 and batch_num: 435\n",
      "Loss of train set: 0.3498914837837219 at epoch: 1 and batch_num: 436\n",
      "Loss of train set: 0.415481299161911 at epoch: 1 and batch_num: 437\n",
      "Loss of train set: 0.48490867018699646 at epoch: 1 and batch_num: 438\n",
      "Loss of train set: 0.2958504557609558 at epoch: 1 and batch_num: 439\n",
      "Loss of train set: 0.3554418683052063 at epoch: 1 and batch_num: 440\n",
      "Loss of train set: 0.5556488633155823 at epoch: 1 and batch_num: 441\n",
      "Loss of train set: 0.516269326210022 at epoch: 1 and batch_num: 442\n",
      "Loss of train set: 0.36311614513397217 at epoch: 1 and batch_num: 443\n",
      "Loss of train set: 0.3786395490169525 at epoch: 1 and batch_num: 444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.29725807905197144 at epoch: 1 and batch_num: 445\n",
      "Loss of train set: 0.27685481309890747 at epoch: 1 and batch_num: 446\n",
      "Loss of train set: 0.3462672531604767 at epoch: 1 and batch_num: 447\n",
      "Loss of train set: 0.3824814260005951 at epoch: 1 and batch_num: 448\n",
      "Loss of train set: 0.23786331713199615 at epoch: 1 and batch_num: 449\n",
      "Loss of train set: 0.3193095326423645 at epoch: 1 and batch_num: 450\n",
      "Loss of train set: 0.25091272592544556 at epoch: 1 and batch_num: 451\n",
      "Loss of train set: 0.43892163038253784 at epoch: 1 and batch_num: 452\n",
      "Loss of train set: 0.34164535999298096 at epoch: 1 and batch_num: 453\n",
      "Loss of train set: 0.4243552088737488 at epoch: 1 and batch_num: 454\n",
      "Loss of train set: 0.22508439421653748 at epoch: 1 and batch_num: 455\n",
      "Loss of train set: 0.20243695378303528 at epoch: 1 and batch_num: 456\n",
      "Loss of train set: 0.4666583240032196 at epoch: 1 and batch_num: 457\n",
      "Loss of train set: 0.19325712323188782 at epoch: 1 and batch_num: 458\n",
      "Loss of train set: 0.4491615891456604 at epoch: 1 and batch_num: 459\n",
      "Loss of train set: 0.4462638795375824 at epoch: 1 and batch_num: 460\n",
      "Loss of train set: 0.4235014021396637 at epoch: 1 and batch_num: 461\n",
      "Loss of train set: 0.36921828985214233 at epoch: 1 and batch_num: 462\n",
      "Loss of train set: 0.2640201151371002 at epoch: 1 and batch_num: 463\n",
      "Loss of train set: 0.26573997735977173 at epoch: 1 and batch_num: 464\n",
      "Loss of train set: 0.4355817437171936 at epoch: 1 and batch_num: 465\n",
      "Loss of train set: 0.2187078446149826 at epoch: 1 and batch_num: 466\n",
      "Loss of train set: 0.3806777894496918 at epoch: 1 and batch_num: 467\n",
      "Loss of train set: 0.30571094155311584 at epoch: 1 and batch_num: 468\n",
      "Loss of train set: 0.3220825791358948 at epoch: 1 and batch_num: 469\n",
      "Loss of train set: 0.33304664492607117 at epoch: 1 and batch_num: 470\n",
      "Loss of train set: 0.3837999701499939 at epoch: 1 and batch_num: 471\n",
      "Loss of train set: 0.2716180682182312 at epoch: 1 and batch_num: 472\n",
      "Loss of train set: 0.35709989070892334 at epoch: 1 and batch_num: 473\n",
      "Loss of train set: 0.3251320719718933 at epoch: 1 and batch_num: 474\n",
      "Loss of train set: 0.39213839173316956 at epoch: 1 and batch_num: 475\n",
      "Loss of train set: 0.39767777919769287 at epoch: 1 and batch_num: 476\n",
      "Loss of train set: 0.3659278154373169 at epoch: 1 and batch_num: 477\n",
      "Loss of train set: 0.37190550565719604 at epoch: 1 and batch_num: 478\n",
      "Loss of train set: 0.3281860053539276 at epoch: 1 and batch_num: 479\n",
      "Loss of train set: 0.36358779668807983 at epoch: 1 and batch_num: 480\n",
      "Loss of train set: 0.21791964769363403 at epoch: 1 and batch_num: 481\n",
      "Loss of train set: 0.37219560146331787 at epoch: 1 and batch_num: 482\n",
      "Loss of train set: 0.5686055421829224 at epoch: 1 and batch_num: 483\n",
      "Loss of train set: 0.39189785718917847 at epoch: 1 and batch_num: 484\n",
      "Loss of train set: 0.37441298365592957 at epoch: 1 and batch_num: 485\n",
      "Loss of train set: 0.3210175633430481 at epoch: 1 and batch_num: 486\n",
      "Loss of train set: 0.4795968532562256 at epoch: 1 and batch_num: 487\n",
      "Loss of train set: 0.28986048698425293 at epoch: 1 and batch_num: 488\n",
      "Loss of train set: 0.38681796193122864 at epoch: 1 and batch_num: 489\n",
      "Loss of train set: 0.28612226247787476 at epoch: 1 and batch_num: 490\n",
      "Loss of train set: 0.4516589045524597 at epoch: 1 and batch_num: 491\n",
      "Loss of train set: 0.43133723735809326 at epoch: 1 and batch_num: 492\n",
      "Loss of train set: 0.3087504506111145 at epoch: 1 and batch_num: 493\n",
      "Loss of train set: 0.34676969051361084 at epoch: 1 and batch_num: 494\n",
      "Loss of train set: 0.40815794467926025 at epoch: 1 and batch_num: 495\n",
      "Loss of train set: 0.39279794692993164 at epoch: 1 and batch_num: 496\n",
      "Loss of train set: 0.23926569521427155 at epoch: 1 and batch_num: 497\n",
      "Loss of train set: 0.33865034580230713 at epoch: 1 and batch_num: 498\n",
      "Loss of train set: 0.3247479498386383 at epoch: 1 and batch_num: 499\n",
      "Loss of train set: 0.5381428599357605 at epoch: 1 and batch_num: 500\n",
      "Loss of train set: 0.42905449867248535 at epoch: 1 and batch_num: 501\n",
      "Loss of train set: 0.5870363712310791 at epoch: 1 and batch_num: 502\n",
      "Loss of train set: 0.29249823093414307 at epoch: 1 and batch_num: 503\n",
      "Loss of train set: 0.41641297936439514 at epoch: 1 and batch_num: 504\n",
      "Loss of train set: 0.2590737044811249 at epoch: 1 and batch_num: 505\n",
      "Loss of train set: 0.4571397304534912 at epoch: 1 and batch_num: 506\n",
      "Loss of train set: 0.27271848917007446 at epoch: 1 and batch_num: 507\n",
      "Loss of train set: 0.3367435038089752 at epoch: 1 and batch_num: 508\n",
      "Loss of train set: 0.43482500314712524 at epoch: 1 and batch_num: 509\n",
      "Loss of train set: 0.4400099515914917 at epoch: 1 and batch_num: 510\n",
      "Loss of train set: 0.3933066129684448 at epoch: 1 and batch_num: 511\n",
      "Loss of train set: 0.4315297603607178 at epoch: 1 and batch_num: 512\n",
      "Loss of train set: 0.378325879573822 at epoch: 1 and batch_num: 513\n",
      "Loss of train set: 0.3540938198566437 at epoch: 1 and batch_num: 514\n",
      "Loss of train set: 0.3744369149208069 at epoch: 1 and batch_num: 515\n",
      "Loss of train set: 0.47519659996032715 at epoch: 1 and batch_num: 516\n",
      "Loss of train set: 0.3893885910511017 at epoch: 1 and batch_num: 517\n",
      "Loss of train set: 0.3881860673427582 at epoch: 1 and batch_num: 518\n",
      "Loss of train set: 0.39120712876319885 at epoch: 1 and batch_num: 519\n",
      "Loss of train set: 0.31163454055786133 at epoch: 1 and batch_num: 520\n",
      "Loss of train set: 0.3213528096675873 at epoch: 1 and batch_num: 521\n",
      "Loss of train set: 0.4673588275909424 at epoch: 1 and batch_num: 522\n",
      "Loss of train set: 0.3241124153137207 at epoch: 1 and batch_num: 523\n",
      "Loss of train set: 0.4757683277130127 at epoch: 1 and batch_num: 524\n",
      "Loss of train set: 0.3326479196548462 at epoch: 1 and batch_num: 525\n",
      "Loss of train set: 0.4197283685207367 at epoch: 1 and batch_num: 526\n",
      "Loss of train set: 0.35931020975112915 at epoch: 1 and batch_num: 527\n",
      "Loss of train set: 0.21232178807258606 at epoch: 1 and batch_num: 528\n",
      "Loss of train set: 0.5352487564086914 at epoch: 1 and batch_num: 529\n",
      "Loss of train set: 0.3973690867424011 at epoch: 1 and batch_num: 530\n",
      "Loss of train set: 0.4066924750804901 at epoch: 1 and batch_num: 531\n",
      "Loss of train set: 0.22478941082954407 at epoch: 1 and batch_num: 532\n",
      "Loss of train set: 0.3359959125518799 at epoch: 1 and batch_num: 533\n",
      "Loss of train set: 0.3820059895515442 at epoch: 1 and batch_num: 534\n",
      "Loss of train set: 0.37505340576171875 at epoch: 1 and batch_num: 535\n",
      "Loss of train set: 0.4050659239292145 at epoch: 1 and batch_num: 536\n",
      "Loss of train set: 0.32189756631851196 at epoch: 1 and batch_num: 537\n",
      "Loss of train set: 0.3537941575050354 at epoch: 1 and batch_num: 538\n",
      "Loss of train set: 0.4186684787273407 at epoch: 1 and batch_num: 539\n",
      "Loss of train set: 0.3573128879070282 at epoch: 1 and batch_num: 540\n",
      "Loss of train set: 0.4416234791278839 at epoch: 1 and batch_num: 541\n",
      "Loss of train set: 0.3558063507080078 at epoch: 1 and batch_num: 542\n",
      "Loss of train set: 0.2952072024345398 at epoch: 1 and batch_num: 543\n",
      "Loss of train set: 0.4258180856704712 at epoch: 1 and batch_num: 544\n",
      "Loss of train set: 0.3283502161502838 at epoch: 1 and batch_num: 545\n",
      "Loss of train set: 0.2429112195968628 at epoch: 1 and batch_num: 546\n",
      "Loss of train set: 0.3785706162452698 at epoch: 1 and batch_num: 547\n",
      "Loss of train set: 0.376960813999176 at epoch: 1 and batch_num: 548\n",
      "Loss of train set: 0.388234406709671 at epoch: 1 and batch_num: 549\n",
      "Loss of train set: 0.35891756415367126 at epoch: 1 and batch_num: 550\n",
      "Loss of train set: 0.44364452362060547 at epoch: 1 and batch_num: 551\n",
      "Loss of train set: 0.4960900843143463 at epoch: 1 and batch_num: 552\n",
      "Loss of train set: 0.25455182790756226 at epoch: 1 and batch_num: 553\n",
      "Loss of train set: 0.3138822615146637 at epoch: 1 and batch_num: 554\n",
      "Loss of train set: 0.4103083610534668 at epoch: 1 and batch_num: 555\n",
      "Loss of train set: 0.2857729196548462 at epoch: 1 and batch_num: 556\n",
      "Loss of train set: 0.5340017676353455 at epoch: 1 and batch_num: 557\n",
      "Loss of train set: 0.265292227268219 at epoch: 1 and batch_num: 558\n",
      "Loss of train set: 0.28225788474082947 at epoch: 1 and batch_num: 559\n",
      "Loss of train set: 0.2940918207168579 at epoch: 1 and batch_num: 560\n",
      "Loss of train set: 0.3844678997993469 at epoch: 1 and batch_num: 561\n",
      "Loss of train set: 0.23883582651615143 at epoch: 1 and batch_num: 562\n",
      "Loss of train set: 0.36321157217025757 at epoch: 1 and batch_num: 563\n",
      "Loss of train set: 0.41873908042907715 at epoch: 1 and batch_num: 564\n",
      "Loss of train set: 0.5015557408332825 at epoch: 1 and batch_num: 565\n",
      "Loss of train set: 0.29180729389190674 at epoch: 1 and batch_num: 566\n",
      "Loss of train set: 0.297018438577652 at epoch: 1 and batch_num: 567\n",
      "Loss of train set: 0.42433199286460876 at epoch: 1 and batch_num: 568\n",
      "Loss of train set: 0.285724937915802 at epoch: 1 and batch_num: 569\n",
      "Loss of train set: 0.37846171855926514 at epoch: 1 and batch_num: 570\n",
      "Loss of train set: 0.3804415464401245 at epoch: 1 and batch_num: 571\n",
      "Loss of train set: 0.2745426893234253 at epoch: 1 and batch_num: 572\n",
      "Loss of train set: 0.37710362672805786 at epoch: 1 and batch_num: 573\n",
      "Loss of train set: 0.3521021008491516 at epoch: 1 and batch_num: 574\n",
      "Loss of train set: 0.2529873847961426 at epoch: 1 and batch_num: 575\n",
      "Loss of train set: 0.30885711312294006 at epoch: 1 and batch_num: 576\n",
      "Loss of train set: 0.29811811447143555 at epoch: 1 and batch_num: 577\n",
      "Loss of train set: 0.30285656452178955 at epoch: 1 and batch_num: 578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3242982029914856 at epoch: 1 and batch_num: 579\n",
      "Loss of train set: 0.3924444317817688 at epoch: 1 and batch_num: 580\n",
      "Loss of train set: 0.27829235792160034 at epoch: 1 and batch_num: 581\n",
      "Loss of train set: 0.38563117384910583 at epoch: 1 and batch_num: 582\n",
      "Loss of train set: 0.3260478675365448 at epoch: 1 and batch_num: 583\n",
      "Loss of train set: 0.42292019724845886 at epoch: 1 and batch_num: 584\n",
      "Loss of train set: 0.4260767698287964 at epoch: 1 and batch_num: 585\n",
      "Loss of train set: 0.5511505603790283 at epoch: 1 and batch_num: 586\n",
      "Loss of train set: 0.3251630961894989 at epoch: 1 and batch_num: 587\n",
      "Loss of train set: 0.5584235191345215 at epoch: 1 and batch_num: 588\n",
      "Loss of train set: 0.3553968369960785 at epoch: 1 and batch_num: 589\n",
      "Loss of train set: 0.4415556490421295 at epoch: 1 and batch_num: 590\n",
      "Loss of train set: 0.6062453985214233 at epoch: 1 and batch_num: 591\n",
      "Loss of train set: 0.31553876399993896 at epoch: 1 and batch_num: 592\n",
      "Loss of train set: 0.4709019660949707 at epoch: 1 and batch_num: 593\n",
      "Loss of train set: 0.5153440237045288 at epoch: 1 and batch_num: 594\n",
      "Loss of train set: 0.3195686340332031 at epoch: 1 and batch_num: 595\n",
      "Loss of train set: 0.5212171673774719 at epoch: 1 and batch_num: 596\n",
      "Loss of train set: 0.29070186614990234 at epoch: 1 and batch_num: 597\n",
      "Loss of train set: 0.6120370626449585 at epoch: 1 and batch_num: 598\n",
      "Loss of train set: 0.3933974802494049 at epoch: 1 and batch_num: 599\n",
      "Loss of train set: 0.39955174922943115 at epoch: 1 and batch_num: 600\n",
      "Loss of train set: 0.34413158893585205 at epoch: 1 and batch_num: 601\n",
      "Loss of train set: 0.4825544059276581 at epoch: 1 and batch_num: 602\n",
      "Loss of train set: 0.29540306329727173 at epoch: 1 and batch_num: 603\n",
      "Loss of train set: 0.32401084899902344 at epoch: 1 and batch_num: 604\n",
      "Loss of train set: 0.5160795450210571 at epoch: 1 and batch_num: 605\n",
      "Loss of train set: 0.3113741874694824 at epoch: 1 and batch_num: 606\n",
      "Loss of train set: 0.3787766695022583 at epoch: 1 and batch_num: 607\n",
      "Loss of train set: 0.24242845177650452 at epoch: 1 and batch_num: 608\n",
      "Loss of train set: 0.6203756928443909 at epoch: 1 and batch_num: 609\n",
      "Loss of train set: 0.35267773270606995 at epoch: 1 and batch_num: 610\n",
      "Loss of train set: 0.3574885427951813 at epoch: 1 and batch_num: 611\n",
      "Loss of train set: 0.3740808963775635 at epoch: 1 and batch_num: 612\n",
      "Loss of train set: 0.3342007100582123 at epoch: 1 and batch_num: 613\n",
      "Loss of train set: 0.3659188747406006 at epoch: 1 and batch_num: 614\n",
      "Loss of train set: 0.2823505997657776 at epoch: 1 and batch_num: 615\n",
      "Loss of train set: 0.2833952307701111 at epoch: 1 and batch_num: 616\n",
      "Loss of train set: 0.2226354479789734 at epoch: 1 and batch_num: 617\n",
      "Loss of train set: 0.16623526811599731 at epoch: 1 and batch_num: 618\n",
      "Loss of train set: 0.2506096661090851 at epoch: 1 and batch_num: 619\n",
      "Loss of train set: 0.43898099660873413 at epoch: 1 and batch_num: 620\n",
      "Loss of train set: 0.49316543340682983 at epoch: 1 and batch_num: 621\n",
      "Loss of train set: 0.2614247798919678 at epoch: 1 and batch_num: 622\n",
      "Loss of train set: 0.31749773025512695 at epoch: 1 and batch_num: 623\n",
      "Loss of train set: 0.31835755705833435 at epoch: 1 and batch_num: 624\n",
      "Loss of train set: 0.26821964979171753 at epoch: 1 and batch_num: 625\n",
      "Loss of train set: 0.27493447065353394 at epoch: 1 and batch_num: 626\n",
      "Loss of train set: 0.3747568130493164 at epoch: 1 and batch_num: 627\n",
      "Loss of train set: 0.30114805698394775 at epoch: 1 and batch_num: 628\n",
      "Loss of train set: 0.32946252822875977 at epoch: 1 and batch_num: 629\n",
      "Loss of train set: 0.20852749049663544 at epoch: 1 and batch_num: 630\n",
      "Loss of train set: 0.4014066159725189 at epoch: 1 and batch_num: 631\n",
      "Loss of train set: 0.33023974299430847 at epoch: 1 and batch_num: 632\n",
      "Loss of train set: 0.28660887479782104 at epoch: 1 and batch_num: 633\n",
      "Loss of train set: 0.29107019305229187 at epoch: 1 and batch_num: 634\n",
      "Loss of train set: 0.37959426641464233 at epoch: 1 and batch_num: 635\n",
      "Loss of train set: 0.391523540019989 at epoch: 1 and batch_num: 636\n",
      "Loss of train set: 0.4008758068084717 at epoch: 1 and batch_num: 637\n",
      "Loss of train set: 0.37229806184768677 at epoch: 1 and batch_num: 638\n",
      "Loss of train set: 0.345406174659729 at epoch: 1 and batch_num: 639\n",
      "Loss of train set: 0.42059171199798584 at epoch: 1 and batch_num: 640\n",
      "Loss of train set: 0.4016035199165344 at epoch: 1 and batch_num: 641\n",
      "Loss of train set: 0.31327494978904724 at epoch: 1 and batch_num: 642\n",
      "Loss of train set: 0.21612751483917236 at epoch: 1 and batch_num: 643\n",
      "Loss of train set: 0.2477377951145172 at epoch: 1 and batch_num: 644\n",
      "Loss of train set: 0.36289626359939575 at epoch: 1 and batch_num: 645\n",
      "Loss of train set: 0.6253880262374878 at epoch: 1 and batch_num: 646\n",
      "Loss of train set: 0.29540348052978516 at epoch: 1 and batch_num: 647\n",
      "Loss of train set: 0.6786084175109863 at epoch: 1 and batch_num: 648\n",
      "Loss of train set: 0.27989405393600464 at epoch: 1 and batch_num: 649\n",
      "Loss of train set: 0.6494978070259094 at epoch: 1 and batch_num: 650\n",
      "Loss of train set: 0.23401570320129395 at epoch: 1 and batch_num: 651\n",
      "Loss of train set: 0.48757994174957275 at epoch: 1 and batch_num: 652\n",
      "Loss of train set: 0.4396146237850189 at epoch: 1 and batch_num: 653\n",
      "Loss of train set: 0.6921646595001221 at epoch: 1 and batch_num: 654\n",
      "Loss of train set: 0.42933857440948486 at epoch: 1 and batch_num: 655\n",
      "Loss of train set: 0.4092090427875519 at epoch: 1 and batch_num: 656\n",
      "Loss of train set: 0.27702704071998596 at epoch: 1 and batch_num: 657\n",
      "Loss of train set: 0.44443798065185547 at epoch: 1 and batch_num: 658\n",
      "Loss of train set: 0.4222867488861084 at epoch: 1 and batch_num: 659\n",
      "Loss of train set: 0.5095434188842773 at epoch: 1 and batch_num: 660\n",
      "Loss of train set: 0.3186447322368622 at epoch: 1 and batch_num: 661\n",
      "Loss of train set: 0.4276103675365448 at epoch: 1 and batch_num: 662\n",
      "Loss of train set: 0.4560328423976898 at epoch: 1 and batch_num: 663\n",
      "Loss of train set: 0.29094117879867554 at epoch: 1 and batch_num: 664\n",
      "Loss of train set: 0.437878280878067 at epoch: 1 and batch_num: 665\n",
      "Loss of train set: 0.3665207624435425 at epoch: 1 and batch_num: 666\n",
      "Loss of train set: 0.26325222849845886 at epoch: 1 and batch_num: 667\n",
      "Loss of train set: 0.4313131272792816 at epoch: 1 and batch_num: 668\n",
      "Loss of train set: 0.4596441984176636 at epoch: 1 and batch_num: 669\n",
      "Loss of train set: 0.3695617914199829 at epoch: 1 and batch_num: 670\n",
      "Loss of train set: 0.35195326805114746 at epoch: 1 and batch_num: 671\n",
      "Loss of train set: 0.4570921063423157 at epoch: 1 and batch_num: 672\n",
      "Loss of train set: 0.41743147373199463 at epoch: 1 and batch_num: 673\n",
      "Loss of train set: 0.321426123380661 at epoch: 1 and batch_num: 674\n",
      "Loss of train set: 0.3852770924568176 at epoch: 1 and batch_num: 675\n",
      "Loss of train set: 0.3988380432128906 at epoch: 1 and batch_num: 676\n",
      "Loss of train set: 0.18249140679836273 at epoch: 1 and batch_num: 677\n",
      "Loss of train set: 0.2945927679538727 at epoch: 1 and batch_num: 678\n",
      "Loss of train set: 0.42293915152549744 at epoch: 1 and batch_num: 679\n",
      "Loss of train set: 0.2782902717590332 at epoch: 1 and batch_num: 680\n",
      "Loss of train set: 0.33155396580696106 at epoch: 1 and batch_num: 681\n",
      "Loss of train set: 0.4739518165588379 at epoch: 1 and batch_num: 682\n",
      "Loss of train set: 0.6102687120437622 at epoch: 1 and batch_num: 683\n",
      "Loss of train set: 0.3427043557167053 at epoch: 1 and batch_num: 684\n",
      "Loss of train set: 0.2605511546134949 at epoch: 1 and batch_num: 685\n",
      "Loss of train set: 0.22146527469158173 at epoch: 1 and batch_num: 686\n",
      "Loss of train set: 0.2080642282962799 at epoch: 1 and batch_num: 687\n",
      "Loss of train set: 0.4616650938987732 at epoch: 1 and batch_num: 688\n",
      "Loss of train set: 0.2860066294670105 at epoch: 1 and batch_num: 689\n",
      "Loss of train set: 0.3284071683883667 at epoch: 1 and batch_num: 690\n",
      "Loss of train set: 0.40101873874664307 at epoch: 1 and batch_num: 691\n",
      "Loss of train set: 0.43203204870224 at epoch: 1 and batch_num: 692\n",
      "Loss of train set: 0.40055468678474426 at epoch: 1 and batch_num: 693\n",
      "Loss of train set: 0.3911040425300598 at epoch: 1 and batch_num: 694\n",
      "Loss of train set: 0.2549581527709961 at epoch: 1 and batch_num: 695\n",
      "Loss of train set: 0.2908841371536255 at epoch: 1 and batch_num: 696\n",
      "Loss of train set: 0.3428289294242859 at epoch: 1 and batch_num: 697\n",
      "Loss of train set: 0.3292655646800995 at epoch: 1 and batch_num: 698\n",
      "Loss of train set: 0.37604087591171265 at epoch: 1 and batch_num: 699\n",
      "Loss of train set: 0.28441599011421204 at epoch: 1 and batch_num: 700\n",
      "Loss of train set: 0.46941307187080383 at epoch: 1 and batch_num: 701\n",
      "Loss of train set: 0.42857545614242554 at epoch: 1 and batch_num: 702\n",
      "Loss of train set: 0.40787357091903687 at epoch: 1 and batch_num: 703\n",
      "Loss of train set: 0.32366928458213806 at epoch: 1 and batch_num: 704\n",
      "Loss of train set: 0.42907023429870605 at epoch: 1 and batch_num: 705\n",
      "Loss of train set: 0.33403491973876953 at epoch: 1 and batch_num: 706\n",
      "Loss of train set: 0.5966464877128601 at epoch: 1 and batch_num: 707\n",
      "Loss of train set: 0.3712390661239624 at epoch: 1 and batch_num: 708\n",
      "Loss of train set: 0.3721737265586853 at epoch: 1 and batch_num: 709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.26556575298309326 at epoch: 1 and batch_num: 710\n",
      "Loss of train set: 0.3086255192756653 at epoch: 1 and batch_num: 711\n",
      "Loss of train set: 0.44281136989593506 at epoch: 1 and batch_num: 712\n",
      "Loss of train set: 0.48453718423843384 at epoch: 1 and batch_num: 713\n",
      "Loss of train set: 0.3010103702545166 at epoch: 1 and batch_num: 714\n",
      "Loss of train set: 0.5719151496887207 at epoch: 1 and batch_num: 715\n",
      "Loss of train set: 0.578802227973938 at epoch: 1 and batch_num: 716\n",
      "Loss of train set: 0.21490171551704407 at epoch: 1 and batch_num: 717\n",
      "Loss of train set: 0.38559070229530334 at epoch: 1 and batch_num: 718\n",
      "Loss of train set: 0.33568596839904785 at epoch: 1 and batch_num: 719\n",
      "Loss of train set: 0.2924855351448059 at epoch: 1 and batch_num: 720\n",
      "Loss of train set: 0.35746780037879944 at epoch: 1 and batch_num: 721\n",
      "Loss of train set: 0.40829557180404663 at epoch: 1 and batch_num: 722\n",
      "Loss of train set: 0.39083409309387207 at epoch: 1 and batch_num: 723\n",
      "Loss of train set: 0.36823606491088867 at epoch: 1 and batch_num: 724\n",
      "Loss of train set: 0.5204362869262695 at epoch: 1 and batch_num: 725\n",
      "Loss of train set: 0.4284999370574951 at epoch: 1 and batch_num: 726\n",
      "Loss of train set: 0.1684717983007431 at epoch: 1 and batch_num: 727\n",
      "Loss of train set: 0.34895241260528564 at epoch: 1 and batch_num: 728\n",
      "Loss of train set: 0.20120495557785034 at epoch: 1 and batch_num: 729\n",
      "Loss of train set: 0.23504221439361572 at epoch: 1 and batch_num: 730\n",
      "Loss of train set: 0.2373356968164444 at epoch: 1 and batch_num: 731\n",
      "Loss of train set: 0.32840925455093384 at epoch: 1 and batch_num: 732\n",
      "Loss of train set: 0.29161161184310913 at epoch: 1 and batch_num: 733\n",
      "Loss of train set: 0.3518354892730713 at epoch: 1 and batch_num: 734\n",
      "Loss of train set: 0.2849907875061035 at epoch: 1 and batch_num: 735\n",
      "Loss of train set: 0.30019766092300415 at epoch: 1 and batch_num: 736\n",
      "Loss of train set: 0.21205216646194458 at epoch: 1 and batch_num: 737\n",
      "Loss of train set: 0.30370020866394043 at epoch: 1 and batch_num: 738\n",
      "Loss of train set: 0.4196773171424866 at epoch: 1 and batch_num: 739\n",
      "Loss of train set: 0.2757902145385742 at epoch: 1 and batch_num: 740\n",
      "Loss of train set: 0.4121062755584717 at epoch: 1 and batch_num: 741\n",
      "Loss of train set: 0.35164615511894226 at epoch: 1 and batch_num: 742\n",
      "Loss of train set: 0.3344108760356903 at epoch: 1 and batch_num: 743\n",
      "Loss of train set: 0.451931893825531 at epoch: 1 and batch_num: 744\n",
      "Loss of train set: 0.15574824810028076 at epoch: 1 and batch_num: 745\n",
      "Loss of train set: 0.3975697159767151 at epoch: 1 and batch_num: 746\n",
      "Loss of train set: 0.4672433137893677 at epoch: 1 and batch_num: 747\n",
      "Loss of train set: 0.3676074743270874 at epoch: 1 and batch_num: 748\n",
      "Loss of train set: 0.2295936644077301 at epoch: 1 and batch_num: 749\n",
      "Loss of train set: 0.23714762926101685 at epoch: 1 and batch_num: 750\n",
      "Loss of train set: 0.2513866424560547 at epoch: 1 and batch_num: 751\n",
      "Loss of train set: 0.3561164140701294 at epoch: 1 and batch_num: 752\n",
      "Loss of train set: 0.3194694519042969 at epoch: 1 and batch_num: 753\n",
      "Loss of train set: 0.2748061418533325 at epoch: 1 and batch_num: 754\n",
      "Loss of train set: 0.39625775814056396 at epoch: 1 and batch_num: 755\n",
      "Loss of train set: 0.3960726857185364 at epoch: 1 and batch_num: 756\n",
      "Loss of train set: 0.5003402829170227 at epoch: 1 and batch_num: 757\n",
      "Loss of train set: 0.354017972946167 at epoch: 1 and batch_num: 758\n",
      "Loss of train set: 0.27324730157852173 at epoch: 1 and batch_num: 759\n",
      "Loss of train set: 0.35587847232818604 at epoch: 1 and batch_num: 760\n",
      "Loss of train set: 0.28370201587677 at epoch: 1 and batch_num: 761\n",
      "Loss of train set: 0.2681189775466919 at epoch: 1 and batch_num: 762\n",
      "Loss of train set: 0.5815649032592773 at epoch: 1 and batch_num: 763\n",
      "Loss of train set: 0.4606640934944153 at epoch: 1 and batch_num: 764\n",
      "Loss of train set: 0.3328293263912201 at epoch: 1 and batch_num: 765\n",
      "Loss of train set: 0.27226099371910095 at epoch: 1 and batch_num: 766\n",
      "Loss of train set: 0.36139464378356934 at epoch: 1 and batch_num: 767\n",
      "Loss of train set: 0.480205774307251 at epoch: 1 and batch_num: 768\n",
      "Loss of train set: 0.48051518201828003 at epoch: 1 and batch_num: 769\n",
      "Loss of train set: 0.34604939818382263 at epoch: 1 and batch_num: 770\n",
      "Loss of train set: 0.47693687677383423 at epoch: 1 and batch_num: 771\n",
      "Loss of train set: 0.2476060390472412 at epoch: 1 and batch_num: 772\n",
      "Loss of train set: 0.35916244983673096 at epoch: 1 and batch_num: 773\n",
      "Loss of train set: 0.26485496759414673 at epoch: 1 and batch_num: 774\n",
      "Loss of train set: 0.4327230453491211 at epoch: 1 and batch_num: 775\n",
      "Loss of train set: 0.35856378078460693 at epoch: 1 and batch_num: 776\n",
      "Loss of train set: 0.28815215826034546 at epoch: 1 and batch_num: 777\n",
      "Loss of train set: 0.3640190362930298 at epoch: 1 and batch_num: 778\n",
      "Loss of train set: 0.26352983713150024 at epoch: 1 and batch_num: 779\n",
      "Loss of train set: 0.5226033926010132 at epoch: 1 and batch_num: 780\n",
      "Loss of train set: 0.2189687192440033 at epoch: 1 and batch_num: 781\n",
      "Loss of train set: 0.2979017198085785 at epoch: 1 and batch_num: 782\n",
      "Loss of train set: 0.4805050790309906 at epoch: 1 and batch_num: 783\n",
      "Loss of train set: 0.5910167694091797 at epoch: 1 and batch_num: 784\n",
      "Loss of train set: 0.21028953790664673 at epoch: 1 and batch_num: 785\n",
      "Loss of train set: 0.5549811124801636 at epoch: 1 and batch_num: 786\n",
      "Loss of train set: 0.35110804438591003 at epoch: 1 and batch_num: 787\n",
      "Loss of train set: 0.1622365564107895 at epoch: 1 and batch_num: 788\n",
      "Loss of train set: 0.5001775026321411 at epoch: 1 and batch_num: 789\n",
      "Loss of train set: 0.3681463599205017 at epoch: 1 and batch_num: 790\n",
      "Loss of train set: 0.2919098734855652 at epoch: 1 and batch_num: 791\n",
      "Loss of train set: 0.2647894322872162 at epoch: 1 and batch_num: 792\n",
      "Loss of train set: 0.3388911485671997 at epoch: 1 and batch_num: 793\n",
      "Loss of train set: 0.33105212450027466 at epoch: 1 and batch_num: 794\n",
      "Loss of train set: 0.481955885887146 at epoch: 1 and batch_num: 795\n",
      "Loss of train set: 0.5804332494735718 at epoch: 1 and batch_num: 796\n",
      "Loss of train set: 0.2539946138858795 at epoch: 1 and batch_num: 797\n",
      "Loss of train set: 0.21825970709323883 at epoch: 1 and batch_num: 798\n",
      "Loss of train set: 0.32841312885284424 at epoch: 1 and batch_num: 799\n",
      "Loss of train set: 0.3719707727432251 at epoch: 1 and batch_num: 800\n",
      "Loss of train set: 0.22593705356121063 at epoch: 1 and batch_num: 801\n",
      "Loss of train set: 0.43137305974960327 at epoch: 1 and batch_num: 802\n",
      "Loss of train set: 0.24468345940113068 at epoch: 1 and batch_num: 803\n",
      "Loss of train set: 0.39791178703308105 at epoch: 1 and batch_num: 804\n",
      "Loss of train set: 0.25819259881973267 at epoch: 1 and batch_num: 805\n",
      "Loss of train set: 0.3304353356361389 at epoch: 1 and batch_num: 806\n",
      "Loss of train set: 0.38125479221343994 at epoch: 1 and batch_num: 807\n",
      "Loss of train set: 0.22253550589084625 at epoch: 1 and batch_num: 808\n",
      "Loss of train set: 0.40574103593826294 at epoch: 1 and batch_num: 809\n",
      "Loss of train set: 0.34613943099975586 at epoch: 1 and batch_num: 810\n",
      "Loss of train set: 0.34590965509414673 at epoch: 1 and batch_num: 811\n",
      "Loss of train set: 0.6105016469955444 at epoch: 1 and batch_num: 812\n",
      "Loss of train set: 0.277328222990036 at epoch: 1 and batch_num: 813\n",
      "Loss of train set: 0.23887908458709717 at epoch: 1 and batch_num: 814\n",
      "Loss of train set: 0.385393351316452 at epoch: 1 and batch_num: 815\n",
      "Loss of train set: 0.578025758266449 at epoch: 1 and batch_num: 816\n",
      "Loss of train set: 0.16640669107437134 at epoch: 1 and batch_num: 817\n",
      "Loss of train set: 0.5071389675140381 at epoch: 1 and batch_num: 818\n",
      "Loss of train set: 0.5452849864959717 at epoch: 1 and batch_num: 819\n",
      "Loss of train set: 0.307969868183136 at epoch: 1 and batch_num: 820\n",
      "Loss of train set: 0.37736237049102783 at epoch: 1 and batch_num: 821\n",
      "Loss of train set: 0.36010295152664185 at epoch: 1 and batch_num: 822\n",
      "Loss of train set: 0.34844064712524414 at epoch: 1 and batch_num: 823\n",
      "Loss of train set: 0.40859609842300415 at epoch: 1 and batch_num: 824\n",
      "Loss of train set: 0.3397182822227478 at epoch: 1 and batch_num: 825\n",
      "Loss of train set: 0.3333292305469513 at epoch: 1 and batch_num: 826\n",
      "Loss of train set: 0.29766392707824707 at epoch: 1 and batch_num: 827\n",
      "Loss of train set: 0.27794545888900757 at epoch: 1 and batch_num: 828\n",
      "Loss of train set: 0.2442120611667633 at epoch: 1 and batch_num: 829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.33890414237976074 at epoch: 1 and batch_num: 830\n",
      "Loss of train set: 0.2555154860019684 at epoch: 1 and batch_num: 831\n",
      "Loss of train set: 0.5110238790512085 at epoch: 1 and batch_num: 832\n",
      "Loss of train set: 0.41295063495635986 at epoch: 1 and batch_num: 833\n",
      "Loss of train set: 0.23925158381462097 at epoch: 1 and batch_num: 834\n",
      "Loss of train set: 0.32336196303367615 at epoch: 1 and batch_num: 835\n",
      "Loss of train set: 0.542203426361084 at epoch: 1 and batch_num: 836\n",
      "Loss of train set: 0.2961232662200928 at epoch: 1 and batch_num: 837\n",
      "Loss of train set: 0.36730051040649414 at epoch: 1 and batch_num: 838\n",
      "Loss of train set: 0.28214067220687866 at epoch: 1 and batch_num: 839\n",
      "Loss of train set: 0.5393984317779541 at epoch: 1 and batch_num: 840\n",
      "Loss of train set: 0.3409457206726074 at epoch: 1 and batch_num: 841\n",
      "Loss of train set: 0.29511094093322754 at epoch: 1 and batch_num: 842\n",
      "Loss of train set: 0.3624495565891266 at epoch: 1 and batch_num: 843\n",
      "Loss of train set: 0.49088817834854126 at epoch: 1 and batch_num: 844\n",
      "Loss of train set: 0.43912816047668457 at epoch: 1 and batch_num: 845\n",
      "Loss of train set: 0.42065533995628357 at epoch: 1 and batch_num: 846\n",
      "Loss of train set: 0.30699604749679565 at epoch: 1 and batch_num: 847\n",
      "Loss of train set: 0.2571083903312683 at epoch: 1 and batch_num: 848\n",
      "Loss of train set: 0.4227263033390045 at epoch: 1 and batch_num: 849\n",
      "Loss of train set: 0.4091572165489197 at epoch: 1 and batch_num: 850\n",
      "Loss of train set: 0.2961152195930481 at epoch: 1 and batch_num: 851\n",
      "Loss of train set: 0.32964932918548584 at epoch: 1 and batch_num: 852\n",
      "Loss of train set: 0.41947776079177856 at epoch: 1 and batch_num: 853\n",
      "Loss of train set: 0.4234358072280884 at epoch: 1 and batch_num: 854\n",
      "Loss of train set: 0.45977962017059326 at epoch: 1 and batch_num: 855\n",
      "Loss of train set: 0.38931936025619507 at epoch: 1 and batch_num: 856\n",
      "Loss of train set: 0.32642361521720886 at epoch: 1 and batch_num: 857\n",
      "Loss of train set: 0.25582969188690186 at epoch: 1 and batch_num: 858\n",
      "Loss of train set: 0.2867906987667084 at epoch: 1 and batch_num: 859\n",
      "Loss of train set: 0.40953996777534485 at epoch: 1 and batch_num: 860\n",
      "Loss of train set: 0.35254526138305664 at epoch: 1 and batch_num: 861\n",
      "Loss of train set: 0.42737677693367004 at epoch: 1 and batch_num: 862\n",
      "Loss of train set: 0.3676827549934387 at epoch: 1 and batch_num: 863\n",
      "Loss of train set: 0.34374701976776123 at epoch: 1 and batch_num: 864\n",
      "Loss of train set: 0.39357051253318787 at epoch: 1 and batch_num: 865\n",
      "Loss of train set: 0.521517813205719 at epoch: 1 and batch_num: 866\n",
      "Loss of train set: 0.33311885595321655 at epoch: 1 and batch_num: 867\n",
      "Loss of train set: 0.3441064953804016 at epoch: 1 and batch_num: 868\n",
      "Loss of train set: 0.43556714057922363 at epoch: 1 and batch_num: 869\n",
      "Loss of train set: 0.3013392388820648 at epoch: 1 and batch_num: 870\n",
      "Loss of train set: 0.30890578031539917 at epoch: 1 and batch_num: 871\n",
      "Loss of train set: 0.3503856062889099 at epoch: 1 and batch_num: 872\n",
      "Loss of train set: 0.5236457586288452 at epoch: 1 and batch_num: 873\n",
      "Loss of train set: 0.6045981645584106 at epoch: 1 and batch_num: 874\n",
      "Loss of train set: 0.3167821168899536 at epoch: 1 and batch_num: 875\n",
      "Loss of train set: 0.24003082513809204 at epoch: 1 and batch_num: 876\n",
      "Loss of train set: 0.35075682401657104 at epoch: 1 and batch_num: 877\n",
      "Loss of train set: 0.35517242550849915 at epoch: 1 and batch_num: 878\n",
      "Loss of train set: 0.34109145402908325 at epoch: 1 and batch_num: 879\n",
      "Loss of train set: 0.4353187084197998 at epoch: 1 and batch_num: 880\n",
      "Loss of train set: 0.46017611026763916 at epoch: 1 and batch_num: 881\n",
      "Loss of train set: 0.36115607619285583 at epoch: 1 and batch_num: 882\n",
      "Loss of train set: 0.3831723928451538 at epoch: 1 and batch_num: 883\n",
      "Loss of train set: 0.5309826731681824 at epoch: 1 and batch_num: 884\n",
      "Loss of train set: 0.34508803486824036 at epoch: 1 and batch_num: 885\n",
      "Loss of train set: 0.2885470390319824 at epoch: 1 and batch_num: 886\n",
      "Loss of train set: 0.3558691442012787 at epoch: 1 and batch_num: 887\n",
      "Loss of train set: 0.24965375661849976 at epoch: 1 and batch_num: 888\n",
      "Loss of train set: 0.24848303198814392 at epoch: 1 and batch_num: 889\n",
      "Loss of train set: 0.26380372047424316 at epoch: 1 and batch_num: 890\n",
      "Loss of train set: 0.39339137077331543 at epoch: 1 and batch_num: 891\n",
      "Loss of train set: 0.5142220258712769 at epoch: 1 and batch_num: 892\n",
      "Loss of train set: 0.35650062561035156 at epoch: 1 and batch_num: 893\n",
      "Loss of train set: 0.4515501856803894 at epoch: 1 and batch_num: 894\n",
      "Loss of train set: 0.36774951219558716 at epoch: 1 and batch_num: 895\n",
      "Loss of train set: 0.4869901239871979 at epoch: 1 and batch_num: 896\n",
      "Loss of train set: 0.5463947057723999 at epoch: 1 and batch_num: 897\n",
      "Loss of train set: 0.3264903426170349 at epoch: 1 and batch_num: 898\n",
      "Loss of train set: 0.23834183812141418 at epoch: 1 and batch_num: 899\n",
      "Loss of train set: 0.27277112007141113 at epoch: 1 and batch_num: 900\n",
      "Loss of train set: 0.23722954094409943 at epoch: 1 and batch_num: 901\n",
      "Loss of train set: 0.3128121495246887 at epoch: 1 and batch_num: 902\n",
      "Loss of train set: 0.48031085729599 at epoch: 1 and batch_num: 903\n",
      "Loss of train set: 0.49769705533981323 at epoch: 1 and batch_num: 904\n",
      "Loss of train set: 0.4880310893058777 at epoch: 1 and batch_num: 905\n",
      "Loss of train set: 0.19548915326595306 at epoch: 1 and batch_num: 906\n",
      "Loss of train set: 0.4476524591445923 at epoch: 1 and batch_num: 907\n",
      "Loss of train set: 0.38767677545547485 at epoch: 1 and batch_num: 908\n",
      "Loss of train set: 0.704558253288269 at epoch: 1 and batch_num: 909\n",
      "Loss of train set: 0.4037422239780426 at epoch: 1 and batch_num: 910\n",
      "Loss of train set: 0.5536203384399414 at epoch: 1 and batch_num: 911\n",
      "Loss of train set: 0.3277905285358429 at epoch: 1 and batch_num: 912\n",
      "Loss of train set: 0.5372701287269592 at epoch: 1 and batch_num: 913\n",
      "Loss of train set: 0.44160035252571106 at epoch: 1 and batch_num: 914\n",
      "Loss of train set: 0.34314948320388794 at epoch: 1 and batch_num: 915\n",
      "Loss of train set: 0.23920688033103943 at epoch: 1 and batch_num: 916\n",
      "Loss of train set: 0.31879448890686035 at epoch: 1 and batch_num: 917\n",
      "Loss of train set: 0.4071810245513916 at epoch: 1 and batch_num: 918\n",
      "Loss of train set: 0.4774637818336487 at epoch: 1 and batch_num: 919\n",
      "Loss of train set: 0.37768664956092834 at epoch: 1 and batch_num: 920\n",
      "Loss of train set: 0.49021273851394653 at epoch: 1 and batch_num: 921\n",
      "Loss of train set: 0.3327089548110962 at epoch: 1 and batch_num: 922\n",
      "Loss of train set: 0.49538129568099976 at epoch: 1 and batch_num: 923\n",
      "Loss of train set: 0.3986523747444153 at epoch: 1 and batch_num: 924\n",
      "Loss of train set: 0.2095108926296234 at epoch: 1 and batch_num: 925\n",
      "Loss of train set: 0.3455975651741028 at epoch: 1 and batch_num: 926\n",
      "Loss of train set: 0.4182661473751068 at epoch: 1 and batch_num: 927\n",
      "Loss of train set: 0.2941586971282959 at epoch: 1 and batch_num: 928\n",
      "Loss of train set: 0.44368284940719604 at epoch: 1 and batch_num: 929\n",
      "Loss of train set: 0.3173931837081909 at epoch: 1 and batch_num: 930\n",
      "Loss of train set: 0.30740153789520264 at epoch: 1 and batch_num: 931\n",
      "Loss of train set: 0.3080633878707886 at epoch: 1 and batch_num: 932\n",
      "Loss of train set: 0.31762534379959106 at epoch: 1 and batch_num: 933\n",
      "Loss of train set: 0.409454345703125 at epoch: 1 and batch_num: 934\n",
      "Loss of train set: 0.5358197689056396 at epoch: 1 and batch_num: 935\n",
      "Loss of train set: 0.6643561720848083 at epoch: 1 and batch_num: 936\n",
      "Loss of train set: 0.24142351746559143 at epoch: 1 and batch_num: 937\n",
      "Accuracy of train set: 0.86835\n",
      "Loss of test set: 0.6777094602584839 at epoch: 1 and batch_num: 0\n",
      "Loss of test set: 0.59998619556427 at epoch: 1 and batch_num: 1\n",
      "Loss of test set: 0.4767715632915497 at epoch: 1 and batch_num: 2\n",
      "Loss of test set: 0.3107789158821106 at epoch: 1 and batch_num: 3\n",
      "Loss of test set: 0.5519702434539795 at epoch: 1 and batch_num: 4\n",
      "Loss of test set: 0.4132038354873657 at epoch: 1 and batch_num: 5\n",
      "Loss of test set: 0.5521506071090698 at epoch: 1 and batch_num: 6\n",
      "Loss of test set: 0.44611769914627075 at epoch: 1 and batch_num: 7\n",
      "Loss of test set: 0.29052817821502686 at epoch: 1 and batch_num: 8\n",
      "Loss of test set: 0.5194817781448364 at epoch: 1 and batch_num: 9\n",
      "Loss of test set: 0.43168503046035767 at epoch: 1 and batch_num: 10\n",
      "Loss of test set: 0.6130830645561218 at epoch: 1 and batch_num: 11\n",
      "Loss of test set: 0.21776244044303894 at epoch: 1 and batch_num: 12\n",
      "Loss of test set: 0.3511336147785187 at epoch: 1 and batch_num: 13\n",
      "Loss of test set: 0.42393219470977783 at epoch: 1 and batch_num: 14\n",
      "Loss of test set: 0.4766576886177063 at epoch: 1 and batch_num: 15\n",
      "Loss of test set: 0.3121817111968994 at epoch: 1 and batch_num: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.4613851308822632 at epoch: 1 and batch_num: 17\n",
      "Loss of test set: 0.3347603678703308 at epoch: 1 and batch_num: 18\n",
      "Loss of test set: 0.6339390277862549 at epoch: 1 and batch_num: 19\n",
      "Loss of test set: 0.3793758153915405 at epoch: 1 and batch_num: 20\n",
      "Loss of test set: 0.40524420142173767 at epoch: 1 and batch_num: 21\n",
      "Loss of test set: 0.2720924913883209 at epoch: 1 and batch_num: 22\n",
      "Loss of test set: 0.40952423214912415 at epoch: 1 and batch_num: 23\n",
      "Loss of test set: 0.28766727447509766 at epoch: 1 and batch_num: 24\n",
      "Loss of test set: 0.5941420793533325 at epoch: 1 and batch_num: 25\n",
      "Loss of test set: 0.3178987205028534 at epoch: 1 and batch_num: 26\n",
      "Loss of test set: 0.4208369553089142 at epoch: 1 and batch_num: 27\n",
      "Loss of test set: 0.23791161179542542 at epoch: 1 and batch_num: 28\n",
      "Loss of test set: 0.42967158555984497 at epoch: 1 and batch_num: 29\n",
      "Loss of test set: 0.3260662257671356 at epoch: 1 and batch_num: 30\n",
      "Loss of test set: 0.42729565501213074 at epoch: 1 and batch_num: 31\n",
      "Loss of test set: 0.5056294798851013 at epoch: 1 and batch_num: 32\n",
      "Loss of test set: 0.4237993657588959 at epoch: 1 and batch_num: 33\n",
      "Loss of test set: 0.37984171509742737 at epoch: 1 and batch_num: 34\n",
      "Loss of test set: 0.40749046206474304 at epoch: 1 and batch_num: 35\n",
      "Loss of test set: 0.6013600826263428 at epoch: 1 and batch_num: 36\n",
      "Loss of test set: 0.4044729471206665 at epoch: 1 and batch_num: 37\n",
      "Loss of test set: 0.38465237617492676 at epoch: 1 and batch_num: 38\n",
      "Loss of test set: 0.3383253812789917 at epoch: 1 and batch_num: 39\n",
      "Loss of test set: 0.24041488766670227 at epoch: 1 and batch_num: 40\n",
      "Loss of test set: 0.4520280659198761 at epoch: 1 and batch_num: 41\n",
      "Loss of test set: 0.5647262334823608 at epoch: 1 and batch_num: 42\n",
      "Loss of test set: 0.4350462853908539 at epoch: 1 and batch_num: 43\n",
      "Loss of test set: 0.35139000415802 at epoch: 1 and batch_num: 44\n",
      "Loss of test set: 0.4467860758304596 at epoch: 1 and batch_num: 45\n",
      "Loss of test set: 0.4278019070625305 at epoch: 1 and batch_num: 46\n",
      "Loss of test set: 0.3293114900588989 at epoch: 1 and batch_num: 47\n",
      "Loss of test set: 0.33894792199134827 at epoch: 1 and batch_num: 48\n",
      "Loss of test set: 0.44942909479141235 at epoch: 1 and batch_num: 49\n",
      "Loss of test set: 0.45112529397010803 at epoch: 1 and batch_num: 50\n",
      "Loss of test set: 0.32546675205230713 at epoch: 1 and batch_num: 51\n",
      "Loss of test set: 0.3527441620826721 at epoch: 1 and batch_num: 52\n",
      "Loss of test set: 0.3752996325492859 at epoch: 1 and batch_num: 53\n",
      "Loss of test set: 0.17923739552497864 at epoch: 1 and batch_num: 54\n",
      "Loss of test set: 0.33396923542022705 at epoch: 1 and batch_num: 55\n",
      "Loss of test set: 0.33510681986808777 at epoch: 1 and batch_num: 56\n",
      "Loss of test set: 0.36949723958969116 at epoch: 1 and batch_num: 57\n",
      "Loss of test set: 0.3565978407859802 at epoch: 1 and batch_num: 58\n",
      "Loss of test set: 0.46777820587158203 at epoch: 1 and batch_num: 59\n",
      "Loss of test set: 0.4381628632545471 at epoch: 1 and batch_num: 60\n",
      "Loss of test set: 0.3466036319732666 at epoch: 1 and batch_num: 61\n",
      "Loss of test set: 0.28589755296707153 at epoch: 1 and batch_num: 62\n",
      "Loss of test set: 0.6694513559341431 at epoch: 1 and batch_num: 63\n",
      "Loss of test set: 0.43910151720046997 at epoch: 1 and batch_num: 64\n",
      "Loss of test set: 0.39340975880622864 at epoch: 1 and batch_num: 65\n",
      "Loss of test set: 0.3995274305343628 at epoch: 1 and batch_num: 66\n",
      "Loss of test set: 0.4883211851119995 at epoch: 1 and batch_num: 67\n",
      "Loss of test set: 0.5431907176971436 at epoch: 1 and batch_num: 68\n",
      "Loss of test set: 0.4725263714790344 at epoch: 1 and batch_num: 69\n",
      "Loss of test set: 0.47920000553131104 at epoch: 1 and batch_num: 70\n",
      "Loss of test set: 0.36339429020881653 at epoch: 1 and batch_num: 71\n",
      "Loss of test set: 0.31790220737457275 at epoch: 1 and batch_num: 72\n",
      "Loss of test set: 0.5023256540298462 at epoch: 1 and batch_num: 73\n",
      "Loss of test set: 0.384024441242218 at epoch: 1 and batch_num: 74\n",
      "Loss of test set: 0.45628345012664795 at epoch: 1 and batch_num: 75\n",
      "Loss of test set: 0.25641876459121704 at epoch: 1 and batch_num: 76\n",
      "Loss of test set: 0.48510658740997314 at epoch: 1 and batch_num: 77\n",
      "Loss of test set: 0.3014046549797058 at epoch: 1 and batch_num: 78\n",
      "Loss of test set: 0.5172991752624512 at epoch: 1 and batch_num: 79\n",
      "Loss of test set: 0.33103838562965393 at epoch: 1 and batch_num: 80\n",
      "Loss of test set: 0.25665056705474854 at epoch: 1 and batch_num: 81\n",
      "Loss of test set: 0.3292911648750305 at epoch: 1 and batch_num: 82\n",
      "Loss of test set: 0.5659759044647217 at epoch: 1 and batch_num: 83\n",
      "Loss of test set: 0.36272335052490234 at epoch: 1 and batch_num: 84\n",
      "Loss of test set: 0.5841416120529175 at epoch: 1 and batch_num: 85\n",
      "Loss of test set: 0.32590872049331665 at epoch: 1 and batch_num: 86\n",
      "Loss of test set: 0.46759554743766785 at epoch: 1 and batch_num: 87\n",
      "Loss of test set: 0.5119602084159851 at epoch: 1 and batch_num: 88\n",
      "Loss of test set: 0.5167950987815857 at epoch: 1 and batch_num: 89\n",
      "Loss of test set: 0.29832470417022705 at epoch: 1 and batch_num: 90\n",
      "Loss of test set: 0.5843744277954102 at epoch: 1 and batch_num: 91\n",
      "Loss of test set: 0.3942658305168152 at epoch: 1 and batch_num: 92\n",
      "Loss of test set: 0.28063470125198364 at epoch: 1 and batch_num: 93\n",
      "Loss of test set: 0.5090406537055969 at epoch: 1 and batch_num: 94\n",
      "Loss of test set: 0.5587859749794006 at epoch: 1 and batch_num: 95\n",
      "Loss of test set: 0.35129645466804504 at epoch: 1 and batch_num: 96\n",
      "Loss of test set: 0.3368864059448242 at epoch: 1 and batch_num: 97\n",
      "Loss of test set: 0.4101950526237488 at epoch: 1 and batch_num: 98\n",
      "Loss of test set: 0.5057182908058167 at epoch: 1 and batch_num: 99\n",
      "Loss of test set: 0.34010517597198486 at epoch: 1 and batch_num: 100\n",
      "Loss of test set: 0.33063578605651855 at epoch: 1 and batch_num: 101\n",
      "Loss of test set: 0.35646122694015503 at epoch: 1 and batch_num: 102\n",
      "Loss of test set: 0.47464847564697266 at epoch: 1 and batch_num: 103\n",
      "Loss of test set: 0.3953332006931305 at epoch: 1 and batch_num: 104\n",
      "Loss of test set: 0.6904659271240234 at epoch: 1 and batch_num: 105\n",
      "Loss of test set: 0.5531318187713623 at epoch: 1 and batch_num: 106\n",
      "Loss of test set: 0.560212254524231 at epoch: 1 and batch_num: 107\n",
      "Loss of test set: 0.4402959942817688 at epoch: 1 and batch_num: 108\n",
      "Loss of test set: 0.28391033411026 at epoch: 1 and batch_num: 109\n",
      "Loss of test set: 0.36970868706703186 at epoch: 1 and batch_num: 110\n",
      "Loss of test set: 0.327359139919281 at epoch: 1 and batch_num: 111\n",
      "Loss of test set: 0.4335551857948303 at epoch: 1 and batch_num: 112\n",
      "Loss of test set: 0.27242782711982727 at epoch: 1 and batch_num: 113\n",
      "Loss of test set: 0.28226229548454285 at epoch: 1 and batch_num: 114\n",
      "Loss of test set: 0.3795928359031677 at epoch: 1 and batch_num: 115\n",
      "Loss of test set: 0.36144930124282837 at epoch: 1 and batch_num: 116\n",
      "Loss of test set: 0.19508406519889832 at epoch: 1 and batch_num: 117\n",
      "Loss of test set: 0.3300400376319885 at epoch: 1 and batch_num: 118\n",
      "Loss of test set: 0.33858755230903625 at epoch: 1 and batch_num: 119\n",
      "Loss of test set: 0.336165189743042 at epoch: 1 and batch_num: 120\n",
      "Loss of test set: 0.4728642404079437 at epoch: 1 and batch_num: 121\n",
      "Loss of test set: 0.4761018753051758 at epoch: 1 and batch_num: 122\n",
      "Loss of test set: 0.5748482942581177 at epoch: 1 and batch_num: 123\n",
      "Loss of test set: 0.3470075726509094 at epoch: 1 and batch_num: 124\n",
      "Loss of test set: 0.2449728548526764 at epoch: 1 and batch_num: 125\n",
      "Loss of test set: 0.377233624458313 at epoch: 1 and batch_num: 126\n",
      "Loss of test set: 0.3978699743747711 at epoch: 1 and batch_num: 127\n",
      "Loss of test set: 0.49750620126724243 at epoch: 1 and batch_num: 128\n",
      "Loss of test set: 0.3763943016529083 at epoch: 1 and batch_num: 129\n",
      "Loss of test set: 0.32838064432144165 at epoch: 1 and batch_num: 130\n",
      "Loss of test set: 0.5995233058929443 at epoch: 1 and batch_num: 131\n",
      "Loss of test set: 0.3343210220336914 at epoch: 1 and batch_num: 132\n",
      "Loss of test set: 0.3100174069404602 at epoch: 1 and batch_num: 133\n",
      "Loss of test set: 0.475908488035202 at epoch: 1 and batch_num: 134\n",
      "Loss of test set: 0.3607851266860962 at epoch: 1 and batch_num: 135\n",
      "Loss of test set: 0.4225536286830902 at epoch: 1 and batch_num: 136\n",
      "Loss of test set: 0.5367907285690308 at epoch: 1 and batch_num: 137\n",
      "Loss of test set: 0.6486543416976929 at epoch: 1 and batch_num: 138\n",
      "Loss of test set: 0.49223241209983826 at epoch: 1 and batch_num: 139\n",
      "Loss of test set: 0.38859423995018005 at epoch: 1 and batch_num: 140\n",
      "Loss of test set: 0.38570666313171387 at epoch: 1 and batch_num: 141\n",
      "Loss of test set: 0.33689558506011963 at epoch: 1 and batch_num: 142\n",
      "Loss of test set: 0.4810946583747864 at epoch: 1 and batch_num: 143\n",
      "Loss of test set: 0.2936086058616638 at epoch: 1 and batch_num: 144\n",
      "Loss of test set: 0.34317684173583984 at epoch: 1 and batch_num: 145\n",
      "Loss of test set: 0.2669060230255127 at epoch: 1 and batch_num: 146\n",
      "Loss of test set: 0.40804746747016907 at epoch: 1 and batch_num: 147\n",
      "Loss of test set: 0.46441176533699036 at epoch: 1 and batch_num: 148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.5269691944122314 at epoch: 1 and batch_num: 149\n",
      "Loss of test set: 0.32767218351364136 at epoch: 1 and batch_num: 150\n",
      "Loss of test set: 0.224827378988266 at epoch: 1 and batch_num: 151\n",
      "Loss of test set: 0.40513718128204346 at epoch: 1 and batch_num: 152\n",
      "Loss of test set: 0.42385420203208923 at epoch: 1 and batch_num: 153\n",
      "Loss of test set: 0.5091288089752197 at epoch: 1 and batch_num: 154\n",
      "Loss of test set: 0.5182786583900452 at epoch: 1 and batch_num: 155\n",
      "Loss of test set: 0.7919517755508423 at epoch: 1 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8519\n",
      "Loss of train set: 0.2594914734363556 at epoch: 2 and batch_num: 0\n",
      "Loss of train set: 0.3920285403728485 at epoch: 2 and batch_num: 1\n",
      "Loss of train set: 0.4676814675331116 at epoch: 2 and batch_num: 2\n",
      "Loss of train set: 0.35826852917671204 at epoch: 2 and batch_num: 3\n",
      "Loss of train set: 0.4283817708492279 at epoch: 2 and batch_num: 4\n",
      "Loss of train set: 0.38054803013801575 at epoch: 2 and batch_num: 5\n",
      "Loss of train set: 0.24846620857715607 at epoch: 2 and batch_num: 6\n",
      "Loss of train set: 0.37088900804519653 at epoch: 2 and batch_num: 7\n",
      "Loss of train set: 0.28199344873428345 at epoch: 2 and batch_num: 8\n",
      "Loss of train set: 0.303896963596344 at epoch: 2 and batch_num: 9\n",
      "Loss of train set: 0.48714980483055115 at epoch: 2 and batch_num: 10\n",
      "Loss of train set: 0.4428495764732361 at epoch: 2 and batch_num: 11\n",
      "Loss of train set: 0.41190212965011597 at epoch: 2 and batch_num: 12\n",
      "Loss of train set: 0.2895260155200958 at epoch: 2 and batch_num: 13\n",
      "Loss of train set: 0.45722493529319763 at epoch: 2 and batch_num: 14\n",
      "Loss of train set: 0.3654690384864807 at epoch: 2 and batch_num: 15\n",
      "Loss of train set: 0.29122984409332275 at epoch: 2 and batch_num: 16\n",
      "Loss of train set: 0.42370739579200745 at epoch: 2 and batch_num: 17\n",
      "Loss of train set: 0.239877849817276 at epoch: 2 and batch_num: 18\n",
      "Loss of train set: 0.5495465993881226 at epoch: 2 and batch_num: 19\n",
      "Loss of train set: 0.35404491424560547 at epoch: 2 and batch_num: 20\n",
      "Loss of train set: 0.4303674101829529 at epoch: 2 and batch_num: 21\n",
      "Loss of train set: 0.3644789457321167 at epoch: 2 and batch_num: 22\n",
      "Loss of train set: 0.25524458289146423 at epoch: 2 and batch_num: 23\n",
      "Loss of train set: 0.337043821811676 at epoch: 2 and batch_num: 24\n",
      "Loss of train set: 0.3665078580379486 at epoch: 2 and batch_num: 25\n",
      "Loss of train set: 0.2028711438179016 at epoch: 2 and batch_num: 26\n",
      "Loss of train set: 0.35499849915504456 at epoch: 2 and batch_num: 27\n",
      "Loss of train set: 0.5199427604675293 at epoch: 2 and batch_num: 28\n",
      "Loss of train set: 0.5425212383270264 at epoch: 2 and batch_num: 29\n",
      "Loss of train set: 0.44304585456848145 at epoch: 2 and batch_num: 30\n",
      "Loss of train set: 0.4145832061767578 at epoch: 2 and batch_num: 31\n",
      "Loss of train set: 0.35233545303344727 at epoch: 2 and batch_num: 32\n",
      "Loss of train set: 0.42964231967926025 at epoch: 2 and batch_num: 33\n",
      "Loss of train set: 0.2923188805580139 at epoch: 2 and batch_num: 34\n",
      "Loss of train set: 0.39884060621261597 at epoch: 2 and batch_num: 35\n",
      "Loss of train set: 0.17206168174743652 at epoch: 2 and batch_num: 36\n",
      "Loss of train set: 0.2713679075241089 at epoch: 2 and batch_num: 37\n",
      "Loss of train set: 0.3770923316478729 at epoch: 2 and batch_num: 38\n",
      "Loss of train set: 0.44024330377578735 at epoch: 2 and batch_num: 39\n",
      "Loss of train set: 0.5362586975097656 at epoch: 2 and batch_num: 40\n",
      "Loss of train set: 0.41929861903190613 at epoch: 2 and batch_num: 41\n",
      "Loss of train set: 0.3224586844444275 at epoch: 2 and batch_num: 42\n",
      "Loss of train set: 0.33752936124801636 at epoch: 2 and batch_num: 43\n",
      "Loss of train set: 0.5534563660621643 at epoch: 2 and batch_num: 44\n",
      "Loss of train set: 0.2232503592967987 at epoch: 2 and batch_num: 45\n",
      "Loss of train set: 0.2202615737915039 at epoch: 2 and batch_num: 46\n",
      "Loss of train set: 0.4279051423072815 at epoch: 2 and batch_num: 47\n",
      "Loss of train set: 0.4912009835243225 at epoch: 2 and batch_num: 48\n",
      "Loss of train set: 0.3498765826225281 at epoch: 2 and batch_num: 49\n",
      "Loss of train set: 0.2750192880630493 at epoch: 2 and batch_num: 50\n",
      "Loss of train set: 0.32273250818252563 at epoch: 2 and batch_num: 51\n",
      "Loss of train set: 0.2698570489883423 at epoch: 2 and batch_num: 52\n",
      "Loss of train set: 0.4592646062374115 at epoch: 2 and batch_num: 53\n",
      "Loss of train set: 0.42094579339027405 at epoch: 2 and batch_num: 54\n",
      "Loss of train set: 0.2702644169330597 at epoch: 2 and batch_num: 55\n",
      "Loss of train set: 0.2883652448654175 at epoch: 2 and batch_num: 56\n",
      "Loss of train set: 0.4447117745876312 at epoch: 2 and batch_num: 57\n",
      "Loss of train set: 0.36169254779815674 at epoch: 2 and batch_num: 58\n",
      "Loss of train set: 0.5318798422813416 at epoch: 2 and batch_num: 59\n",
      "Loss of train set: 0.35450083017349243 at epoch: 2 and batch_num: 60\n",
      "Loss of train set: 0.20577524602413177 at epoch: 2 and batch_num: 61\n",
      "Loss of train set: 0.46545660495758057 at epoch: 2 and batch_num: 62\n",
      "Loss of train set: 0.33144426345825195 at epoch: 2 and batch_num: 63\n",
      "Loss of train set: 0.36627861857414246 at epoch: 2 and batch_num: 64\n",
      "Loss of train set: 0.5990090370178223 at epoch: 2 and batch_num: 65\n",
      "Loss of train set: 0.3903076946735382 at epoch: 2 and batch_num: 66\n",
      "Loss of train set: 0.27728116512298584 at epoch: 2 and batch_num: 67\n",
      "Loss of train set: 0.6174342036247253 at epoch: 2 and batch_num: 68\n",
      "Loss of train set: 0.3921222984790802 at epoch: 2 and batch_num: 69\n",
      "Loss of train set: 0.3460562527179718 at epoch: 2 and batch_num: 70\n",
      "Loss of train set: 0.410172700881958 at epoch: 2 and batch_num: 71\n",
      "Loss of train set: 0.49930673837661743 at epoch: 2 and batch_num: 72\n",
      "Loss of train set: 0.30365413427352905 at epoch: 2 and batch_num: 73\n",
      "Loss of train set: 0.490098774433136 at epoch: 2 and batch_num: 74\n",
      "Loss of train set: 0.41824793815612793 at epoch: 2 and batch_num: 75\n",
      "Loss of train set: 0.34504610300064087 at epoch: 2 and batch_num: 76\n",
      "Loss of train set: 0.3708434998989105 at epoch: 2 and batch_num: 77\n",
      "Loss of train set: 0.4454352557659149 at epoch: 2 and batch_num: 78\n",
      "Loss of train set: 0.5451366901397705 at epoch: 2 and batch_num: 79\n",
      "Loss of train set: 0.5667569041252136 at epoch: 2 and batch_num: 80\n",
      "Loss of train set: 0.25897136330604553 at epoch: 2 and batch_num: 81\n",
      "Loss of train set: 0.36843186616897583 at epoch: 2 and batch_num: 82\n",
      "Loss of train set: 0.4043048620223999 at epoch: 2 and batch_num: 83\n",
      "Loss of train set: 0.530083417892456 at epoch: 2 and batch_num: 84\n",
      "Loss of train set: 0.4306032657623291 at epoch: 2 and batch_num: 85\n",
      "Loss of train set: 0.25491049885749817 at epoch: 2 and batch_num: 86\n",
      "Loss of train set: 0.3312990963459015 at epoch: 2 and batch_num: 87\n",
      "Loss of train set: 0.3555353283882141 at epoch: 2 and batch_num: 88\n",
      "Loss of train set: 0.3616288900375366 at epoch: 2 and batch_num: 89\n",
      "Loss of train set: 0.33556056022644043 at epoch: 2 and batch_num: 90\n",
      "Loss of train set: 0.7149333953857422 at epoch: 2 and batch_num: 91\n",
      "Loss of train set: 0.4557737112045288 at epoch: 2 and batch_num: 92\n",
      "Loss of train set: 0.4080161452293396 at epoch: 2 and batch_num: 93\n",
      "Loss of train set: 0.5257150530815125 at epoch: 2 and batch_num: 94\n",
      "Loss of train set: 0.3017615079879761 at epoch: 2 and batch_num: 95\n",
      "Loss of train set: 0.4033188819885254 at epoch: 2 and batch_num: 96\n",
      "Loss of train set: 0.28123655915260315 at epoch: 2 and batch_num: 97\n",
      "Loss of train set: 0.399932324886322 at epoch: 2 and batch_num: 98\n",
      "Loss of train set: 0.4122130870819092 at epoch: 2 and batch_num: 99\n",
      "Loss of train set: 0.2241026759147644 at epoch: 2 and batch_num: 100\n",
      "Loss of train set: 0.35174447298049927 at epoch: 2 and batch_num: 101\n",
      "Loss of train set: 0.25954437255859375 at epoch: 2 and batch_num: 102\n",
      "Loss of train set: 0.21836364269256592 at epoch: 2 and batch_num: 103\n",
      "Loss of train set: 0.32362762093544006 at epoch: 2 and batch_num: 104\n",
      "Loss of train set: 0.2443295419216156 at epoch: 2 and batch_num: 105\n",
      "Loss of train set: 0.4661860466003418 at epoch: 2 and batch_num: 106\n",
      "Loss of train set: 0.4336735010147095 at epoch: 2 and batch_num: 107\n",
      "Loss of train set: 0.5806121230125427 at epoch: 2 and batch_num: 108\n",
      "Loss of train set: 0.41464656591415405 at epoch: 2 and batch_num: 109\n",
      "Loss of train set: 0.3695887327194214 at epoch: 2 and batch_num: 110\n",
      "Loss of train set: 0.3664209246635437 at epoch: 2 and batch_num: 111\n",
      "Loss of train set: 0.342502623796463 at epoch: 2 and batch_num: 112\n",
      "Loss of train set: 0.4521833658218384 at epoch: 2 and batch_num: 113\n",
      "Loss of train set: 0.25700318813323975 at epoch: 2 and batch_num: 114\n",
      "Loss of train set: 0.47557494044303894 at epoch: 2 and batch_num: 115\n",
      "Loss of train set: 0.21112319827079773 at epoch: 2 and batch_num: 116\n",
      "Loss of train set: 0.3002506494522095 at epoch: 2 and batch_num: 117\n",
      "Loss of train set: 0.31756001710891724 at epoch: 2 and batch_num: 118\n",
      "Loss of train set: 0.3373050093650818 at epoch: 2 and batch_num: 119\n",
      "Loss of train set: 0.1885317862033844 at epoch: 2 and batch_num: 120\n",
      "Loss of train set: 0.3682233989238739 at epoch: 2 and batch_num: 121\n",
      "Loss of train set: 0.4514540433883667 at epoch: 2 and batch_num: 122\n",
      "Loss of train set: 0.38798075914382935 at epoch: 2 and batch_num: 123\n",
      "Loss of train set: 0.3295687735080719 at epoch: 2 and batch_num: 124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.44387948513031006 at epoch: 2 and batch_num: 125\n",
      "Loss of train set: 0.2468045949935913 at epoch: 2 and batch_num: 126\n",
      "Loss of train set: 0.4844149947166443 at epoch: 2 and batch_num: 127\n",
      "Loss of train set: 0.5031107068061829 at epoch: 2 and batch_num: 128\n",
      "Loss of train set: 0.45078417658805847 at epoch: 2 and batch_num: 129\n",
      "Loss of train set: 0.3526938855648041 at epoch: 2 and batch_num: 130\n",
      "Loss of train set: 0.27499687671661377 at epoch: 2 and batch_num: 131\n",
      "Loss of train set: 0.21639776229858398 at epoch: 2 and batch_num: 132\n",
      "Loss of train set: 0.35455799102783203 at epoch: 2 and batch_num: 133\n",
      "Loss of train set: 0.27836301922798157 at epoch: 2 and batch_num: 134\n",
      "Loss of train set: 0.4017488658428192 at epoch: 2 and batch_num: 135\n",
      "Loss of train set: 0.29326844215393066 at epoch: 2 and batch_num: 136\n",
      "Loss of train set: 0.4871397912502289 at epoch: 2 and batch_num: 137\n",
      "Loss of train set: 0.4466438293457031 at epoch: 2 and batch_num: 138\n",
      "Loss of train set: 0.19966143369674683 at epoch: 2 and batch_num: 139\n",
      "Loss of train set: 0.2533283829689026 at epoch: 2 and batch_num: 140\n",
      "Loss of train set: 0.33938127756118774 at epoch: 2 and batch_num: 141\n",
      "Loss of train set: 0.45448845624923706 at epoch: 2 and batch_num: 142\n",
      "Loss of train set: 0.5163639187812805 at epoch: 2 and batch_num: 143\n",
      "Loss of train set: 0.22580371797084808 at epoch: 2 and batch_num: 144\n",
      "Loss of train set: 0.17637643218040466 at epoch: 2 and batch_num: 145\n",
      "Loss of train set: 0.37344294786453247 at epoch: 2 and batch_num: 146\n",
      "Loss of train set: 0.48642173409461975 at epoch: 2 and batch_num: 147\n",
      "Loss of train set: 0.22319291532039642 at epoch: 2 and batch_num: 148\n",
      "Loss of train set: 0.32702404260635376 at epoch: 2 and batch_num: 149\n",
      "Loss of train set: 0.24544373154640198 at epoch: 2 and batch_num: 150\n",
      "Loss of train set: 0.25299885869026184 at epoch: 2 and batch_num: 151\n",
      "Loss of train set: 0.3192060589790344 at epoch: 2 and batch_num: 152\n",
      "Loss of train set: 0.25771766901016235 at epoch: 2 and batch_num: 153\n",
      "Loss of train set: 0.30060136318206787 at epoch: 2 and batch_num: 154\n",
      "Loss of train set: 0.4451858103275299 at epoch: 2 and batch_num: 155\n",
      "Loss of train set: 0.3303379416465759 at epoch: 2 and batch_num: 156\n",
      "Loss of train set: 0.7070932388305664 at epoch: 2 and batch_num: 157\n",
      "Loss of train set: 0.4217337965965271 at epoch: 2 and batch_num: 158\n",
      "Loss of train set: 0.18974466621875763 at epoch: 2 and batch_num: 159\n",
      "Loss of train set: 0.21028676629066467 at epoch: 2 and batch_num: 160\n",
      "Loss of train set: 0.378312349319458 at epoch: 2 and batch_num: 161\n",
      "Loss of train set: 0.29802894592285156 at epoch: 2 and batch_num: 162\n",
      "Loss of train set: 0.46049031615257263 at epoch: 2 and batch_num: 163\n",
      "Loss of train set: 0.33882421255111694 at epoch: 2 and batch_num: 164\n",
      "Loss of train set: 0.3634779453277588 at epoch: 2 and batch_num: 165\n",
      "Loss of train set: 0.37799885869026184 at epoch: 2 and batch_num: 166\n",
      "Loss of train set: 0.1749267876148224 at epoch: 2 and batch_num: 167\n",
      "Loss of train set: 0.48269766569137573 at epoch: 2 and batch_num: 168\n",
      "Loss of train set: 0.516732394695282 at epoch: 2 and batch_num: 169\n",
      "Loss of train set: 0.5515785217285156 at epoch: 2 and batch_num: 170\n",
      "Loss of train set: 0.676389217376709 at epoch: 2 and batch_num: 171\n",
      "Loss of train set: 0.28560060262680054 at epoch: 2 and batch_num: 172\n",
      "Loss of train set: 0.374952495098114 at epoch: 2 and batch_num: 173\n",
      "Loss of train set: 0.2747432589530945 at epoch: 2 and batch_num: 174\n",
      "Loss of train set: 0.5121777057647705 at epoch: 2 and batch_num: 175\n",
      "Loss of train set: 0.25521260499954224 at epoch: 2 and batch_num: 176\n",
      "Loss of train set: 0.49482065439224243 at epoch: 2 and batch_num: 177\n",
      "Loss of train set: 0.3187604546546936 at epoch: 2 and batch_num: 178\n",
      "Loss of train set: 0.3406895399093628 at epoch: 2 and batch_num: 179\n",
      "Loss of train set: 0.4405132532119751 at epoch: 2 and batch_num: 180\n",
      "Loss of train set: 0.43172574043273926 at epoch: 2 and batch_num: 181\n",
      "Loss of train set: 0.29382792115211487 at epoch: 2 and batch_num: 182\n",
      "Loss of train set: 0.4634317457675934 at epoch: 2 and batch_num: 183\n",
      "Loss of train set: 0.26731377840042114 at epoch: 2 and batch_num: 184\n",
      "Loss of train set: 0.28174737095832825 at epoch: 2 and batch_num: 185\n",
      "Loss of train set: 0.30488985776901245 at epoch: 2 and batch_num: 186\n",
      "Loss of train set: 0.28166306018829346 at epoch: 2 and batch_num: 187\n",
      "Loss of train set: 0.37449368834495544 at epoch: 2 and batch_num: 188\n",
      "Loss of train set: 0.498075932264328 at epoch: 2 and batch_num: 189\n",
      "Loss of train set: 0.34924525022506714 at epoch: 2 and batch_num: 190\n",
      "Loss of train set: 0.33416885137557983 at epoch: 2 and batch_num: 191\n",
      "Loss of train set: 0.2525809109210968 at epoch: 2 and batch_num: 192\n",
      "Loss of train set: 0.2964448928833008 at epoch: 2 and batch_num: 193\n",
      "Loss of train set: 0.26982808113098145 at epoch: 2 and batch_num: 194\n",
      "Loss of train set: 0.2771250307559967 at epoch: 2 and batch_num: 195\n",
      "Loss of train set: 0.4690607190132141 at epoch: 2 and batch_num: 196\n",
      "Loss of train set: 0.2426627278327942 at epoch: 2 and batch_num: 197\n",
      "Loss of train set: 0.35090047121047974 at epoch: 2 and batch_num: 198\n",
      "Loss of train set: 0.5378620624542236 at epoch: 2 and batch_num: 199\n",
      "Loss of train set: 0.2351820021867752 at epoch: 2 and batch_num: 200\n",
      "Loss of train set: 0.3852594792842865 at epoch: 2 and batch_num: 201\n",
      "Loss of train set: 0.3585822284221649 at epoch: 2 and batch_num: 202\n",
      "Loss of train set: 0.29068243503570557 at epoch: 2 and batch_num: 203\n",
      "Loss of train set: 0.5664838552474976 at epoch: 2 and batch_num: 204\n",
      "Loss of train set: 0.5189833045005798 at epoch: 2 and batch_num: 205\n",
      "Loss of train set: 0.22997337579727173 at epoch: 2 and batch_num: 206\n",
      "Loss of train set: 0.3465609550476074 at epoch: 2 and batch_num: 207\n",
      "Loss of train set: 0.46010318398475647 at epoch: 2 and batch_num: 208\n",
      "Loss of train set: 0.381148099899292 at epoch: 2 and batch_num: 209\n",
      "Loss of train set: 0.2020435631275177 at epoch: 2 and batch_num: 210\n",
      "Loss of train set: 0.24268494546413422 at epoch: 2 and batch_num: 211\n",
      "Loss of train set: 0.43660473823547363 at epoch: 2 and batch_num: 212\n",
      "Loss of train set: 0.31982994079589844 at epoch: 2 and batch_num: 213\n",
      "Loss of train set: 0.44205448031425476 at epoch: 2 and batch_num: 214\n",
      "Loss of train set: 0.27312731742858887 at epoch: 2 and batch_num: 215\n",
      "Loss of train set: 0.281424880027771 at epoch: 2 and batch_num: 216\n",
      "Loss of train set: 0.2264959216117859 at epoch: 2 and batch_num: 217\n",
      "Loss of train set: 0.3392994999885559 at epoch: 2 and batch_num: 218\n",
      "Loss of train set: 0.41849035024642944 at epoch: 2 and batch_num: 219\n",
      "Loss of train set: 0.33334881067276 at epoch: 2 and batch_num: 220\n",
      "Loss of train set: 0.30543336272239685 at epoch: 2 and batch_num: 221\n",
      "Loss of train set: 0.3445403575897217 at epoch: 2 and batch_num: 222\n",
      "Loss of train set: 0.34861063957214355 at epoch: 2 and batch_num: 223\n",
      "Loss of train set: 0.36148181557655334 at epoch: 2 and batch_num: 224\n",
      "Loss of train set: 0.3705303966999054 at epoch: 2 and batch_num: 225\n",
      "Loss of train set: 0.33907005190849304 at epoch: 2 and batch_num: 226\n",
      "Loss of train set: 0.19499197602272034 at epoch: 2 and batch_num: 227\n",
      "Loss of train set: 0.28053128719329834 at epoch: 2 and batch_num: 228\n",
      "Loss of train set: 0.41254347562789917 at epoch: 2 and batch_num: 229\n",
      "Loss of train set: 0.2954212725162506 at epoch: 2 and batch_num: 230\n",
      "Loss of train set: 0.22137653827667236 at epoch: 2 and batch_num: 231\n",
      "Loss of train set: 0.48615527153015137 at epoch: 2 and batch_num: 232\n",
      "Loss of train set: 0.36553123593330383 at epoch: 2 and batch_num: 233\n",
      "Loss of train set: 0.39502424001693726 at epoch: 2 and batch_num: 234\n",
      "Loss of train set: 0.3215738534927368 at epoch: 2 and batch_num: 235\n",
      "Loss of train set: 0.44108033180236816 at epoch: 2 and batch_num: 236\n",
      "Loss of train set: 0.3187759518623352 at epoch: 2 and batch_num: 237\n",
      "Loss of train set: 0.4516335725784302 at epoch: 2 and batch_num: 238\n",
      "Loss of train set: 0.2551552653312683 at epoch: 2 and batch_num: 239\n",
      "Loss of train set: 0.319251149892807 at epoch: 2 and batch_num: 240\n",
      "Loss of train set: 0.5472395420074463 at epoch: 2 and batch_num: 241\n",
      "Loss of train set: 0.5471572875976562 at epoch: 2 and batch_num: 242\n",
      "Loss of train set: 0.3096954822540283 at epoch: 2 and batch_num: 243\n",
      "Loss of train set: 0.43280690908432007 at epoch: 2 and batch_num: 244\n",
      "Loss of train set: 0.2708449065685272 at epoch: 2 and batch_num: 245\n",
      "Loss of train set: 0.26125967502593994 at epoch: 2 and batch_num: 246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.32186996936798096 at epoch: 2 and batch_num: 247\n",
      "Loss of train set: 0.2893880605697632 at epoch: 2 and batch_num: 248\n",
      "Loss of train set: 0.29742351174354553 at epoch: 2 and batch_num: 249\n",
      "Loss of train set: 0.2325161248445511 at epoch: 2 and batch_num: 250\n",
      "Loss of train set: 0.46663814783096313 at epoch: 2 and batch_num: 251\n",
      "Loss of train set: 0.42820122838020325 at epoch: 2 and batch_num: 252\n",
      "Loss of train set: 0.4024653136730194 at epoch: 2 and batch_num: 253\n",
      "Loss of train set: 0.3909338712692261 at epoch: 2 and batch_num: 254\n",
      "Loss of train set: 0.41132915019989014 at epoch: 2 and batch_num: 255\n",
      "Loss of train set: 0.40913325548171997 at epoch: 2 and batch_num: 256\n",
      "Loss of train set: 0.465553343296051 at epoch: 2 and batch_num: 257\n",
      "Loss of train set: 0.41237297654151917 at epoch: 2 and batch_num: 258\n",
      "Loss of train set: 0.34835565090179443 at epoch: 2 and batch_num: 259\n",
      "Loss of train set: 0.3346342146396637 at epoch: 2 and batch_num: 260\n",
      "Loss of train set: 0.20496521890163422 at epoch: 2 and batch_num: 261\n",
      "Loss of train set: 0.24565722048282623 at epoch: 2 and batch_num: 262\n",
      "Loss of train set: 0.3960457146167755 at epoch: 2 and batch_num: 263\n",
      "Loss of train set: 0.30396848917007446 at epoch: 2 and batch_num: 264\n",
      "Loss of train set: 0.27631187438964844 at epoch: 2 and batch_num: 265\n",
      "Loss of train set: 0.4445950388908386 at epoch: 2 and batch_num: 266\n",
      "Loss of train set: 0.3587806820869446 at epoch: 2 and batch_num: 267\n",
      "Loss of train set: 0.3834246098995209 at epoch: 2 and batch_num: 268\n",
      "Loss of train set: 0.34466564655303955 at epoch: 2 and batch_num: 269\n",
      "Loss of train set: 0.32678619027137756 at epoch: 2 and batch_num: 270\n",
      "Loss of train set: 0.4597134590148926 at epoch: 2 and batch_num: 271\n",
      "Loss of train set: 0.323052316904068 at epoch: 2 and batch_num: 272\n",
      "Loss of train set: 0.452761173248291 at epoch: 2 and batch_num: 273\n",
      "Loss of train set: 0.37889033555984497 at epoch: 2 and batch_num: 274\n",
      "Loss of train set: 0.5317433476448059 at epoch: 2 and batch_num: 275\n",
      "Loss of train set: 0.3819125294685364 at epoch: 2 and batch_num: 276\n",
      "Loss of train set: 0.2930200695991516 at epoch: 2 and batch_num: 277\n",
      "Loss of train set: 0.29575181007385254 at epoch: 2 and batch_num: 278\n",
      "Loss of train set: 0.23670069873332977 at epoch: 2 and batch_num: 279\n",
      "Loss of train set: 0.4316841959953308 at epoch: 2 and batch_num: 280\n",
      "Loss of train set: 0.4709797203540802 at epoch: 2 and batch_num: 281\n",
      "Loss of train set: 0.28393232822418213 at epoch: 2 and batch_num: 282\n",
      "Loss of train set: 0.4993976354598999 at epoch: 2 and batch_num: 283\n",
      "Loss of train set: 0.47158724069595337 at epoch: 2 and batch_num: 284\n",
      "Loss of train set: 0.4141966700553894 at epoch: 2 and batch_num: 285\n",
      "Loss of train set: 0.4982302188873291 at epoch: 2 and batch_num: 286\n",
      "Loss of train set: 0.4186035394668579 at epoch: 2 and batch_num: 287\n",
      "Loss of train set: 0.3148593306541443 at epoch: 2 and batch_num: 288\n",
      "Loss of train set: 0.30144721269607544 at epoch: 2 and batch_num: 289\n",
      "Loss of train set: 0.21735578775405884 at epoch: 2 and batch_num: 290\n",
      "Loss of train set: 0.41189706325531006 at epoch: 2 and batch_num: 291\n",
      "Loss of train set: 0.2529293894767761 at epoch: 2 and batch_num: 292\n",
      "Loss of train set: 0.22831086814403534 at epoch: 2 and batch_num: 293\n",
      "Loss of train set: 0.3689876198768616 at epoch: 2 and batch_num: 294\n",
      "Loss of train set: 0.24355491995811462 at epoch: 2 and batch_num: 295\n",
      "Loss of train set: 0.35444551706314087 at epoch: 2 and batch_num: 296\n",
      "Loss of train set: 0.25393810868263245 at epoch: 2 and batch_num: 297\n",
      "Loss of train set: 0.5456552505493164 at epoch: 2 and batch_num: 298\n",
      "Loss of train set: 0.23423877358436584 at epoch: 2 and batch_num: 299\n",
      "Loss of train set: 0.34474605321884155 at epoch: 2 and batch_num: 300\n",
      "Loss of train set: 0.1821831464767456 at epoch: 2 and batch_num: 301\n",
      "Loss of train set: 0.38276052474975586 at epoch: 2 and batch_num: 302\n",
      "Loss of train set: 0.38364923000335693 at epoch: 2 and batch_num: 303\n",
      "Loss of train set: 0.39779508113861084 at epoch: 2 and batch_num: 304\n",
      "Loss of train set: 0.47243696451187134 at epoch: 2 and batch_num: 305\n",
      "Loss of train set: 0.3800649046897888 at epoch: 2 and batch_num: 306\n",
      "Loss of train set: 0.5487054586410522 at epoch: 2 and batch_num: 307\n",
      "Loss of train set: 0.49153733253479004 at epoch: 2 and batch_num: 308\n",
      "Loss of train set: 0.4917580485343933 at epoch: 2 and batch_num: 309\n",
      "Loss of train set: 0.3517121374607086 at epoch: 2 and batch_num: 310\n",
      "Loss of train set: 0.3045501708984375 at epoch: 2 and batch_num: 311\n",
      "Loss of train set: 0.4438040256500244 at epoch: 2 and batch_num: 312\n",
      "Loss of train set: 0.356320858001709 at epoch: 2 and batch_num: 313\n",
      "Loss of train set: 0.27179381251335144 at epoch: 2 and batch_num: 314\n",
      "Loss of train set: 0.32717621326446533 at epoch: 2 and batch_num: 315\n",
      "Loss of train set: 0.5629701614379883 at epoch: 2 and batch_num: 316\n",
      "Loss of train set: 0.4920746684074402 at epoch: 2 and batch_num: 317\n",
      "Loss of train set: 0.3884173035621643 at epoch: 2 and batch_num: 318\n",
      "Loss of train set: 0.3697596788406372 at epoch: 2 and batch_num: 319\n",
      "Loss of train set: 0.3501043915748596 at epoch: 2 and batch_num: 320\n",
      "Loss of train set: 0.20219111442565918 at epoch: 2 and batch_num: 321\n",
      "Loss of train set: 0.42074108123779297 at epoch: 2 and batch_num: 322\n",
      "Loss of train set: 0.35885006189346313 at epoch: 2 and batch_num: 323\n",
      "Loss of train set: 0.3492647409439087 at epoch: 2 and batch_num: 324\n",
      "Loss of train set: 0.5091845989227295 at epoch: 2 and batch_num: 325\n",
      "Loss of train set: 0.21997395157814026 at epoch: 2 and batch_num: 326\n",
      "Loss of train set: 0.3905811905860901 at epoch: 2 and batch_num: 327\n",
      "Loss of train set: 0.3454427421092987 at epoch: 2 and batch_num: 328\n",
      "Loss of train set: 0.5756139755249023 at epoch: 2 and batch_num: 329\n",
      "Loss of train set: 0.41917914152145386 at epoch: 2 and batch_num: 330\n",
      "Loss of train set: 0.3269408643245697 at epoch: 2 and batch_num: 331\n",
      "Loss of train set: 0.5510845184326172 at epoch: 2 and batch_num: 332\n",
      "Loss of train set: 0.3079667389392853 at epoch: 2 and batch_num: 333\n",
      "Loss of train set: 0.3926510214805603 at epoch: 2 and batch_num: 334\n",
      "Loss of train set: 0.26931583881378174 at epoch: 2 and batch_num: 335\n",
      "Loss of train set: 0.4579733610153198 at epoch: 2 and batch_num: 336\n",
      "Loss of train set: 0.36524054408073425 at epoch: 2 and batch_num: 337\n",
      "Loss of train set: 0.4257357120513916 at epoch: 2 and batch_num: 338\n",
      "Loss of train set: 0.3603724539279938 at epoch: 2 and batch_num: 339\n",
      "Loss of train set: 0.3999248147010803 at epoch: 2 and batch_num: 340\n",
      "Loss of train set: 0.48652392625808716 at epoch: 2 and batch_num: 341\n",
      "Loss of train set: 0.5816665887832642 at epoch: 2 and batch_num: 342\n",
      "Loss of train set: 0.5875784158706665 at epoch: 2 and batch_num: 343\n",
      "Loss of train set: 0.35579580068588257 at epoch: 2 and batch_num: 344\n",
      "Loss of train set: 0.2851206064224243 at epoch: 2 and batch_num: 345\n",
      "Loss of train set: 0.22508245706558228 at epoch: 2 and batch_num: 346\n",
      "Loss of train set: 0.34229665994644165 at epoch: 2 and batch_num: 347\n",
      "Loss of train set: 0.5570904612541199 at epoch: 2 and batch_num: 348\n",
      "Loss of train set: 0.5022155046463013 at epoch: 2 and batch_num: 349\n",
      "Loss of train set: 0.3601749539375305 at epoch: 2 and batch_num: 350\n",
      "Loss of train set: 0.29214268922805786 at epoch: 2 and batch_num: 351\n",
      "Loss of train set: 0.29486602544784546 at epoch: 2 and batch_num: 352\n",
      "Loss of train set: 0.26114606857299805 at epoch: 2 and batch_num: 353\n",
      "Loss of train set: 0.5051313638687134 at epoch: 2 and batch_num: 354\n",
      "Loss of train set: 0.5182099342346191 at epoch: 2 and batch_num: 355\n",
      "Loss of train set: 0.41945838928222656 at epoch: 2 and batch_num: 356\n",
      "Loss of train set: 0.510825514793396 at epoch: 2 and batch_num: 357\n",
      "Loss of train set: 0.2558073401451111 at epoch: 2 and batch_num: 358\n",
      "Loss of train set: 0.3976733684539795 at epoch: 2 and batch_num: 359\n",
      "Loss of train set: 0.377967894077301 at epoch: 2 and batch_num: 360\n",
      "Loss of train set: 0.2673393785953522 at epoch: 2 and batch_num: 361\n",
      "Loss of train set: 0.3460092842578888 at epoch: 2 and batch_num: 362\n",
      "Loss of train set: 0.37734338641166687 at epoch: 2 and batch_num: 363\n",
      "Loss of train set: 0.4548978805541992 at epoch: 2 and batch_num: 364\n",
      "Loss of train set: 0.36448240280151367 at epoch: 2 and batch_num: 365\n",
      "Loss of train set: 0.26289135217666626 at epoch: 2 and batch_num: 366\n",
      "Loss of train set: 0.43401819467544556 at epoch: 2 and batch_num: 367\n",
      "Loss of train set: 0.21855658292770386 at epoch: 2 and batch_num: 368\n",
      "Loss of train set: 0.30343255400657654 at epoch: 2 and batch_num: 369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.40669313073158264 at epoch: 2 and batch_num: 370\n",
      "Loss of train set: 0.43974408507347107 at epoch: 2 and batch_num: 371\n",
      "Loss of train set: 0.44485968351364136 at epoch: 2 and batch_num: 372\n",
      "Loss of train set: 0.3453119993209839 at epoch: 2 and batch_num: 373\n",
      "Loss of train set: 0.26810967922210693 at epoch: 2 and batch_num: 374\n",
      "Loss of train set: 0.41543900966644287 at epoch: 2 and batch_num: 375\n",
      "Loss of train set: 0.3931903541088104 at epoch: 2 and batch_num: 376\n",
      "Loss of train set: 0.3159489631652832 at epoch: 2 and batch_num: 377\n",
      "Loss of train set: 0.31426405906677246 at epoch: 2 and batch_num: 378\n",
      "Loss of train set: 0.3848831355571747 at epoch: 2 and batch_num: 379\n",
      "Loss of train set: 0.26700323820114136 at epoch: 2 and batch_num: 380\n",
      "Loss of train set: 0.2672433853149414 at epoch: 2 and batch_num: 381\n",
      "Loss of train set: 0.3421512246131897 at epoch: 2 and batch_num: 382\n",
      "Loss of train set: 0.6306793689727783 at epoch: 2 and batch_num: 383\n",
      "Loss of train set: 0.3180665969848633 at epoch: 2 and batch_num: 384\n",
      "Loss of train set: 0.34698382019996643 at epoch: 2 and batch_num: 385\n",
      "Loss of train set: 0.39729270339012146 at epoch: 2 and batch_num: 386\n",
      "Loss of train set: 0.3054423928260803 at epoch: 2 and batch_num: 387\n",
      "Loss of train set: 0.30882731080055237 at epoch: 2 and batch_num: 388\n",
      "Loss of train set: 0.45687851309776306 at epoch: 2 and batch_num: 389\n",
      "Loss of train set: 0.38675224781036377 at epoch: 2 and batch_num: 390\n",
      "Loss of train set: 0.2668491303920746 at epoch: 2 and batch_num: 391\n",
      "Loss of train set: 0.5413691997528076 at epoch: 2 and batch_num: 392\n",
      "Loss of train set: 0.2849469780921936 at epoch: 2 and batch_num: 393\n",
      "Loss of train set: 0.2483043372631073 at epoch: 2 and batch_num: 394\n",
      "Loss of train set: 0.37343209981918335 at epoch: 2 and batch_num: 395\n",
      "Loss of train set: 0.29406142234802246 at epoch: 2 and batch_num: 396\n",
      "Loss of train set: 0.2931254506111145 at epoch: 2 and batch_num: 397\n",
      "Loss of train set: 0.19449713826179504 at epoch: 2 and batch_num: 398\n",
      "Loss of train set: 0.3212956488132477 at epoch: 2 and batch_num: 399\n",
      "Loss of train set: 0.24072736501693726 at epoch: 2 and batch_num: 400\n",
      "Loss of train set: 0.6333708167076111 at epoch: 2 and batch_num: 401\n",
      "Loss of train set: 0.3133731186389923 at epoch: 2 and batch_num: 402\n",
      "Loss of train set: 0.40665608644485474 at epoch: 2 and batch_num: 403\n",
      "Loss of train set: 0.40615612268447876 at epoch: 2 and batch_num: 404\n",
      "Loss of train set: 0.2609046399593353 at epoch: 2 and batch_num: 405\n",
      "Loss of train set: 0.43849125504493713 at epoch: 2 and batch_num: 406\n",
      "Loss of train set: 0.2834533154964447 at epoch: 2 and batch_num: 407\n",
      "Loss of train set: 0.5273218750953674 at epoch: 2 and batch_num: 408\n",
      "Loss of train set: 0.3627811670303345 at epoch: 2 and batch_num: 409\n",
      "Loss of train set: 0.5494534969329834 at epoch: 2 and batch_num: 410\n",
      "Loss of train set: 0.17412245273590088 at epoch: 2 and batch_num: 411\n",
      "Loss of train set: 0.31232935190200806 at epoch: 2 and batch_num: 412\n",
      "Loss of train set: 0.43035879731178284 at epoch: 2 and batch_num: 413\n",
      "Loss of train set: 0.4126192033290863 at epoch: 2 and batch_num: 414\n",
      "Loss of train set: 0.24596351385116577 at epoch: 2 and batch_num: 415\n",
      "Loss of train set: 0.43087807297706604 at epoch: 2 and batch_num: 416\n",
      "Loss of train set: 0.5738343000411987 at epoch: 2 and batch_num: 417\n",
      "Loss of train set: 0.48231199383735657 at epoch: 2 and batch_num: 418\n",
      "Loss of train set: 0.4432927072048187 at epoch: 2 and batch_num: 419\n",
      "Loss of train set: 0.3822811245918274 at epoch: 2 and batch_num: 420\n",
      "Loss of train set: 0.48581522703170776 at epoch: 2 and batch_num: 421\n",
      "Loss of train set: 0.3708203434944153 at epoch: 2 and batch_num: 422\n",
      "Loss of train set: 0.3563212752342224 at epoch: 2 and batch_num: 423\n",
      "Loss of train set: 0.41792136430740356 at epoch: 2 and batch_num: 424\n",
      "Loss of train set: 0.21226021647453308 at epoch: 2 and batch_num: 425\n",
      "Loss of train set: 0.28681501746177673 at epoch: 2 and batch_num: 426\n",
      "Loss of train set: 0.34593117237091064 at epoch: 2 and batch_num: 427\n",
      "Loss of train set: 0.40137535333633423 at epoch: 2 and batch_num: 428\n",
      "Loss of train set: 0.2584512531757355 at epoch: 2 and batch_num: 429\n",
      "Loss of train set: 0.5730358362197876 at epoch: 2 and batch_num: 430\n",
      "Loss of train set: 0.5382996797561646 at epoch: 2 and batch_num: 431\n",
      "Loss of train set: 0.4239806532859802 at epoch: 2 and batch_num: 432\n",
      "Loss of train set: 0.3239705264568329 at epoch: 2 and batch_num: 433\n",
      "Loss of train set: 0.3580726087093353 at epoch: 2 and batch_num: 434\n",
      "Loss of train set: 0.5629828572273254 at epoch: 2 and batch_num: 435\n",
      "Loss of train set: 0.357898086309433 at epoch: 2 and batch_num: 436\n",
      "Loss of train set: 0.2620466351509094 at epoch: 2 and batch_num: 437\n",
      "Loss of train set: 0.3779778480529785 at epoch: 2 and batch_num: 438\n",
      "Loss of train set: 0.3861715793609619 at epoch: 2 and batch_num: 439\n",
      "Loss of train set: 0.29689401388168335 at epoch: 2 and batch_num: 440\n",
      "Loss of train set: 0.4019060730934143 at epoch: 2 and batch_num: 441\n",
      "Loss of train set: 0.25275519490242004 at epoch: 2 and batch_num: 442\n",
      "Loss of train set: 0.47774237394332886 at epoch: 2 and batch_num: 443\n",
      "Loss of train set: 0.5266096591949463 at epoch: 2 and batch_num: 444\n",
      "Loss of train set: 0.4044654965400696 at epoch: 2 and batch_num: 445\n",
      "Loss of train set: 0.3494284152984619 at epoch: 2 and batch_num: 446\n",
      "Loss of train set: 0.4084136188030243 at epoch: 2 and batch_num: 447\n",
      "Loss of train set: 0.39354151487350464 at epoch: 2 and batch_num: 448\n",
      "Loss of train set: 0.4422733187675476 at epoch: 2 and batch_num: 449\n",
      "Loss of train set: 0.5246812701225281 at epoch: 2 and batch_num: 450\n",
      "Loss of train set: 0.3206738829612732 at epoch: 2 and batch_num: 451\n",
      "Loss of train set: 0.2952868342399597 at epoch: 2 and batch_num: 452\n",
      "Loss of train set: 0.6263527870178223 at epoch: 2 and batch_num: 453\n",
      "Loss of train set: 0.5207326412200928 at epoch: 2 and batch_num: 454\n",
      "Loss of train set: 0.263086199760437 at epoch: 2 and batch_num: 455\n",
      "Loss of train set: 0.5060533285140991 at epoch: 2 and batch_num: 456\n",
      "Loss of train set: 0.40991050004959106 at epoch: 2 and batch_num: 457\n",
      "Loss of train set: 0.33157244324684143 at epoch: 2 and batch_num: 458\n",
      "Loss of train set: 0.31008481979370117 at epoch: 2 and batch_num: 459\n",
      "Loss of train set: 0.28102371096611023 at epoch: 2 and batch_num: 460\n",
      "Loss of train set: 0.5127397775650024 at epoch: 2 and batch_num: 461\n",
      "Loss of train set: 0.32354995608329773 at epoch: 2 and batch_num: 462\n",
      "Loss of train set: 0.18420538306236267 at epoch: 2 and batch_num: 463\n",
      "Loss of train set: 0.35518819093704224 at epoch: 2 and batch_num: 464\n",
      "Loss of train set: 0.33301693201065063 at epoch: 2 and batch_num: 465\n",
      "Loss of train set: 0.44854533672332764 at epoch: 2 and batch_num: 466\n",
      "Loss of train set: 0.36968347430229187 at epoch: 2 and batch_num: 467\n",
      "Loss of train set: 0.37464195489883423 at epoch: 2 and batch_num: 468\n",
      "Loss of train set: 0.3798139691352844 at epoch: 2 and batch_num: 469\n",
      "Loss of train set: 0.17481045424938202 at epoch: 2 and batch_num: 470\n",
      "Loss of train set: 0.44088467955589294 at epoch: 2 and batch_num: 471\n",
      "Loss of train set: 0.23672254383563995 at epoch: 2 and batch_num: 472\n",
      "Loss of train set: 0.3355979323387146 at epoch: 2 and batch_num: 473\n",
      "Loss of train set: 0.31842970848083496 at epoch: 2 and batch_num: 474\n",
      "Loss of train set: 0.44863003492355347 at epoch: 2 and batch_num: 475\n",
      "Loss of train set: 0.512036919593811 at epoch: 2 and batch_num: 476\n",
      "Loss of train set: 0.2842450439929962 at epoch: 2 and batch_num: 477\n",
      "Loss of train set: 0.35527274012565613 at epoch: 2 and batch_num: 478\n",
      "Loss of train set: 0.2247679978609085 at epoch: 2 and batch_num: 479\n",
      "Loss of train set: 0.3540523648262024 at epoch: 2 and batch_num: 480\n",
      "Loss of train set: 0.2782433032989502 at epoch: 2 and batch_num: 481\n",
      "Loss of train set: 0.14239287376403809 at epoch: 2 and batch_num: 482\n",
      "Loss of train set: 0.2782364785671234 at epoch: 2 and batch_num: 483\n",
      "Loss of train set: 0.2745066285133362 at epoch: 2 and batch_num: 484\n",
      "Loss of train set: 0.49477526545524597 at epoch: 2 and batch_num: 485\n",
      "Loss of train set: 0.4357674717903137 at epoch: 2 and batch_num: 486\n",
      "Loss of train set: 0.4904899597167969 at epoch: 2 and batch_num: 487\n",
      "Loss of train set: 0.3366071879863739 at epoch: 2 and batch_num: 488\n",
      "Loss of train set: 0.20252802968025208 at epoch: 2 and batch_num: 489\n",
      "Loss of train set: 0.2719290256500244 at epoch: 2 and batch_num: 490\n",
      "Loss of train set: 0.31500929594039917 at epoch: 2 and batch_num: 491\n",
      "Loss of train set: 0.20083072781562805 at epoch: 2 and batch_num: 492\n",
      "Loss of train set: 0.24627448618412018 at epoch: 2 and batch_num: 493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.21772639453411102 at epoch: 2 and batch_num: 494\n",
      "Loss of train set: 0.29997745156288147 at epoch: 2 and batch_num: 495\n",
      "Loss of train set: 0.2343507558107376 at epoch: 2 and batch_num: 496\n",
      "Loss of train set: 0.31430017948150635 at epoch: 2 and batch_num: 497\n",
      "Loss of train set: 0.44655078649520874 at epoch: 2 and batch_num: 498\n",
      "Loss of train set: 0.29890817403793335 at epoch: 2 and batch_num: 499\n",
      "Loss of train set: 0.31196489930152893 at epoch: 2 and batch_num: 500\n",
      "Loss of train set: 0.25875324010849 at epoch: 2 and batch_num: 501\n",
      "Loss of train set: 0.4387657046318054 at epoch: 2 and batch_num: 502\n",
      "Loss of train set: 0.21789714694023132 at epoch: 2 and batch_num: 503\n",
      "Loss of train set: 0.3863503336906433 at epoch: 2 and batch_num: 504\n",
      "Loss of train set: 0.3710055947303772 at epoch: 2 and batch_num: 505\n",
      "Loss of train set: 0.28372424840927124 at epoch: 2 and batch_num: 506\n",
      "Loss of train set: 0.25108802318573 at epoch: 2 and batch_num: 507\n",
      "Loss of train set: 0.46521228551864624 at epoch: 2 and batch_num: 508\n",
      "Loss of train set: 0.36310791969299316 at epoch: 2 and batch_num: 509\n",
      "Loss of train set: 0.3839064836502075 at epoch: 2 and batch_num: 510\n",
      "Loss of train set: 0.3767848610877991 at epoch: 2 and batch_num: 511\n",
      "Loss of train set: 0.25528469681739807 at epoch: 2 and batch_num: 512\n",
      "Loss of train set: 0.3863632380962372 at epoch: 2 and batch_num: 513\n",
      "Loss of train set: 0.40464410185813904 at epoch: 2 and batch_num: 514\n",
      "Loss of train set: 0.37574338912963867 at epoch: 2 and batch_num: 515\n",
      "Loss of train set: 0.46017199754714966 at epoch: 2 and batch_num: 516\n",
      "Loss of train set: 0.459549218416214 at epoch: 2 and batch_num: 517\n",
      "Loss of train set: 0.2493361234664917 at epoch: 2 and batch_num: 518\n",
      "Loss of train set: 0.4244508147239685 at epoch: 2 and batch_num: 519\n",
      "Loss of train set: 0.4252455234527588 at epoch: 2 and batch_num: 520\n",
      "Loss of train set: 0.3546493947505951 at epoch: 2 and batch_num: 521\n",
      "Loss of train set: 0.35773196816444397 at epoch: 2 and batch_num: 522\n",
      "Loss of train set: 0.3462154269218445 at epoch: 2 and batch_num: 523\n",
      "Loss of train set: 0.4620750844478607 at epoch: 2 and batch_num: 524\n",
      "Loss of train set: 0.44231802225112915 at epoch: 2 and batch_num: 525\n",
      "Loss of train set: 0.369634211063385 at epoch: 2 and batch_num: 526\n",
      "Loss of train set: 0.2619055211544037 at epoch: 2 and batch_num: 527\n",
      "Loss of train set: 0.37502044439315796 at epoch: 2 and batch_num: 528\n",
      "Loss of train set: 0.25628724694252014 at epoch: 2 and batch_num: 529\n",
      "Loss of train set: 0.27642136812210083 at epoch: 2 and batch_num: 530\n",
      "Loss of train set: 0.35549628734588623 at epoch: 2 and batch_num: 531\n",
      "Loss of train set: 0.534083366394043 at epoch: 2 and batch_num: 532\n",
      "Loss of train set: 0.4228367209434509 at epoch: 2 and batch_num: 533\n",
      "Loss of train set: 0.17444771528244019 at epoch: 2 and batch_num: 534\n",
      "Loss of train set: 0.30524319410324097 at epoch: 2 and batch_num: 535\n",
      "Loss of train set: 0.38890042901039124 at epoch: 2 and batch_num: 536\n",
      "Loss of train set: 0.2861725091934204 at epoch: 2 and batch_num: 537\n",
      "Loss of train set: 0.27711015939712524 at epoch: 2 and batch_num: 538\n",
      "Loss of train set: 0.31772738695144653 at epoch: 2 and batch_num: 539\n",
      "Loss of train set: 0.3827893137931824 at epoch: 2 and batch_num: 540\n",
      "Loss of train set: 0.3711073100566864 at epoch: 2 and batch_num: 541\n",
      "Loss of train set: 0.3010903000831604 at epoch: 2 and batch_num: 542\n",
      "Loss of train set: 0.36430782079696655 at epoch: 2 and batch_num: 543\n",
      "Loss of train set: 0.31627142429351807 at epoch: 2 and batch_num: 544\n",
      "Loss of train set: 0.2810521721839905 at epoch: 2 and batch_num: 545\n",
      "Loss of train set: 0.24074561893939972 at epoch: 2 and batch_num: 546\n",
      "Loss of train set: 0.22983017563819885 at epoch: 2 and batch_num: 547\n",
      "Loss of train set: 0.46892961859703064 at epoch: 2 and batch_num: 548\n",
      "Loss of train set: 0.38229674100875854 at epoch: 2 and batch_num: 549\n",
      "Loss of train set: 0.2299014776945114 at epoch: 2 and batch_num: 550\n",
      "Loss of train set: 0.24012941122055054 at epoch: 2 and batch_num: 551\n",
      "Loss of train set: 0.38852059841156006 at epoch: 2 and batch_num: 552\n",
      "Loss of train set: 0.42404669523239136 at epoch: 2 and batch_num: 553\n",
      "Loss of train set: 0.3841511011123657 at epoch: 2 and batch_num: 554\n",
      "Loss of train set: 0.4114762842655182 at epoch: 2 and batch_num: 555\n",
      "Loss of train set: 0.43451976776123047 at epoch: 2 and batch_num: 556\n",
      "Loss of train set: 0.5829576253890991 at epoch: 2 and batch_num: 557\n",
      "Loss of train set: 0.493762731552124 at epoch: 2 and batch_num: 558\n",
      "Loss of train set: 0.2679068446159363 at epoch: 2 and batch_num: 559\n",
      "Loss of train set: 0.2535439431667328 at epoch: 2 and batch_num: 560\n",
      "Loss of train set: 0.23427529633045197 at epoch: 2 and batch_num: 561\n",
      "Loss of train set: 0.2805699110031128 at epoch: 2 and batch_num: 562\n",
      "Loss of train set: 0.22816452383995056 at epoch: 2 and batch_num: 563\n",
      "Loss of train set: 0.2871168553829193 at epoch: 2 and batch_num: 564\n",
      "Loss of train set: 0.29048168659210205 at epoch: 2 and batch_num: 565\n",
      "Loss of train set: 0.2822175920009613 at epoch: 2 and batch_num: 566\n",
      "Loss of train set: 0.2307852804660797 at epoch: 2 and batch_num: 567\n",
      "Loss of train set: 0.30897367000579834 at epoch: 2 and batch_num: 568\n",
      "Loss of train set: 0.4229608178138733 at epoch: 2 and batch_num: 569\n",
      "Loss of train set: 0.40048009157180786 at epoch: 2 and batch_num: 570\n",
      "Loss of train set: 0.3761519193649292 at epoch: 2 and batch_num: 571\n",
      "Loss of train set: 0.3821393847465515 at epoch: 2 and batch_num: 572\n",
      "Loss of train set: 0.2826565206050873 at epoch: 2 and batch_num: 573\n",
      "Loss of train set: 0.3296363949775696 at epoch: 2 and batch_num: 574\n",
      "Loss of train set: 0.2955526113510132 at epoch: 2 and batch_num: 575\n",
      "Loss of train set: 0.2740762233734131 at epoch: 2 and batch_num: 576\n",
      "Loss of train set: 0.4018014073371887 at epoch: 2 and batch_num: 577\n",
      "Loss of train set: 0.31753602623939514 at epoch: 2 and batch_num: 578\n",
      "Loss of train set: 0.4182247519493103 at epoch: 2 and batch_num: 579\n",
      "Loss of train set: 0.3418315052986145 at epoch: 2 and batch_num: 580\n",
      "Loss of train set: 0.23333033919334412 at epoch: 2 and batch_num: 581\n",
      "Loss of train set: 0.36347272992134094 at epoch: 2 and batch_num: 582\n",
      "Loss of train set: 0.4236287474632263 at epoch: 2 and batch_num: 583\n",
      "Loss of train set: 0.43924951553344727 at epoch: 2 and batch_num: 584\n",
      "Loss of train set: 0.305655300617218 at epoch: 2 and batch_num: 585\n",
      "Loss of train set: 0.19439658522605896 at epoch: 2 and batch_num: 586\n",
      "Loss of train set: 0.3720501661300659 at epoch: 2 and batch_num: 587\n",
      "Loss of train set: 0.32949739694595337 at epoch: 2 and batch_num: 588\n",
      "Loss of train set: 0.22158277034759521 at epoch: 2 and batch_num: 589\n",
      "Loss of train set: 0.4298402965068817 at epoch: 2 and batch_num: 590\n",
      "Loss of train set: 0.38115161657333374 at epoch: 2 and batch_num: 591\n",
      "Loss of train set: 0.3792148232460022 at epoch: 2 and batch_num: 592\n",
      "Loss of train set: 0.4023149311542511 at epoch: 2 and batch_num: 593\n",
      "Loss of train set: 0.29184985160827637 at epoch: 2 and batch_num: 594\n",
      "Loss of train set: 0.3862852454185486 at epoch: 2 and batch_num: 595\n",
      "Loss of train set: 0.4682711958885193 at epoch: 2 and batch_num: 596\n",
      "Loss of train set: 0.36886078119277954 at epoch: 2 and batch_num: 597\n",
      "Loss of train set: 0.3703383803367615 at epoch: 2 and batch_num: 598\n",
      "Loss of train set: 0.36658763885498047 at epoch: 2 and batch_num: 599\n",
      "Loss of train set: 0.2859721779823303 at epoch: 2 and batch_num: 600\n",
      "Loss of train set: 0.45432722568511963 at epoch: 2 and batch_num: 601\n",
      "Loss of train set: 0.25744226574897766 at epoch: 2 and batch_num: 602\n",
      "Loss of train set: 0.40347588062286377 at epoch: 2 and batch_num: 603\n",
      "Loss of train set: 0.34190988540649414 at epoch: 2 and batch_num: 604\n",
      "Loss of train set: 0.5064070224761963 at epoch: 2 and batch_num: 605\n",
      "Loss of train set: 0.5576251745223999 at epoch: 2 and batch_num: 606\n",
      "Loss of train set: 0.44714781641960144 at epoch: 2 and batch_num: 607\n",
      "Loss of train set: 0.175362691283226 at epoch: 2 and batch_num: 608\n",
      "Loss of train set: 0.3408767580986023 at epoch: 2 and batch_num: 609\n",
      "Loss of train set: 0.26417773962020874 at epoch: 2 and batch_num: 610\n",
      "Loss of train set: 0.5755957365036011 at epoch: 2 and batch_num: 611\n",
      "Loss of train set: 0.39512187242507935 at epoch: 2 and batch_num: 612\n",
      "Loss of train set: 0.33615943789482117 at epoch: 2 and batch_num: 613\n",
      "Loss of train set: 0.4099671542644501 at epoch: 2 and batch_num: 614\n",
      "Loss of train set: 0.3701968491077423 at epoch: 2 and batch_num: 615\n",
      "Loss of train set: 0.25527703762054443 at epoch: 2 and batch_num: 616\n",
      "Loss of train set: 0.3869679570198059 at epoch: 2 and batch_num: 617\n",
      "Loss of train set: 0.2789011597633362 at epoch: 2 and batch_num: 618\n",
      "Loss of train set: 0.24663421511650085 at epoch: 2 and batch_num: 619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.4305481016635895 at epoch: 2 and batch_num: 620\n",
      "Loss of train set: 0.21755072474479675 at epoch: 2 and batch_num: 621\n",
      "Loss of train set: 0.30302780866622925 at epoch: 2 and batch_num: 622\n",
      "Loss of train set: 0.2729795575141907 at epoch: 2 and batch_num: 623\n",
      "Loss of train set: 0.25775888562202454 at epoch: 2 and batch_num: 624\n",
      "Loss of train set: 0.3749161660671234 at epoch: 2 and batch_num: 625\n",
      "Loss of train set: 0.39281851053237915 at epoch: 2 and batch_num: 626\n",
      "Loss of train set: 0.2920130491256714 at epoch: 2 and batch_num: 627\n",
      "Loss of train set: 0.41537588834762573 at epoch: 2 and batch_num: 628\n",
      "Loss of train set: 0.46259933710098267 at epoch: 2 and batch_num: 629\n",
      "Loss of train set: 0.4907442331314087 at epoch: 2 and batch_num: 630\n",
      "Loss of train set: 0.3882972300052643 at epoch: 2 and batch_num: 631\n",
      "Loss of train set: 0.4343458116054535 at epoch: 2 and batch_num: 632\n",
      "Loss of train set: 0.32555508613586426 at epoch: 2 and batch_num: 633\n",
      "Loss of train set: 0.21753358840942383 at epoch: 2 and batch_num: 634\n",
      "Loss of train set: 0.38196712732315063 at epoch: 2 and batch_num: 635\n",
      "Loss of train set: 0.2910192608833313 at epoch: 2 and batch_num: 636\n",
      "Loss of train set: 0.49926522374153137 at epoch: 2 and batch_num: 637\n",
      "Loss of train set: 0.32658982276916504 at epoch: 2 and batch_num: 638\n",
      "Loss of train set: 0.4036710262298584 at epoch: 2 and batch_num: 639\n",
      "Loss of train set: 0.4652332365512848 at epoch: 2 and batch_num: 640\n",
      "Loss of train set: 0.2916867136955261 at epoch: 2 and batch_num: 641\n",
      "Loss of train set: 0.2510308623313904 at epoch: 2 and batch_num: 642\n",
      "Loss of train set: 0.4016261100769043 at epoch: 2 and batch_num: 643\n",
      "Loss of train set: 0.253065288066864 at epoch: 2 and batch_num: 644\n",
      "Loss of train set: 0.4578225612640381 at epoch: 2 and batch_num: 645\n",
      "Loss of train set: 0.1478617787361145 at epoch: 2 and batch_num: 646\n",
      "Loss of train set: 0.45593878626823425 at epoch: 2 and batch_num: 647\n",
      "Loss of train set: 0.460806667804718 at epoch: 2 and batch_num: 648\n",
      "Loss of train set: 0.33429864048957825 at epoch: 2 and batch_num: 649\n",
      "Loss of train set: 0.2619435787200928 at epoch: 2 and batch_num: 650\n",
      "Loss of train set: 0.3652106821537018 at epoch: 2 and batch_num: 651\n",
      "Loss of train set: 0.5010718107223511 at epoch: 2 and batch_num: 652\n",
      "Loss of train set: 0.31206923723220825 at epoch: 2 and batch_num: 653\n",
      "Loss of train set: 0.3869313597679138 at epoch: 2 and batch_num: 654\n",
      "Loss of train set: 0.30332502722740173 at epoch: 2 and batch_num: 655\n",
      "Loss of train set: 0.4442841708660126 at epoch: 2 and batch_num: 656\n",
      "Loss of train set: 0.4642351269721985 at epoch: 2 and batch_num: 657\n",
      "Loss of train set: 0.2605589032173157 at epoch: 2 and batch_num: 658\n",
      "Loss of train set: 0.37793445587158203 at epoch: 2 and batch_num: 659\n",
      "Loss of train set: 0.3263736963272095 at epoch: 2 and batch_num: 660\n",
      "Loss of train set: 0.39336830377578735 at epoch: 2 and batch_num: 661\n",
      "Loss of train set: 0.3911709189414978 at epoch: 2 and batch_num: 662\n",
      "Loss of train set: 0.3953336477279663 at epoch: 2 and batch_num: 663\n",
      "Loss of train set: 0.3739382028579712 at epoch: 2 and batch_num: 664\n",
      "Loss of train set: 0.29734718799591064 at epoch: 2 and batch_num: 665\n",
      "Loss of train set: 0.3266017436981201 at epoch: 2 and batch_num: 666\n",
      "Loss of train set: 0.4025927484035492 at epoch: 2 and batch_num: 667\n",
      "Loss of train set: 0.22024913132190704 at epoch: 2 and batch_num: 668\n",
      "Loss of train set: 0.47755905985832214 at epoch: 2 and batch_num: 669\n",
      "Loss of train set: 0.14682920277118683 at epoch: 2 and batch_num: 670\n",
      "Loss of train set: 0.3867114186286926 at epoch: 2 and batch_num: 671\n",
      "Loss of train set: 0.4220834970474243 at epoch: 2 and batch_num: 672\n",
      "Loss of train set: 0.3343496322631836 at epoch: 2 and batch_num: 673\n",
      "Loss of train set: 0.294913113117218 at epoch: 2 and batch_num: 674\n",
      "Loss of train set: 0.2952871322631836 at epoch: 2 and batch_num: 675\n",
      "Loss of train set: 0.28113675117492676 at epoch: 2 and batch_num: 676\n",
      "Loss of train set: 0.4294549226760864 at epoch: 2 and batch_num: 677\n",
      "Loss of train set: 0.35092079639434814 at epoch: 2 and batch_num: 678\n",
      "Loss of train set: 0.38870906829833984 at epoch: 2 and batch_num: 679\n",
      "Loss of train set: 0.5220440626144409 at epoch: 2 and batch_num: 680\n",
      "Loss of train set: 0.5476367473602295 at epoch: 2 and batch_num: 681\n",
      "Loss of train set: 0.3751022219657898 at epoch: 2 and batch_num: 682\n",
      "Loss of train set: 0.32987144589424133 at epoch: 2 and batch_num: 683\n",
      "Loss of train set: 0.313558429479599 at epoch: 2 and batch_num: 684\n",
      "Loss of train set: 0.4207686483860016 at epoch: 2 and batch_num: 685\n",
      "Loss of train set: 0.5721074342727661 at epoch: 2 and batch_num: 686\n",
      "Loss of train set: 0.3670993447303772 at epoch: 2 and batch_num: 687\n",
      "Loss of train set: 0.6582168340682983 at epoch: 2 and batch_num: 688\n",
      "Loss of train set: 0.23652908205986023 at epoch: 2 and batch_num: 689\n",
      "Loss of train set: 0.29144516587257385 at epoch: 2 and batch_num: 690\n",
      "Loss of train set: 0.3033638596534729 at epoch: 2 and batch_num: 691\n",
      "Loss of train set: 0.39646488428115845 at epoch: 2 and batch_num: 692\n",
      "Loss of train set: 0.3383522927761078 at epoch: 2 and batch_num: 693\n",
      "Loss of train set: 0.39136838912963867 at epoch: 2 and batch_num: 694\n",
      "Loss of train set: 0.3612785339355469 at epoch: 2 and batch_num: 695\n",
      "Loss of train set: 0.23935160040855408 at epoch: 2 and batch_num: 696\n",
      "Loss of train set: 0.26893723011016846 at epoch: 2 and batch_num: 697\n",
      "Loss of train set: 0.22501423954963684 at epoch: 2 and batch_num: 698\n",
      "Loss of train set: 0.32262754440307617 at epoch: 2 and batch_num: 699\n",
      "Loss of train set: 0.25751399993896484 at epoch: 2 and batch_num: 700\n",
      "Loss of train set: 0.4013574719429016 at epoch: 2 and batch_num: 701\n",
      "Loss of train set: 0.3385605812072754 at epoch: 2 and batch_num: 702\n",
      "Loss of train set: 0.28082501888275146 at epoch: 2 and batch_num: 703\n",
      "Loss of train set: 0.33194512128829956 at epoch: 2 and batch_num: 704\n",
      "Loss of train set: 0.3316335082054138 at epoch: 2 and batch_num: 705\n",
      "Loss of train set: 0.33152976632118225 at epoch: 2 and batch_num: 706\n",
      "Loss of train set: 0.2617875337600708 at epoch: 2 and batch_num: 707\n",
      "Loss of train set: 0.4088181257247925 at epoch: 2 and batch_num: 708\n",
      "Loss of train set: 0.23280148208141327 at epoch: 2 and batch_num: 709\n",
      "Loss of train set: 0.31593507528305054 at epoch: 2 and batch_num: 710\n",
      "Loss of train set: 0.2715744376182556 at epoch: 2 and batch_num: 711\n",
      "Loss of train set: 0.30725201964378357 at epoch: 2 and batch_num: 712\n",
      "Loss of train set: 0.44254422187805176 at epoch: 2 and batch_num: 713\n",
      "Loss of train set: 0.24520711600780487 at epoch: 2 and batch_num: 714\n",
      "Loss of train set: 0.2848171591758728 at epoch: 2 and batch_num: 715\n",
      "Loss of train set: 0.35225656628608704 at epoch: 2 and batch_num: 716\n",
      "Loss of train set: 0.42495304346084595 at epoch: 2 and batch_num: 717\n",
      "Loss of train set: 0.4022008776664734 at epoch: 2 and batch_num: 718\n",
      "Loss of train set: 0.2810334265232086 at epoch: 2 and batch_num: 719\n",
      "Loss of train set: 0.2076936960220337 at epoch: 2 and batch_num: 720\n",
      "Loss of train set: 0.35660994052886963 at epoch: 2 and batch_num: 721\n",
      "Loss of train set: 0.2684457004070282 at epoch: 2 and batch_num: 722\n",
      "Loss of train set: 0.31280460953712463 at epoch: 2 and batch_num: 723\n",
      "Loss of train set: 0.33283525705337524 at epoch: 2 and batch_num: 724\n",
      "Loss of train set: 0.30992186069488525 at epoch: 2 and batch_num: 725\n",
      "Loss of train set: 0.452525794506073 at epoch: 2 and batch_num: 726\n",
      "Loss of train set: 0.5121141076087952 at epoch: 2 and batch_num: 727\n",
      "Loss of train set: 0.6311427354812622 at epoch: 2 and batch_num: 728\n",
      "Loss of train set: 0.3207935094833374 at epoch: 2 and batch_num: 729\n",
      "Loss of train set: 0.30488142371177673 at epoch: 2 and batch_num: 730\n",
      "Loss of train set: 0.5630558729171753 at epoch: 2 and batch_num: 731\n",
      "Loss of train set: 0.34288665652275085 at epoch: 2 and batch_num: 732\n",
      "Loss of train set: 0.47056302428245544 at epoch: 2 and batch_num: 733\n",
      "Loss of train set: 0.35721540451049805 at epoch: 2 and batch_num: 734\n",
      "Loss of train set: 0.2413523644208908 at epoch: 2 and batch_num: 735\n",
      "Loss of train set: 0.3328729271888733 at epoch: 2 and batch_num: 736\n",
      "Loss of train set: 0.4704533815383911 at epoch: 2 and batch_num: 737\n",
      "Loss of train set: 0.39596691727638245 at epoch: 2 and batch_num: 738\n",
      "Loss of train set: 0.43157681822776794 at epoch: 2 and batch_num: 739\n",
      "Loss of train set: 0.48579999804496765 at epoch: 2 and batch_num: 740\n",
      "Loss of train set: 0.36308103799819946 at epoch: 2 and batch_num: 741\n",
      "Loss of train set: 0.38249146938323975 at epoch: 2 and batch_num: 742\n",
      "Loss of train set: 0.5370319485664368 at epoch: 2 and batch_num: 743\n",
      "Loss of train set: 0.2635413408279419 at epoch: 2 and batch_num: 744\n",
      "Loss of train set: 0.24840232729911804 at epoch: 2 and batch_num: 745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2931292653083801 at epoch: 2 and batch_num: 746\n",
      "Loss of train set: 0.30626147985458374 at epoch: 2 and batch_num: 747\n",
      "Loss of train set: 0.3972514271736145 at epoch: 2 and batch_num: 748\n",
      "Loss of train set: 0.4636181890964508 at epoch: 2 and batch_num: 749\n",
      "Loss of train set: 0.30363646149635315 at epoch: 2 and batch_num: 750\n",
      "Loss of train set: 0.4022686779499054 at epoch: 2 and batch_num: 751\n",
      "Loss of train set: 0.4310694932937622 at epoch: 2 and batch_num: 752\n",
      "Loss of train set: 0.2781248092651367 at epoch: 2 and batch_num: 753\n",
      "Loss of train set: 0.4495573043823242 at epoch: 2 and batch_num: 754\n",
      "Loss of train set: 0.4995322525501251 at epoch: 2 and batch_num: 755\n",
      "Loss of train set: 0.24864289164543152 at epoch: 2 and batch_num: 756\n",
      "Loss of train set: 0.5050042271614075 at epoch: 2 and batch_num: 757\n",
      "Loss of train set: 0.5501735210418701 at epoch: 2 and batch_num: 758\n",
      "Loss of train set: 0.3409837782382965 at epoch: 2 and batch_num: 759\n",
      "Loss of train set: 0.38941019773483276 at epoch: 2 and batch_num: 760\n",
      "Loss of train set: 0.311013787984848 at epoch: 2 and batch_num: 761\n",
      "Loss of train set: 0.379849374294281 at epoch: 2 and batch_num: 762\n",
      "Loss of train set: 0.15383830666542053 at epoch: 2 and batch_num: 763\n",
      "Loss of train set: 0.29377734661102295 at epoch: 2 and batch_num: 764\n",
      "Loss of train set: 0.34922200441360474 at epoch: 2 and batch_num: 765\n",
      "Loss of train set: 0.3525810241699219 at epoch: 2 and batch_num: 766\n",
      "Loss of train set: 0.32633912563323975 at epoch: 2 and batch_num: 767\n",
      "Loss of train set: 0.27334681153297424 at epoch: 2 and batch_num: 768\n",
      "Loss of train set: 0.43696892261505127 at epoch: 2 and batch_num: 769\n",
      "Loss of train set: 0.4197753369808197 at epoch: 2 and batch_num: 770\n",
      "Loss of train set: 0.23654361069202423 at epoch: 2 and batch_num: 771\n",
      "Loss of train set: 0.36546558141708374 at epoch: 2 and batch_num: 772\n",
      "Loss of train set: 0.2574991285800934 at epoch: 2 and batch_num: 773\n",
      "Loss of train set: 0.6140602827072144 at epoch: 2 and batch_num: 774\n",
      "Loss of train set: 0.2636451721191406 at epoch: 2 and batch_num: 775\n",
      "Loss of train set: 0.36523953080177307 at epoch: 2 and batch_num: 776\n",
      "Loss of train set: 0.22249379754066467 at epoch: 2 and batch_num: 777\n",
      "Loss of train set: 0.3935791254043579 at epoch: 2 and batch_num: 778\n",
      "Loss of train set: 0.2769632935523987 at epoch: 2 and batch_num: 779\n",
      "Loss of train set: 0.35569682717323303 at epoch: 2 and batch_num: 780\n",
      "Loss of train set: 0.3232494592666626 at epoch: 2 and batch_num: 781\n",
      "Loss of train set: 0.2527221143245697 at epoch: 2 and batch_num: 782\n",
      "Loss of train set: 0.2919051945209503 at epoch: 2 and batch_num: 783\n",
      "Loss of train set: 0.25888651609420776 at epoch: 2 and batch_num: 784\n",
      "Loss of train set: 0.3139408826828003 at epoch: 2 and batch_num: 785\n",
      "Loss of train set: 0.3708028793334961 at epoch: 2 and batch_num: 786\n",
      "Loss of train set: 0.5425907969474792 at epoch: 2 and batch_num: 787\n",
      "Loss of train set: 0.17616203427314758 at epoch: 2 and batch_num: 788\n",
      "Loss of train set: 0.2702329456806183 at epoch: 2 and batch_num: 789\n",
      "Loss of train set: 0.5507476925849915 at epoch: 2 and batch_num: 790\n",
      "Loss of train set: 0.3773370087146759 at epoch: 2 and batch_num: 791\n",
      "Loss of train set: 0.30630192160606384 at epoch: 2 and batch_num: 792\n",
      "Loss of train set: 0.578345000743866 at epoch: 2 and batch_num: 793\n",
      "Loss of train set: 0.34554868936538696 at epoch: 2 and batch_num: 794\n",
      "Loss of train set: 0.3499513268470764 at epoch: 2 and batch_num: 795\n",
      "Loss of train set: 0.3707049489021301 at epoch: 2 and batch_num: 796\n",
      "Loss of train set: 0.2658698558807373 at epoch: 2 and batch_num: 797\n",
      "Loss of train set: 0.34633105993270874 at epoch: 2 and batch_num: 798\n",
      "Loss of train set: 0.34364748001098633 at epoch: 2 and batch_num: 799\n",
      "Loss of train set: 0.24056170880794525 at epoch: 2 and batch_num: 800\n",
      "Loss of train set: 0.3268281817436218 at epoch: 2 and batch_num: 801\n",
      "Loss of train set: 0.5033260583877563 at epoch: 2 and batch_num: 802\n",
      "Loss of train set: 0.286321222782135 at epoch: 2 and batch_num: 803\n",
      "Loss of train set: 0.3876407742500305 at epoch: 2 and batch_num: 804\n",
      "Loss of train set: 0.2554819583892822 at epoch: 2 and batch_num: 805\n",
      "Loss of train set: 0.4783948063850403 at epoch: 2 and batch_num: 806\n",
      "Loss of train set: 0.20774194598197937 at epoch: 2 and batch_num: 807\n",
      "Loss of train set: 0.40554970502853394 at epoch: 2 and batch_num: 808\n",
      "Loss of train set: 0.6394789218902588 at epoch: 2 and batch_num: 809\n",
      "Loss of train set: 0.34750795364379883 at epoch: 2 and batch_num: 810\n",
      "Loss of train set: 0.3917723596096039 at epoch: 2 and batch_num: 811\n",
      "Loss of train set: 0.3595174551010132 at epoch: 2 and batch_num: 812\n",
      "Loss of train set: 0.23072442412376404 at epoch: 2 and batch_num: 813\n",
      "Loss of train set: 0.3194791376590729 at epoch: 2 and batch_num: 814\n",
      "Loss of train set: 0.35099610686302185 at epoch: 2 and batch_num: 815\n",
      "Loss of train set: 0.3580509424209595 at epoch: 2 and batch_num: 816\n",
      "Loss of train set: 0.36043494939804077 at epoch: 2 and batch_num: 817\n",
      "Loss of train set: 0.3135761618614197 at epoch: 2 and batch_num: 818\n",
      "Loss of train set: 0.40676718950271606 at epoch: 2 and batch_num: 819\n",
      "Loss of train set: 0.44160938262939453 at epoch: 2 and batch_num: 820\n",
      "Loss of train set: 0.40261051058769226 at epoch: 2 and batch_num: 821\n",
      "Loss of train set: 0.24379420280456543 at epoch: 2 and batch_num: 822\n",
      "Loss of train set: 0.4501497745513916 at epoch: 2 and batch_num: 823\n",
      "Loss of train set: 0.2591506242752075 at epoch: 2 and batch_num: 824\n",
      "Loss of train set: 0.40884873270988464 at epoch: 2 and batch_num: 825\n",
      "Loss of train set: 0.2826368808746338 at epoch: 2 and batch_num: 826\n",
      "Loss of train set: 0.38973745703697205 at epoch: 2 and batch_num: 827\n",
      "Loss of train set: 0.3258035182952881 at epoch: 2 and batch_num: 828\n",
      "Loss of train set: 0.33456432819366455 at epoch: 2 and batch_num: 829\n",
      "Loss of train set: 0.4917451739311218 at epoch: 2 and batch_num: 830\n",
      "Loss of train set: 0.45178404450416565 at epoch: 2 and batch_num: 831\n",
      "Loss of train set: 0.4211950898170471 at epoch: 2 and batch_num: 832\n",
      "Loss of train set: 0.2983415424823761 at epoch: 2 and batch_num: 833\n",
      "Loss of train set: 0.44477105140686035 at epoch: 2 and batch_num: 834\n",
      "Loss of train set: 0.4177513122558594 at epoch: 2 and batch_num: 835\n",
      "Loss of train set: 0.15696093440055847 at epoch: 2 and batch_num: 836\n",
      "Loss of train set: 0.3041122257709503 at epoch: 2 and batch_num: 837\n",
      "Loss of train set: 0.37351784110069275 at epoch: 2 and batch_num: 838\n",
      "Loss of train set: 0.3631170094013214 at epoch: 2 and batch_num: 839\n",
      "Loss of train set: 0.45887529850006104 at epoch: 2 and batch_num: 840\n",
      "Loss of train set: 0.4256901741027832 at epoch: 2 and batch_num: 841\n",
      "Loss of train set: 0.5207033157348633 at epoch: 2 and batch_num: 842\n",
      "Loss of train set: 0.4922494888305664 at epoch: 2 and batch_num: 843\n",
      "Loss of train set: 0.5773651003837585 at epoch: 2 and batch_num: 844\n",
      "Loss of train set: 0.301154226064682 at epoch: 2 and batch_num: 845\n",
      "Loss of train set: 0.25954437255859375 at epoch: 2 and batch_num: 846\n",
      "Loss of train set: 0.47650009393692017 at epoch: 2 and batch_num: 847\n",
      "Loss of train set: 0.3549785614013672 at epoch: 2 and batch_num: 848\n",
      "Loss of train set: 0.3251802921295166 at epoch: 2 and batch_num: 849\n",
      "Loss of train set: 0.2491074502468109 at epoch: 2 and batch_num: 850\n",
      "Loss of train set: 0.3150795102119446 at epoch: 2 and batch_num: 851\n",
      "Loss of train set: 0.3108236789703369 at epoch: 2 and batch_num: 852\n",
      "Loss of train set: 0.41031819581985474 at epoch: 2 and batch_num: 853\n",
      "Loss of train set: 0.26197588443756104 at epoch: 2 and batch_num: 854\n",
      "Loss of train set: 0.3269997239112854 at epoch: 2 and batch_num: 855\n",
      "Loss of train set: 0.23584863543510437 at epoch: 2 and batch_num: 856\n",
      "Loss of train set: 0.3167514204978943 at epoch: 2 and batch_num: 857\n",
      "Loss of train set: 0.5155057311058044 at epoch: 2 and batch_num: 858\n",
      "Loss of train set: 0.48495933413505554 at epoch: 2 and batch_num: 859\n",
      "Loss of train set: 0.3495382070541382 at epoch: 2 and batch_num: 860\n",
      "Loss of train set: 0.4031103253364563 at epoch: 2 and batch_num: 861\n",
      "Loss of train set: 0.3709569573402405 at epoch: 2 and batch_num: 862\n",
      "Loss of train set: 0.6405648589134216 at epoch: 2 and batch_num: 863\n",
      "Loss of train set: 0.44821837544441223 at epoch: 2 and batch_num: 864\n",
      "Loss of train set: 0.3166660666465759 at epoch: 2 and batch_num: 865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.25223568081855774 at epoch: 2 and batch_num: 866\n",
      "Loss of train set: 0.4905925393104553 at epoch: 2 and batch_num: 867\n",
      "Loss of train set: 0.3699924349784851 at epoch: 2 and batch_num: 868\n",
      "Loss of train set: 0.2758708596229553 at epoch: 2 and batch_num: 869\n",
      "Loss of train set: 0.42891839146614075 at epoch: 2 and batch_num: 870\n",
      "Loss of train set: 0.3891327977180481 at epoch: 2 and batch_num: 871\n",
      "Loss of train set: 0.36850959062576294 at epoch: 2 and batch_num: 872\n",
      "Loss of train set: 0.592562735080719 at epoch: 2 and batch_num: 873\n",
      "Loss of train set: 0.3886052668094635 at epoch: 2 and batch_num: 874\n",
      "Loss of train set: 0.3593737483024597 at epoch: 2 and batch_num: 875\n",
      "Loss of train set: 0.5642231702804565 at epoch: 2 and batch_num: 876\n",
      "Loss of train set: 0.5907999277114868 at epoch: 2 and batch_num: 877\n",
      "Loss of train set: 0.30039799213409424 at epoch: 2 and batch_num: 878\n",
      "Loss of train set: 0.34299176931381226 at epoch: 2 and batch_num: 879\n",
      "Loss of train set: 0.3363844156265259 at epoch: 2 and batch_num: 880\n",
      "Loss of train set: 0.3587791919708252 at epoch: 2 and batch_num: 881\n",
      "Loss of train set: 0.2754000425338745 at epoch: 2 and batch_num: 882\n",
      "Loss of train set: 0.4945337772369385 at epoch: 2 and batch_num: 883\n",
      "Loss of train set: 0.35404616594314575 at epoch: 2 and batch_num: 884\n",
      "Loss of train set: 0.23937495052814484 at epoch: 2 and batch_num: 885\n",
      "Loss of train set: 0.33602046966552734 at epoch: 2 and batch_num: 886\n",
      "Loss of train set: 0.4825708568096161 at epoch: 2 and batch_num: 887\n",
      "Loss of train set: 0.4002839922904968 at epoch: 2 and batch_num: 888\n",
      "Loss of train set: 0.36028385162353516 at epoch: 2 and batch_num: 889\n",
      "Loss of train set: 0.49266475439071655 at epoch: 2 and batch_num: 890\n",
      "Loss of train set: 0.5558998584747314 at epoch: 2 and batch_num: 891\n",
      "Loss of train set: 0.304610013961792 at epoch: 2 and batch_num: 892\n",
      "Loss of train set: 0.26487651467323303 at epoch: 2 and batch_num: 893\n",
      "Loss of train set: 0.2005620300769806 at epoch: 2 and batch_num: 894\n",
      "Loss of train set: 0.2196313738822937 at epoch: 2 and batch_num: 895\n",
      "Loss of train set: 0.29299965500831604 at epoch: 2 and batch_num: 896\n",
      "Loss of train set: 0.27687007188796997 at epoch: 2 and batch_num: 897\n",
      "Loss of train set: 0.3200666904449463 at epoch: 2 and batch_num: 898\n",
      "Loss of train set: 0.4036314785480499 at epoch: 2 and batch_num: 899\n",
      "Loss of train set: 0.3099866807460785 at epoch: 2 and batch_num: 900\n",
      "Loss of train set: 0.5935050249099731 at epoch: 2 and batch_num: 901\n",
      "Loss of train set: 0.28635480999946594 at epoch: 2 and batch_num: 902\n",
      "Loss of train set: 0.28585904836654663 at epoch: 2 and batch_num: 903\n",
      "Loss of train set: 0.4583987891674042 at epoch: 2 and batch_num: 904\n",
      "Loss of train set: 0.23681876063346863 at epoch: 2 and batch_num: 905\n",
      "Loss of train set: 0.4279901385307312 at epoch: 2 and batch_num: 906\n",
      "Loss of train set: 0.33607810735702515 at epoch: 2 and batch_num: 907\n",
      "Loss of train set: 0.3036346137523651 at epoch: 2 and batch_num: 908\n",
      "Loss of train set: 0.3707481324672699 at epoch: 2 and batch_num: 909\n",
      "Loss of train set: 0.4560156464576721 at epoch: 2 and batch_num: 910\n",
      "Loss of train set: 0.3488476276397705 at epoch: 2 and batch_num: 911\n",
      "Loss of train set: 0.32674920558929443 at epoch: 2 and batch_num: 912\n",
      "Loss of train set: 0.32376646995544434 at epoch: 2 and batch_num: 913\n",
      "Loss of train set: 0.584662675857544 at epoch: 2 and batch_num: 914\n",
      "Loss of train set: 0.31866371631622314 at epoch: 2 and batch_num: 915\n",
      "Loss of train set: 0.2885977029800415 at epoch: 2 and batch_num: 916\n",
      "Loss of train set: 0.34015417098999023 at epoch: 2 and batch_num: 917\n",
      "Loss of train set: 0.5024521350860596 at epoch: 2 and batch_num: 918\n",
      "Loss of train set: 0.49041542410850525 at epoch: 2 and batch_num: 919\n",
      "Loss of train set: 0.3547017276287079 at epoch: 2 and batch_num: 920\n",
      "Loss of train set: 0.37317973375320435 at epoch: 2 and batch_num: 921\n",
      "Loss of train set: 0.3014988601207733 at epoch: 2 and batch_num: 922\n",
      "Loss of train set: 0.3540048599243164 at epoch: 2 and batch_num: 923\n",
      "Loss of train set: 0.35410743951797485 at epoch: 2 and batch_num: 924\n",
      "Loss of train set: 0.52897047996521 at epoch: 2 and batch_num: 925\n",
      "Loss of train set: 0.437073290348053 at epoch: 2 and batch_num: 926\n",
      "Loss of train set: 0.3854485750198364 at epoch: 2 and batch_num: 927\n",
      "Loss of train set: 0.4511011838912964 at epoch: 2 and batch_num: 928\n",
      "Loss of train set: 0.40423595905303955 at epoch: 2 and batch_num: 929\n",
      "Loss of train set: 0.4022060036659241 at epoch: 2 and batch_num: 930\n",
      "Loss of train set: 0.39589405059814453 at epoch: 2 and batch_num: 931\n",
      "Loss of train set: 0.3168811798095703 at epoch: 2 and batch_num: 932\n",
      "Loss of train set: 0.3974825441837311 at epoch: 2 and batch_num: 933\n",
      "Loss of train set: 0.31691214442253113 at epoch: 2 and batch_num: 934\n",
      "Loss of train set: 0.39752617478370667 at epoch: 2 and batch_num: 935\n",
      "Loss of train set: 0.4325703978538513 at epoch: 2 and batch_num: 936\n",
      "Loss of train set: 0.5129703283309937 at epoch: 2 and batch_num: 937\n",
      "Accuracy of train set: 0.8699166666666667\n",
      "Loss of test set: 0.3393460512161255 at epoch: 2 and batch_num: 0\n",
      "Loss of test set: 0.38281741738319397 at epoch: 2 and batch_num: 1\n",
      "Loss of test set: 0.37928110361099243 at epoch: 2 and batch_num: 2\n",
      "Loss of test set: 0.29760465025901794 at epoch: 2 and batch_num: 3\n",
      "Loss of test set: 0.42637211084365845 at epoch: 2 and batch_num: 4\n",
      "Loss of test set: 0.39184683561325073 at epoch: 2 and batch_num: 5\n",
      "Loss of test set: 0.5178796052932739 at epoch: 2 and batch_num: 6\n",
      "Loss of test set: 0.2844412624835968 at epoch: 2 and batch_num: 7\n",
      "Loss of test set: 0.3657284677028656 at epoch: 2 and batch_num: 8\n",
      "Loss of test set: 0.34397533535957336 at epoch: 2 and batch_num: 9\n",
      "Loss of test set: 0.40358245372772217 at epoch: 2 and batch_num: 10\n",
      "Loss of test set: 0.45557913184165955 at epoch: 2 and batch_num: 11\n",
      "Loss of test set: 0.3653556704521179 at epoch: 2 and batch_num: 12\n",
      "Loss of test set: 0.4372944235801697 at epoch: 2 and batch_num: 13\n",
      "Loss of test set: 0.4893810749053955 at epoch: 2 and batch_num: 14\n",
      "Loss of test set: 0.23951290547847748 at epoch: 2 and batch_num: 15\n",
      "Loss of test set: 0.34299835562705994 at epoch: 2 and batch_num: 16\n",
      "Loss of test set: 0.5489249229431152 at epoch: 2 and batch_num: 17\n",
      "Loss of test set: 0.44977906346321106 at epoch: 2 and batch_num: 18\n",
      "Loss of test set: 0.4730796217918396 at epoch: 2 and batch_num: 19\n",
      "Loss of test set: 0.35489487648010254 at epoch: 2 and batch_num: 20\n",
      "Loss of test set: 0.4166513681411743 at epoch: 2 and batch_num: 21\n",
      "Loss of test set: 0.4200740456581116 at epoch: 2 and batch_num: 22\n",
      "Loss of test set: 0.35232558846473694 at epoch: 2 and batch_num: 23\n",
      "Loss of test set: 0.27081817388534546 at epoch: 2 and batch_num: 24\n",
      "Loss of test set: 0.43610480427742004 at epoch: 2 and batch_num: 25\n",
      "Loss of test set: 0.3906051516532898 at epoch: 2 and batch_num: 26\n",
      "Loss of test set: 0.36905980110168457 at epoch: 2 and batch_num: 27\n",
      "Loss of test set: 0.5326002836227417 at epoch: 2 and batch_num: 28\n",
      "Loss of test set: 0.39999914169311523 at epoch: 2 and batch_num: 29\n",
      "Loss of test set: 0.3160170912742615 at epoch: 2 and batch_num: 30\n",
      "Loss of test set: 0.5468142032623291 at epoch: 2 and batch_num: 31\n",
      "Loss of test set: 0.37078529596328735 at epoch: 2 and batch_num: 32\n",
      "Loss of test set: 0.30136704444885254 at epoch: 2 and batch_num: 33\n",
      "Loss of test set: 0.3086603879928589 at epoch: 2 and batch_num: 34\n",
      "Loss of test set: 0.2866491675376892 at epoch: 2 and batch_num: 35\n",
      "Loss of test set: 0.37272757291793823 at epoch: 2 and batch_num: 36\n",
      "Loss of test set: 0.47109299898147583 at epoch: 2 and batch_num: 37\n",
      "Loss of test set: 0.4397309124469757 at epoch: 2 and batch_num: 38\n",
      "Loss of test set: 0.435508668422699 at epoch: 2 and batch_num: 39\n",
      "Loss of test set: 0.49448540806770325 at epoch: 2 and batch_num: 40\n",
      "Loss of test set: 0.527521014213562 at epoch: 2 and batch_num: 41\n",
      "Loss of test set: 0.38661831617355347 at epoch: 2 and batch_num: 42\n",
      "Loss of test set: 0.5738844275474548 at epoch: 2 and batch_num: 43\n",
      "Loss of test set: 0.40765219926834106 at epoch: 2 and batch_num: 44\n",
      "Loss of test set: 0.4650390148162842 at epoch: 2 and batch_num: 45\n",
      "Loss of test set: 0.3278011977672577 at epoch: 2 and batch_num: 46\n",
      "Loss of test set: 0.4305567741394043 at epoch: 2 and batch_num: 47\n",
      "Loss of test set: 0.2691188454627991 at epoch: 2 and batch_num: 48\n",
      "Loss of test set: 0.30817365646362305 at epoch: 2 and batch_num: 49\n",
      "Loss of test set: 0.3372536897659302 at epoch: 2 and batch_num: 50\n",
      "Loss of test set: 0.5200004577636719 at epoch: 2 and batch_num: 51\n",
      "Loss of test set: 0.6399787664413452 at epoch: 2 and batch_num: 52\n",
      "Loss of test set: 0.24480432271957397 at epoch: 2 and batch_num: 53\n",
      "Loss of test set: 0.469470351934433 at epoch: 2 and batch_num: 54\n",
      "Loss of test set: 0.3809775710105896 at epoch: 2 and batch_num: 55\n",
      "Loss of test set: 0.4236428439617157 at epoch: 2 and batch_num: 56\n",
      "Loss of test set: 0.6174341440200806 at epoch: 2 and batch_num: 57\n",
      "Loss of test set: 0.6026962995529175 at epoch: 2 and batch_num: 58\n",
      "Loss of test set: 0.5458599925041199 at epoch: 2 and batch_num: 59\n",
      "Loss of test set: 0.3985899090766907 at epoch: 2 and batch_num: 60\n",
      "Loss of test set: 0.4771113991737366 at epoch: 2 and batch_num: 61\n",
      "Loss of test set: 0.7226766347885132 at epoch: 2 and batch_num: 62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.3804761469364166 at epoch: 2 and batch_num: 63\n",
      "Loss of test set: 0.4637799859046936 at epoch: 2 and batch_num: 64\n",
      "Loss of test set: 0.4055601954460144 at epoch: 2 and batch_num: 65\n",
      "Loss of test set: 0.3964419662952423 at epoch: 2 and batch_num: 66\n",
      "Loss of test set: 0.3676512837409973 at epoch: 2 and batch_num: 67\n",
      "Loss of test set: 0.3583683967590332 at epoch: 2 and batch_num: 68\n",
      "Loss of test set: 0.5128660202026367 at epoch: 2 and batch_num: 69\n",
      "Loss of test set: 0.32388585805892944 at epoch: 2 and batch_num: 70\n",
      "Loss of test set: 0.49348169565200806 at epoch: 2 and batch_num: 71\n",
      "Loss of test set: 0.3155672252178192 at epoch: 2 and batch_num: 72\n",
      "Loss of test set: 0.28992825746536255 at epoch: 2 and batch_num: 73\n",
      "Loss of test set: 0.49606001377105713 at epoch: 2 and batch_num: 74\n",
      "Loss of test set: 0.43314486742019653 at epoch: 2 and batch_num: 75\n",
      "Loss of test set: 0.4554835259914398 at epoch: 2 and batch_num: 76\n",
      "Loss of test set: 0.5158458948135376 at epoch: 2 and batch_num: 77\n",
      "Loss of test set: 0.5774874687194824 at epoch: 2 and batch_num: 78\n",
      "Loss of test set: 0.4516260623931885 at epoch: 2 and batch_num: 79\n",
      "Loss of test set: 0.5224697589874268 at epoch: 2 and batch_num: 80\n",
      "Loss of test set: 0.5433142185211182 at epoch: 2 and batch_num: 81\n",
      "Loss of test set: 0.44145894050598145 at epoch: 2 and batch_num: 82\n",
      "Loss of test set: 0.35332685708999634 at epoch: 2 and batch_num: 83\n",
      "Loss of test set: 0.2684926986694336 at epoch: 2 and batch_num: 84\n",
      "Loss of test set: 0.3947063088417053 at epoch: 2 and batch_num: 85\n",
      "Loss of test set: 0.8017551302909851 at epoch: 2 and batch_num: 86\n",
      "Loss of test set: 0.2437669336795807 at epoch: 2 and batch_num: 87\n",
      "Loss of test set: 0.6724351644515991 at epoch: 2 and batch_num: 88\n",
      "Loss of test set: 0.39521530270576477 at epoch: 2 and batch_num: 89\n",
      "Loss of test set: 0.48400765657424927 at epoch: 2 and batch_num: 90\n",
      "Loss of test set: 0.3620646595954895 at epoch: 2 and batch_num: 91\n",
      "Loss of test set: 0.36239302158355713 at epoch: 2 and batch_num: 92\n",
      "Loss of test set: 0.5938976407051086 at epoch: 2 and batch_num: 93\n",
      "Loss of test set: 0.422802209854126 at epoch: 2 and batch_num: 94\n",
      "Loss of test set: 0.27894797921180725 at epoch: 2 and batch_num: 95\n",
      "Loss of test set: 0.3178064823150635 at epoch: 2 and batch_num: 96\n",
      "Loss of test set: 0.3670547902584076 at epoch: 2 and batch_num: 97\n",
      "Loss of test set: 0.3076131343841553 at epoch: 2 and batch_num: 98\n",
      "Loss of test set: 0.36281657218933105 at epoch: 2 and batch_num: 99\n",
      "Loss of test set: 0.26523682475090027 at epoch: 2 and batch_num: 100\n",
      "Loss of test set: 0.43079158663749695 at epoch: 2 and batch_num: 101\n",
      "Loss of test set: 0.3167250454425812 at epoch: 2 and batch_num: 102\n",
      "Loss of test set: 0.2365514039993286 at epoch: 2 and batch_num: 103\n",
      "Loss of test set: 0.5161920785903931 at epoch: 2 and batch_num: 104\n",
      "Loss of test set: 0.37949419021606445 at epoch: 2 and batch_num: 105\n",
      "Loss of test set: 0.4848254919052124 at epoch: 2 and batch_num: 106\n",
      "Loss of test set: 0.3499016761779785 at epoch: 2 and batch_num: 107\n",
      "Loss of test set: 0.4592301845550537 at epoch: 2 and batch_num: 108\n",
      "Loss of test set: 0.40793561935424805 at epoch: 2 and batch_num: 109\n",
      "Loss of test set: 0.3779250979423523 at epoch: 2 and batch_num: 110\n",
      "Loss of test set: 0.2668623924255371 at epoch: 2 and batch_num: 111\n",
      "Loss of test set: 0.37407153844833374 at epoch: 2 and batch_num: 112\n",
      "Loss of test set: 0.3555852770805359 at epoch: 2 and batch_num: 113\n",
      "Loss of test set: 0.3192445635795593 at epoch: 2 and batch_num: 114\n",
      "Loss of test set: 0.23126712441444397 at epoch: 2 and batch_num: 115\n",
      "Loss of test set: 0.42172878980636597 at epoch: 2 and batch_num: 116\n",
      "Loss of test set: 0.4406725764274597 at epoch: 2 and batch_num: 117\n",
      "Loss of test set: 0.4006049633026123 at epoch: 2 and batch_num: 118\n",
      "Loss of test set: 0.4289442300796509 at epoch: 2 and batch_num: 119\n",
      "Loss of test set: 0.36504065990448 at epoch: 2 and batch_num: 120\n",
      "Loss of test set: 0.4117077589035034 at epoch: 2 and batch_num: 121\n",
      "Loss of test set: 0.6570001840591431 at epoch: 2 and batch_num: 122\n",
      "Loss of test set: 0.5958456993103027 at epoch: 2 and batch_num: 123\n",
      "Loss of test set: 0.18096716701984406 at epoch: 2 and batch_num: 124\n",
      "Loss of test set: 0.27172911167144775 at epoch: 2 and batch_num: 125\n",
      "Loss of test set: 0.4292411804199219 at epoch: 2 and batch_num: 126\n",
      "Loss of test set: 0.30984175205230713 at epoch: 2 and batch_num: 127\n",
      "Loss of test set: 0.4048464596271515 at epoch: 2 and batch_num: 128\n",
      "Loss of test set: 0.2734445631504059 at epoch: 2 and batch_num: 129\n",
      "Loss of test set: 0.4439866244792938 at epoch: 2 and batch_num: 130\n",
      "Loss of test set: 0.5544501543045044 at epoch: 2 and batch_num: 131\n",
      "Loss of test set: 0.39925575256347656 at epoch: 2 and batch_num: 132\n",
      "Loss of test set: 0.4842459559440613 at epoch: 2 and batch_num: 133\n",
      "Loss of test set: 0.2602054476737976 at epoch: 2 and batch_num: 134\n",
      "Loss of test set: 0.6533219218254089 at epoch: 2 and batch_num: 135\n",
      "Loss of test set: 0.3996363580226898 at epoch: 2 and batch_num: 136\n",
      "Loss of test set: 0.27224695682525635 at epoch: 2 and batch_num: 137\n",
      "Loss of test set: 0.2483547031879425 at epoch: 2 and batch_num: 138\n",
      "Loss of test set: 0.3564947545528412 at epoch: 2 and batch_num: 139\n",
      "Loss of test set: 0.3321743607521057 at epoch: 2 and batch_num: 140\n",
      "Loss of test set: 0.4344785809516907 at epoch: 2 and batch_num: 141\n",
      "Loss of test set: 0.2654416859149933 at epoch: 2 and batch_num: 142\n",
      "Loss of test set: 0.3281617760658264 at epoch: 2 and batch_num: 143\n",
      "Loss of test set: 0.4620533287525177 at epoch: 2 and batch_num: 144\n",
      "Loss of test set: 0.3485015630722046 at epoch: 2 and batch_num: 145\n",
      "Loss of test set: 0.4804167151451111 at epoch: 2 and batch_num: 146\n",
      "Loss of test set: 0.39121493697166443 at epoch: 2 and batch_num: 147\n",
      "Loss of test set: 0.3551621437072754 at epoch: 2 and batch_num: 148\n",
      "Loss of test set: 0.27365654706954956 at epoch: 2 and batch_num: 149\n",
      "Loss of test set: 0.29066160321235657 at epoch: 2 and batch_num: 150\n",
      "Loss of test set: 0.5032922029495239 at epoch: 2 and batch_num: 151\n",
      "Loss of test set: 0.33968448638916016 at epoch: 2 and batch_num: 152\n",
      "Loss of test set: 0.29876381158828735 at epoch: 2 and batch_num: 153\n",
      "Loss of test set: 0.405214786529541 at epoch: 2 and batch_num: 154\n",
      "Loss of test set: 0.49146199226379395 at epoch: 2 and batch_num: 155\n",
      "Loss of test set: 0.3594614267349243 at epoch: 2 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8572\n",
      "Loss of train set: 0.23921766877174377 at epoch: 3 and batch_num: 0\n",
      "Loss of train set: 0.5283249020576477 at epoch: 3 and batch_num: 1\n",
      "Loss of train set: 0.30857568979263306 at epoch: 3 and batch_num: 2\n",
      "Loss of train set: 0.3144144117832184 at epoch: 3 and batch_num: 3\n",
      "Loss of train set: 0.40094682574272156 at epoch: 3 and batch_num: 4\n",
      "Loss of train set: 0.2675991654396057 at epoch: 3 and batch_num: 5\n",
      "Loss of train set: 0.4667203724384308 at epoch: 3 and batch_num: 6\n",
      "Loss of train set: 0.2746664881706238 at epoch: 3 and batch_num: 7\n",
      "Loss of train set: 0.5270763039588928 at epoch: 3 and batch_num: 8\n",
      "Loss of train set: 0.30003446340560913 at epoch: 3 and batch_num: 9\n",
      "Loss of train set: 0.33165135979652405 at epoch: 3 and batch_num: 10\n",
      "Loss of train set: 0.4492678940296173 at epoch: 3 and batch_num: 11\n",
      "Loss of train set: 0.4791732430458069 at epoch: 3 and batch_num: 12\n",
      "Loss of train set: 0.2020128071308136 at epoch: 3 and batch_num: 13\n",
      "Loss of train set: 0.33103668689727783 at epoch: 3 and batch_num: 14\n",
      "Loss of train set: 0.370593786239624 at epoch: 3 and batch_num: 15\n",
      "Loss of train set: 0.3967777490615845 at epoch: 3 and batch_num: 16\n",
      "Loss of train set: 0.2548972964286804 at epoch: 3 and batch_num: 17\n",
      "Loss of train set: 0.42273518443107605 at epoch: 3 and batch_num: 18\n",
      "Loss of train set: 0.3839617669582367 at epoch: 3 and batch_num: 19\n",
      "Loss of train set: 0.4664478600025177 at epoch: 3 and batch_num: 20\n",
      "Loss of train set: 0.2348642349243164 at epoch: 3 and batch_num: 21\n",
      "Loss of train set: 0.3225494623184204 at epoch: 3 and batch_num: 22\n",
      "Loss of train set: 0.39893075823783875 at epoch: 3 and batch_num: 23\n",
      "Loss of train set: 0.4490317702293396 at epoch: 3 and batch_num: 24\n",
      "Loss of train set: 0.45959311723709106 at epoch: 3 and batch_num: 25\n",
      "Loss of train set: 0.30159056186676025 at epoch: 3 and batch_num: 26\n",
      "Loss of train set: 0.361289918422699 at epoch: 3 and batch_num: 27\n",
      "Loss of train set: 0.25562363862991333 at epoch: 3 and batch_num: 28\n",
      "Loss of train set: 0.28047049045562744 at epoch: 3 and batch_num: 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.47403115034103394 at epoch: 3 and batch_num: 30\n",
      "Loss of train set: 0.2418273538351059 at epoch: 3 and batch_num: 31\n",
      "Loss of train set: 0.42834562063217163 at epoch: 3 and batch_num: 32\n",
      "Loss of train set: 0.45934396982192993 at epoch: 3 and batch_num: 33\n",
      "Loss of train set: 0.24437546730041504 at epoch: 3 and batch_num: 34\n",
      "Loss of train set: 0.40084195137023926 at epoch: 3 and batch_num: 35\n",
      "Loss of train set: 0.37992995977401733 at epoch: 3 and batch_num: 36\n",
      "Loss of train set: 0.5151453018188477 at epoch: 3 and batch_num: 37\n",
      "Loss of train set: 0.35357046127319336 at epoch: 3 and batch_num: 38\n",
      "Loss of train set: 0.4476476311683655 at epoch: 3 and batch_num: 39\n",
      "Loss of train set: 0.35907575488090515 at epoch: 3 and batch_num: 40\n",
      "Loss of train set: 0.34820684790611267 at epoch: 3 and batch_num: 41\n",
      "Loss of train set: 0.4008493423461914 at epoch: 3 and batch_num: 42\n",
      "Loss of train set: 0.40929514169692993 at epoch: 3 and batch_num: 43\n",
      "Loss of train set: 0.3234340250492096 at epoch: 3 and batch_num: 44\n",
      "Loss of train set: 0.3575226366519928 at epoch: 3 and batch_num: 45\n",
      "Loss of train set: 0.30322209000587463 at epoch: 3 and batch_num: 46\n",
      "Loss of train set: 0.45061594247817993 at epoch: 3 and batch_num: 47\n",
      "Loss of train set: 0.4300122857093811 at epoch: 3 and batch_num: 48\n",
      "Loss of train set: 0.3263081908226013 at epoch: 3 and batch_num: 49\n",
      "Loss of train set: 0.3615710437297821 at epoch: 3 and batch_num: 50\n",
      "Loss of train set: 0.43253469467163086 at epoch: 3 and batch_num: 51\n",
      "Loss of train set: 0.4032313823699951 at epoch: 3 and batch_num: 52\n",
      "Loss of train set: 0.37519848346710205 at epoch: 3 and batch_num: 53\n",
      "Loss of train set: 0.33634892106056213 at epoch: 3 and batch_num: 54\n",
      "Loss of train set: 0.35511288046836853 at epoch: 3 and batch_num: 55\n",
      "Loss of train set: 0.29595229029655457 at epoch: 3 and batch_num: 56\n",
      "Loss of train set: 0.3933166265487671 at epoch: 3 and batch_num: 57\n",
      "Loss of train set: 0.4085097908973694 at epoch: 3 and batch_num: 58\n",
      "Loss of train set: 0.47582268714904785 at epoch: 3 and batch_num: 59\n",
      "Loss of train set: 0.34955060482025146 at epoch: 3 and batch_num: 60\n",
      "Loss of train set: 0.37339162826538086 at epoch: 3 and batch_num: 61\n",
      "Loss of train set: 0.34206050634384155 at epoch: 3 and batch_num: 62\n",
      "Loss of train set: 0.4097653031349182 at epoch: 3 and batch_num: 63\n",
      "Loss of train set: 0.4847850203514099 at epoch: 3 and batch_num: 64\n",
      "Loss of train set: 0.26734066009521484 at epoch: 3 and batch_num: 65\n",
      "Loss of train set: 0.28789186477661133 at epoch: 3 and batch_num: 66\n",
      "Loss of train set: 0.48933345079421997 at epoch: 3 and batch_num: 67\n",
      "Loss of train set: 0.20345255732536316 at epoch: 3 and batch_num: 68\n",
      "Loss of train set: 0.38281458616256714 at epoch: 3 and batch_num: 69\n",
      "Loss of train set: 0.30057427287101746 at epoch: 3 and batch_num: 70\n",
      "Loss of train set: 0.33456629514694214 at epoch: 3 and batch_num: 71\n",
      "Loss of train set: 0.43095821142196655 at epoch: 3 and batch_num: 72\n",
      "Loss of train set: 0.2608223557472229 at epoch: 3 and batch_num: 73\n",
      "Loss of train set: 0.2871348261833191 at epoch: 3 and batch_num: 74\n",
      "Loss of train set: 0.17141178250312805 at epoch: 3 and batch_num: 75\n",
      "Loss of train set: 0.46515512466430664 at epoch: 3 and batch_num: 76\n",
      "Loss of train set: 0.2760758697986603 at epoch: 3 and batch_num: 77\n",
      "Loss of train set: 0.21742165088653564 at epoch: 3 and batch_num: 78\n",
      "Loss of train set: 0.3488357663154602 at epoch: 3 and batch_num: 79\n",
      "Loss of train set: 0.4811429977416992 at epoch: 3 and batch_num: 80\n",
      "Loss of train set: 0.37036558985710144 at epoch: 3 and batch_num: 81\n",
      "Loss of train set: 0.4790370464324951 at epoch: 3 and batch_num: 82\n",
      "Loss of train set: 0.4046761691570282 at epoch: 3 and batch_num: 83\n",
      "Loss of train set: 0.294699490070343 at epoch: 3 and batch_num: 84\n",
      "Loss of train set: 0.289584755897522 at epoch: 3 and batch_num: 85\n",
      "Loss of train set: 0.30288809537887573 at epoch: 3 and batch_num: 86\n",
      "Loss of train set: 0.23309612274169922 at epoch: 3 and batch_num: 87\n",
      "Loss of train set: 0.47542327642440796 at epoch: 3 and batch_num: 88\n",
      "Loss of train set: 0.40857598185539246 at epoch: 3 and batch_num: 89\n",
      "Loss of train set: 0.5319634675979614 at epoch: 3 and batch_num: 90\n",
      "Loss of train set: 0.4114339053630829 at epoch: 3 and batch_num: 91\n",
      "Loss of train set: 0.4353776276111603 at epoch: 3 and batch_num: 92\n",
      "Loss of train set: 0.41710543632507324 at epoch: 3 and batch_num: 93\n",
      "Loss of train set: 0.5944380760192871 at epoch: 3 and batch_num: 94\n",
      "Loss of train set: 0.3308483362197876 at epoch: 3 and batch_num: 95\n",
      "Loss of train set: 0.1998189389705658 at epoch: 3 and batch_num: 96\n",
      "Loss of train set: 0.48774221539497375 at epoch: 3 and batch_num: 97\n",
      "Loss of train set: 0.39177095890045166 at epoch: 3 and batch_num: 98\n",
      "Loss of train set: 0.30942875146865845 at epoch: 3 and batch_num: 99\n",
      "Loss of train set: 0.22759422659873962 at epoch: 3 and batch_num: 100\n",
      "Loss of train set: 0.4100883901119232 at epoch: 3 and batch_num: 101\n",
      "Loss of train set: 0.5018696784973145 at epoch: 3 and batch_num: 102\n",
      "Loss of train set: 0.34891587495803833 at epoch: 3 and batch_num: 103\n",
      "Loss of train set: 0.3808589577674866 at epoch: 3 and batch_num: 104\n",
      "Loss of train set: 0.39751136302948 at epoch: 3 and batch_num: 105\n",
      "Loss of train set: 0.4748961329460144 at epoch: 3 and batch_num: 106\n",
      "Loss of train set: 0.4233136773109436 at epoch: 3 and batch_num: 107\n",
      "Loss of train set: 0.3645968735218048 at epoch: 3 and batch_num: 108\n",
      "Loss of train set: 0.4391227066516876 at epoch: 3 and batch_num: 109\n",
      "Loss of train set: 0.29719412326812744 at epoch: 3 and batch_num: 110\n",
      "Loss of train set: 0.44679781794548035 at epoch: 3 and batch_num: 111\n",
      "Loss of train set: 0.3787882924079895 at epoch: 3 and batch_num: 112\n",
      "Loss of train set: 0.24179425835609436 at epoch: 3 and batch_num: 113\n",
      "Loss of train set: 0.1956542730331421 at epoch: 3 and batch_num: 114\n",
      "Loss of train set: 0.48687148094177246 at epoch: 3 and batch_num: 115\n",
      "Loss of train set: 0.24163421988487244 at epoch: 3 and batch_num: 116\n",
      "Loss of train set: 0.3276362121105194 at epoch: 3 and batch_num: 117\n",
      "Loss of train set: 0.32712364196777344 at epoch: 3 and batch_num: 118\n",
      "Loss of train set: 0.3631899654865265 at epoch: 3 and batch_num: 119\n",
      "Loss of train set: 0.2619333863258362 at epoch: 3 and batch_num: 120\n",
      "Loss of train set: 0.4326542019844055 at epoch: 3 and batch_num: 121\n",
      "Loss of train set: 0.47812917828559875 at epoch: 3 and batch_num: 122\n",
      "Loss of train set: 0.2967088222503662 at epoch: 3 and batch_num: 123\n",
      "Loss of train set: 0.3823774456977844 at epoch: 3 and batch_num: 124\n",
      "Loss of train set: 0.4513113498687744 at epoch: 3 and batch_num: 125\n",
      "Loss of train set: 0.4655263423919678 at epoch: 3 and batch_num: 126\n",
      "Loss of train set: 0.47212058305740356 at epoch: 3 and batch_num: 127\n",
      "Loss of train set: 0.406798779964447 at epoch: 3 and batch_num: 128\n",
      "Loss of train set: 0.23854020237922668 at epoch: 3 and batch_num: 129\n",
      "Loss of train set: 0.30433815717697144 at epoch: 3 and batch_num: 130\n",
      "Loss of train set: 0.37913110852241516 at epoch: 3 and batch_num: 131\n",
      "Loss of train set: 0.17761635780334473 at epoch: 3 and batch_num: 132\n",
      "Loss of train set: 0.3557560443878174 at epoch: 3 and batch_num: 133\n",
      "Loss of train set: 0.42537885904312134 at epoch: 3 and batch_num: 134\n",
      "Loss of train set: 0.39852267503738403 at epoch: 3 and batch_num: 135\n",
      "Loss of train set: 0.44223201274871826 at epoch: 3 and batch_num: 136\n",
      "Loss of train set: 0.30158352851867676 at epoch: 3 and batch_num: 137\n",
      "Loss of train set: 0.39292681217193604 at epoch: 3 and batch_num: 138\n",
      "Loss of train set: 0.3360751271247864 at epoch: 3 and batch_num: 139\n",
      "Loss of train set: 0.459964394569397 at epoch: 3 and batch_num: 140\n",
      "Loss of train set: 0.45239758491516113 at epoch: 3 and batch_num: 141\n",
      "Loss of train set: 0.4656592607498169 at epoch: 3 and batch_num: 142\n",
      "Loss of train set: 0.3218221664428711 at epoch: 3 and batch_num: 143\n",
      "Loss of train set: 0.7013493776321411 at epoch: 3 and batch_num: 144\n",
      "Loss of train set: 0.3986469805240631 at epoch: 3 and batch_num: 145\n",
      "Loss of train set: 0.43660151958465576 at epoch: 3 and batch_num: 146\n",
      "Loss of train set: 0.4118634760379791 at epoch: 3 and batch_num: 147\n",
      "Loss of train set: 0.28696078062057495 at epoch: 3 and batch_num: 148\n",
      "Loss of train set: 0.196494460105896 at epoch: 3 and batch_num: 149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3854299783706665 at epoch: 3 and batch_num: 150\n",
      "Loss of train set: 0.2797486186027527 at epoch: 3 and batch_num: 151\n",
      "Loss of train set: 0.4167001247406006 at epoch: 3 and batch_num: 152\n",
      "Loss of train set: 0.5700344443321228 at epoch: 3 and batch_num: 153\n",
      "Loss of train set: 0.47594666481018066 at epoch: 3 and batch_num: 154\n",
      "Loss of train set: 0.5442844033241272 at epoch: 3 and batch_num: 155\n",
      "Loss of train set: 0.21296176314353943 at epoch: 3 and batch_num: 156\n",
      "Loss of train set: 0.4390031397342682 at epoch: 3 and batch_num: 157\n",
      "Loss of train set: 0.3423312306404114 at epoch: 3 and batch_num: 158\n",
      "Loss of train set: 0.3826282322406769 at epoch: 3 and batch_num: 159\n",
      "Loss of train set: 0.2814168334007263 at epoch: 3 and batch_num: 160\n",
      "Loss of train set: 0.4690384566783905 at epoch: 3 and batch_num: 161\n",
      "Loss of train set: 0.220234215259552 at epoch: 3 and batch_num: 162\n",
      "Loss of train set: 0.359974205493927 at epoch: 3 and batch_num: 163\n",
      "Loss of train set: 0.44448477029800415 at epoch: 3 and batch_num: 164\n",
      "Loss of train set: 0.271671324968338 at epoch: 3 and batch_num: 165\n",
      "Loss of train set: 0.1286011040210724 at epoch: 3 and batch_num: 166\n",
      "Loss of train set: 0.3290977478027344 at epoch: 3 and batch_num: 167\n",
      "Loss of train set: 0.4216845631599426 at epoch: 3 and batch_num: 168\n",
      "Loss of train set: 0.26571571826934814 at epoch: 3 and batch_num: 169\n",
      "Loss of train set: 0.3384014964103699 at epoch: 3 and batch_num: 170\n",
      "Loss of train set: 0.5077195167541504 at epoch: 3 and batch_num: 171\n",
      "Loss of train set: 0.31506189703941345 at epoch: 3 and batch_num: 172\n",
      "Loss of train set: 0.4236217141151428 at epoch: 3 and batch_num: 173\n",
      "Loss of train set: 0.26656869053840637 at epoch: 3 and batch_num: 174\n",
      "Loss of train set: 0.2695311903953552 at epoch: 3 and batch_num: 175\n",
      "Loss of train set: 0.3319035768508911 at epoch: 3 and batch_num: 176\n",
      "Loss of train set: 0.31519627571105957 at epoch: 3 and batch_num: 177\n",
      "Loss of train set: 0.4835438132286072 at epoch: 3 and batch_num: 178\n",
      "Loss of train set: 0.309114933013916 at epoch: 3 and batch_num: 179\n",
      "Loss of train set: 0.4004966616630554 at epoch: 3 and batch_num: 180\n",
      "Loss of train set: 0.4397306740283966 at epoch: 3 and batch_num: 181\n",
      "Loss of train set: 0.3279455304145813 at epoch: 3 and batch_num: 182\n",
      "Loss of train set: 0.3932992219924927 at epoch: 3 and batch_num: 183\n",
      "Loss of train set: 0.3976075053215027 at epoch: 3 and batch_num: 184\n",
      "Loss of train set: 0.4140179753303528 at epoch: 3 and batch_num: 185\n",
      "Loss of train set: 0.3428349196910858 at epoch: 3 and batch_num: 186\n",
      "Loss of train set: 0.3218161165714264 at epoch: 3 and batch_num: 187\n",
      "Loss of train set: 0.27292680740356445 at epoch: 3 and batch_num: 188\n",
      "Loss of train set: 0.41590654850006104 at epoch: 3 and batch_num: 189\n",
      "Loss of train set: 0.29973363876342773 at epoch: 3 and batch_num: 190\n",
      "Loss of train set: 0.23477603495121002 at epoch: 3 and batch_num: 191\n",
      "Loss of train set: 0.28494584560394287 at epoch: 3 and batch_num: 192\n",
      "Loss of train set: 0.40852776169776917 at epoch: 3 and batch_num: 193\n",
      "Loss of train set: 0.4553869962692261 at epoch: 3 and batch_num: 194\n",
      "Loss of train set: 0.4454629421234131 at epoch: 3 and batch_num: 195\n",
      "Loss of train set: 0.3665710687637329 at epoch: 3 and batch_num: 196\n",
      "Loss of train set: 0.2668062448501587 at epoch: 3 and batch_num: 197\n",
      "Loss of train set: 0.34793657064437866 at epoch: 3 and batch_num: 198\n",
      "Loss of train set: 0.4034317135810852 at epoch: 3 and batch_num: 199\n",
      "Loss of train set: 0.36257204413414 at epoch: 3 and batch_num: 200\n",
      "Loss of train set: 0.3145379424095154 at epoch: 3 and batch_num: 201\n",
      "Loss of train set: 0.3335860073566437 at epoch: 3 and batch_num: 202\n",
      "Loss of train set: 0.31787917017936707 at epoch: 3 and batch_num: 203\n",
      "Loss of train set: 0.3283625543117523 at epoch: 3 and batch_num: 204\n",
      "Loss of train set: 0.5033844709396362 at epoch: 3 and batch_num: 205\n",
      "Loss of train set: 0.35079628229141235 at epoch: 3 and batch_num: 206\n",
      "Loss of train set: 0.3902454972267151 at epoch: 3 and batch_num: 207\n",
      "Loss of train set: 0.22869113087654114 at epoch: 3 and batch_num: 208\n",
      "Loss of train set: 0.47853049635887146 at epoch: 3 and batch_num: 209\n",
      "Loss of train set: 0.6683304905891418 at epoch: 3 and batch_num: 210\n",
      "Loss of train set: 0.3798169493675232 at epoch: 3 and batch_num: 211\n",
      "Loss of train set: 0.6858575344085693 at epoch: 3 and batch_num: 212\n",
      "Loss of train set: 0.512877345085144 at epoch: 3 and batch_num: 213\n",
      "Loss of train set: 0.2021181285381317 at epoch: 3 and batch_num: 214\n",
      "Loss of train set: 0.2661448121070862 at epoch: 3 and batch_num: 215\n",
      "Loss of train set: 0.3956034481525421 at epoch: 3 and batch_num: 216\n",
      "Loss of train set: 0.40471816062927246 at epoch: 3 and batch_num: 217\n",
      "Loss of train set: 0.3827033042907715 at epoch: 3 and batch_num: 218\n",
      "Loss of train set: 0.35048675537109375 at epoch: 3 and batch_num: 219\n",
      "Loss of train set: 0.24003371596336365 at epoch: 3 and batch_num: 220\n",
      "Loss of train set: 0.46118271350860596 at epoch: 3 and batch_num: 221\n",
      "Loss of train set: 0.34600716829299927 at epoch: 3 and batch_num: 222\n",
      "Loss of train set: 0.3866227865219116 at epoch: 3 and batch_num: 223\n",
      "Loss of train set: 0.31336352229118347 at epoch: 3 and batch_num: 224\n",
      "Loss of train set: 0.41796356439590454 at epoch: 3 and batch_num: 225\n",
      "Loss of train set: 0.4789232313632965 at epoch: 3 and batch_num: 226\n",
      "Loss of train set: 0.3129568099975586 at epoch: 3 and batch_num: 227\n",
      "Loss of train set: 0.25438424944877625 at epoch: 3 and batch_num: 228\n",
      "Loss of train set: 0.2658904194831848 at epoch: 3 and batch_num: 229\n",
      "Loss of train set: 0.2668779492378235 at epoch: 3 and batch_num: 230\n",
      "Loss of train set: 0.3740881383419037 at epoch: 3 and batch_num: 231\n",
      "Loss of train set: 0.24045249819755554 at epoch: 3 and batch_num: 232\n",
      "Loss of train set: 0.24183550477027893 at epoch: 3 and batch_num: 233\n",
      "Loss of train set: 0.4311125874519348 at epoch: 3 and batch_num: 234\n",
      "Loss of train set: 0.289876788854599 at epoch: 3 and batch_num: 235\n",
      "Loss of train set: 0.2930232584476471 at epoch: 3 and batch_num: 236\n",
      "Loss of train set: 0.39393550157546997 at epoch: 3 and batch_num: 237\n",
      "Loss of train set: 0.26752156019210815 at epoch: 3 and batch_num: 238\n",
      "Loss of train set: 0.49025654792785645 at epoch: 3 and batch_num: 239\n",
      "Loss of train set: 0.3978239595890045 at epoch: 3 and batch_num: 240\n",
      "Loss of train set: 0.45886456966400146 at epoch: 3 and batch_num: 241\n",
      "Loss of train set: 0.36200594902038574 at epoch: 3 and batch_num: 242\n",
      "Loss of train set: 0.2855771780014038 at epoch: 3 and batch_num: 243\n",
      "Loss of train set: 0.4081078767776489 at epoch: 3 and batch_num: 244\n",
      "Loss of train set: 0.3710375428199768 at epoch: 3 and batch_num: 245\n",
      "Loss of train set: 0.3180280327796936 at epoch: 3 and batch_num: 246\n",
      "Loss of train set: 0.38432687520980835 at epoch: 3 and batch_num: 247\n",
      "Loss of train set: 0.2692487835884094 at epoch: 3 and batch_num: 248\n",
      "Loss of train set: 0.33334749937057495 at epoch: 3 and batch_num: 249\n",
      "Loss of train set: 0.3977415859699249 at epoch: 3 and batch_num: 250\n",
      "Loss of train set: 0.29499343037605286 at epoch: 3 and batch_num: 251\n",
      "Loss of train set: 0.3196501135826111 at epoch: 3 and batch_num: 252\n",
      "Loss of train set: 0.21551308035850525 at epoch: 3 and batch_num: 253\n",
      "Loss of train set: 0.3804193139076233 at epoch: 3 and batch_num: 254\n",
      "Loss of train set: 0.2312239706516266 at epoch: 3 and batch_num: 255\n",
      "Loss of train set: 0.379555344581604 at epoch: 3 and batch_num: 256\n",
      "Loss of train set: 0.12749198079109192 at epoch: 3 and batch_num: 257\n",
      "Loss of train set: 0.25911515951156616 at epoch: 3 and batch_num: 258\n",
      "Loss of train set: 0.3465404808521271 at epoch: 3 and batch_num: 259\n",
      "Loss of train set: 0.35295170545578003 at epoch: 3 and batch_num: 260\n",
      "Loss of train set: 0.4671891927719116 at epoch: 3 and batch_num: 261\n",
      "Loss of train set: 0.5101809501647949 at epoch: 3 and batch_num: 262\n",
      "Loss of train set: 0.2205720692873001 at epoch: 3 and batch_num: 263\n",
      "Loss of train set: 0.397696852684021 at epoch: 3 and batch_num: 264\n",
      "Loss of train set: 0.2617168724536896 at epoch: 3 and batch_num: 265\n",
      "Loss of train set: 0.2792477011680603 at epoch: 3 and batch_num: 266\n",
      "Loss of train set: 0.34795305132865906 at epoch: 3 and batch_num: 267\n",
      "Loss of train set: 0.4634166955947876 at epoch: 3 and batch_num: 268\n",
      "Loss of train set: 0.22441400587558746 at epoch: 3 and batch_num: 269\n",
      "Loss of train set: 0.3005193769931793 at epoch: 3 and batch_num: 270\n",
      "Loss of train set: 0.1996212899684906 at epoch: 3 and batch_num: 271\n",
      "Loss of train set: 0.32437247037887573 at epoch: 3 and batch_num: 272\n",
      "Loss of train set: 0.47754034399986267 at epoch: 3 and batch_num: 273\n",
      "Loss of train set: 0.3559562563896179 at epoch: 3 and batch_num: 274\n",
      "Loss of train set: 0.4261208772659302 at epoch: 3 and batch_num: 275\n",
      "Loss of train set: 0.42002835869789124 at epoch: 3 and batch_num: 276\n",
      "Loss of train set: 0.25515609979629517 at epoch: 3 and batch_num: 277\n",
      "Loss of train set: 0.34141623973846436 at epoch: 3 and batch_num: 278\n",
      "Loss of train set: 0.2178766429424286 at epoch: 3 and batch_num: 279\n",
      "Loss of train set: 0.2841070592403412 at epoch: 3 and batch_num: 280\n",
      "Loss of train set: 0.3864249587059021 at epoch: 3 and batch_num: 281\n",
      "Loss of train set: 0.22264420986175537 at epoch: 3 and batch_num: 282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.41621166467666626 at epoch: 3 and batch_num: 283\n",
      "Loss of train set: 0.38472503423690796 at epoch: 3 and batch_num: 284\n",
      "Loss of train set: 0.23187488317489624 at epoch: 3 and batch_num: 285\n",
      "Loss of train set: 0.4577639698982239 at epoch: 3 and batch_num: 286\n",
      "Loss of train set: 0.3202451467514038 at epoch: 3 and batch_num: 287\n",
      "Loss of train set: 0.28284984827041626 at epoch: 3 and batch_num: 288\n",
      "Loss of train set: 0.3306998014450073 at epoch: 3 and batch_num: 289\n",
      "Loss of train set: 0.362399160861969 at epoch: 3 and batch_num: 290\n",
      "Loss of train set: 0.30888935923576355 at epoch: 3 and batch_num: 291\n",
      "Loss of train set: 0.2705376148223877 at epoch: 3 and batch_num: 292\n",
      "Loss of train set: 0.49889129400253296 at epoch: 3 and batch_num: 293\n",
      "Loss of train set: 0.4437445402145386 at epoch: 3 and batch_num: 294\n",
      "Loss of train set: 0.35621893405914307 at epoch: 3 and batch_num: 295\n",
      "Loss of train set: 0.43743056058883667 at epoch: 3 and batch_num: 296\n",
      "Loss of train set: 0.3871270418167114 at epoch: 3 and batch_num: 297\n",
      "Loss of train set: 0.24207332730293274 at epoch: 3 and batch_num: 298\n",
      "Loss of train set: 0.371410071849823 at epoch: 3 and batch_num: 299\n",
      "Loss of train set: 0.3242550492286682 at epoch: 3 and batch_num: 300\n",
      "Loss of train set: 0.20952486991882324 at epoch: 3 and batch_num: 301\n",
      "Loss of train set: 0.2886866629123688 at epoch: 3 and batch_num: 302\n",
      "Loss of train set: 0.4903406798839569 at epoch: 3 and batch_num: 303\n",
      "Loss of train set: 0.31553220748901367 at epoch: 3 and batch_num: 304\n",
      "Loss of train set: 0.5517099499702454 at epoch: 3 and batch_num: 305\n",
      "Loss of train set: 0.3000190854072571 at epoch: 3 and batch_num: 306\n",
      "Loss of train set: 0.38133499026298523 at epoch: 3 and batch_num: 307\n",
      "Loss of train set: 0.41673538088798523 at epoch: 3 and batch_num: 308\n",
      "Loss of train set: 0.3194286525249481 at epoch: 3 and batch_num: 309\n",
      "Loss of train set: 0.2721986472606659 at epoch: 3 and batch_num: 310\n",
      "Loss of train set: 0.32975560426712036 at epoch: 3 and batch_num: 311\n",
      "Loss of train set: 0.35786598920822144 at epoch: 3 and batch_num: 312\n",
      "Loss of train set: 0.39339011907577515 at epoch: 3 and batch_num: 313\n",
      "Loss of train set: 0.2911485731601715 at epoch: 3 and batch_num: 314\n",
      "Loss of train set: 0.39246872067451477 at epoch: 3 and batch_num: 315\n",
      "Loss of train set: 0.5940731763839722 at epoch: 3 and batch_num: 316\n",
      "Loss of train set: 0.307256281375885 at epoch: 3 and batch_num: 317\n",
      "Loss of train set: 0.4960688352584839 at epoch: 3 and batch_num: 318\n",
      "Loss of train set: 0.30585891008377075 at epoch: 3 and batch_num: 319\n",
      "Loss of train set: 0.4484240412712097 at epoch: 3 and batch_num: 320\n",
      "Loss of train set: 0.28513550758361816 at epoch: 3 and batch_num: 321\n",
      "Loss of train set: 0.3149561882019043 at epoch: 3 and batch_num: 322\n",
      "Loss of train set: 0.23204770684242249 at epoch: 3 and batch_num: 323\n",
      "Loss of train set: 0.3714587986469269 at epoch: 3 and batch_num: 324\n",
      "Loss of train set: 0.3414763808250427 at epoch: 3 and batch_num: 325\n",
      "Loss of train set: 0.3168787658214569 at epoch: 3 and batch_num: 326\n",
      "Loss of train set: 0.43372222781181335 at epoch: 3 and batch_num: 327\n",
      "Loss of train set: 0.27648407220840454 at epoch: 3 and batch_num: 328\n",
      "Loss of train set: 0.4375048279762268 at epoch: 3 and batch_num: 329\n",
      "Loss of train set: 0.3641212582588196 at epoch: 3 and batch_num: 330\n",
      "Loss of train set: 0.3831390142440796 at epoch: 3 and batch_num: 331\n",
      "Loss of train set: 0.23744753003120422 at epoch: 3 and batch_num: 332\n",
      "Loss of train set: 0.34392213821411133 at epoch: 3 and batch_num: 333\n",
      "Loss of train set: 0.3901381194591522 at epoch: 3 and batch_num: 334\n",
      "Loss of train set: 0.36875277757644653 at epoch: 3 and batch_num: 335\n",
      "Loss of train set: 0.4887944459915161 at epoch: 3 and batch_num: 336\n",
      "Loss of train set: 0.3239585757255554 at epoch: 3 and batch_num: 337\n",
      "Loss of train set: 0.4154863953590393 at epoch: 3 and batch_num: 338\n",
      "Loss of train set: 0.27166178822517395 at epoch: 3 and batch_num: 339\n",
      "Loss of train set: 0.3095066547393799 at epoch: 3 and batch_num: 340\n",
      "Loss of train set: 0.31131047010421753 at epoch: 3 and batch_num: 341\n",
      "Loss of train set: 0.4233616888523102 at epoch: 3 and batch_num: 342\n",
      "Loss of train set: 0.3277375400066376 at epoch: 3 and batch_num: 343\n",
      "Loss of train set: 0.44485098123550415 at epoch: 3 and batch_num: 344\n",
      "Loss of train set: 0.20754384994506836 at epoch: 3 and batch_num: 345\n",
      "Loss of train set: 0.37831827998161316 at epoch: 3 and batch_num: 346\n",
      "Loss of train set: 0.387016236782074 at epoch: 3 and batch_num: 347\n",
      "Loss of train set: 0.4036343991756439 at epoch: 3 and batch_num: 348\n",
      "Loss of train set: 0.29263001680374146 at epoch: 3 and batch_num: 349\n",
      "Loss of train set: 0.41423773765563965 at epoch: 3 and batch_num: 350\n",
      "Loss of train set: 0.2991996109485626 at epoch: 3 and batch_num: 351\n",
      "Loss of train set: 0.31792014837265015 at epoch: 3 and batch_num: 352\n",
      "Loss of train set: 0.34924641251564026 at epoch: 3 and batch_num: 353\n",
      "Loss of train set: 0.3411092460155487 at epoch: 3 and batch_num: 354\n",
      "Loss of train set: 0.1840251237154007 at epoch: 3 and batch_num: 355\n",
      "Loss of train set: 0.21017968654632568 at epoch: 3 and batch_num: 356\n",
      "Loss of train set: 0.3785857558250427 at epoch: 3 and batch_num: 357\n",
      "Loss of train set: 0.43258628249168396 at epoch: 3 and batch_num: 358\n",
      "Loss of train set: 0.4378569722175598 at epoch: 3 and batch_num: 359\n",
      "Loss of train set: 0.46296098828315735 at epoch: 3 and batch_num: 360\n",
      "Loss of train set: 0.4351535439491272 at epoch: 3 and batch_num: 361\n",
      "Loss of train set: 0.2863258123397827 at epoch: 3 and batch_num: 362\n",
      "Loss of train set: 0.44969993829727173 at epoch: 3 and batch_num: 363\n",
      "Loss of train set: 0.5234955549240112 at epoch: 3 and batch_num: 364\n",
      "Loss of train set: 0.41158419847488403 at epoch: 3 and batch_num: 365\n",
      "Loss of train set: 0.31991833448410034 at epoch: 3 and batch_num: 366\n",
      "Loss of train set: 0.3438180088996887 at epoch: 3 and batch_num: 367\n",
      "Loss of train set: 0.41963666677474976 at epoch: 3 and batch_num: 368\n",
      "Loss of train set: 0.22407634556293488 at epoch: 3 and batch_num: 369\n",
      "Loss of train set: 0.2940485179424286 at epoch: 3 and batch_num: 370\n",
      "Loss of train set: 0.2973586618900299 at epoch: 3 and batch_num: 371\n",
      "Loss of train set: 0.377996027469635 at epoch: 3 and batch_num: 372\n",
      "Loss of train set: 0.2595241069793701 at epoch: 3 and batch_num: 373\n",
      "Loss of train set: 0.3690859079360962 at epoch: 3 and batch_num: 374\n",
      "Loss of train set: 0.2604305148124695 at epoch: 3 and batch_num: 375\n",
      "Loss of train set: 0.39462369680404663 at epoch: 3 and batch_num: 376\n",
      "Loss of train set: 0.2752014398574829 at epoch: 3 and batch_num: 377\n",
      "Loss of train set: 0.25849229097366333 at epoch: 3 and batch_num: 378\n",
      "Loss of train set: 0.28139734268188477 at epoch: 3 and batch_num: 379\n",
      "Loss of train set: 0.37398430705070496 at epoch: 3 and batch_num: 380\n",
      "Loss of train set: 0.4193181097507477 at epoch: 3 and batch_num: 381\n",
      "Loss of train set: 0.49395841360092163 at epoch: 3 and batch_num: 382\n",
      "Loss of train set: 0.25800570845603943 at epoch: 3 and batch_num: 383\n",
      "Loss of train set: 0.48825106024742126 at epoch: 3 and batch_num: 384\n",
      "Loss of train set: 0.3466225862503052 at epoch: 3 and batch_num: 385\n",
      "Loss of train set: 0.5033563375473022 at epoch: 3 and batch_num: 386\n",
      "Loss of train set: 0.26782143115997314 at epoch: 3 and batch_num: 387\n",
      "Loss of train set: 0.33686816692352295 at epoch: 3 and batch_num: 388\n",
      "Loss of train set: 0.39770078659057617 at epoch: 3 and batch_num: 389\n",
      "Loss of train set: 0.17565380036830902 at epoch: 3 and batch_num: 390\n",
      "Loss of train set: 0.2676158845424652 at epoch: 3 and batch_num: 391\n",
      "Loss of train set: 0.41518837213516235 at epoch: 3 and batch_num: 392\n",
      "Loss of train set: 0.44936370849609375 at epoch: 3 and batch_num: 393\n",
      "Loss of train set: 0.3222840428352356 at epoch: 3 and batch_num: 394\n",
      "Loss of train set: 0.35957270860671997 at epoch: 3 and batch_num: 395\n",
      "Loss of train set: 0.2669638395309448 at epoch: 3 and batch_num: 396\n",
      "Loss of train set: 0.37810882925987244 at epoch: 3 and batch_num: 397\n",
      "Loss of train set: 0.3813210725784302 at epoch: 3 and batch_num: 398\n",
      "Loss of train set: 0.22182106971740723 at epoch: 3 and batch_num: 399\n",
      "Loss of train set: 0.3657035827636719 at epoch: 3 and batch_num: 400\n",
      "Loss of train set: 0.4097787141799927 at epoch: 3 and batch_num: 401\n",
      "Loss of train set: 0.3411440849304199 at epoch: 3 and batch_num: 402\n",
      "Loss of train set: 0.3555004596710205 at epoch: 3 and batch_num: 403\n",
      "Loss of train set: 0.35074764490127563 at epoch: 3 and batch_num: 404\n",
      "Loss of train set: 0.32939663529396057 at epoch: 3 and batch_num: 405\n",
      "Loss of train set: 0.7854037284851074 at epoch: 3 and batch_num: 406\n",
      "Loss of train set: 0.2818717956542969 at epoch: 3 and batch_num: 407\n",
      "Loss of train set: 0.4866574704647064 at epoch: 3 and batch_num: 408\n",
      "Loss of train set: 0.3290982246398926 at epoch: 3 and batch_num: 409\n",
      "Loss of train set: 0.35573720932006836 at epoch: 3 and batch_num: 410\n",
      "Loss of train set: 0.5664349794387817 at epoch: 3 and batch_num: 411\n",
      "Loss of train set: 0.5628606081008911 at epoch: 3 and batch_num: 412\n",
      "Loss of train set: 0.21077197790145874 at epoch: 3 and batch_num: 413\n",
      "Loss of train set: 0.3035186529159546 at epoch: 3 and batch_num: 414\n",
      "Loss of train set: 0.32451581954956055 at epoch: 3 and batch_num: 415\n",
      "Loss of train set: 0.756434977054596 at epoch: 3 and batch_num: 416\n",
      "Loss of train set: 0.3386732339859009 at epoch: 3 and batch_num: 417\n",
      "Loss of train set: 0.25368911027908325 at epoch: 3 and batch_num: 418\n",
      "Loss of train set: 0.47279107570648193 at epoch: 3 and batch_num: 419\n",
      "Loss of train set: 0.31065690517425537 at epoch: 3 and batch_num: 420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.22705936431884766 at epoch: 3 and batch_num: 421\n",
      "Loss of train set: 0.4616886377334595 at epoch: 3 and batch_num: 422\n",
      "Loss of train set: 0.5175651907920837 at epoch: 3 and batch_num: 423\n",
      "Loss of train set: 0.34398093819618225 at epoch: 3 and batch_num: 424\n",
      "Loss of train set: 0.4099167287349701 at epoch: 3 and batch_num: 425\n",
      "Loss of train set: 0.28293120861053467 at epoch: 3 and batch_num: 426\n",
      "Loss of train set: 0.5339188575744629 at epoch: 3 and batch_num: 427\n",
      "Loss of train set: 0.2735292315483093 at epoch: 3 and batch_num: 428\n",
      "Loss of train set: 0.7019764184951782 at epoch: 3 and batch_num: 429\n",
      "Loss of train set: 0.44580960273742676 at epoch: 3 and batch_num: 430\n",
      "Loss of train set: 0.3921690583229065 at epoch: 3 and batch_num: 431\n",
      "Loss of train set: 0.435700923204422 at epoch: 3 and batch_num: 432\n",
      "Loss of train set: 0.6004925966262817 at epoch: 3 and batch_num: 433\n",
      "Loss of train set: 0.34456828236579895 at epoch: 3 and batch_num: 434\n",
      "Loss of train set: 0.3012270927429199 at epoch: 3 and batch_num: 435\n",
      "Loss of train set: 0.34202760457992554 at epoch: 3 and batch_num: 436\n",
      "Loss of train set: 0.5105586051940918 at epoch: 3 and batch_num: 437\n",
      "Loss of train set: 0.25773537158966064 at epoch: 3 and batch_num: 438\n",
      "Loss of train set: 0.31289607286453247 at epoch: 3 and batch_num: 439\n",
      "Loss of train set: 0.43241095542907715 at epoch: 3 and batch_num: 440\n",
      "Loss of train set: 0.26943641901016235 at epoch: 3 and batch_num: 441\n",
      "Loss of train set: 0.3518928289413452 at epoch: 3 and batch_num: 442\n",
      "Loss of train set: 0.3506726026535034 at epoch: 3 and batch_num: 443\n",
      "Loss of train set: 0.3396584987640381 at epoch: 3 and batch_num: 444\n",
      "Loss of train set: 0.4780263304710388 at epoch: 3 and batch_num: 445\n",
      "Loss of train set: 0.2592282295227051 at epoch: 3 and batch_num: 446\n",
      "Loss of train set: 0.34622418880462646 at epoch: 3 and batch_num: 447\n",
      "Loss of train set: 0.5119546055793762 at epoch: 3 and batch_num: 448\n",
      "Loss of train set: 0.300822913646698 at epoch: 3 and batch_num: 449\n",
      "Loss of train set: 0.6652539372444153 at epoch: 3 and batch_num: 450\n",
      "Loss of train set: 0.32455605268478394 at epoch: 3 and batch_num: 451\n",
      "Loss of train set: 0.2516283690929413 at epoch: 3 and batch_num: 452\n",
      "Loss of train set: 0.18177659809589386 at epoch: 3 and batch_num: 453\n",
      "Loss of train set: 0.3495938181877136 at epoch: 3 and batch_num: 454\n",
      "Loss of train set: 0.27568814158439636 at epoch: 3 and batch_num: 455\n",
      "Loss of train set: 0.36172395944595337 at epoch: 3 and batch_num: 456\n",
      "Loss of train set: 0.253751277923584 at epoch: 3 and batch_num: 457\n",
      "Loss of train set: 0.41180282831192017 at epoch: 3 and batch_num: 458\n",
      "Loss of train set: 0.2292121946811676 at epoch: 3 and batch_num: 459\n",
      "Loss of train set: 0.2921665906906128 at epoch: 3 and batch_num: 460\n",
      "Loss of train set: 0.3902941346168518 at epoch: 3 and batch_num: 461\n",
      "Loss of train set: 0.3442831039428711 at epoch: 3 and batch_num: 462\n",
      "Loss of train set: 0.5868628025054932 at epoch: 3 and batch_num: 463\n",
      "Loss of train set: 0.5434768199920654 at epoch: 3 and batch_num: 464\n",
      "Loss of train set: 0.5349769592285156 at epoch: 3 and batch_num: 465\n",
      "Loss of train set: 0.5873806476593018 at epoch: 3 and batch_num: 466\n",
      "Loss of train set: 0.4596761465072632 at epoch: 3 and batch_num: 467\n",
      "Loss of train set: 0.3787643313407898 at epoch: 3 and batch_num: 468\n",
      "Loss of train set: 0.199162557721138 at epoch: 3 and batch_num: 469\n",
      "Loss of train set: 0.3410567045211792 at epoch: 3 and batch_num: 470\n",
      "Loss of train set: 0.22592586278915405 at epoch: 3 and batch_num: 471\n",
      "Loss of train set: 0.2145194113254547 at epoch: 3 and batch_num: 472\n",
      "Loss of train set: 0.13919195532798767 at epoch: 3 and batch_num: 473\n",
      "Loss of train set: 0.3934025466442108 at epoch: 3 and batch_num: 474\n",
      "Loss of train set: 0.26605579257011414 at epoch: 3 and batch_num: 475\n",
      "Loss of train set: 0.3535185754299164 at epoch: 3 and batch_num: 476\n",
      "Loss of train set: 0.39840584993362427 at epoch: 3 and batch_num: 477\n",
      "Loss of train set: 0.31360697746276855 at epoch: 3 and batch_num: 478\n",
      "Loss of train set: 0.25351017713546753 at epoch: 3 and batch_num: 479\n",
      "Loss of train set: 0.2898464798927307 at epoch: 3 and batch_num: 480\n",
      "Loss of train set: 0.3459622263908386 at epoch: 3 and batch_num: 481\n",
      "Loss of train set: 0.26935479044914246 at epoch: 3 and batch_num: 482\n",
      "Loss of train set: 0.3991280198097229 at epoch: 3 and batch_num: 483\n",
      "Loss of train set: 0.23604601621627808 at epoch: 3 and batch_num: 484\n",
      "Loss of train set: 0.5087711215019226 at epoch: 3 and batch_num: 485\n",
      "Loss of train set: 0.40312907099723816 at epoch: 3 and batch_num: 486\n",
      "Loss of train set: 0.4180435836315155 at epoch: 3 and batch_num: 487\n",
      "Loss of train set: 0.31333038210868835 at epoch: 3 and batch_num: 488\n",
      "Loss of train set: 0.3321698307991028 at epoch: 3 and batch_num: 489\n",
      "Loss of train set: 0.32329246401786804 at epoch: 3 and batch_num: 490\n",
      "Loss of train set: 0.2742599546909332 at epoch: 3 and batch_num: 491\n",
      "Loss of train set: 0.20236946642398834 at epoch: 3 and batch_num: 492\n",
      "Loss of train set: 0.4150860011577606 at epoch: 3 and batch_num: 493\n",
      "Loss of train set: 0.32613539695739746 at epoch: 3 and batch_num: 494\n",
      "Loss of train set: 0.3302628993988037 at epoch: 3 and batch_num: 495\n",
      "Loss of train set: 0.4307035207748413 at epoch: 3 and batch_num: 496\n",
      "Loss of train set: 0.6308140754699707 at epoch: 3 and batch_num: 497\n",
      "Loss of train set: 0.3424432575702667 at epoch: 3 and batch_num: 498\n",
      "Loss of train set: 0.3287607431411743 at epoch: 3 and batch_num: 499\n",
      "Loss of train set: 0.3984297513961792 at epoch: 3 and batch_num: 500\n",
      "Loss of train set: 0.3869835138320923 at epoch: 3 and batch_num: 501\n",
      "Loss of train set: 0.2710232436656952 at epoch: 3 and batch_num: 502\n",
      "Loss of train set: 0.3747643232345581 at epoch: 3 and batch_num: 503\n",
      "Loss of train set: 0.38742101192474365 at epoch: 3 and batch_num: 504\n",
      "Loss of train set: 0.3195367455482483 at epoch: 3 and batch_num: 505\n",
      "Loss of train set: 0.5711861252784729 at epoch: 3 and batch_num: 506\n",
      "Loss of train set: 0.3909710645675659 at epoch: 3 and batch_num: 507\n",
      "Loss of train set: 0.28812500834465027 at epoch: 3 and batch_num: 508\n",
      "Loss of train set: 0.4022581875324249 at epoch: 3 and batch_num: 509\n",
      "Loss of train set: 0.23784346878528595 at epoch: 3 and batch_num: 510\n",
      "Loss of train set: 0.27972376346588135 at epoch: 3 and batch_num: 511\n",
      "Loss of train set: 0.37605658173561096 at epoch: 3 and batch_num: 512\n",
      "Loss of train set: 0.3170774579048157 at epoch: 3 and batch_num: 513\n",
      "Loss of train set: 0.2865840494632721 at epoch: 3 and batch_num: 514\n",
      "Loss of train set: 0.5655573606491089 at epoch: 3 and batch_num: 515\n",
      "Loss of train set: 0.37323686480522156 at epoch: 3 and batch_num: 516\n",
      "Loss of train set: 0.40691861510276794 at epoch: 3 and batch_num: 517\n",
      "Loss of train set: 0.3495630621910095 at epoch: 3 and batch_num: 518\n",
      "Loss of train set: 0.3720550835132599 at epoch: 3 and batch_num: 519\n",
      "Loss of train set: 0.3724896013736725 at epoch: 3 and batch_num: 520\n",
      "Loss of train set: 0.2756398618221283 at epoch: 3 and batch_num: 521\n",
      "Loss of train set: 0.3934706449508667 at epoch: 3 and batch_num: 522\n",
      "Loss of train set: 0.2455402910709381 at epoch: 3 and batch_num: 523\n",
      "Loss of train set: 0.5049134492874146 at epoch: 3 and batch_num: 524\n",
      "Loss of train set: 0.540304958820343 at epoch: 3 and batch_num: 525\n",
      "Loss of train set: 0.4207805395126343 at epoch: 3 and batch_num: 526\n",
      "Loss of train set: 0.30814328789711 at epoch: 3 and batch_num: 527\n",
      "Loss of train set: 0.27267444133758545 at epoch: 3 and batch_num: 528\n",
      "Loss of train set: 0.2963849902153015 at epoch: 3 and batch_num: 529\n",
      "Loss of train set: 0.19195106625556946 at epoch: 3 and batch_num: 530\n",
      "Loss of train set: 0.35371655225753784 at epoch: 3 and batch_num: 531\n",
      "Loss of train set: 0.24468404054641724 at epoch: 3 and batch_num: 532\n",
      "Loss of train set: 0.2585962414741516 at epoch: 3 and batch_num: 533\n",
      "Loss of train set: 0.2246851623058319 at epoch: 3 and batch_num: 534\n",
      "Loss of train set: 0.34867018461227417 at epoch: 3 and batch_num: 535\n",
      "Loss of train set: 0.38145947456359863 at epoch: 3 and batch_num: 536\n",
      "Loss of train set: 0.4118031859397888 at epoch: 3 and batch_num: 537\n",
      "Loss of train set: 0.34637314081192017 at epoch: 3 and batch_num: 538\n",
      "Loss of train set: 0.6144136786460876 at epoch: 3 and batch_num: 539\n",
      "Loss of train set: 0.5943766236305237 at epoch: 3 and batch_num: 540\n",
      "Loss of train set: 0.4806782603263855 at epoch: 3 and batch_num: 541\n",
      "Loss of train set: 0.3818807005882263 at epoch: 3 and batch_num: 542\n",
      "Loss of train set: 0.2972652316093445 at epoch: 3 and batch_num: 543\n",
      "Loss of train set: 0.27014511823654175 at epoch: 3 and batch_num: 544\n",
      "Loss of train set: 0.47722840309143066 at epoch: 3 and batch_num: 545\n",
      "Loss of train set: 0.29693537950515747 at epoch: 3 and batch_num: 546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.4009113311767578 at epoch: 3 and batch_num: 547\n",
      "Loss of train set: 0.4570685625076294 at epoch: 3 and batch_num: 548\n",
      "Loss of train set: 0.4155636131763458 at epoch: 3 and batch_num: 549\n",
      "Loss of train set: 0.4066839814186096 at epoch: 3 and batch_num: 550\n",
      "Loss of train set: 0.46854734420776367 at epoch: 3 and batch_num: 551\n",
      "Loss of train set: 0.43259742856025696 at epoch: 3 and batch_num: 552\n",
      "Loss of train set: 0.21448522806167603 at epoch: 3 and batch_num: 553\n",
      "Loss of train set: 0.21468448638916016 at epoch: 3 and batch_num: 554\n",
      "Loss of train set: 0.29532724618911743 at epoch: 3 and batch_num: 555\n",
      "Loss of train set: 0.37025758624076843 at epoch: 3 and batch_num: 556\n",
      "Loss of train set: 0.35836437344551086 at epoch: 3 and batch_num: 557\n",
      "Loss of train set: 0.37372642755508423 at epoch: 3 and batch_num: 558\n",
      "Loss of train set: 0.29808780550956726 at epoch: 3 and batch_num: 559\n",
      "Loss of train set: 0.2510404586791992 at epoch: 3 and batch_num: 560\n",
      "Loss of train set: 0.35182589292526245 at epoch: 3 and batch_num: 561\n",
      "Loss of train set: 0.3137965202331543 at epoch: 3 and batch_num: 562\n",
      "Loss of train set: 0.3326692581176758 at epoch: 3 and batch_num: 563\n",
      "Loss of train set: 0.16226458549499512 at epoch: 3 and batch_num: 564\n",
      "Loss of train set: 0.4359397888183594 at epoch: 3 and batch_num: 565\n",
      "Loss of train set: 0.25863897800445557 at epoch: 3 and batch_num: 566\n",
      "Loss of train set: 0.38560783863067627 at epoch: 3 and batch_num: 567\n",
      "Loss of train set: 0.3860793709754944 at epoch: 3 and batch_num: 568\n",
      "Loss of train set: 0.5145401358604431 at epoch: 3 and batch_num: 569\n",
      "Loss of train set: 0.2381490170955658 at epoch: 3 and batch_num: 570\n",
      "Loss of train set: 0.3207678198814392 at epoch: 3 and batch_num: 571\n",
      "Loss of train set: 0.5208638906478882 at epoch: 3 and batch_num: 572\n",
      "Loss of train set: 0.23704183101654053 at epoch: 3 and batch_num: 573\n",
      "Loss of train set: 0.23928028345108032 at epoch: 3 and batch_num: 574\n",
      "Loss of train set: 0.3204580843448639 at epoch: 3 and batch_num: 575\n",
      "Loss of train set: 0.3253493309020996 at epoch: 3 and batch_num: 576\n",
      "Loss of train set: 0.24731752276420593 at epoch: 3 and batch_num: 577\n",
      "Loss of train set: 0.48082318902015686 at epoch: 3 and batch_num: 578\n",
      "Loss of train set: 0.275823175907135 at epoch: 3 and batch_num: 579\n",
      "Loss of train set: 0.5147148370742798 at epoch: 3 and batch_num: 580\n",
      "Loss of train set: 0.33385854959487915 at epoch: 3 and batch_num: 581\n",
      "Loss of train set: 0.26993998885154724 at epoch: 3 and batch_num: 582\n",
      "Loss of train set: 0.23883621394634247 at epoch: 3 and batch_num: 583\n",
      "Loss of train set: 0.38098886609077454 at epoch: 3 and batch_num: 584\n",
      "Loss of train set: 0.2602762281894684 at epoch: 3 and batch_num: 585\n",
      "Loss of train set: 0.5270009636878967 at epoch: 3 and batch_num: 586\n",
      "Loss of train set: 0.4615589678287506 at epoch: 3 and batch_num: 587\n",
      "Loss of train set: 0.47790032625198364 at epoch: 3 and batch_num: 588\n",
      "Loss of train set: 0.5339301228523254 at epoch: 3 and batch_num: 589\n",
      "Loss of train set: 0.4060734510421753 at epoch: 3 and batch_num: 590\n",
      "Loss of train set: 0.2852713465690613 at epoch: 3 and batch_num: 591\n",
      "Loss of train set: 0.3385833501815796 at epoch: 3 and batch_num: 592\n",
      "Loss of train set: 0.2859732508659363 at epoch: 3 and batch_num: 593\n",
      "Loss of train set: 0.3752104640007019 at epoch: 3 and batch_num: 594\n",
      "Loss of train set: 0.4821130335330963 at epoch: 3 and batch_num: 595\n",
      "Loss of train set: 0.2443949282169342 at epoch: 3 and batch_num: 596\n",
      "Loss of train set: 0.37440526485443115 at epoch: 3 and batch_num: 597\n",
      "Loss of train set: 0.2598344385623932 at epoch: 3 and batch_num: 598\n",
      "Loss of train set: 0.3557305634021759 at epoch: 3 and batch_num: 599\n",
      "Loss of train set: 0.3978414237499237 at epoch: 3 and batch_num: 600\n",
      "Loss of train set: 0.4930216073989868 at epoch: 3 and batch_num: 601\n",
      "Loss of train set: 0.4291462302207947 at epoch: 3 and batch_num: 602\n",
      "Loss of train set: 0.404619038105011 at epoch: 3 and batch_num: 603\n",
      "Loss of train set: 0.34089529514312744 at epoch: 3 and batch_num: 604\n",
      "Loss of train set: 0.3927694857120514 at epoch: 3 and batch_num: 605\n",
      "Loss of train set: 0.32080820202827454 at epoch: 3 and batch_num: 606\n",
      "Loss of train set: 0.39859455823898315 at epoch: 3 and batch_num: 607\n",
      "Loss of train set: 0.2652713358402252 at epoch: 3 and batch_num: 608\n",
      "Loss of train set: 0.1928684413433075 at epoch: 3 and batch_num: 609\n",
      "Loss of train set: 0.39087867736816406 at epoch: 3 and batch_num: 610\n",
      "Loss of train set: 0.3987598717212677 at epoch: 3 and batch_num: 611\n",
      "Loss of train set: 0.5398759841918945 at epoch: 3 and batch_num: 612\n",
      "Loss of train set: 0.3608045279979706 at epoch: 3 and batch_num: 613\n",
      "Loss of train set: 0.19222310185432434 at epoch: 3 and batch_num: 614\n",
      "Loss of train set: 0.21942177414894104 at epoch: 3 and batch_num: 615\n",
      "Loss of train set: 0.3211085796356201 at epoch: 3 and batch_num: 616\n",
      "Loss of train set: 0.4357236623764038 at epoch: 3 and batch_num: 617\n",
      "Loss of train set: 0.45789098739624023 at epoch: 3 and batch_num: 618\n",
      "Loss of train set: 0.38536691665649414 at epoch: 3 and batch_num: 619\n",
      "Loss of train set: 0.30788856744766235 at epoch: 3 and batch_num: 620\n",
      "Loss of train set: 0.4036242961883545 at epoch: 3 and batch_num: 621\n",
      "Loss of train set: 0.30727332830429077 at epoch: 3 and batch_num: 622\n",
      "Loss of train set: 0.2979302406311035 at epoch: 3 and batch_num: 623\n",
      "Loss of train set: 0.4650620222091675 at epoch: 3 and batch_num: 624\n",
      "Loss of train set: 0.2873556911945343 at epoch: 3 and batch_num: 625\n",
      "Loss of train set: 0.34344229102134705 at epoch: 3 and batch_num: 626\n",
      "Loss of train set: 0.5785481929779053 at epoch: 3 and batch_num: 627\n",
      "Loss of train set: 0.25861647725105286 at epoch: 3 and batch_num: 628\n",
      "Loss of train set: 0.28742682933807373 at epoch: 3 and batch_num: 629\n",
      "Loss of train set: 0.33153781294822693 at epoch: 3 and batch_num: 630\n",
      "Loss of train set: 0.40795469284057617 at epoch: 3 and batch_num: 631\n",
      "Loss of train set: 0.4225437343120575 at epoch: 3 and batch_num: 632\n",
      "Loss of train set: 0.3631734251976013 at epoch: 3 and batch_num: 633\n",
      "Loss of train set: 0.4218468964099884 at epoch: 3 and batch_num: 634\n",
      "Loss of train set: 0.3629192113876343 at epoch: 3 and batch_num: 635\n",
      "Loss of train set: 0.48968425393104553 at epoch: 3 and batch_num: 636\n",
      "Loss of train set: 0.21195468306541443 at epoch: 3 and batch_num: 637\n",
      "Loss of train set: 0.2331320345401764 at epoch: 3 and batch_num: 638\n",
      "Loss of train set: 0.36093050241470337 at epoch: 3 and batch_num: 639\n",
      "Loss of train set: 0.3038839101791382 at epoch: 3 and batch_num: 640\n",
      "Loss of train set: 0.39377686381340027 at epoch: 3 and batch_num: 641\n",
      "Loss of train set: 0.35090047121047974 at epoch: 3 and batch_num: 642\n",
      "Loss of train set: 0.41349345445632935 at epoch: 3 and batch_num: 643\n",
      "Loss of train set: 0.3797697126865387 at epoch: 3 and batch_num: 644\n",
      "Loss of train set: 0.23485174775123596 at epoch: 3 and batch_num: 645\n",
      "Loss of train set: 0.5132819414138794 at epoch: 3 and batch_num: 646\n",
      "Loss of train set: 0.4122922122478485 at epoch: 3 and batch_num: 647\n",
      "Loss of train set: 0.4129306674003601 at epoch: 3 and batch_num: 648\n",
      "Loss of train set: 0.26809513568878174 at epoch: 3 and batch_num: 649\n",
      "Loss of train set: 0.34350264072418213 at epoch: 3 and batch_num: 650\n",
      "Loss of train set: 0.45772379636764526 at epoch: 3 and batch_num: 651\n",
      "Loss of train set: 0.45079904794692993 at epoch: 3 and batch_num: 652\n",
      "Loss of train set: 0.4518040418624878 at epoch: 3 and batch_num: 653\n",
      "Loss of train set: 0.4259856939315796 at epoch: 3 and batch_num: 654\n",
      "Loss of train set: 0.1471118927001953 at epoch: 3 and batch_num: 655\n",
      "Loss of train set: 0.37607330083847046 at epoch: 3 and batch_num: 656\n",
      "Loss of train set: 0.15481524169445038 at epoch: 3 and batch_num: 657\n",
      "Loss of train set: 0.26946866512298584 at epoch: 3 and batch_num: 658\n",
      "Loss of train set: 0.43933001160621643 at epoch: 3 and batch_num: 659\n",
      "Loss of train set: 0.36990973353385925 at epoch: 3 and batch_num: 660\n",
      "Loss of train set: 0.24874255061149597 at epoch: 3 and batch_num: 661\n",
      "Loss of train set: 0.3436810076236725 at epoch: 3 and batch_num: 662\n",
      "Loss of train set: 0.34668463468551636 at epoch: 3 and batch_num: 663\n",
      "Loss of train set: 0.2961265742778778 at epoch: 3 and batch_num: 664\n",
      "Loss of train set: 0.3385898470878601 at epoch: 3 and batch_num: 665\n",
      "Loss of train set: 0.3902474641799927 at epoch: 3 and batch_num: 666\n",
      "Loss of train set: 0.5983531475067139 at epoch: 3 and batch_num: 667\n",
      "Loss of train set: 0.3003956377506256 at epoch: 3 and batch_num: 668\n",
      "Loss of train set: 0.5685408115386963 at epoch: 3 and batch_num: 669\n",
      "Loss of train set: 0.45662063360214233 at epoch: 3 and batch_num: 670\n",
      "Loss of train set: 0.521401047706604 at epoch: 3 and batch_num: 671\n",
      "Loss of train set: 0.24948352575302124 at epoch: 3 and batch_num: 672\n",
      "Loss of train set: 0.3621780276298523 at epoch: 3 and batch_num: 673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.25173723697662354 at epoch: 3 and batch_num: 674\n",
      "Loss of train set: 0.28569889068603516 at epoch: 3 and batch_num: 675\n",
      "Loss of train set: 0.22582606971263885 at epoch: 3 and batch_num: 676\n",
      "Loss of train set: 0.30519556999206543 at epoch: 3 and batch_num: 677\n",
      "Loss of train set: 0.33532801270484924 at epoch: 3 and batch_num: 678\n",
      "Loss of train set: 0.5287194848060608 at epoch: 3 and batch_num: 679\n",
      "Loss of train set: 0.3269439935684204 at epoch: 3 and batch_num: 680\n",
      "Loss of train set: 0.48553407192230225 at epoch: 3 and batch_num: 681\n",
      "Loss of train set: 0.3388320207595825 at epoch: 3 and batch_num: 682\n",
      "Loss of train set: 0.5812841057777405 at epoch: 3 and batch_num: 683\n",
      "Loss of train set: 0.5588445067405701 at epoch: 3 and batch_num: 684\n",
      "Loss of train set: 0.41381245851516724 at epoch: 3 and batch_num: 685\n",
      "Loss of train set: 0.38061201572418213 at epoch: 3 and batch_num: 686\n",
      "Loss of train set: 0.30696818232536316 at epoch: 3 and batch_num: 687\n",
      "Loss of train set: 0.29686009883880615 at epoch: 3 and batch_num: 688\n",
      "Loss of train set: 0.28082144260406494 at epoch: 3 and batch_num: 689\n",
      "Loss of train set: 0.2948830723762512 at epoch: 3 and batch_num: 690\n",
      "Loss of train set: 0.4039079248905182 at epoch: 3 and batch_num: 691\n",
      "Loss of train set: 0.3076004385948181 at epoch: 3 and batch_num: 692\n",
      "Loss of train set: 0.44335228204727173 at epoch: 3 and batch_num: 693\n",
      "Loss of train set: 0.2725125551223755 at epoch: 3 and batch_num: 694\n",
      "Loss of train set: 0.2992643415927887 at epoch: 3 and batch_num: 695\n",
      "Loss of train set: 0.5230640172958374 at epoch: 3 and batch_num: 696\n",
      "Loss of train set: 0.2643561363220215 at epoch: 3 and batch_num: 697\n",
      "Loss of train set: 0.19873908162117004 at epoch: 3 and batch_num: 698\n",
      "Loss of train set: 0.25638866424560547 at epoch: 3 and batch_num: 699\n",
      "Loss of train set: 0.3701104521751404 at epoch: 3 and batch_num: 700\n",
      "Loss of train set: 0.40005385875701904 at epoch: 3 and batch_num: 701\n",
      "Loss of train set: 0.4306718707084656 at epoch: 3 and batch_num: 702\n",
      "Loss of train set: 0.27547839283943176 at epoch: 3 and batch_num: 703\n",
      "Loss of train set: 0.2143823504447937 at epoch: 3 and batch_num: 704\n",
      "Loss of train set: 0.34498322010040283 at epoch: 3 and batch_num: 705\n",
      "Loss of train set: 0.506171703338623 at epoch: 3 and batch_num: 706\n",
      "Loss of train set: 0.4407479166984558 at epoch: 3 and batch_num: 707\n",
      "Loss of train set: 0.3383376896381378 at epoch: 3 and batch_num: 708\n",
      "Loss of train set: 0.4181559085845947 at epoch: 3 and batch_num: 709\n",
      "Loss of train set: 0.31403499841690063 at epoch: 3 and batch_num: 710\n",
      "Loss of train set: 0.34860870242118835 at epoch: 3 and batch_num: 711\n",
      "Loss of train set: 0.35871922969818115 at epoch: 3 and batch_num: 712\n",
      "Loss of train set: 0.29153338074684143 at epoch: 3 and batch_num: 713\n",
      "Loss of train set: 0.3347671627998352 at epoch: 3 and batch_num: 714\n",
      "Loss of train set: 0.3506379723548889 at epoch: 3 and batch_num: 715\n",
      "Loss of train set: 0.4226294755935669 at epoch: 3 and batch_num: 716\n",
      "Loss of train set: 0.3857230246067047 at epoch: 3 and batch_num: 717\n",
      "Loss of train set: 0.44579529762268066 at epoch: 3 and batch_num: 718\n",
      "Loss of train set: 0.3478125333786011 at epoch: 3 and batch_num: 719\n",
      "Loss of train set: 0.23953552544116974 at epoch: 3 and batch_num: 720\n",
      "Loss of train set: 0.3077131509780884 at epoch: 3 and batch_num: 721\n",
      "Loss of train set: 0.3930063545703888 at epoch: 3 and batch_num: 722\n",
      "Loss of train set: 0.26799458265304565 at epoch: 3 and batch_num: 723\n",
      "Loss of train set: 0.23771165311336517 at epoch: 3 and batch_num: 724\n",
      "Loss of train set: 0.3017956018447876 at epoch: 3 and batch_num: 725\n",
      "Loss of train set: 0.31042057275772095 at epoch: 3 and batch_num: 726\n",
      "Loss of train set: 0.38707929849624634 at epoch: 3 and batch_num: 727\n",
      "Loss of train set: 0.49007782340049744 at epoch: 3 and batch_num: 728\n",
      "Loss of train set: 0.36005353927612305 at epoch: 3 and batch_num: 729\n",
      "Loss of train set: 0.3011115491390228 at epoch: 3 and batch_num: 730\n",
      "Loss of train set: 0.31244248151779175 at epoch: 3 and batch_num: 731\n",
      "Loss of train set: 0.46125903725624084 at epoch: 3 and batch_num: 732\n",
      "Loss of train set: 0.18175315856933594 at epoch: 3 and batch_num: 733\n",
      "Loss of train set: 0.30379945039749146 at epoch: 3 and batch_num: 734\n",
      "Loss of train set: 0.3602348268032074 at epoch: 3 and batch_num: 735\n",
      "Loss of train set: 0.3224996328353882 at epoch: 3 and batch_num: 736\n",
      "Loss of train set: 0.28224730491638184 at epoch: 3 and batch_num: 737\n",
      "Loss of train set: 0.5062050819396973 at epoch: 3 and batch_num: 738\n",
      "Loss of train set: 0.44054579734802246 at epoch: 3 and batch_num: 739\n",
      "Loss of train set: 0.4960078001022339 at epoch: 3 and batch_num: 740\n",
      "Loss of train set: 0.4805513024330139 at epoch: 3 and batch_num: 741\n",
      "Loss of train set: 0.38922011852264404 at epoch: 3 and batch_num: 742\n",
      "Loss of train set: 0.3805736303329468 at epoch: 3 and batch_num: 743\n",
      "Loss of train set: 0.33990877866744995 at epoch: 3 and batch_num: 744\n",
      "Loss of train set: 0.4636216163635254 at epoch: 3 and batch_num: 745\n",
      "Loss of train set: 0.19424647092819214 at epoch: 3 and batch_num: 746\n",
      "Loss of train set: 0.2471565157175064 at epoch: 3 and batch_num: 747\n",
      "Loss of train set: 0.5033808946609497 at epoch: 3 and batch_num: 748\n",
      "Loss of train set: 0.29317760467529297 at epoch: 3 and batch_num: 749\n",
      "Loss of train set: 0.33075568079948425 at epoch: 3 and batch_num: 750\n",
      "Loss of train set: 0.3527289927005768 at epoch: 3 and batch_num: 751\n",
      "Loss of train set: 0.3206430673599243 at epoch: 3 and batch_num: 752\n",
      "Loss of train set: 0.5085816383361816 at epoch: 3 and batch_num: 753\n",
      "Loss of train set: 0.29619085788726807 at epoch: 3 and batch_num: 754\n",
      "Loss of train set: 0.42057865858078003 at epoch: 3 and batch_num: 755\n",
      "Loss of train set: 0.24568837881088257 at epoch: 3 and batch_num: 756\n",
      "Loss of train set: 0.3356352746486664 at epoch: 3 and batch_num: 757\n",
      "Loss of train set: 0.2779979705810547 at epoch: 3 and batch_num: 758\n",
      "Loss of train set: 0.33375871181488037 at epoch: 3 and batch_num: 759\n",
      "Loss of train set: 0.28722238540649414 at epoch: 3 and batch_num: 760\n",
      "Loss of train set: 0.3021906614303589 at epoch: 3 and batch_num: 761\n",
      "Loss of train set: 0.3564189076423645 at epoch: 3 and batch_num: 762\n",
      "Loss of train set: 0.3155338168144226 at epoch: 3 and batch_num: 763\n",
      "Loss of train set: 0.3876238465309143 at epoch: 3 and batch_num: 764\n",
      "Loss of train set: 0.29729026556015015 at epoch: 3 and batch_num: 765\n",
      "Loss of train set: 0.31314384937286377 at epoch: 3 and batch_num: 766\n",
      "Loss of train set: 0.3748381733894348 at epoch: 3 and batch_num: 767\n",
      "Loss of train set: 0.3195697069168091 at epoch: 3 and batch_num: 768\n",
      "Loss of train set: 0.30122387409210205 at epoch: 3 and batch_num: 769\n",
      "Loss of train set: 0.31347572803497314 at epoch: 3 and batch_num: 770\n",
      "Loss of train set: 0.24918222427368164 at epoch: 3 and batch_num: 771\n",
      "Loss of train set: 0.26826971769332886 at epoch: 3 and batch_num: 772\n",
      "Loss of train set: 0.21332982182502747 at epoch: 3 and batch_num: 773\n",
      "Loss of train set: 0.19992968440055847 at epoch: 3 and batch_num: 774\n",
      "Loss of train set: 0.3936998248100281 at epoch: 3 and batch_num: 775\n",
      "Loss of train set: 0.3886357545852661 at epoch: 3 and batch_num: 776\n",
      "Loss of train set: 0.28889232873916626 at epoch: 3 and batch_num: 777\n",
      "Loss of train set: 0.4240487217903137 at epoch: 3 and batch_num: 778\n",
      "Loss of train set: 0.40208011865615845 at epoch: 3 and batch_num: 779\n",
      "Loss of train set: 0.2876821756362915 at epoch: 3 and batch_num: 780\n",
      "Loss of train set: 0.47001880407333374 at epoch: 3 and batch_num: 781\n",
      "Loss of train set: 0.3338850438594818 at epoch: 3 and batch_num: 782\n",
      "Loss of train set: 0.2263677567243576 at epoch: 3 and batch_num: 783\n",
      "Loss of train set: 0.38158220052719116 at epoch: 3 and batch_num: 784\n",
      "Loss of train set: 0.5131118297576904 at epoch: 3 and batch_num: 785\n",
      "Loss of train set: 0.2996392548084259 at epoch: 3 and batch_num: 786\n",
      "Loss of train set: 0.3671633303165436 at epoch: 3 and batch_num: 787\n",
      "Loss of train set: 0.34443122148513794 at epoch: 3 and batch_num: 788\n",
      "Loss of train set: 0.3642441928386688 at epoch: 3 and batch_num: 789\n",
      "Loss of train set: 0.29801204800605774 at epoch: 3 and batch_num: 790\n",
      "Loss of train set: 0.32039985060691833 at epoch: 3 and batch_num: 791\n",
      "Loss of train set: 0.3859909176826477 at epoch: 3 and batch_num: 792\n",
      "Loss of train set: 0.25917547941207886 at epoch: 3 and batch_num: 793\n",
      "Loss of train set: 0.28936767578125 at epoch: 3 and batch_num: 794\n",
      "Loss of train set: 0.27219027280807495 at epoch: 3 and batch_num: 795\n",
      "Loss of train set: 0.4938429296016693 at epoch: 3 and batch_num: 796\n",
      "Loss of train set: 0.4120669662952423 at epoch: 3 and batch_num: 797\n",
      "Loss of train set: 0.36162930727005005 at epoch: 3 and batch_num: 798\n",
      "Loss of train set: 0.25787943601608276 at epoch: 3 and batch_num: 799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2561814486980438 at epoch: 3 and batch_num: 800\n",
      "Loss of train set: 0.4479371905326843 at epoch: 3 and batch_num: 801\n",
      "Loss of train set: 0.3310825526714325 at epoch: 3 and batch_num: 802\n",
      "Loss of train set: 0.35538914799690247 at epoch: 3 and batch_num: 803\n",
      "Loss of train set: 0.28641384840011597 at epoch: 3 and batch_num: 804\n",
      "Loss of train set: 0.3578549027442932 at epoch: 3 and batch_num: 805\n",
      "Loss of train set: 0.37929078936576843 at epoch: 3 and batch_num: 806\n",
      "Loss of train set: 0.365389347076416 at epoch: 3 and batch_num: 807\n",
      "Loss of train set: 0.36604368686676025 at epoch: 3 and batch_num: 808\n",
      "Loss of train set: 0.36864298582077026 at epoch: 3 and batch_num: 809\n",
      "Loss of train set: 0.4915613830089569 at epoch: 3 and batch_num: 810\n",
      "Loss of train set: 0.37004002928733826 at epoch: 3 and batch_num: 811\n",
      "Loss of train set: 0.3742353916168213 at epoch: 3 and batch_num: 812\n",
      "Loss of train set: 0.41149282455444336 at epoch: 3 and batch_num: 813\n",
      "Loss of train set: 0.4111141264438629 at epoch: 3 and batch_num: 814\n",
      "Loss of train set: 0.430549681186676 at epoch: 3 and batch_num: 815\n",
      "Loss of train set: 0.25105908513069153 at epoch: 3 and batch_num: 816\n",
      "Loss of train set: 0.23025748133659363 at epoch: 3 and batch_num: 817\n",
      "Loss of train set: 0.32631173729896545 at epoch: 3 and batch_num: 818\n",
      "Loss of train set: 0.3514806628227234 at epoch: 3 and batch_num: 819\n",
      "Loss of train set: 0.3116510808467865 at epoch: 3 and batch_num: 820\n",
      "Loss of train set: 0.2991916537284851 at epoch: 3 and batch_num: 821\n",
      "Loss of train set: 0.34675443172454834 at epoch: 3 and batch_num: 822\n",
      "Loss of train set: 0.16321659088134766 at epoch: 3 and batch_num: 823\n",
      "Loss of train set: 0.3189083933830261 at epoch: 3 and batch_num: 824\n",
      "Loss of train set: 0.3699166178703308 at epoch: 3 and batch_num: 825\n",
      "Loss of train set: 0.19468346238136292 at epoch: 3 and batch_num: 826\n",
      "Loss of train set: 0.6008800268173218 at epoch: 3 and batch_num: 827\n",
      "Loss of train set: 0.4384400248527527 at epoch: 3 and batch_num: 828\n",
      "Loss of train set: 0.4518245756626129 at epoch: 3 and batch_num: 829\n",
      "Loss of train set: 0.37497562170028687 at epoch: 3 and batch_num: 830\n",
      "Loss of train set: 0.2601911127567291 at epoch: 3 and batch_num: 831\n",
      "Loss of train set: 0.2333676517009735 at epoch: 3 and batch_num: 832\n",
      "Loss of train set: 0.36085766553878784 at epoch: 3 and batch_num: 833\n",
      "Loss of train set: 0.3148877024650574 at epoch: 3 and batch_num: 834\n",
      "Loss of train set: 0.33936548233032227 at epoch: 3 and batch_num: 835\n",
      "Loss of train set: 0.29876071214675903 at epoch: 3 and batch_num: 836\n",
      "Loss of train set: 0.41185417771339417 at epoch: 3 and batch_num: 837\n",
      "Loss of train set: 0.40416717529296875 at epoch: 3 and batch_num: 838\n",
      "Loss of train set: 0.3697223663330078 at epoch: 3 and batch_num: 839\n",
      "Loss of train set: 0.40522921085357666 at epoch: 3 and batch_num: 840\n",
      "Loss of train set: 0.42030245065689087 at epoch: 3 and batch_num: 841\n",
      "Loss of train set: 0.2606350779533386 at epoch: 3 and batch_num: 842\n",
      "Loss of train set: 0.2940521538257599 at epoch: 3 and batch_num: 843\n",
      "Loss of train set: 0.3397482633590698 at epoch: 3 and batch_num: 844\n",
      "Loss of train set: 0.2721446454524994 at epoch: 3 and batch_num: 845\n",
      "Loss of train set: 0.34113362431526184 at epoch: 3 and batch_num: 846\n",
      "Loss of train set: 0.2516616880893707 at epoch: 3 and batch_num: 847\n",
      "Loss of train set: 0.26380929350852966 at epoch: 3 and batch_num: 848\n",
      "Loss of train set: 0.5523483753204346 at epoch: 3 and batch_num: 849\n",
      "Loss of train set: 0.30774521827697754 at epoch: 3 and batch_num: 850\n",
      "Loss of train set: 0.4483588933944702 at epoch: 3 and batch_num: 851\n",
      "Loss of train set: 0.34396642446517944 at epoch: 3 and batch_num: 852\n",
      "Loss of train set: 0.3551764488220215 at epoch: 3 and batch_num: 853\n",
      "Loss of train set: 0.3266957998275757 at epoch: 3 and batch_num: 854\n",
      "Loss of train set: 0.33511677384376526 at epoch: 3 and batch_num: 855\n",
      "Loss of train set: 0.32798415422439575 at epoch: 3 and batch_num: 856\n",
      "Loss of train set: 0.21920457482337952 at epoch: 3 and batch_num: 857\n",
      "Loss of train set: 0.44945028424263 at epoch: 3 and batch_num: 858\n",
      "Loss of train set: 0.3507233262062073 at epoch: 3 and batch_num: 859\n",
      "Loss of train set: 0.21822500228881836 at epoch: 3 and batch_num: 860\n",
      "Loss of train set: 0.22938783466815948 at epoch: 3 and batch_num: 861\n",
      "Loss of train set: 0.3162171244621277 at epoch: 3 and batch_num: 862\n",
      "Loss of train set: 0.3061959147453308 at epoch: 3 and batch_num: 863\n",
      "Loss of train set: 0.3813644051551819 at epoch: 3 and batch_num: 864\n",
      "Loss of train set: 0.2858121991157532 at epoch: 3 and batch_num: 865\n",
      "Loss of train set: 0.3246266841888428 at epoch: 3 and batch_num: 866\n",
      "Loss of train set: 0.37516748905181885 at epoch: 3 and batch_num: 867\n",
      "Loss of train set: 0.3844314217567444 at epoch: 3 and batch_num: 868\n",
      "Loss of train set: 0.29458969831466675 at epoch: 3 and batch_num: 869\n",
      "Loss of train set: 0.47705674171447754 at epoch: 3 and batch_num: 870\n",
      "Loss of train set: 0.4467385411262512 at epoch: 3 and batch_num: 871\n",
      "Loss of train set: 0.4897087514400482 at epoch: 3 and batch_num: 872\n",
      "Loss of train set: 0.24663999676704407 at epoch: 3 and batch_num: 873\n",
      "Loss of train set: 0.34576261043548584 at epoch: 3 and batch_num: 874\n",
      "Loss of train set: 0.23801037669181824 at epoch: 3 and batch_num: 875\n",
      "Loss of train set: 0.34149014949798584 at epoch: 3 and batch_num: 876\n",
      "Loss of train set: 0.321511447429657 at epoch: 3 and batch_num: 877\n",
      "Loss of train set: 0.2994491457939148 at epoch: 3 and batch_num: 878\n",
      "Loss of train set: 0.3140500783920288 at epoch: 3 and batch_num: 879\n",
      "Loss of train set: 0.4120196998119354 at epoch: 3 and batch_num: 880\n",
      "Loss of train set: 0.3153877258300781 at epoch: 3 and batch_num: 881\n",
      "Loss of train set: 0.3864484429359436 at epoch: 3 and batch_num: 882\n",
      "Loss of train set: 0.4388888478279114 at epoch: 3 and batch_num: 883\n",
      "Loss of train set: 0.47036275267601013 at epoch: 3 and batch_num: 884\n",
      "Loss of train set: 0.2134462296962738 at epoch: 3 and batch_num: 885\n",
      "Loss of train set: 0.44454342126846313 at epoch: 3 and batch_num: 886\n",
      "Loss of train set: 0.32949185371398926 at epoch: 3 and batch_num: 887\n",
      "Loss of train set: 0.3928018808364868 at epoch: 3 and batch_num: 888\n",
      "Loss of train set: 0.40108856558799744 at epoch: 3 and batch_num: 889\n",
      "Loss of train set: 0.3116309642791748 at epoch: 3 and batch_num: 890\n",
      "Loss of train set: 0.2766505777835846 at epoch: 3 and batch_num: 891\n",
      "Loss of train set: 0.2601740062236786 at epoch: 3 and batch_num: 892\n",
      "Loss of train set: 0.3047837018966675 at epoch: 3 and batch_num: 893\n",
      "Loss of train set: 0.3599339723587036 at epoch: 3 and batch_num: 894\n",
      "Loss of train set: 0.3331717252731323 at epoch: 3 and batch_num: 895\n",
      "Loss of train set: 0.2642325162887573 at epoch: 3 and batch_num: 896\n",
      "Loss of train set: 0.3542632460594177 at epoch: 3 and batch_num: 897\n",
      "Loss of train set: 0.5075170993804932 at epoch: 3 and batch_num: 898\n",
      "Loss of train set: 0.3834840655326843 at epoch: 3 and batch_num: 899\n",
      "Loss of train set: 0.29733869433403015 at epoch: 3 and batch_num: 900\n",
      "Loss of train set: 0.4547427296638489 at epoch: 3 and batch_num: 901\n",
      "Loss of train set: 0.2816633880138397 at epoch: 3 and batch_num: 902\n",
      "Loss of train set: 0.4296487271785736 at epoch: 3 and batch_num: 903\n",
      "Loss of train set: 0.22672848403453827 at epoch: 3 and batch_num: 904\n",
      "Loss of train set: 0.2451377958059311 at epoch: 3 and batch_num: 905\n",
      "Loss of train set: 0.37543749809265137 at epoch: 3 and batch_num: 906\n",
      "Loss of train set: 0.29243117570877075 at epoch: 3 and batch_num: 907\n",
      "Loss of train set: 0.602787971496582 at epoch: 3 and batch_num: 908\n",
      "Loss of train set: 0.23234498500823975 at epoch: 3 and batch_num: 909\n",
      "Loss of train set: 0.30090343952178955 at epoch: 3 and batch_num: 910\n",
      "Loss of train set: 0.21253390610218048 at epoch: 3 and batch_num: 911\n",
      "Loss of train set: 0.49747782945632935 at epoch: 3 and batch_num: 912\n",
      "Loss of train set: 0.4295222759246826 at epoch: 3 and batch_num: 913\n",
      "Loss of train set: 0.2868114709854126 at epoch: 3 and batch_num: 914\n",
      "Loss of train set: 0.37116739153862 at epoch: 3 and batch_num: 915\n",
      "Loss of train set: 0.25786375999450684 at epoch: 3 and batch_num: 916\n",
      "Loss of train set: 0.410935640335083 at epoch: 3 and batch_num: 917\n",
      "Loss of train set: 0.19040974974632263 at epoch: 3 and batch_num: 918\n",
      "Loss of train set: 0.5177679061889648 at epoch: 3 and batch_num: 919\n",
      "Loss of train set: 0.2810932397842407 at epoch: 3 and batch_num: 920\n",
      "Loss of train set: 0.23364165425300598 at epoch: 3 and batch_num: 921\n",
      "Loss of train set: 0.4808391332626343 at epoch: 3 and batch_num: 922\n",
      "Loss of train set: 0.40560808777809143 at epoch: 3 and batch_num: 923\n",
      "Loss of train set: 0.3061072826385498 at epoch: 3 and batch_num: 924\n",
      "Loss of train set: 0.22589179873466492 at epoch: 3 and batch_num: 925\n",
      "Loss of train set: 0.299277126789093 at epoch: 3 and batch_num: 926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2279684841632843 at epoch: 3 and batch_num: 927\n",
      "Loss of train set: 0.23977836966514587 at epoch: 3 and batch_num: 928\n",
      "Loss of train set: 0.19684594869613647 at epoch: 3 and batch_num: 929\n",
      "Loss of train set: 0.38638076186180115 at epoch: 3 and batch_num: 930\n",
      "Loss of train set: 0.36340853571891785 at epoch: 3 and batch_num: 931\n",
      "Loss of train set: 0.2766600251197815 at epoch: 3 and batch_num: 932\n",
      "Loss of train set: 0.23626670241355896 at epoch: 3 and batch_num: 933\n",
      "Loss of train set: 0.25380831956863403 at epoch: 3 and batch_num: 934\n",
      "Loss of train set: 0.41394519805908203 at epoch: 3 and batch_num: 935\n",
      "Loss of train set: 0.3449236750602722 at epoch: 3 and batch_num: 936\n",
      "Loss of train set: 0.3130996823310852 at epoch: 3 and batch_num: 937\n",
      "Accuracy of train set: 0.8724666666666666\n",
      "Loss of test set: 0.5723856687545776 at epoch: 3 and batch_num: 0\n",
      "Loss of test set: 0.4097191095352173 at epoch: 3 and batch_num: 1\n",
      "Loss of test set: 0.47945737838745117 at epoch: 3 and batch_num: 2\n",
      "Loss of test set: 0.3774529695510864 at epoch: 3 and batch_num: 3\n",
      "Loss of test set: 0.23192322254180908 at epoch: 3 and batch_num: 4\n",
      "Loss of test set: 0.4975164234638214 at epoch: 3 and batch_num: 5\n",
      "Loss of test set: 0.40035367012023926 at epoch: 3 and batch_num: 6\n",
      "Loss of test set: 0.35069072246551514 at epoch: 3 and batch_num: 7\n",
      "Loss of test set: 0.5890928506851196 at epoch: 3 and batch_num: 8\n",
      "Loss of test set: 0.23282739520072937 at epoch: 3 and batch_num: 9\n",
      "Loss of test set: 0.29110074043273926 at epoch: 3 and batch_num: 10\n",
      "Loss of test set: 0.6366358399391174 at epoch: 3 and batch_num: 11\n",
      "Loss of test set: 0.31934893131256104 at epoch: 3 and batch_num: 12\n",
      "Loss of test set: 0.39389389753341675 at epoch: 3 and batch_num: 13\n",
      "Loss of test set: 0.5284326672554016 at epoch: 3 and batch_num: 14\n",
      "Loss of test set: 0.3537410497665405 at epoch: 3 and batch_num: 15\n",
      "Loss of test set: 0.3852781057357788 at epoch: 3 and batch_num: 16\n",
      "Loss of test set: 0.3364022374153137 at epoch: 3 and batch_num: 17\n",
      "Loss of test set: 0.24683988094329834 at epoch: 3 and batch_num: 18\n",
      "Loss of test set: 0.4429951310157776 at epoch: 3 and batch_num: 19\n",
      "Loss of test set: 0.4425872564315796 at epoch: 3 and batch_num: 20\n",
      "Loss of test set: 0.46792519092559814 at epoch: 3 and batch_num: 21\n",
      "Loss of test set: 0.41944420337677 at epoch: 3 and batch_num: 22\n",
      "Loss of test set: 0.3547511696815491 at epoch: 3 and batch_num: 23\n",
      "Loss of test set: 0.569639265537262 at epoch: 3 and batch_num: 24\n",
      "Loss of test set: 0.4114161729812622 at epoch: 3 and batch_num: 25\n",
      "Loss of test set: 0.2570391297340393 at epoch: 3 and batch_num: 26\n",
      "Loss of test set: 0.4223569631576538 at epoch: 3 and batch_num: 27\n",
      "Loss of test set: 0.28453969955444336 at epoch: 3 and batch_num: 28\n",
      "Loss of test set: 0.4269004464149475 at epoch: 3 and batch_num: 29\n",
      "Loss of test set: 0.36207571625709534 at epoch: 3 and batch_num: 30\n",
      "Loss of test set: 0.4904480576515198 at epoch: 3 and batch_num: 31\n",
      "Loss of test set: 0.45444032549858093 at epoch: 3 and batch_num: 32\n",
      "Loss of test set: 0.4296104311943054 at epoch: 3 and batch_num: 33\n",
      "Loss of test set: 0.3980085551738739 at epoch: 3 and batch_num: 34\n",
      "Loss of test set: 0.34957438707351685 at epoch: 3 and batch_num: 35\n",
      "Loss of test set: 0.675412654876709 at epoch: 3 and batch_num: 36\n",
      "Loss of test set: 0.5684802532196045 at epoch: 3 and batch_num: 37\n",
      "Loss of test set: 0.44825077056884766 at epoch: 3 and batch_num: 38\n",
      "Loss of test set: 0.25906869769096375 at epoch: 3 and batch_num: 39\n",
      "Loss of test set: 0.36583513021469116 at epoch: 3 and batch_num: 40\n",
      "Loss of test set: 0.22192013263702393 at epoch: 3 and batch_num: 41\n",
      "Loss of test set: 0.5345270037651062 at epoch: 3 and batch_num: 42\n",
      "Loss of test set: 0.2018200159072876 at epoch: 3 and batch_num: 43\n",
      "Loss of test set: 0.35827410221099854 at epoch: 3 and batch_num: 44\n",
      "Loss of test set: 0.34524717926979065 at epoch: 3 and batch_num: 45\n",
      "Loss of test set: 0.42328715324401855 at epoch: 3 and batch_num: 46\n",
      "Loss of test set: 0.5360903143882751 at epoch: 3 and batch_num: 47\n",
      "Loss of test set: 0.32767120003700256 at epoch: 3 and batch_num: 48\n",
      "Loss of test set: 0.4169846475124359 at epoch: 3 and batch_num: 49\n",
      "Loss of test set: 0.5454051494598389 at epoch: 3 and batch_num: 50\n",
      "Loss of test set: 0.48403769731521606 at epoch: 3 and batch_num: 51\n",
      "Loss of test set: 0.24352706968784332 at epoch: 3 and batch_num: 52\n",
      "Loss of test set: 0.6168378591537476 at epoch: 3 and batch_num: 53\n",
      "Loss of test set: 0.31919947266578674 at epoch: 3 and batch_num: 54\n",
      "Loss of test set: 0.5482887029647827 at epoch: 3 and batch_num: 55\n",
      "Loss of test set: 0.5190167427062988 at epoch: 3 and batch_num: 56\n",
      "Loss of test set: 0.6564173102378845 at epoch: 3 and batch_num: 57\n",
      "Loss of test set: 0.3736696243286133 at epoch: 3 and batch_num: 58\n",
      "Loss of test set: 0.37468039989471436 at epoch: 3 and batch_num: 59\n",
      "Loss of test set: 0.5559084415435791 at epoch: 3 and batch_num: 60\n",
      "Loss of test set: 0.29946812987327576 at epoch: 3 and batch_num: 61\n",
      "Loss of test set: 0.4525916576385498 at epoch: 3 and batch_num: 62\n",
      "Loss of test set: 0.3924359381198883 at epoch: 3 and batch_num: 63\n",
      "Loss of test set: 0.3010241389274597 at epoch: 3 and batch_num: 64\n",
      "Loss of test set: 0.4093437194824219 at epoch: 3 and batch_num: 65\n",
      "Loss of test set: 0.3851042091846466 at epoch: 3 and batch_num: 66\n",
      "Loss of test set: 0.3748425245285034 at epoch: 3 and batch_num: 67\n",
      "Loss of test set: 0.19004184007644653 at epoch: 3 and batch_num: 68\n",
      "Loss of test set: 0.4928622543811798 at epoch: 3 and batch_num: 69\n",
      "Loss of test set: 0.2970486283302307 at epoch: 3 and batch_num: 70\n",
      "Loss of test set: 0.24921908974647522 at epoch: 3 and batch_num: 71\n",
      "Loss of test set: 0.3706394135951996 at epoch: 3 and batch_num: 72\n",
      "Loss of test set: 0.6046692132949829 at epoch: 3 and batch_num: 73\n",
      "Loss of test set: 0.2594802677631378 at epoch: 3 and batch_num: 74\n",
      "Loss of test set: 0.3170817494392395 at epoch: 3 and batch_num: 75\n",
      "Loss of test set: 0.40119171142578125 at epoch: 3 and batch_num: 76\n",
      "Loss of test set: 0.4719659984111786 at epoch: 3 and batch_num: 77\n",
      "Loss of test set: 0.5054791569709778 at epoch: 3 and batch_num: 78\n",
      "Loss of test set: 0.37932369112968445 at epoch: 3 and batch_num: 79\n",
      "Loss of test set: 0.4862287640571594 at epoch: 3 and batch_num: 80\n",
      "Loss of test set: 0.27251917123794556 at epoch: 3 and batch_num: 81\n",
      "Loss of test set: 0.5817978382110596 at epoch: 3 and batch_num: 82\n",
      "Loss of test set: 0.37636035680770874 at epoch: 3 and batch_num: 83\n",
      "Loss of test set: 0.3363579213619232 at epoch: 3 and batch_num: 84\n",
      "Loss of test set: 0.3031199276447296 at epoch: 3 and batch_num: 85\n",
      "Loss of test set: 0.4556955099105835 at epoch: 3 and batch_num: 86\n",
      "Loss of test set: 0.3891778588294983 at epoch: 3 and batch_num: 87\n",
      "Loss of test set: 0.417493999004364 at epoch: 3 and batch_num: 88\n",
      "Loss of test set: 0.3999708294868469 at epoch: 3 and batch_num: 89\n",
      "Loss of test set: 0.5147408843040466 at epoch: 3 and batch_num: 90\n",
      "Loss of test set: 0.36779141426086426 at epoch: 3 and batch_num: 91\n",
      "Loss of test set: 0.38055866956710815 at epoch: 3 and batch_num: 92\n",
      "Loss of test set: 0.34297966957092285 at epoch: 3 and batch_num: 93\n",
      "Loss of test set: 0.47698089480400085 at epoch: 3 and batch_num: 94\n",
      "Loss of test set: 0.6080135107040405 at epoch: 3 and batch_num: 95\n",
      "Loss of test set: 0.5278823971748352 at epoch: 3 and batch_num: 96\n",
      "Loss of test set: 0.35742679238319397 at epoch: 3 and batch_num: 97\n",
      "Loss of test set: 0.3403107523918152 at epoch: 3 and batch_num: 98\n",
      "Loss of test set: 0.39155399799346924 at epoch: 3 and batch_num: 99\n",
      "Loss of test set: 0.42194846272468567 at epoch: 3 and batch_num: 100\n",
      "Loss of test set: 0.29729095101356506 at epoch: 3 and batch_num: 101\n",
      "Loss of test set: 0.3584006428718567 at epoch: 3 and batch_num: 102\n",
      "Loss of test set: 0.5612211227416992 at epoch: 3 and batch_num: 103\n",
      "Loss of test set: 0.48689666390419006 at epoch: 3 and batch_num: 104\n",
      "Loss of test set: 0.489152729511261 at epoch: 3 and batch_num: 105\n",
      "Loss of test set: 0.4281916618347168 at epoch: 3 and batch_num: 106\n",
      "Loss of test set: 0.5376030802726746 at epoch: 3 and batch_num: 107\n",
      "Loss of test set: 0.3450951874256134 at epoch: 3 and batch_num: 108\n",
      "Loss of test set: 0.4628668427467346 at epoch: 3 and batch_num: 109\n",
      "Loss of test set: 0.41418394446372986 at epoch: 3 and batch_num: 110\n",
      "Loss of test set: 0.37254464626312256 at epoch: 3 and batch_num: 111\n",
      "Loss of test set: 0.4188355803489685 at epoch: 3 and batch_num: 112\n",
      "Loss of test set: 0.4397457242012024 at epoch: 3 and batch_num: 113\n",
      "Loss of test set: 0.19963249564170837 at epoch: 3 and batch_num: 114\n",
      "Loss of test set: 0.6824823617935181 at epoch: 3 and batch_num: 115\n",
      "Loss of test set: 0.4813745617866516 at epoch: 3 and batch_num: 116\n",
      "Loss of test set: 0.36534038186073303 at epoch: 3 and batch_num: 117\n",
      "Loss of test set: 0.42474862933158875 at epoch: 3 and batch_num: 118\n",
      "Loss of test set: 0.24978607892990112 at epoch: 3 and batch_num: 119\n",
      "Loss of test set: 0.40453100204467773 at epoch: 3 and batch_num: 120\n",
      "Loss of test set: 0.20547960698604584 at epoch: 3 and batch_num: 121\n",
      "Loss of test set: 0.41226863861083984 at epoch: 3 and batch_num: 122\n",
      "Loss of test set: 0.4085533916950226 at epoch: 3 and batch_num: 123\n",
      "Loss of test set: 0.447490394115448 at epoch: 3 and batch_num: 124\n",
      "Loss of test set: 0.46900486946105957 at epoch: 3 and batch_num: 125\n",
      "Loss of test set: 0.40343207120895386 at epoch: 3 and batch_num: 126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.3454568088054657 at epoch: 3 and batch_num: 127\n",
      "Loss of test set: 0.30514174699783325 at epoch: 3 and batch_num: 128\n",
      "Loss of test set: 0.26817628741264343 at epoch: 3 and batch_num: 129\n",
      "Loss of test set: 0.41621577739715576 at epoch: 3 and batch_num: 130\n",
      "Loss of test set: 0.41705113649368286 at epoch: 3 and batch_num: 131\n",
      "Loss of test set: 0.31749412417411804 at epoch: 3 and batch_num: 132\n",
      "Loss of test set: 0.27568912506103516 at epoch: 3 and batch_num: 133\n",
      "Loss of test set: 0.35817089676856995 at epoch: 3 and batch_num: 134\n",
      "Loss of test set: 0.30293768644332886 at epoch: 3 and batch_num: 135\n",
      "Loss of test set: 0.5715955495834351 at epoch: 3 and batch_num: 136\n",
      "Loss of test set: 0.6154892444610596 at epoch: 3 and batch_num: 137\n",
      "Loss of test set: 0.3352344036102295 at epoch: 3 and batch_num: 138\n",
      "Loss of test set: 0.7479535341262817 at epoch: 3 and batch_num: 139\n",
      "Loss of test set: 0.34657806158065796 at epoch: 3 and batch_num: 140\n",
      "Loss of test set: 0.312117338180542 at epoch: 3 and batch_num: 141\n",
      "Loss of test set: 0.523540735244751 at epoch: 3 and batch_num: 142\n",
      "Loss of test set: 0.3916519284248352 at epoch: 3 and batch_num: 143\n",
      "Loss of test set: 0.5436970591545105 at epoch: 3 and batch_num: 144\n",
      "Loss of test set: 0.4553077816963196 at epoch: 3 and batch_num: 145\n",
      "Loss of test set: 0.5436022281646729 at epoch: 3 and batch_num: 146\n",
      "Loss of test set: 0.428724467754364 at epoch: 3 and batch_num: 147\n",
      "Loss of test set: 0.45112141966819763 at epoch: 3 and batch_num: 148\n",
      "Loss of test set: 0.21655763685703278 at epoch: 3 and batch_num: 149\n",
      "Loss of test set: 0.44497400522232056 at epoch: 3 and batch_num: 150\n",
      "Loss of test set: 0.3987557291984558 at epoch: 3 and batch_num: 151\n",
      "Loss of test set: 0.30917391180992126 at epoch: 3 and batch_num: 152\n",
      "Loss of test set: 0.4066679775714874 at epoch: 3 and batch_num: 153\n",
      "Loss of test set: 0.3267578184604645 at epoch: 3 and batch_num: 154\n",
      "Loss of test set: 0.41837042570114136 at epoch: 3 and batch_num: 155\n",
      "Loss of test set: 0.2761048674583435 at epoch: 3 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8538\n",
      "Loss of train set: 0.3331185579299927 at epoch: 4 and batch_num: 0\n",
      "Loss of train set: 0.4024309515953064 at epoch: 4 and batch_num: 1\n",
      "Loss of train set: 0.1730232834815979 at epoch: 4 and batch_num: 2\n",
      "Loss of train set: 0.3051181435585022 at epoch: 4 and batch_num: 3\n",
      "Loss of train set: 0.46670788526535034 at epoch: 4 and batch_num: 4\n",
      "Loss of train set: 0.22945693135261536 at epoch: 4 and batch_num: 5\n",
      "Loss of train set: 0.3296842575073242 at epoch: 4 and batch_num: 6\n",
      "Loss of train set: 0.3957585394382477 at epoch: 4 and batch_num: 7\n",
      "Loss of train set: 0.24030375480651855 at epoch: 4 and batch_num: 8\n",
      "Loss of train set: 0.4546983242034912 at epoch: 4 and batch_num: 9\n",
      "Loss of train set: 0.25748148560523987 at epoch: 4 and batch_num: 10\n",
      "Loss of train set: 0.5352909564971924 at epoch: 4 and batch_num: 11\n",
      "Loss of train set: 0.2700672447681427 at epoch: 4 and batch_num: 12\n",
      "Loss of train set: 0.2957037091255188 at epoch: 4 and batch_num: 13\n",
      "Loss of train set: 0.3528240919113159 at epoch: 4 and batch_num: 14\n",
      "Loss of train set: 0.5112201571464539 at epoch: 4 and batch_num: 15\n",
      "Loss of train set: 0.371076762676239 at epoch: 4 and batch_num: 16\n",
      "Loss of train set: 0.4365808963775635 at epoch: 4 and batch_num: 17\n",
      "Loss of train set: 0.39844995737075806 at epoch: 4 and batch_num: 18\n",
      "Loss of train set: 0.42873552441596985 at epoch: 4 and batch_num: 19\n",
      "Loss of train set: 0.40838736295700073 at epoch: 4 and batch_num: 20\n",
      "Loss of train set: 0.37768083810806274 at epoch: 4 and batch_num: 21\n",
      "Loss of train set: 0.38116297125816345 at epoch: 4 and batch_num: 22\n",
      "Loss of train set: 0.5075880289077759 at epoch: 4 and batch_num: 23\n",
      "Loss of train set: 0.21021389961242676 at epoch: 4 and batch_num: 24\n",
      "Loss of train set: 0.4095780551433563 at epoch: 4 and batch_num: 25\n",
      "Loss of train set: 0.33034539222717285 at epoch: 4 and batch_num: 26\n",
      "Loss of train set: 0.25539982318878174 at epoch: 4 and batch_num: 27\n",
      "Loss of train set: 0.3908788561820984 at epoch: 4 and batch_num: 28\n",
      "Loss of train set: 0.26377415657043457 at epoch: 4 and batch_num: 29\n",
      "Loss of train set: 0.2585110664367676 at epoch: 4 and batch_num: 30\n",
      "Loss of train set: 0.21712812781333923 at epoch: 4 and batch_num: 31\n",
      "Loss of train set: 0.2954965829849243 at epoch: 4 and batch_num: 32\n",
      "Loss of train set: 0.4442160129547119 at epoch: 4 and batch_num: 33\n",
      "Loss of train set: 0.44770824909210205 at epoch: 4 and batch_num: 34\n",
      "Loss of train set: 0.3412362337112427 at epoch: 4 and batch_num: 35\n",
      "Loss of train set: 0.35540902614593506 at epoch: 4 and batch_num: 36\n",
      "Loss of train set: 0.4678182005882263 at epoch: 4 and batch_num: 37\n",
      "Loss of train set: 0.29428213834762573 at epoch: 4 and batch_num: 38\n",
      "Loss of train set: 0.2800617814064026 at epoch: 4 and batch_num: 39\n",
      "Loss of train set: 0.3227089047431946 at epoch: 4 and batch_num: 40\n",
      "Loss of train set: 0.3534752130508423 at epoch: 4 and batch_num: 41\n",
      "Loss of train set: 0.5185636878013611 at epoch: 4 and batch_num: 42\n",
      "Loss of train set: 0.2944050431251526 at epoch: 4 and batch_num: 43\n",
      "Loss of train set: 0.336015522480011 at epoch: 4 and batch_num: 44\n",
      "Loss of train set: 0.3442445695400238 at epoch: 4 and batch_num: 45\n",
      "Loss of train set: 0.391905814409256 at epoch: 4 and batch_num: 46\n",
      "Loss of train set: 0.42848122119903564 at epoch: 4 and batch_num: 47\n",
      "Loss of train set: 0.4695708751678467 at epoch: 4 and batch_num: 48\n",
      "Loss of train set: 0.27568042278289795 at epoch: 4 and batch_num: 49\n",
      "Loss of train set: 0.32930415868759155 at epoch: 4 and batch_num: 50\n",
      "Loss of train set: 0.3423416018486023 at epoch: 4 and batch_num: 51\n",
      "Loss of train set: 0.3539779782295227 at epoch: 4 and batch_num: 52\n",
      "Loss of train set: 0.3454080820083618 at epoch: 4 and batch_num: 53\n",
      "Loss of train set: 0.42792317271232605 at epoch: 4 and batch_num: 54\n",
      "Loss of train set: 0.3157675862312317 at epoch: 4 and batch_num: 55\n",
      "Loss of train set: 0.31649208068847656 at epoch: 4 and batch_num: 56\n",
      "Loss of train set: 0.2764049172401428 at epoch: 4 and batch_num: 57\n",
      "Loss of train set: 0.3150066137313843 at epoch: 4 and batch_num: 58\n",
      "Loss of train set: 0.3060927987098694 at epoch: 4 and batch_num: 59\n",
      "Loss of train set: 0.37023431062698364 at epoch: 4 and batch_num: 60\n",
      "Loss of train set: 0.3795614242553711 at epoch: 4 and batch_num: 61\n",
      "Loss of train set: 0.4247599244117737 at epoch: 4 and batch_num: 62\n",
      "Loss of train set: 0.3132469654083252 at epoch: 4 and batch_num: 63\n",
      "Loss of train set: 0.2886670231819153 at epoch: 4 and batch_num: 64\n",
      "Loss of train set: 0.2754380702972412 at epoch: 4 and batch_num: 65\n",
      "Loss of train set: 0.48815083503723145 at epoch: 4 and batch_num: 66\n",
      "Loss of train set: 0.40798047184944153 at epoch: 4 and batch_num: 67\n",
      "Loss of train set: 0.2931462526321411 at epoch: 4 and batch_num: 68\n",
      "Loss of train set: 0.45307183265686035 at epoch: 4 and batch_num: 69\n",
      "Loss of train set: 0.2747073769569397 at epoch: 4 and batch_num: 70\n",
      "Loss of train set: 0.29610976576805115 at epoch: 4 and batch_num: 71\n",
      "Loss of train set: 0.3870049715042114 at epoch: 4 and batch_num: 72\n",
      "Loss of train set: 0.3111598789691925 at epoch: 4 and batch_num: 73\n",
      "Loss of train set: 0.5305428504943848 at epoch: 4 and batch_num: 74\n",
      "Loss of train set: 0.4176436960697174 at epoch: 4 and batch_num: 75\n",
      "Loss of train set: 0.28081414103507996 at epoch: 4 and batch_num: 76\n",
      "Loss of train set: 0.2522214651107788 at epoch: 4 and batch_num: 77\n",
      "Loss of train set: 0.3293098211288452 at epoch: 4 and batch_num: 78\n",
      "Loss of train set: 0.16925856471061707 at epoch: 4 and batch_num: 79\n",
      "Loss of train set: 0.25219643115997314 at epoch: 4 and batch_num: 80\n",
      "Loss of train set: 0.5148566365242004 at epoch: 4 and batch_num: 81\n",
      "Loss of train set: 0.3261755704879761 at epoch: 4 and batch_num: 82\n",
      "Loss of train set: 0.289234459400177 at epoch: 4 and batch_num: 83\n",
      "Loss of train set: 0.29725581407546997 at epoch: 4 and batch_num: 84\n",
      "Loss of train set: 0.342836856842041 at epoch: 4 and batch_num: 85\n",
      "Loss of train set: 0.4262600541114807 at epoch: 4 and batch_num: 86\n",
      "Loss of train set: 0.25697964429855347 at epoch: 4 and batch_num: 87\n",
      "Loss of train set: 0.1193239614367485 at epoch: 4 and batch_num: 88\n",
      "Loss of train set: 0.3608638048171997 at epoch: 4 and batch_num: 89\n",
      "Loss of train set: 0.34959936141967773 at epoch: 4 and batch_num: 90\n",
      "Loss of train set: 0.359036386013031 at epoch: 4 and batch_num: 91\n",
      "Loss of train set: 0.3090347349643707 at epoch: 4 and batch_num: 92\n",
      "Loss of train set: 0.5114413499832153 at epoch: 4 and batch_num: 93\n",
      "Loss of train set: 0.3023410439491272 at epoch: 4 and batch_num: 94\n",
      "Loss of train set: 0.6794157028198242 at epoch: 4 and batch_num: 95\n",
      "Loss of train set: 0.271194726228714 at epoch: 4 and batch_num: 96\n",
      "Loss of train set: 0.3267190456390381 at epoch: 4 and batch_num: 97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2560054063796997 at epoch: 4 and batch_num: 98\n",
      "Loss of train set: 0.4236893355846405 at epoch: 4 and batch_num: 99\n",
      "Loss of train set: 0.399283766746521 at epoch: 4 and batch_num: 100\n",
      "Loss of train set: 0.3034120202064514 at epoch: 4 and batch_num: 101\n",
      "Loss of train set: 0.25213149189949036 at epoch: 4 and batch_num: 102\n",
      "Loss of train set: 0.4199574589729309 at epoch: 4 and batch_num: 103\n",
      "Loss of train set: 0.3143118619918823 at epoch: 4 and batch_num: 104\n",
      "Loss of train set: 0.3151547312736511 at epoch: 4 and batch_num: 105\n",
      "Loss of train set: 0.2921695113182068 at epoch: 4 and batch_num: 106\n",
      "Loss of train set: 0.2287590503692627 at epoch: 4 and batch_num: 107\n",
      "Loss of train set: 0.36687612533569336 at epoch: 4 and batch_num: 108\n",
      "Loss of train set: 0.37382882833480835 at epoch: 4 and batch_num: 109\n",
      "Loss of train set: 0.5383939146995544 at epoch: 4 and batch_num: 110\n",
      "Loss of train set: 0.2708171606063843 at epoch: 4 and batch_num: 111\n",
      "Loss of train set: 0.2780067026615143 at epoch: 4 and batch_num: 112\n",
      "Loss of train set: 0.3589600920677185 at epoch: 4 and batch_num: 113\n",
      "Loss of train set: 0.5274633169174194 at epoch: 4 and batch_num: 114\n",
      "Loss of train set: 0.3295360207557678 at epoch: 4 and batch_num: 115\n",
      "Loss of train set: 0.30854111909866333 at epoch: 4 and batch_num: 116\n",
      "Loss of train set: 0.5615897178649902 at epoch: 4 and batch_num: 117\n",
      "Loss of train set: 0.3626924753189087 at epoch: 4 and batch_num: 118\n",
      "Loss of train set: 0.2664080858230591 at epoch: 4 and batch_num: 119\n",
      "Loss of train set: 0.25759363174438477 at epoch: 4 and batch_num: 120\n",
      "Loss of train set: 0.4112073481082916 at epoch: 4 and batch_num: 121\n",
      "Loss of train set: 0.24734631180763245 at epoch: 4 and batch_num: 122\n",
      "Loss of train set: 0.288698673248291 at epoch: 4 and batch_num: 123\n",
      "Loss of train set: 0.4637582302093506 at epoch: 4 and batch_num: 124\n",
      "Loss of train set: 0.4461333453655243 at epoch: 4 and batch_num: 125\n",
      "Loss of train set: 0.3165012001991272 at epoch: 4 and batch_num: 126\n",
      "Loss of train set: 0.2563813030719757 at epoch: 4 and batch_num: 127\n",
      "Loss of train set: 0.22739821672439575 at epoch: 4 and batch_num: 128\n",
      "Loss of train set: 0.356782466173172 at epoch: 4 and batch_num: 129\n",
      "Loss of train set: 0.30290690064430237 at epoch: 4 and batch_num: 130\n",
      "Loss of train set: 0.56236732006073 at epoch: 4 and batch_num: 131\n",
      "Loss of train set: 0.2788419723510742 at epoch: 4 and batch_num: 132\n",
      "Loss of train set: 0.3035793900489807 at epoch: 4 and batch_num: 133\n",
      "Loss of train set: 0.43521803617477417 at epoch: 4 and batch_num: 134\n",
      "Loss of train set: 0.48878785967826843 at epoch: 4 and batch_num: 135\n",
      "Loss of train set: 0.3588237166404724 at epoch: 4 and batch_num: 136\n",
      "Loss of train set: 0.32655712962150574 at epoch: 4 and batch_num: 137\n",
      "Loss of train set: 0.3699459731578827 at epoch: 4 and batch_num: 138\n",
      "Loss of train set: 0.32452449202537537 at epoch: 4 and batch_num: 139\n",
      "Loss of train set: 0.3889172077178955 at epoch: 4 and batch_num: 140\n",
      "Loss of train set: 0.2874235510826111 at epoch: 4 and batch_num: 141\n",
      "Loss of train set: 0.5257993936538696 at epoch: 4 and batch_num: 142\n",
      "Loss of train set: 0.29680541157722473 at epoch: 4 and batch_num: 143\n",
      "Loss of train set: 0.24107712507247925 at epoch: 4 and batch_num: 144\n",
      "Loss of train set: 0.3796583116054535 at epoch: 4 and batch_num: 145\n",
      "Loss of train set: 0.33791351318359375 at epoch: 4 and batch_num: 146\n",
      "Loss of train set: 0.40189772844314575 at epoch: 4 and batch_num: 147\n",
      "Loss of train set: 0.34964174032211304 at epoch: 4 and batch_num: 148\n",
      "Loss of train set: 0.35729464888572693 at epoch: 4 and batch_num: 149\n",
      "Loss of train set: 0.208842933177948 at epoch: 4 and batch_num: 150\n",
      "Loss of train set: 0.2592265009880066 at epoch: 4 and batch_num: 151\n",
      "Loss of train set: 0.331265926361084 at epoch: 4 and batch_num: 152\n",
      "Loss of train set: 0.29169562458992004 at epoch: 4 and batch_num: 153\n",
      "Loss of train set: 0.29370200634002686 at epoch: 4 and batch_num: 154\n",
      "Loss of train set: 0.3104200065135956 at epoch: 4 and batch_num: 155\n",
      "Loss of train set: 0.19927722215652466 at epoch: 4 and batch_num: 156\n",
      "Loss of train set: 0.34313827753067017 at epoch: 4 and batch_num: 157\n",
      "Loss of train set: 0.4615927040576935 at epoch: 4 and batch_num: 158\n",
      "Loss of train set: 0.2731952667236328 at epoch: 4 and batch_num: 159\n",
      "Loss of train set: 0.3635140061378479 at epoch: 4 and batch_num: 160\n",
      "Loss of train set: 0.24547594785690308 at epoch: 4 and batch_num: 161\n",
      "Loss of train set: 0.31588831543922424 at epoch: 4 and batch_num: 162\n",
      "Loss of train set: 0.31558963656425476 at epoch: 4 and batch_num: 163\n",
      "Loss of train set: 0.30295392870903015 at epoch: 4 and batch_num: 164\n",
      "Loss of train set: 0.31115877628326416 at epoch: 4 and batch_num: 165\n",
      "Loss of train set: 0.40441811084747314 at epoch: 4 and batch_num: 166\n",
      "Loss of train set: 0.32908564805984497 at epoch: 4 and batch_num: 167\n",
      "Loss of train set: 0.40775346755981445 at epoch: 4 and batch_num: 168\n",
      "Loss of train set: 0.21506983041763306 at epoch: 4 and batch_num: 169\n",
      "Loss of train set: 0.2655688524246216 at epoch: 4 and batch_num: 170\n",
      "Loss of train set: 0.32036909461021423 at epoch: 4 and batch_num: 171\n",
      "Loss of train set: 0.39963299036026 at epoch: 4 and batch_num: 172\n",
      "Loss of train set: 0.2344762235879898 at epoch: 4 and batch_num: 173\n",
      "Loss of train set: 0.391336053609848 at epoch: 4 and batch_num: 174\n",
      "Loss of train set: 0.25722944736480713 at epoch: 4 and batch_num: 175\n",
      "Loss of train set: 0.31122052669525146 at epoch: 4 and batch_num: 176\n",
      "Loss of train set: 0.3890343904495239 at epoch: 4 and batch_num: 177\n",
      "Loss of train set: 0.15071740746498108 at epoch: 4 and batch_num: 178\n",
      "Loss of train set: 0.4333571493625641 at epoch: 4 and batch_num: 179\n",
      "Loss of train set: 0.285625696182251 at epoch: 4 and batch_num: 180\n",
      "Loss of train set: 0.4084472060203552 at epoch: 4 and batch_num: 181\n",
      "Loss of train set: 0.3609232008457184 at epoch: 4 and batch_num: 182\n",
      "Loss of train set: 0.5598893165588379 at epoch: 4 and batch_num: 183\n",
      "Loss of train set: 0.25830286741256714 at epoch: 4 and batch_num: 184\n",
      "Loss of train set: 0.5225342512130737 at epoch: 4 and batch_num: 185\n",
      "Loss of train set: 0.2657319903373718 at epoch: 4 and batch_num: 186\n",
      "Loss of train set: 0.36414238810539246 at epoch: 4 and batch_num: 187\n",
      "Loss of train set: 0.5011487007141113 at epoch: 4 and batch_num: 188\n",
      "Loss of train set: 0.3339783549308777 at epoch: 4 and batch_num: 189\n",
      "Loss of train set: 0.27443066239356995 at epoch: 4 and batch_num: 190\n",
      "Loss of train set: 0.3522074222564697 at epoch: 4 and batch_num: 191\n",
      "Loss of train set: 0.2504979074001312 at epoch: 4 and batch_num: 192\n",
      "Loss of train set: 0.4844949245452881 at epoch: 4 and batch_num: 193\n",
      "Loss of train set: 0.21517017483711243 at epoch: 4 and batch_num: 194\n",
      "Loss of train set: 0.5051409602165222 at epoch: 4 and batch_num: 195\n",
      "Loss of train set: 0.2437957525253296 at epoch: 4 and batch_num: 196\n",
      "Loss of train set: 0.4476112723350525 at epoch: 4 and batch_num: 197\n",
      "Loss of train set: 0.3855469226837158 at epoch: 4 and batch_num: 198\n",
      "Loss of train set: 0.6136913895606995 at epoch: 4 and batch_num: 199\n",
      "Loss of train set: 0.20767712593078613 at epoch: 4 and batch_num: 200\n",
      "Loss of train set: 0.42623263597488403 at epoch: 4 and batch_num: 201\n",
      "Loss of train set: 0.34475186467170715 at epoch: 4 and batch_num: 202\n",
      "Loss of train set: 0.31411561369895935 at epoch: 4 and batch_num: 203\n",
      "Loss of train set: 0.49019497632980347 at epoch: 4 and batch_num: 204\n",
      "Loss of train set: 0.4835042357444763 at epoch: 4 and batch_num: 205\n",
      "Loss of train set: 0.458840012550354 at epoch: 4 and batch_num: 206\n",
      "Loss of train set: 0.3851104974746704 at epoch: 4 and batch_num: 207\n",
      "Loss of train set: 0.3588904142379761 at epoch: 4 and batch_num: 208\n",
      "Loss of train set: 0.6624233722686768 at epoch: 4 and batch_num: 209\n",
      "Loss of train set: 0.3171702027320862 at epoch: 4 and batch_num: 210\n",
      "Loss of train set: 0.34477487206459045 at epoch: 4 and batch_num: 211\n",
      "Loss of train set: 0.38608047366142273 at epoch: 4 and batch_num: 212\n",
      "Loss of train set: 0.491161972284317 at epoch: 4 and batch_num: 213\n",
      "Loss of train set: 0.4206642508506775 at epoch: 4 and batch_num: 214\n",
      "Loss of train set: 0.4695371687412262 at epoch: 4 and batch_num: 215\n",
      "Loss of train set: 0.38241297006607056 at epoch: 4 and batch_num: 216\n",
      "Loss of train set: 0.2150346040725708 at epoch: 4 and batch_num: 217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.20133063197135925 at epoch: 4 and batch_num: 218\n",
      "Loss of train set: 0.2639216184616089 at epoch: 4 and batch_num: 219\n",
      "Loss of train set: 0.3607422709465027 at epoch: 4 and batch_num: 220\n",
      "Loss of train set: 0.4060354232788086 at epoch: 4 and batch_num: 221\n",
      "Loss of train set: 0.45482510328292847 at epoch: 4 and batch_num: 222\n",
      "Loss of train set: 0.44192764163017273 at epoch: 4 and batch_num: 223\n",
      "Loss of train set: 0.3709757328033447 at epoch: 4 and batch_num: 224\n",
      "Loss of train set: 0.3361068069934845 at epoch: 4 and batch_num: 225\n",
      "Loss of train set: 0.3225145936012268 at epoch: 4 and batch_num: 226\n",
      "Loss of train set: 0.48567089438438416 at epoch: 4 and batch_num: 227\n",
      "Loss of train set: 0.34853076934814453 at epoch: 4 and batch_num: 228\n",
      "Loss of train set: 0.2800992727279663 at epoch: 4 and batch_num: 229\n",
      "Loss of train set: 0.29599136114120483 at epoch: 4 and batch_num: 230\n",
      "Loss of train set: 0.44652172923088074 at epoch: 4 and batch_num: 231\n",
      "Loss of train set: 0.38821470737457275 at epoch: 4 and batch_num: 232\n",
      "Loss of train set: 0.3941847085952759 at epoch: 4 and batch_num: 233\n",
      "Loss of train set: 0.26924923062324524 at epoch: 4 and batch_num: 234\n",
      "Loss of train set: 0.4019051790237427 at epoch: 4 and batch_num: 235\n",
      "Loss of train set: 0.48741188645362854 at epoch: 4 and batch_num: 236\n",
      "Loss of train set: 0.5667029619216919 at epoch: 4 and batch_num: 237\n",
      "Loss of train set: 0.37761491537094116 at epoch: 4 and batch_num: 238\n",
      "Loss of train set: 0.4433283805847168 at epoch: 4 and batch_num: 239\n",
      "Loss of train set: 0.23823405802249908 at epoch: 4 and batch_num: 240\n",
      "Loss of train set: 0.31772398948669434 at epoch: 4 and batch_num: 241\n",
      "Loss of train set: 0.2702176868915558 at epoch: 4 and batch_num: 242\n",
      "Loss of train set: 0.24033725261688232 at epoch: 4 and batch_num: 243\n",
      "Loss of train set: 0.2768988311290741 at epoch: 4 and batch_num: 244\n",
      "Loss of train set: 0.46110135316848755 at epoch: 4 and batch_num: 245\n",
      "Loss of train set: 0.25928497314453125 at epoch: 4 and batch_num: 246\n",
      "Loss of train set: 0.5364114046096802 at epoch: 4 and batch_num: 247\n",
      "Loss of train set: 0.5644327402114868 at epoch: 4 and batch_num: 248\n",
      "Loss of train set: 0.25804272294044495 at epoch: 4 and batch_num: 249\n",
      "Loss of train set: 0.33745741844177246 at epoch: 4 and batch_num: 250\n",
      "Loss of train set: 0.4428181052207947 at epoch: 4 and batch_num: 251\n",
      "Loss of train set: 0.6095027327537537 at epoch: 4 and batch_num: 252\n",
      "Loss of train set: 0.33334824442863464 at epoch: 4 and batch_num: 253\n",
      "Loss of train set: 0.31802329421043396 at epoch: 4 and batch_num: 254\n",
      "Loss of train set: 0.3802836537361145 at epoch: 4 and batch_num: 255\n",
      "Loss of train set: 0.4455282688140869 at epoch: 4 and batch_num: 256\n",
      "Loss of train set: 0.35363131761550903 at epoch: 4 and batch_num: 257\n",
      "Loss of train set: 0.25598838925361633 at epoch: 4 and batch_num: 258\n",
      "Loss of train set: 0.42611485719680786 at epoch: 4 and batch_num: 259\n",
      "Loss of train set: 0.37736207246780396 at epoch: 4 and batch_num: 260\n",
      "Loss of train set: 0.6691678762435913 at epoch: 4 and batch_num: 261\n",
      "Loss of train set: 0.4507180452346802 at epoch: 4 and batch_num: 262\n",
      "Loss of train set: 0.2932192385196686 at epoch: 4 and batch_num: 263\n",
      "Loss of train set: 0.43135616183280945 at epoch: 4 and batch_num: 264\n",
      "Loss of train set: 0.35123008489608765 at epoch: 4 and batch_num: 265\n",
      "Loss of train set: 0.3711777925491333 at epoch: 4 and batch_num: 266\n",
      "Loss of train set: 0.33496928215026855 at epoch: 4 and batch_num: 267\n",
      "Loss of train set: 0.29912614822387695 at epoch: 4 and batch_num: 268\n",
      "Loss of train set: 0.45244717597961426 at epoch: 4 and batch_num: 269\n",
      "Loss of train set: 0.278738796710968 at epoch: 4 and batch_num: 270\n",
      "Loss of train set: 0.35359182953834534 at epoch: 4 and batch_num: 271\n",
      "Loss of train set: 0.31557175517082214 at epoch: 4 and batch_num: 272\n",
      "Loss of train set: 0.23334769904613495 at epoch: 4 and batch_num: 273\n",
      "Loss of train set: 0.3882538676261902 at epoch: 4 and batch_num: 274\n",
      "Loss of train set: 0.5403082966804504 at epoch: 4 and batch_num: 275\n",
      "Loss of train set: 0.3163378834724426 at epoch: 4 and batch_num: 276\n",
      "Loss of train set: 0.28086453676223755 at epoch: 4 and batch_num: 277\n",
      "Loss of train set: 0.2724085748195648 at epoch: 4 and batch_num: 278\n",
      "Loss of train set: 0.3994792103767395 at epoch: 4 and batch_num: 279\n",
      "Loss of train set: 0.24107421934604645 at epoch: 4 and batch_num: 280\n",
      "Loss of train set: 0.30020421743392944 at epoch: 4 and batch_num: 281\n",
      "Loss of train set: 0.37380796670913696 at epoch: 4 and batch_num: 282\n",
      "Loss of train set: 0.5303463935852051 at epoch: 4 and batch_num: 283\n",
      "Loss of train set: 0.21335440874099731 at epoch: 4 and batch_num: 284\n",
      "Loss of train set: 0.20181512832641602 at epoch: 4 and batch_num: 285\n",
      "Loss of train set: 0.39426225423812866 at epoch: 4 and batch_num: 286\n",
      "Loss of train set: 0.31731516122817993 at epoch: 4 and batch_num: 287\n",
      "Loss of train set: 0.35136687755584717 at epoch: 4 and batch_num: 288\n",
      "Loss of train set: 0.43170568346977234 at epoch: 4 and batch_num: 289\n",
      "Loss of train set: 0.3219844698905945 at epoch: 4 and batch_num: 290\n",
      "Loss of train set: 0.3721105754375458 at epoch: 4 and batch_num: 291\n",
      "Loss of train set: 0.39108747243881226 at epoch: 4 and batch_num: 292\n",
      "Loss of train set: 0.25165966153144836 at epoch: 4 and batch_num: 293\n",
      "Loss of train set: 0.2740408778190613 at epoch: 4 and batch_num: 294\n",
      "Loss of train set: 0.3080829381942749 at epoch: 4 and batch_num: 295\n",
      "Loss of train set: 0.35579437017440796 at epoch: 4 and batch_num: 296\n",
      "Loss of train set: 0.5438967943191528 at epoch: 4 and batch_num: 297\n",
      "Loss of train set: 0.42701834440231323 at epoch: 4 and batch_num: 298\n",
      "Loss of train set: 0.4663759469985962 at epoch: 4 and batch_num: 299\n",
      "Loss of train set: 0.3899257779121399 at epoch: 4 and batch_num: 300\n",
      "Loss of train set: 0.25687772035598755 at epoch: 4 and batch_num: 301\n",
      "Loss of train set: 0.24665452539920807 at epoch: 4 and batch_num: 302\n",
      "Loss of train set: 0.46004149317741394 at epoch: 4 and batch_num: 303\n",
      "Loss of train set: 0.31116482615470886 at epoch: 4 and batch_num: 304\n",
      "Loss of train set: 0.4402439594268799 at epoch: 4 and batch_num: 305\n",
      "Loss of train set: 0.4165363311767578 at epoch: 4 and batch_num: 306\n",
      "Loss of train set: 0.4498807191848755 at epoch: 4 and batch_num: 307\n",
      "Loss of train set: 0.30618131160736084 at epoch: 4 and batch_num: 308\n",
      "Loss of train set: 0.44563084840774536 at epoch: 4 and batch_num: 309\n",
      "Loss of train set: 0.23300181329250336 at epoch: 4 and batch_num: 310\n",
      "Loss of train set: 0.28527602553367615 at epoch: 4 and batch_num: 311\n",
      "Loss of train set: 0.24840930104255676 at epoch: 4 and batch_num: 312\n",
      "Loss of train set: 0.331248939037323 at epoch: 4 and batch_num: 313\n",
      "Loss of train set: 0.34055715799331665 at epoch: 4 and batch_num: 314\n",
      "Loss of train set: 0.26902323961257935 at epoch: 4 and batch_num: 315\n",
      "Loss of train set: 0.6057741045951843 at epoch: 4 and batch_num: 316\n",
      "Loss of train set: 0.24354679882526398 at epoch: 4 and batch_num: 317\n",
      "Loss of train set: 0.43977493047714233 at epoch: 4 and batch_num: 318\n",
      "Loss of train set: 0.4451024532318115 at epoch: 4 and batch_num: 319\n",
      "Loss of train set: 0.3996855616569519 at epoch: 4 and batch_num: 320\n",
      "Loss of train set: 0.5468226671218872 at epoch: 4 and batch_num: 321\n",
      "Loss of train set: 0.3227561414241791 at epoch: 4 and batch_num: 322\n",
      "Loss of train set: 0.31325972080230713 at epoch: 4 and batch_num: 323\n",
      "Loss of train set: 0.34248679876327515 at epoch: 4 and batch_num: 324\n",
      "Loss of train set: 0.38194191455841064 at epoch: 4 and batch_num: 325\n",
      "Loss of train set: 0.23814518749713898 at epoch: 4 and batch_num: 326\n",
      "Loss of train set: 0.30625230073928833 at epoch: 4 and batch_num: 327\n",
      "Loss of train set: 0.23640140891075134 at epoch: 4 and batch_num: 328\n",
      "Loss of train set: 0.26360493898391724 at epoch: 4 and batch_num: 329\n",
      "Loss of train set: 0.24643456935882568 at epoch: 4 and batch_num: 330\n",
      "Loss of train set: 0.26987046003341675 at epoch: 4 and batch_num: 331\n",
      "Loss of train set: 0.2727510929107666 at epoch: 4 and batch_num: 332\n",
      "Loss of train set: 0.46167993545532227 at epoch: 4 and batch_num: 333\n",
      "Loss of train set: 0.3025602698326111 at epoch: 4 and batch_num: 334\n",
      "Loss of train set: 0.2586885094642639 at epoch: 4 and batch_num: 335\n",
      "Loss of train set: 0.5033493041992188 at epoch: 4 and batch_num: 336\n",
      "Loss of train set: 0.42133551836013794 at epoch: 4 and batch_num: 337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.22225061058998108 at epoch: 4 and batch_num: 338\n",
      "Loss of train set: 0.3915135860443115 at epoch: 4 and batch_num: 339\n",
      "Loss of train set: 0.33562231063842773 at epoch: 4 and batch_num: 340\n",
      "Loss of train set: 0.23954857885837555 at epoch: 4 and batch_num: 341\n",
      "Loss of train set: 0.4026848077774048 at epoch: 4 and batch_num: 342\n",
      "Loss of train set: 0.31409090757369995 at epoch: 4 and batch_num: 343\n",
      "Loss of train set: 0.5514094829559326 at epoch: 4 and batch_num: 344\n",
      "Loss of train set: 0.2204158902168274 at epoch: 4 and batch_num: 345\n",
      "Loss of train set: 0.454693078994751 at epoch: 4 and batch_num: 346\n",
      "Loss of train set: 0.25963228940963745 at epoch: 4 and batch_num: 347\n",
      "Loss of train set: 0.4793913960456848 at epoch: 4 and batch_num: 348\n",
      "Loss of train set: 0.35435453057289124 at epoch: 4 and batch_num: 349\n",
      "Loss of train set: 0.5085902214050293 at epoch: 4 and batch_num: 350\n",
      "Loss of train set: 0.3666827380657196 at epoch: 4 and batch_num: 351\n",
      "Loss of train set: 0.3575541377067566 at epoch: 4 and batch_num: 352\n",
      "Loss of train set: 0.35296350717544556 at epoch: 4 and batch_num: 353\n",
      "Loss of train set: 0.17704011499881744 at epoch: 4 and batch_num: 354\n",
      "Loss of train set: 0.40872377157211304 at epoch: 4 and batch_num: 355\n",
      "Loss of train set: 0.2273724377155304 at epoch: 4 and batch_num: 356\n",
      "Loss of train set: 0.3932935297489166 at epoch: 4 and batch_num: 357\n",
      "Loss of train set: 0.4480445384979248 at epoch: 4 and batch_num: 358\n",
      "Loss of train set: 0.3316037058830261 at epoch: 4 and batch_num: 359\n",
      "Loss of train set: 0.373859167098999 at epoch: 4 and batch_num: 360\n",
      "Loss of train set: 0.3456493020057678 at epoch: 4 and batch_num: 361\n",
      "Loss of train set: 0.47335970401763916 at epoch: 4 and batch_num: 362\n",
      "Loss of train set: 0.36729639768600464 at epoch: 4 and batch_num: 363\n",
      "Loss of train set: 0.41546890139579773 at epoch: 4 and batch_num: 364\n",
      "Loss of train set: 0.4506908655166626 at epoch: 4 and batch_num: 365\n",
      "Loss of train set: 0.3951778709888458 at epoch: 4 and batch_num: 366\n",
      "Loss of train set: 0.4445011019706726 at epoch: 4 and batch_num: 367\n",
      "Loss of train set: 0.3519132137298584 at epoch: 4 and batch_num: 368\n",
      "Loss of train set: 0.2857711911201477 at epoch: 4 and batch_num: 369\n",
      "Loss of train set: 0.32712459564208984 at epoch: 4 and batch_num: 370\n",
      "Loss of train set: 0.2277769148349762 at epoch: 4 and batch_num: 371\n",
      "Loss of train set: 0.29361066222190857 at epoch: 4 and batch_num: 372\n",
      "Loss of train set: 0.3625484108924866 at epoch: 4 and batch_num: 373\n",
      "Loss of train set: 0.19316957890987396 at epoch: 4 and batch_num: 374\n",
      "Loss of train set: 0.33863571286201477 at epoch: 4 and batch_num: 375\n",
      "Loss of train set: 0.21441711485385895 at epoch: 4 and batch_num: 376\n",
      "Loss of train set: 0.5096036195755005 at epoch: 4 and batch_num: 377\n",
      "Loss of train set: 0.32034242153167725 at epoch: 4 and batch_num: 378\n",
      "Loss of train set: 0.28726011514663696 at epoch: 4 and batch_num: 379\n",
      "Loss of train set: 0.3578449785709381 at epoch: 4 and batch_num: 380\n",
      "Loss of train set: 0.3412879705429077 at epoch: 4 and batch_num: 381\n",
      "Loss of train set: 0.3142906129360199 at epoch: 4 and batch_num: 382\n",
      "Loss of train set: 0.2618890404701233 at epoch: 4 and batch_num: 383\n",
      "Loss of train set: 0.33547139167785645 at epoch: 4 and batch_num: 384\n",
      "Loss of train set: 0.47831082344055176 at epoch: 4 and batch_num: 385\n",
      "Loss of train set: 0.3024594783782959 at epoch: 4 and batch_num: 386\n",
      "Loss of train set: 0.3645588159561157 at epoch: 4 and batch_num: 387\n",
      "Loss of train set: 0.18227428197860718 at epoch: 4 and batch_num: 388\n",
      "Loss of train set: 0.3966731131076813 at epoch: 4 and batch_num: 389\n",
      "Loss of train set: 0.3276432156562805 at epoch: 4 and batch_num: 390\n",
      "Loss of train set: 0.30841928720474243 at epoch: 4 and batch_num: 391\n",
      "Loss of train set: 0.3551769256591797 at epoch: 4 and batch_num: 392\n",
      "Loss of train set: 0.31918710470199585 at epoch: 4 and batch_num: 393\n",
      "Loss of train set: 0.2721058130264282 at epoch: 4 and batch_num: 394\n",
      "Loss of train set: 0.3963402509689331 at epoch: 4 and batch_num: 395\n",
      "Loss of train set: 0.40188100934028625 at epoch: 4 and batch_num: 396\n",
      "Loss of train set: 0.35732483863830566 at epoch: 4 and batch_num: 397\n",
      "Loss of train set: 0.4259297549724579 at epoch: 4 and batch_num: 398\n",
      "Loss of train set: 0.26384878158569336 at epoch: 4 and batch_num: 399\n",
      "Loss of train set: 0.3284614682197571 at epoch: 4 and batch_num: 400\n",
      "Loss of train set: 0.2768472731113434 at epoch: 4 and batch_num: 401\n",
      "Loss of train set: 0.327508807182312 at epoch: 4 and batch_num: 402\n",
      "Loss of train set: 0.30813831090927124 at epoch: 4 and batch_num: 403\n",
      "Loss of train set: 0.47004634141921997 at epoch: 4 and batch_num: 404\n",
      "Loss of train set: 0.476657509803772 at epoch: 4 and batch_num: 405\n",
      "Loss of train set: 0.3891086280345917 at epoch: 4 and batch_num: 406\n",
      "Loss of train set: 0.36539095640182495 at epoch: 4 and batch_num: 407\n",
      "Loss of train set: 0.4006161093711853 at epoch: 4 and batch_num: 408\n",
      "Loss of train set: 0.2851106524467468 at epoch: 4 and batch_num: 409\n",
      "Loss of train set: 0.3284926116466522 at epoch: 4 and batch_num: 410\n",
      "Loss of train set: 0.2940957546234131 at epoch: 4 and batch_num: 411\n",
      "Loss of train set: 0.2581941485404968 at epoch: 4 and batch_num: 412\n",
      "Loss of train set: 0.2760237157344818 at epoch: 4 and batch_num: 413\n",
      "Loss of train set: 0.2902662754058838 at epoch: 4 and batch_num: 414\n",
      "Loss of train set: 0.41081613302230835 at epoch: 4 and batch_num: 415\n",
      "Loss of train set: 0.3559064567089081 at epoch: 4 and batch_num: 416\n",
      "Loss of train set: 0.44110050797462463 at epoch: 4 and batch_num: 417\n",
      "Loss of train set: 0.2285994589328766 at epoch: 4 and batch_num: 418\n",
      "Loss of train set: 0.37671801447868347 at epoch: 4 and batch_num: 419\n",
      "Loss of train set: 0.24167987704277039 at epoch: 4 and batch_num: 420\n",
      "Loss of train set: 0.4105067551136017 at epoch: 4 and batch_num: 421\n",
      "Loss of train set: 0.36472082138061523 at epoch: 4 and batch_num: 422\n",
      "Loss of train set: 0.365863561630249 at epoch: 4 and batch_num: 423\n",
      "Loss of train set: 0.29528552293777466 at epoch: 4 and batch_num: 424\n",
      "Loss of train set: 0.3348381519317627 at epoch: 4 and batch_num: 425\n",
      "Loss of train set: 0.24609974026679993 at epoch: 4 and batch_num: 426\n",
      "Loss of train set: 0.42544621229171753 at epoch: 4 and batch_num: 427\n",
      "Loss of train set: 0.41582414507865906 at epoch: 4 and batch_num: 428\n",
      "Loss of train set: 0.4247765839099884 at epoch: 4 and batch_num: 429\n",
      "Loss of train set: 0.178569495677948 at epoch: 4 and batch_num: 430\n",
      "Loss of train set: 0.19665399193763733 at epoch: 4 and batch_num: 431\n",
      "Loss of train set: 0.21097218990325928 at epoch: 4 and batch_num: 432\n",
      "Loss of train set: 0.3725106120109558 at epoch: 4 and batch_num: 433\n",
      "Loss of train set: 0.3119141459465027 at epoch: 4 and batch_num: 434\n",
      "Loss of train set: 0.4901321530342102 at epoch: 4 and batch_num: 435\n",
      "Loss of train set: 0.36450058221817017 at epoch: 4 and batch_num: 436\n",
      "Loss of train set: 0.3164903521537781 at epoch: 4 and batch_num: 437\n",
      "Loss of train set: 0.3280472159385681 at epoch: 4 and batch_num: 438\n",
      "Loss of train set: 0.37813064455986023 at epoch: 4 and batch_num: 439\n",
      "Loss of train set: 0.20709939301013947 at epoch: 4 and batch_num: 440\n",
      "Loss of train set: 0.29840371012687683 at epoch: 4 and batch_num: 441\n",
      "Loss of train set: 0.21015837788581848 at epoch: 4 and batch_num: 442\n",
      "Loss of train set: 0.2568778395652771 at epoch: 4 and batch_num: 443\n",
      "Loss of train set: 0.32889223098754883 at epoch: 4 and batch_num: 444\n",
      "Loss of train set: 0.37833720445632935 at epoch: 4 and batch_num: 445\n",
      "Loss of train set: 0.25400424003601074 at epoch: 4 and batch_num: 446\n",
      "Loss of train set: 0.25857013463974 at epoch: 4 and batch_num: 447\n",
      "Loss of train set: 0.2220112532377243 at epoch: 4 and batch_num: 448\n",
      "Loss of train set: 0.37778908014297485 at epoch: 4 and batch_num: 449\n",
      "Loss of train set: 0.40123021602630615 at epoch: 4 and batch_num: 450\n",
      "Loss of train set: 0.2705482244491577 at epoch: 4 and batch_num: 451\n",
      "Loss of train set: 0.29375728964805603 at epoch: 4 and batch_num: 452\n",
      "Loss of train set: 0.6028702855110168 at epoch: 4 and batch_num: 453\n",
      "Loss of train set: 0.33584219217300415 at epoch: 4 and batch_num: 454\n",
      "Loss of train set: 0.37001439929008484 at epoch: 4 and batch_num: 455\n",
      "Loss of train set: 0.22173519432544708 at epoch: 4 and batch_num: 456\n",
      "Loss of train set: 0.3831823468208313 at epoch: 4 and batch_num: 457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.45484107732772827 at epoch: 4 and batch_num: 458\n",
      "Loss of train set: 0.26876312494277954 at epoch: 4 and batch_num: 459\n",
      "Loss of train set: 0.6683852672576904 at epoch: 4 and batch_num: 460\n",
      "Loss of train set: 0.3630589246749878 at epoch: 4 and batch_num: 461\n",
      "Loss of train set: 0.18489477038383484 at epoch: 4 and batch_num: 462\n",
      "Loss of train set: 0.2154092788696289 at epoch: 4 and batch_num: 463\n",
      "Loss of train set: 0.3479018211364746 at epoch: 4 and batch_num: 464\n",
      "Loss of train set: 0.2683812379837036 at epoch: 4 and batch_num: 465\n",
      "Loss of train set: 0.36555108428001404 at epoch: 4 and batch_num: 466\n",
      "Loss of train set: 0.3002437353134155 at epoch: 4 and batch_num: 467\n",
      "Loss of train set: 0.5879657864570618 at epoch: 4 and batch_num: 468\n",
      "Loss of train set: 0.5115981101989746 at epoch: 4 and batch_num: 469\n",
      "Loss of train set: 0.34005993604660034 at epoch: 4 and batch_num: 470\n",
      "Loss of train set: 0.41396135091781616 at epoch: 4 and batch_num: 471\n",
      "Loss of train set: 0.4417703151702881 at epoch: 4 and batch_num: 472\n",
      "Loss of train set: 0.390232115983963 at epoch: 4 and batch_num: 473\n",
      "Loss of train set: 0.6259406805038452 at epoch: 4 and batch_num: 474\n",
      "Loss of train set: 0.49779680371284485 at epoch: 4 and batch_num: 475\n",
      "Loss of train set: 0.37914085388183594 at epoch: 4 and batch_num: 476\n",
      "Loss of train set: 0.3727151155471802 at epoch: 4 and batch_num: 477\n",
      "Loss of train set: 0.39987295866012573 at epoch: 4 and batch_num: 478\n",
      "Loss of train set: 0.3262818157672882 at epoch: 4 and batch_num: 479\n",
      "Loss of train set: 0.3379772901535034 at epoch: 4 and batch_num: 480\n",
      "Loss of train set: 0.21675167977809906 at epoch: 4 and batch_num: 481\n",
      "Loss of train set: 0.3532227575778961 at epoch: 4 and batch_num: 482\n",
      "Loss of train set: 0.2795913517475128 at epoch: 4 and batch_num: 483\n",
      "Loss of train set: 0.4744941294193268 at epoch: 4 and batch_num: 484\n",
      "Loss of train set: 0.5104825496673584 at epoch: 4 and batch_num: 485\n",
      "Loss of train set: 0.3413187265396118 at epoch: 4 and batch_num: 486\n",
      "Loss of train set: 0.2176743596792221 at epoch: 4 and batch_num: 487\n",
      "Loss of train set: 0.4146204888820648 at epoch: 4 and batch_num: 488\n",
      "Loss of train set: 0.3205074071884155 at epoch: 4 and batch_num: 489\n",
      "Loss of train set: 0.44099947810173035 at epoch: 4 and batch_num: 490\n",
      "Loss of train set: 0.44657307863235474 at epoch: 4 and batch_num: 491\n",
      "Loss of train set: 0.17691519856452942 at epoch: 4 and batch_num: 492\n",
      "Loss of train set: 0.49227091670036316 at epoch: 4 and batch_num: 493\n",
      "Loss of train set: 0.35901618003845215 at epoch: 4 and batch_num: 494\n",
      "Loss of train set: 0.3165648281574249 at epoch: 4 and batch_num: 495\n",
      "Loss of train set: 0.18950575590133667 at epoch: 4 and batch_num: 496\n",
      "Loss of train set: 0.37258124351501465 at epoch: 4 and batch_num: 497\n",
      "Loss of train set: 0.3212326765060425 at epoch: 4 and batch_num: 498\n",
      "Loss of train set: 0.19937027990818024 at epoch: 4 and batch_num: 499\n",
      "Loss of train set: 0.3571070730686188 at epoch: 4 and batch_num: 500\n",
      "Loss of train set: 0.26180171966552734 at epoch: 4 and batch_num: 501\n",
      "Loss of train set: 0.17459911108016968 at epoch: 4 and batch_num: 502\n",
      "Loss of train set: 0.46064430475234985 at epoch: 4 and batch_num: 503\n",
      "Loss of train set: 0.431518018245697 at epoch: 4 and batch_num: 504\n",
      "Loss of train set: 0.3434479236602783 at epoch: 4 and batch_num: 505\n",
      "Loss of train set: 0.38847777247428894 at epoch: 4 and batch_num: 506\n",
      "Loss of train set: 0.3924691677093506 at epoch: 4 and batch_num: 507\n",
      "Loss of train set: 0.45677614212036133 at epoch: 4 and batch_num: 508\n",
      "Loss of train set: 0.39802098274230957 at epoch: 4 and batch_num: 509\n",
      "Loss of train set: 0.24282702803611755 at epoch: 4 and batch_num: 510\n",
      "Loss of train set: 0.38689854741096497 at epoch: 4 and batch_num: 511\n",
      "Loss of train set: 0.20763862133026123 at epoch: 4 and batch_num: 512\n",
      "Loss of train set: 0.3244459331035614 at epoch: 4 and batch_num: 513\n",
      "Loss of train set: 0.23721277713775635 at epoch: 4 and batch_num: 514\n",
      "Loss of train set: 0.38944709300994873 at epoch: 4 and batch_num: 515\n",
      "Loss of train set: 0.3653433918952942 at epoch: 4 and batch_num: 516\n",
      "Loss of train set: 0.5320417881011963 at epoch: 4 and batch_num: 517\n",
      "Loss of train set: 0.4529709219932556 at epoch: 4 and batch_num: 518\n",
      "Loss of train set: 0.19516339898109436 at epoch: 4 and batch_num: 519\n",
      "Loss of train set: 0.2540214955806732 at epoch: 4 and batch_num: 520\n",
      "Loss of train set: 0.38582947850227356 at epoch: 4 and batch_num: 521\n",
      "Loss of train set: 0.5863171815872192 at epoch: 4 and batch_num: 522\n",
      "Loss of train set: 0.4075624644756317 at epoch: 4 and batch_num: 523\n",
      "Loss of train set: 0.41419488191604614 at epoch: 4 and batch_num: 524\n",
      "Loss of train set: 0.38515859842300415 at epoch: 4 and batch_num: 525\n",
      "Loss of train set: 0.2536603808403015 at epoch: 4 and batch_num: 526\n",
      "Loss of train set: 0.3115065097808838 at epoch: 4 and batch_num: 527\n",
      "Loss of train set: 0.33687663078308105 at epoch: 4 and batch_num: 528\n",
      "Loss of train set: 0.2313893735408783 at epoch: 4 and batch_num: 529\n",
      "Loss of train set: 0.33401429653167725 at epoch: 4 and batch_num: 530\n",
      "Loss of train set: 0.5390045642852783 at epoch: 4 and batch_num: 531\n",
      "Loss of train set: 0.39909565448760986 at epoch: 4 and batch_num: 532\n",
      "Loss of train set: 0.31719154119491577 at epoch: 4 and batch_num: 533\n",
      "Loss of train set: 0.5055661201477051 at epoch: 4 and batch_num: 534\n",
      "Loss of train set: 0.3941934108734131 at epoch: 4 and batch_num: 535\n",
      "Loss of train set: 0.3766138255596161 at epoch: 4 and batch_num: 536\n",
      "Loss of train set: 0.26994067430496216 at epoch: 4 and batch_num: 537\n",
      "Loss of train set: 0.2769230604171753 at epoch: 4 and batch_num: 538\n",
      "Loss of train set: 0.33631038665771484 at epoch: 4 and batch_num: 539\n",
      "Loss of train set: 0.5433025360107422 at epoch: 4 and batch_num: 540\n",
      "Loss of train set: 0.292774498462677 at epoch: 4 and batch_num: 541\n",
      "Loss of train set: 0.3092435598373413 at epoch: 4 and batch_num: 542\n",
      "Loss of train set: 0.5790276527404785 at epoch: 4 and batch_num: 543\n",
      "Loss of train set: 0.3381814956665039 at epoch: 4 and batch_num: 544\n",
      "Loss of train set: 0.41766148805618286 at epoch: 4 and batch_num: 545\n",
      "Loss of train set: 0.2084442377090454 at epoch: 4 and batch_num: 546\n",
      "Loss of train set: 0.33177003264427185 at epoch: 4 and batch_num: 547\n",
      "Loss of train set: 0.3168585002422333 at epoch: 4 and batch_num: 548\n",
      "Loss of train set: 0.4214100241661072 at epoch: 4 and batch_num: 549\n",
      "Loss of train set: 0.3760855793952942 at epoch: 4 and batch_num: 550\n",
      "Loss of train set: 0.42030322551727295 at epoch: 4 and batch_num: 551\n",
      "Loss of train set: 0.4299156069755554 at epoch: 4 and batch_num: 552\n",
      "Loss of train set: 0.31381744146347046 at epoch: 4 and batch_num: 553\n",
      "Loss of train set: 0.1790352612733841 at epoch: 4 and batch_num: 554\n",
      "Loss of train set: 0.25522446632385254 at epoch: 4 and batch_num: 555\n",
      "Loss of train set: 0.2034454643726349 at epoch: 4 and batch_num: 556\n",
      "Loss of train set: 0.43237125873565674 at epoch: 4 and batch_num: 557\n",
      "Loss of train set: 0.1841449737548828 at epoch: 4 and batch_num: 558\n",
      "Loss of train set: 0.36639440059661865 at epoch: 4 and batch_num: 559\n",
      "Loss of train set: 0.4233914017677307 at epoch: 4 and batch_num: 560\n",
      "Loss of train set: 0.45390450954437256 at epoch: 4 and batch_num: 561\n",
      "Loss of train set: 0.44665926694869995 at epoch: 4 and batch_num: 562\n",
      "Loss of train set: 0.2972405254840851 at epoch: 4 and batch_num: 563\n",
      "Loss of train set: 0.34500205516815186 at epoch: 4 and batch_num: 564\n",
      "Loss of train set: 0.39082348346710205 at epoch: 4 and batch_num: 565\n",
      "Loss of train set: 0.37229007482528687 at epoch: 4 and batch_num: 566\n",
      "Loss of train set: 0.4358227849006653 at epoch: 4 and batch_num: 567\n",
      "Loss of train set: 0.43315011262893677 at epoch: 4 and batch_num: 568\n",
      "Loss of train set: 0.36069756746292114 at epoch: 4 and batch_num: 569\n",
      "Loss of train set: 0.30679479241371155 at epoch: 4 and batch_num: 570\n",
      "Loss of train set: 0.2581207752227783 at epoch: 4 and batch_num: 571\n",
      "Loss of train set: 0.3037838339805603 at epoch: 4 and batch_num: 572\n",
      "Loss of train set: 0.4378461539745331 at epoch: 4 and batch_num: 573\n",
      "Loss of train set: 0.3648521900177002 at epoch: 4 and batch_num: 574\n",
      "Loss of train set: 0.21580243110656738 at epoch: 4 and batch_num: 575\n",
      "Loss of train set: 0.3778834939002991 at epoch: 4 and batch_num: 576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.593529224395752 at epoch: 4 and batch_num: 577\n",
      "Loss of train set: 0.33185362815856934 at epoch: 4 and batch_num: 578\n",
      "Loss of train set: 0.16386198997497559 at epoch: 4 and batch_num: 579\n",
      "Loss of train set: 0.476624071598053 at epoch: 4 and batch_num: 580\n",
      "Loss of train set: 0.37171077728271484 at epoch: 4 and batch_num: 581\n",
      "Loss of train set: 0.28165072202682495 at epoch: 4 and batch_num: 582\n",
      "Loss of train set: 0.2774900794029236 at epoch: 4 and batch_num: 583\n",
      "Loss of train set: 0.4573044776916504 at epoch: 4 and batch_num: 584\n",
      "Loss of train set: 0.23686957359313965 at epoch: 4 and batch_num: 585\n",
      "Loss of train set: 0.4319879412651062 at epoch: 4 and batch_num: 586\n",
      "Loss of train set: 0.3578300476074219 at epoch: 4 and batch_num: 587\n",
      "Loss of train set: 0.3066129684448242 at epoch: 4 and batch_num: 588\n",
      "Loss of train set: 0.41677621006965637 at epoch: 4 and batch_num: 589\n",
      "Loss of train set: 0.3898746967315674 at epoch: 4 and batch_num: 590\n",
      "Loss of train set: 0.430062860250473 at epoch: 4 and batch_num: 591\n",
      "Loss of train set: 0.3461804687976837 at epoch: 4 and batch_num: 592\n",
      "Loss of train set: 0.28471463918685913 at epoch: 4 and batch_num: 593\n",
      "Loss of train set: 0.2723868489265442 at epoch: 4 and batch_num: 594\n",
      "Loss of train set: 0.48238521814346313 at epoch: 4 and batch_num: 595\n",
      "Loss of train set: 0.19074681401252747 at epoch: 4 and batch_num: 596\n",
      "Loss of train set: 0.5034610033035278 at epoch: 4 and batch_num: 597\n",
      "Loss of train set: 0.28484010696411133 at epoch: 4 and batch_num: 598\n",
      "Loss of train set: 0.6913055777549744 at epoch: 4 and batch_num: 599\n",
      "Loss of train set: 0.33788275718688965 at epoch: 4 and batch_num: 600\n",
      "Loss of train set: 0.27240967750549316 at epoch: 4 and batch_num: 601\n",
      "Loss of train set: 0.32166141271591187 at epoch: 4 and batch_num: 602\n",
      "Loss of train set: 0.49461036920547485 at epoch: 4 and batch_num: 603\n",
      "Loss of train set: 0.44676536321640015 at epoch: 4 and batch_num: 604\n",
      "Loss of train set: 0.2807796597480774 at epoch: 4 and batch_num: 605\n",
      "Loss of train set: 0.33517295122146606 at epoch: 4 and batch_num: 606\n",
      "Loss of train set: 0.27770471572875977 at epoch: 4 and batch_num: 607\n",
      "Loss of train set: 0.34237998723983765 at epoch: 4 and batch_num: 608\n",
      "Loss of train set: 0.33137932419776917 at epoch: 4 and batch_num: 609\n",
      "Loss of train set: 0.4066426157951355 at epoch: 4 and batch_num: 610\n",
      "Loss of train set: 0.3266768455505371 at epoch: 4 and batch_num: 611\n",
      "Loss of train set: 0.38652682304382324 at epoch: 4 and batch_num: 612\n",
      "Loss of train set: 0.3066011071205139 at epoch: 4 and batch_num: 613\n",
      "Loss of train set: 0.3996444344520569 at epoch: 4 and batch_num: 614\n",
      "Loss of train set: 0.33282843232154846 at epoch: 4 and batch_num: 615\n",
      "Loss of train set: 0.25981423258781433 at epoch: 4 and batch_num: 616\n",
      "Loss of train set: 0.23487669229507446 at epoch: 4 and batch_num: 617\n",
      "Loss of train set: 0.32216620445251465 at epoch: 4 and batch_num: 618\n",
      "Loss of train set: 0.4638867974281311 at epoch: 4 and batch_num: 619\n",
      "Loss of train set: 0.2986154556274414 at epoch: 4 and batch_num: 620\n",
      "Loss of train set: 0.31423184275627136 at epoch: 4 and batch_num: 621\n",
      "Loss of train set: 0.41670092940330505 at epoch: 4 and batch_num: 622\n",
      "Loss of train set: 0.27204057574272156 at epoch: 4 and batch_num: 623\n",
      "Loss of train set: 0.4445486068725586 at epoch: 4 and batch_num: 624\n",
      "Loss of train set: 0.33766889572143555 at epoch: 4 and batch_num: 625\n",
      "Loss of train set: 0.3188092112541199 at epoch: 4 and batch_num: 626\n",
      "Loss of train set: 0.2855433523654938 at epoch: 4 and batch_num: 627\n",
      "Loss of train set: 0.22403833270072937 at epoch: 4 and batch_num: 628\n",
      "Loss of train set: 0.2779315412044525 at epoch: 4 and batch_num: 629\n",
      "Loss of train set: 0.3200617730617523 at epoch: 4 and batch_num: 630\n",
      "Loss of train set: 0.26861682534217834 at epoch: 4 and batch_num: 631\n",
      "Loss of train set: 0.3270307183265686 at epoch: 4 and batch_num: 632\n",
      "Loss of train set: 0.48137372732162476 at epoch: 4 and batch_num: 633\n",
      "Loss of train set: 0.3062213063240051 at epoch: 4 and batch_num: 634\n",
      "Loss of train set: 0.200486958026886 at epoch: 4 and batch_num: 635\n",
      "Loss of train set: 0.5185283422470093 at epoch: 4 and batch_num: 636\n",
      "Loss of train set: 0.3273887038230896 at epoch: 4 and batch_num: 637\n",
      "Loss of train set: 0.3688034415245056 at epoch: 4 and batch_num: 638\n",
      "Loss of train set: 0.5144285559654236 at epoch: 4 and batch_num: 639\n",
      "Loss of train set: 0.37487921118736267 at epoch: 4 and batch_num: 640\n",
      "Loss of train set: 0.33929580450057983 at epoch: 4 and batch_num: 641\n",
      "Loss of train set: 0.3451528251171112 at epoch: 4 and batch_num: 642\n",
      "Loss of train set: 0.7057797312736511 at epoch: 4 and batch_num: 643\n",
      "Loss of train set: 0.22957943379878998 at epoch: 4 and batch_num: 644\n",
      "Loss of train set: 0.4379999339580536 at epoch: 4 and batch_num: 645\n",
      "Loss of train set: 0.4966585040092468 at epoch: 4 and batch_num: 646\n",
      "Loss of train set: 0.27613651752471924 at epoch: 4 and batch_num: 647\n",
      "Loss of train set: 0.3548130691051483 at epoch: 4 and batch_num: 648\n",
      "Loss of train set: 0.37023940682411194 at epoch: 4 and batch_num: 649\n",
      "Loss of train set: 0.2856155335903168 at epoch: 4 and batch_num: 650\n",
      "Loss of train set: 0.31970080733299255 at epoch: 4 and batch_num: 651\n",
      "Loss of train set: 0.2658359706401825 at epoch: 4 and batch_num: 652\n",
      "Loss of train set: 0.46319079399108887 at epoch: 4 and batch_num: 653\n",
      "Loss of train set: 0.2685675024986267 at epoch: 4 and batch_num: 654\n",
      "Loss of train set: 0.6168020367622375 at epoch: 4 and batch_num: 655\n",
      "Loss of train set: 0.3687766492366791 at epoch: 4 and batch_num: 656\n",
      "Loss of train set: 0.3625374734401703 at epoch: 4 and batch_num: 657\n",
      "Loss of train set: 0.30011269450187683 at epoch: 4 and batch_num: 658\n",
      "Loss of train set: 0.30448246002197266 at epoch: 4 and batch_num: 659\n",
      "Loss of train set: 0.22237658500671387 at epoch: 4 and batch_num: 660\n",
      "Loss of train set: 0.43498921394348145 at epoch: 4 and batch_num: 661\n",
      "Loss of train set: 0.4647384285926819 at epoch: 4 and batch_num: 662\n",
      "Loss of train set: 0.2873433530330658 at epoch: 4 and batch_num: 663\n",
      "Loss of train set: 0.2994305491447449 at epoch: 4 and batch_num: 664\n",
      "Loss of train set: 0.35353371500968933 at epoch: 4 and batch_num: 665\n",
      "Loss of train set: 0.1911294013261795 at epoch: 4 and batch_num: 666\n",
      "Loss of train set: 0.2878831624984741 at epoch: 4 and batch_num: 667\n",
      "Loss of train set: 0.5274912714958191 at epoch: 4 and batch_num: 668\n",
      "Loss of train set: 0.3267401158809662 at epoch: 4 and batch_num: 669\n",
      "Loss of train set: 0.33509334921836853 at epoch: 4 and batch_num: 670\n",
      "Loss of train set: 0.35407495498657227 at epoch: 4 and batch_num: 671\n",
      "Loss of train set: 0.21723443269729614 at epoch: 4 and batch_num: 672\n",
      "Loss of train set: 0.3255641460418701 at epoch: 4 and batch_num: 673\n",
      "Loss of train set: 0.23050907254219055 at epoch: 4 and batch_num: 674\n",
      "Loss of train set: 0.3259482979774475 at epoch: 4 and batch_num: 675\n",
      "Loss of train set: 0.37626856565475464 at epoch: 4 and batch_num: 676\n",
      "Loss of train set: 0.2738714814186096 at epoch: 4 and batch_num: 677\n",
      "Loss of train set: 0.3895886540412903 at epoch: 4 and batch_num: 678\n",
      "Loss of train set: 0.27202776074409485 at epoch: 4 and batch_num: 679\n",
      "Loss of train set: 0.3487231433391571 at epoch: 4 and batch_num: 680\n",
      "Loss of train set: 0.472098171710968 at epoch: 4 and batch_num: 681\n",
      "Loss of train set: 0.393307626247406 at epoch: 4 and batch_num: 682\n",
      "Loss of train set: 0.396358460187912 at epoch: 4 and batch_num: 683\n",
      "Loss of train set: 0.2630074620246887 at epoch: 4 and batch_num: 684\n",
      "Loss of train set: 0.2779540419578552 at epoch: 4 and batch_num: 685\n",
      "Loss of train set: 0.2933950126171112 at epoch: 4 and batch_num: 686\n",
      "Loss of train set: 0.24948319792747498 at epoch: 4 and batch_num: 687\n",
      "Loss of train set: 0.2845239043235779 at epoch: 4 and batch_num: 688\n",
      "Loss of train set: 0.5086411833763123 at epoch: 4 and batch_num: 689\n",
      "Loss of train set: 0.45806777477264404 at epoch: 4 and batch_num: 690\n",
      "Loss of train set: 0.2914133667945862 at epoch: 4 and batch_num: 691\n",
      "Loss of train set: 0.18147021532058716 at epoch: 4 and batch_num: 692\n",
      "Loss of train set: 0.37310975790023804 at epoch: 4 and batch_num: 693\n",
      "Loss of train set: 0.31408536434173584 at epoch: 4 and batch_num: 694\n",
      "Loss of train set: 0.18993613123893738 at epoch: 4 and batch_num: 695\n",
      "Loss of train set: 0.4051154851913452 at epoch: 4 and batch_num: 696\n",
      "Loss of train set: 0.25604167580604553 at epoch: 4 and batch_num: 697\n",
      "Loss of train set: 0.30415043234825134 at epoch: 4 and batch_num: 698\n",
      "Loss of train set: 0.5610001683235168 at epoch: 4 and batch_num: 699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2610170245170593 at epoch: 4 and batch_num: 700\n",
      "Loss of train set: 0.30710193514823914 at epoch: 4 and batch_num: 701\n",
      "Loss of train set: 0.3805635869503021 at epoch: 4 and batch_num: 702\n",
      "Loss of train set: 0.3065524697303772 at epoch: 4 and batch_num: 703\n",
      "Loss of train set: 0.38599544763565063 at epoch: 4 and batch_num: 704\n",
      "Loss of train set: 0.3387543261051178 at epoch: 4 and batch_num: 705\n",
      "Loss of train set: 0.24682305753231049 at epoch: 4 and batch_num: 706\n",
      "Loss of train set: 0.36304500699043274 at epoch: 4 and batch_num: 707\n",
      "Loss of train set: 0.49451136589050293 at epoch: 4 and batch_num: 708\n",
      "Loss of train set: 0.3579583764076233 at epoch: 4 and batch_num: 709\n",
      "Loss of train set: 0.32265129685401917 at epoch: 4 and batch_num: 710\n",
      "Loss of train set: 0.16854330897331238 at epoch: 4 and batch_num: 711\n",
      "Loss of train set: 0.2771632969379425 at epoch: 4 and batch_num: 712\n",
      "Loss of train set: 0.29563987255096436 at epoch: 4 and batch_num: 713\n",
      "Loss of train set: 0.26066064834594727 at epoch: 4 and batch_num: 714\n",
      "Loss of train set: 0.28196001052856445 at epoch: 4 and batch_num: 715\n",
      "Loss of train set: 0.33402979373931885 at epoch: 4 and batch_num: 716\n",
      "Loss of train set: 0.5019245147705078 at epoch: 4 and batch_num: 717\n",
      "Loss of train set: 0.5775918960571289 at epoch: 4 and batch_num: 718\n",
      "Loss of train set: 0.4015868306159973 at epoch: 4 and batch_num: 719\n",
      "Loss of train set: 0.30231228470802307 at epoch: 4 and batch_num: 720\n",
      "Loss of train set: 0.459685742855072 at epoch: 4 and batch_num: 721\n",
      "Loss of train set: 0.4072534441947937 at epoch: 4 and batch_num: 722\n",
      "Loss of train set: 0.28229230642318726 at epoch: 4 and batch_num: 723\n",
      "Loss of train set: 0.35643085837364197 at epoch: 4 and batch_num: 724\n",
      "Loss of train set: 0.37455490231513977 at epoch: 4 and batch_num: 725\n",
      "Loss of train set: 0.4891514480113983 at epoch: 4 and batch_num: 726\n",
      "Loss of train set: 0.30341121554374695 at epoch: 4 and batch_num: 727\n",
      "Loss of train set: 0.4147034287452698 at epoch: 4 and batch_num: 728\n",
      "Loss of train set: 0.2877804636955261 at epoch: 4 and batch_num: 729\n",
      "Loss of train set: 0.381600022315979 at epoch: 4 and batch_num: 730\n",
      "Loss of train set: 0.19133082032203674 at epoch: 4 and batch_num: 731\n",
      "Loss of train set: 0.24040184915065765 at epoch: 4 and batch_num: 732\n",
      "Loss of train set: 0.33466434478759766 at epoch: 4 and batch_num: 733\n",
      "Loss of train set: 0.29763156175613403 at epoch: 4 and batch_num: 734\n",
      "Loss of train set: 0.575986921787262 at epoch: 4 and batch_num: 735\n",
      "Loss of train set: 0.3733953833580017 at epoch: 4 and batch_num: 736\n",
      "Loss of train set: 0.380868136882782 at epoch: 4 and batch_num: 737\n",
      "Loss of train set: 0.3506562411785126 at epoch: 4 and batch_num: 738\n",
      "Loss of train set: 0.2788965106010437 at epoch: 4 and batch_num: 739\n",
      "Loss of train set: 0.32390540838241577 at epoch: 4 and batch_num: 740\n",
      "Loss of train set: 0.3845328092575073 at epoch: 4 and batch_num: 741\n",
      "Loss of train set: 0.45438534021377563 at epoch: 4 and batch_num: 742\n",
      "Loss of train set: 0.31634876132011414 at epoch: 4 and batch_num: 743\n",
      "Loss of train set: 0.28603076934814453 at epoch: 4 and batch_num: 744\n",
      "Loss of train set: 0.33979374170303345 at epoch: 4 and batch_num: 745\n",
      "Loss of train set: 0.14754557609558105 at epoch: 4 and batch_num: 746\n",
      "Loss of train set: 0.43824225664138794 at epoch: 4 and batch_num: 747\n",
      "Loss of train set: 0.3635447025299072 at epoch: 4 and batch_num: 748\n",
      "Loss of train set: 0.371524453163147 at epoch: 4 and batch_num: 749\n",
      "Loss of train set: 0.24915309250354767 at epoch: 4 and batch_num: 750\n",
      "Loss of train set: 0.3097587823867798 at epoch: 4 and batch_num: 751\n",
      "Loss of train set: 0.37370556592941284 at epoch: 4 and batch_num: 752\n",
      "Loss of train set: 0.305642694234848 at epoch: 4 and batch_num: 753\n",
      "Loss of train set: 0.3553858697414398 at epoch: 4 and batch_num: 754\n",
      "Loss of train set: 0.3512624204158783 at epoch: 4 and batch_num: 755\n",
      "Loss of train set: 0.21677947044372559 at epoch: 4 and batch_num: 756\n",
      "Loss of train set: 0.2621285915374756 at epoch: 4 and batch_num: 757\n",
      "Loss of train set: 0.3544553816318512 at epoch: 4 and batch_num: 758\n",
      "Loss of train set: 0.4643423855304718 at epoch: 4 and batch_num: 759\n",
      "Loss of train set: 0.4094061851501465 at epoch: 4 and batch_num: 760\n",
      "Loss of train set: 0.5984399914741516 at epoch: 4 and batch_num: 761\n",
      "Loss of train set: 0.4100154638290405 at epoch: 4 and batch_num: 762\n",
      "Loss of train set: 0.3078266978263855 at epoch: 4 and batch_num: 763\n",
      "Loss of train set: 0.4528639018535614 at epoch: 4 and batch_num: 764\n",
      "Loss of train set: 0.42319566011428833 at epoch: 4 and batch_num: 765\n",
      "Loss of train set: 0.3931538760662079 at epoch: 4 and batch_num: 766\n",
      "Loss of train set: 0.26461291313171387 at epoch: 4 and batch_num: 767\n",
      "Loss of train set: 0.2918694317340851 at epoch: 4 and batch_num: 768\n",
      "Loss of train set: 0.37198054790496826 at epoch: 4 and batch_num: 769\n",
      "Loss of train set: 0.3896421790122986 at epoch: 4 and batch_num: 770\n",
      "Loss of train set: 0.2801711857318878 at epoch: 4 and batch_num: 771\n",
      "Loss of train set: 0.32356423139572144 at epoch: 4 and batch_num: 772\n",
      "Loss of train set: 0.3164166510105133 at epoch: 4 and batch_num: 773\n",
      "Loss of train set: 0.3270576000213623 at epoch: 4 and batch_num: 774\n",
      "Loss of train set: 0.2996096611022949 at epoch: 4 and batch_num: 775\n",
      "Loss of train set: 0.23896054923534393 at epoch: 4 and batch_num: 776\n",
      "Loss of train set: 0.47750285267829895 at epoch: 4 and batch_num: 777\n",
      "Loss of train set: 0.4808357357978821 at epoch: 4 and batch_num: 778\n",
      "Loss of train set: 0.25227656960487366 at epoch: 4 and batch_num: 779\n",
      "Loss of train set: 0.2357100546360016 at epoch: 4 and batch_num: 780\n",
      "Loss of train set: 0.4474513530731201 at epoch: 4 and batch_num: 781\n",
      "Loss of train set: 0.43754857778549194 at epoch: 4 and batch_num: 782\n",
      "Loss of train set: 0.3693251609802246 at epoch: 4 and batch_num: 783\n",
      "Loss of train set: 0.5160037875175476 at epoch: 4 and batch_num: 784\n",
      "Loss of train set: 0.25459688901901245 at epoch: 4 and batch_num: 785\n",
      "Loss of train set: 0.4479789733886719 at epoch: 4 and batch_num: 786\n",
      "Loss of train set: 0.2348623126745224 at epoch: 4 and batch_num: 787\n",
      "Loss of train set: 0.37586909532546997 at epoch: 4 and batch_num: 788\n",
      "Loss of train set: 0.5025596022605896 at epoch: 4 and batch_num: 789\n",
      "Loss of train set: 0.2993425726890564 at epoch: 4 and batch_num: 790\n",
      "Loss of train set: 0.35537293553352356 at epoch: 4 and batch_num: 791\n",
      "Loss of train set: 0.3442639410495758 at epoch: 4 and batch_num: 792\n",
      "Loss of train set: 0.46314194798469543 at epoch: 4 and batch_num: 793\n",
      "Loss of train set: 0.3574081361293793 at epoch: 4 and batch_num: 794\n",
      "Loss of train set: 0.3129972517490387 at epoch: 4 and batch_num: 795\n",
      "Loss of train set: 0.4457087814807892 at epoch: 4 and batch_num: 796\n",
      "Loss of train set: 0.3410933017730713 at epoch: 4 and batch_num: 797\n",
      "Loss of train set: 0.3582577705383301 at epoch: 4 and batch_num: 798\n",
      "Loss of train set: 0.21554189920425415 at epoch: 4 and batch_num: 799\n",
      "Loss of train set: 0.5713226795196533 at epoch: 4 and batch_num: 800\n",
      "Loss of train set: 0.3198855221271515 at epoch: 4 and batch_num: 801\n",
      "Loss of train set: 0.4238262176513672 at epoch: 4 and batch_num: 802\n",
      "Loss of train set: 0.35641878843307495 at epoch: 4 and batch_num: 803\n",
      "Loss of train set: 0.22830793261528015 at epoch: 4 and batch_num: 804\n",
      "Loss of train set: 0.40262511372566223 at epoch: 4 and batch_num: 805\n",
      "Loss of train set: 0.4913642704486847 at epoch: 4 and batch_num: 806\n",
      "Loss of train set: 0.29911598563194275 at epoch: 4 and batch_num: 807\n",
      "Loss of train set: 0.19310952723026276 at epoch: 4 and batch_num: 808\n",
      "Loss of train set: 0.39913350343704224 at epoch: 4 and batch_num: 809\n",
      "Loss of train set: 0.33193737268447876 at epoch: 4 and batch_num: 810\n",
      "Loss of train set: 0.22783750295639038 at epoch: 4 and batch_num: 811\n",
      "Loss of train set: 0.514461100101471 at epoch: 4 and batch_num: 812\n",
      "Loss of train set: 0.3193786144256592 at epoch: 4 and batch_num: 813\n",
      "Loss of train set: 0.36490464210510254 at epoch: 4 and batch_num: 814\n",
      "Loss of train set: 0.40175291895866394 at epoch: 4 and batch_num: 815\n",
      "Loss of train set: 0.46563953161239624 at epoch: 4 and batch_num: 816\n",
      "Loss of train set: 0.35640838742256165 at epoch: 4 and batch_num: 817\n",
      "Loss of train set: 0.2784885764122009 at epoch: 4 and batch_num: 818\n",
      "Loss of train set: 0.2799500823020935 at epoch: 4 and batch_num: 819\n",
      "Loss of train set: 0.19662582874298096 at epoch: 4 and batch_num: 820\n",
      "Loss of train set: 0.37324059009552 at epoch: 4 and batch_num: 821\n",
      "Loss of train set: 0.4562501311302185 at epoch: 4 and batch_num: 822\n",
      "Loss of train set: 0.44375619292259216 at epoch: 4 and batch_num: 823\n",
      "Loss of train set: 0.3904370665550232 at epoch: 4 and batch_num: 824\n",
      "Loss of train set: 0.32209497690200806 at epoch: 4 and batch_num: 825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2552722096443176 at epoch: 4 and batch_num: 826\n",
      "Loss of train set: 0.4100149869918823 at epoch: 4 and batch_num: 827\n",
      "Loss of train set: 0.24098604917526245 at epoch: 4 and batch_num: 828\n",
      "Loss of train set: 0.34866946935653687 at epoch: 4 and batch_num: 829\n",
      "Loss of train set: 0.2294107973575592 at epoch: 4 and batch_num: 830\n",
      "Loss of train set: 0.2985087037086487 at epoch: 4 and batch_num: 831\n",
      "Loss of train set: 0.4123185873031616 at epoch: 4 and batch_num: 832\n",
      "Loss of train set: 0.3980070650577545 at epoch: 4 and batch_num: 833\n",
      "Loss of train set: 0.30045241117477417 at epoch: 4 and batch_num: 834\n",
      "Loss of train set: 0.4968576431274414 at epoch: 4 and batch_num: 835\n",
      "Loss of train set: 0.24387763440608978 at epoch: 4 and batch_num: 836\n",
      "Loss of train set: 0.4575902819633484 at epoch: 4 and batch_num: 837\n",
      "Loss of train set: 0.327401340007782 at epoch: 4 and batch_num: 838\n",
      "Loss of train set: 0.412584125995636 at epoch: 4 and batch_num: 839\n",
      "Loss of train set: 0.21803495287895203 at epoch: 4 and batch_num: 840\n",
      "Loss of train set: 0.24989604949951172 at epoch: 4 and batch_num: 841\n",
      "Loss of train set: 0.39178556203842163 at epoch: 4 and batch_num: 842\n",
      "Loss of train set: 0.19104236364364624 at epoch: 4 and batch_num: 843\n",
      "Loss of train set: 0.411475270986557 at epoch: 4 and batch_num: 844\n",
      "Loss of train set: 0.31447190046310425 at epoch: 4 and batch_num: 845\n",
      "Loss of train set: 0.2392035871744156 at epoch: 4 and batch_num: 846\n",
      "Loss of train set: 0.3386276066303253 at epoch: 4 and batch_num: 847\n",
      "Loss of train set: 0.27427488565444946 at epoch: 4 and batch_num: 848\n",
      "Loss of train set: 0.2957448959350586 at epoch: 4 and batch_num: 849\n",
      "Loss of train set: 0.3349755108356476 at epoch: 4 and batch_num: 850\n",
      "Loss of train set: 0.3357354998588562 at epoch: 4 and batch_num: 851\n",
      "Loss of train set: 0.35875654220581055 at epoch: 4 and batch_num: 852\n",
      "Loss of train set: 0.4073284864425659 at epoch: 4 and batch_num: 853\n",
      "Loss of train set: 0.37623679637908936 at epoch: 4 and batch_num: 854\n",
      "Loss of train set: 0.3164409399032593 at epoch: 4 and batch_num: 855\n",
      "Loss of train set: 0.28941619396209717 at epoch: 4 and batch_num: 856\n",
      "Loss of train set: 0.4811522364616394 at epoch: 4 and batch_num: 857\n",
      "Loss of train set: 0.1726401150226593 at epoch: 4 and batch_num: 858\n",
      "Loss of train set: 0.4112454652786255 at epoch: 4 and batch_num: 859\n",
      "Loss of train set: 0.3315003514289856 at epoch: 4 and batch_num: 860\n",
      "Loss of train set: 0.3580549955368042 at epoch: 4 and batch_num: 861\n",
      "Loss of train set: 0.36532843112945557 at epoch: 4 and batch_num: 862\n",
      "Loss of train set: 0.4433029294013977 at epoch: 4 and batch_num: 863\n",
      "Loss of train set: 0.23018789291381836 at epoch: 4 and batch_num: 864\n",
      "Loss of train set: 0.464839905500412 at epoch: 4 and batch_num: 865\n",
      "Loss of train set: 0.4460050165653229 at epoch: 4 and batch_num: 866\n",
      "Loss of train set: 0.36150115728378296 at epoch: 4 and batch_num: 867\n",
      "Loss of train set: 0.32418373227119446 at epoch: 4 and batch_num: 868\n",
      "Loss of train set: 0.36117222905158997 at epoch: 4 and batch_num: 869\n",
      "Loss of train set: 0.3392190635204315 at epoch: 4 and batch_num: 870\n",
      "Loss of train set: 0.38441070914268494 at epoch: 4 and batch_num: 871\n",
      "Loss of train set: 0.2946983575820923 at epoch: 4 and batch_num: 872\n",
      "Loss of train set: 0.2953646183013916 at epoch: 4 and batch_num: 873\n",
      "Loss of train set: 0.27204710245132446 at epoch: 4 and batch_num: 874\n",
      "Loss of train set: 0.22336125373840332 at epoch: 4 and batch_num: 875\n",
      "Loss of train set: 0.37174445390701294 at epoch: 4 and batch_num: 876\n",
      "Loss of train set: 0.38642174005508423 at epoch: 4 and batch_num: 877\n",
      "Loss of train set: 0.3062112331390381 at epoch: 4 and batch_num: 878\n",
      "Loss of train set: 0.27324676513671875 at epoch: 4 and batch_num: 879\n",
      "Loss of train set: 0.27511152625083923 at epoch: 4 and batch_num: 880\n",
      "Loss of train set: 0.3473159074783325 at epoch: 4 and batch_num: 881\n",
      "Loss of train set: 0.2599257826805115 at epoch: 4 and batch_num: 882\n",
      "Loss of train set: 0.289828896522522 at epoch: 4 and batch_num: 883\n",
      "Loss of train set: 0.3792227804660797 at epoch: 4 and batch_num: 884\n",
      "Loss of train set: 0.2734701633453369 at epoch: 4 and batch_num: 885\n",
      "Loss of train set: 0.24877557158470154 at epoch: 4 and batch_num: 886\n",
      "Loss of train set: 0.43190139532089233 at epoch: 4 and batch_num: 887\n",
      "Loss of train set: 0.23965507745742798 at epoch: 4 and batch_num: 888\n",
      "Loss of train set: 0.259123831987381 at epoch: 4 and batch_num: 889\n",
      "Loss of train set: 0.41115468740463257 at epoch: 4 and batch_num: 890\n",
      "Loss of train set: 0.23169314861297607 at epoch: 4 and batch_num: 891\n",
      "Loss of train set: 0.39131978154182434 at epoch: 4 and batch_num: 892\n",
      "Loss of train set: 0.2524385452270508 at epoch: 4 and batch_num: 893\n",
      "Loss of train set: 0.33219921588897705 at epoch: 4 and batch_num: 894\n",
      "Loss of train set: 0.45515888929367065 at epoch: 4 and batch_num: 895\n",
      "Loss of train set: 0.27309420704841614 at epoch: 4 and batch_num: 896\n",
      "Loss of train set: 0.3507024049758911 at epoch: 4 and batch_num: 897\n",
      "Loss of train set: 0.24660640954971313 at epoch: 4 and batch_num: 898\n",
      "Loss of train set: 0.24938228726387024 at epoch: 4 and batch_num: 899\n",
      "Loss of train set: 0.39642664790153503 at epoch: 4 and batch_num: 900\n",
      "Loss of train set: 0.3124358654022217 at epoch: 4 and batch_num: 901\n",
      "Loss of train set: 0.6430066823959351 at epoch: 4 and batch_num: 902\n",
      "Loss of train set: 0.3797183632850647 at epoch: 4 and batch_num: 903\n",
      "Loss of train set: 0.29258498549461365 at epoch: 4 and batch_num: 904\n",
      "Loss of train set: 0.21843618154525757 at epoch: 4 and batch_num: 905\n",
      "Loss of train set: 0.48287487030029297 at epoch: 4 and batch_num: 906\n",
      "Loss of train set: 0.323504775762558 at epoch: 4 and batch_num: 907\n",
      "Loss of train set: 0.340684175491333 at epoch: 4 and batch_num: 908\n",
      "Loss of train set: 0.31542402505874634 at epoch: 4 and batch_num: 909\n",
      "Loss of train set: 0.31568413972854614 at epoch: 4 and batch_num: 910\n",
      "Loss of train set: 0.40948835015296936 at epoch: 4 and batch_num: 911\n",
      "Loss of train set: 0.29242315888404846 at epoch: 4 and batch_num: 912\n",
      "Loss of train set: 0.5159409046173096 at epoch: 4 and batch_num: 913\n",
      "Loss of train set: 0.3290325403213501 at epoch: 4 and batch_num: 914\n",
      "Loss of train set: 0.4103490114212036 at epoch: 4 and batch_num: 915\n",
      "Loss of train set: 0.38292449712753296 at epoch: 4 and batch_num: 916\n",
      "Loss of train set: 0.3231169581413269 at epoch: 4 and batch_num: 917\n",
      "Loss of train set: 0.31520146131515503 at epoch: 4 and batch_num: 918\n",
      "Loss of train set: 0.2555902898311615 at epoch: 4 and batch_num: 919\n",
      "Loss of train set: 0.27519330382347107 at epoch: 4 and batch_num: 920\n",
      "Loss of train set: 0.4036754369735718 at epoch: 4 and batch_num: 921\n",
      "Loss of train set: 0.3631383180618286 at epoch: 4 and batch_num: 922\n",
      "Loss of train set: 0.31581079959869385 at epoch: 4 and batch_num: 923\n",
      "Loss of train set: 0.46607378125190735 at epoch: 4 and batch_num: 924\n",
      "Loss of train set: 0.3293483257293701 at epoch: 4 and batch_num: 925\n",
      "Loss of train set: 0.4960275888442993 at epoch: 4 and batch_num: 926\n",
      "Loss of train set: 0.5414573550224304 at epoch: 4 and batch_num: 927\n",
      "Loss of train set: 0.36652839183807373 at epoch: 4 and batch_num: 928\n",
      "Loss of train set: 0.3151535093784332 at epoch: 4 and batch_num: 929\n",
      "Loss of train set: 0.3862721621990204 at epoch: 4 and batch_num: 930\n",
      "Loss of train set: 0.40617820620536804 at epoch: 4 and batch_num: 931\n",
      "Loss of train set: 0.3474918007850647 at epoch: 4 and batch_num: 932\n",
      "Loss of train set: 0.3305514454841614 at epoch: 4 and batch_num: 933\n",
      "Loss of train set: 0.4067287743091583 at epoch: 4 and batch_num: 934\n",
      "Loss of train set: 0.43751513957977295 at epoch: 4 and batch_num: 935\n",
      "Loss of train set: 0.26904770731925964 at epoch: 4 and batch_num: 936\n",
      "Loss of train set: 0.40425339341163635 at epoch: 4 and batch_num: 937\n",
      "Accuracy of train set: 0.8734833333333333\n",
      "Loss of test set: 0.2746274769306183 at epoch: 4 and batch_num: 0\n",
      "Loss of test set: 0.23179125785827637 at epoch: 4 and batch_num: 1\n",
      "Loss of test set: 0.14059031009674072 at epoch: 4 and batch_num: 2\n",
      "Loss of test set: 0.6624687314033508 at epoch: 4 and batch_num: 3\n",
      "Loss of test set: 0.2292456477880478 at epoch: 4 and batch_num: 4\n",
      "Loss of test set: 0.4660128951072693 at epoch: 4 and batch_num: 5\n",
      "Loss of test set: 0.7793174982070923 at epoch: 4 and batch_num: 6\n",
      "Loss of test set: 0.5087129473686218 at epoch: 4 and batch_num: 7\n",
      "Loss of test set: 0.4554612338542938 at epoch: 4 and batch_num: 8\n",
      "Loss of test set: 0.42170417308807373 at epoch: 4 and batch_num: 9\n",
      "Loss of test set: 0.27976900339126587 at epoch: 4 and batch_num: 10\n",
      "Loss of test set: 0.5230629444122314 at epoch: 4 and batch_num: 11\n",
      "Loss of test set: 0.580983579158783 at epoch: 4 and batch_num: 12\n",
      "Loss of test set: 0.28038734197616577 at epoch: 4 and batch_num: 13\n",
      "Loss of test set: 0.436784565448761 at epoch: 4 and batch_num: 14\n",
      "Loss of test set: 0.4373090863227844 at epoch: 4 and batch_num: 15\n",
      "Loss of test set: 0.4895003139972687 at epoch: 4 and batch_num: 16\n",
      "Loss of test set: 0.33861833810806274 at epoch: 4 and batch_num: 17\n",
      "Loss of test set: 0.25187361240386963 at epoch: 4 and batch_num: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.31251513957977295 at epoch: 4 and batch_num: 19\n",
      "Loss of test set: 0.380281001329422 at epoch: 4 and batch_num: 20\n",
      "Loss of test set: 0.4162091910839081 at epoch: 4 and batch_num: 21\n",
      "Loss of test set: 0.22640348970890045 at epoch: 4 and batch_num: 22\n",
      "Loss of test set: 0.28617149591445923 at epoch: 4 and batch_num: 23\n",
      "Loss of test set: 0.3639892339706421 at epoch: 4 and batch_num: 24\n",
      "Loss of test set: 0.33126020431518555 at epoch: 4 and batch_num: 25\n",
      "Loss of test set: 0.32226455211639404 at epoch: 4 and batch_num: 26\n",
      "Loss of test set: 0.49530327320098877 at epoch: 4 and batch_num: 27\n",
      "Loss of test set: 0.43728145956993103 at epoch: 4 and batch_num: 28\n",
      "Loss of test set: 0.6336638927459717 at epoch: 4 and batch_num: 29\n",
      "Loss of test set: 0.2306765913963318 at epoch: 4 and batch_num: 30\n",
      "Loss of test set: 0.2268698364496231 at epoch: 4 and batch_num: 31\n",
      "Loss of test set: 0.45467087626457214 at epoch: 4 and batch_num: 32\n",
      "Loss of test set: 0.42176416516304016 at epoch: 4 and batch_num: 33\n",
      "Loss of test set: 0.3634389638900757 at epoch: 4 and batch_num: 34\n",
      "Loss of test set: 0.3736816346645355 at epoch: 4 and batch_num: 35\n",
      "Loss of test set: 0.47253915667533875 at epoch: 4 and batch_num: 36\n",
      "Loss of test set: 0.34533095359802246 at epoch: 4 and batch_num: 37\n",
      "Loss of test set: 0.38591206073760986 at epoch: 4 and batch_num: 38\n",
      "Loss of test set: 0.43910956382751465 at epoch: 4 and batch_num: 39\n",
      "Loss of test set: 0.5174134969711304 at epoch: 4 and batch_num: 40\n",
      "Loss of test set: 0.18299582600593567 at epoch: 4 and batch_num: 41\n",
      "Loss of test set: 0.5282261371612549 at epoch: 4 and batch_num: 42\n",
      "Loss of test set: 0.7490332722663879 at epoch: 4 and batch_num: 43\n",
      "Loss of test set: 0.30224698781967163 at epoch: 4 and batch_num: 44\n",
      "Loss of test set: 0.39040374755859375 at epoch: 4 and batch_num: 45\n",
      "Loss of test set: 0.4968094825744629 at epoch: 4 and batch_num: 46\n",
      "Loss of test set: 0.544079601764679 at epoch: 4 and batch_num: 47\n",
      "Loss of test set: 0.40382009744644165 at epoch: 4 and batch_num: 48\n",
      "Loss of test set: 0.4360750913619995 at epoch: 4 and batch_num: 49\n",
      "Loss of test set: 0.4160435199737549 at epoch: 4 and batch_num: 50\n",
      "Loss of test set: 0.33645981550216675 at epoch: 4 and batch_num: 51\n",
      "Loss of test set: 0.35587990283966064 at epoch: 4 and batch_num: 52\n",
      "Loss of test set: 0.6443153619766235 at epoch: 4 and batch_num: 53\n",
      "Loss of test set: 0.31924301385879517 at epoch: 4 and batch_num: 54\n",
      "Loss of test set: 0.45172369480133057 at epoch: 4 and batch_num: 55\n",
      "Loss of test set: 0.3216613233089447 at epoch: 4 and batch_num: 56\n",
      "Loss of test set: 0.5221700668334961 at epoch: 4 and batch_num: 57\n",
      "Loss of test set: 0.3220316171646118 at epoch: 4 and batch_num: 58\n",
      "Loss of test set: 0.4367343485355377 at epoch: 4 and batch_num: 59\n",
      "Loss of test set: 0.33352991938591003 at epoch: 4 and batch_num: 60\n",
      "Loss of test set: 0.3009740710258484 at epoch: 4 and batch_num: 61\n",
      "Loss of test set: 0.4540306031703949 at epoch: 4 and batch_num: 62\n",
      "Loss of test set: 0.28028860688209534 at epoch: 4 and batch_num: 63\n",
      "Loss of test set: 0.3524831533432007 at epoch: 4 and batch_num: 64\n",
      "Loss of test set: 0.42176133394241333 at epoch: 4 and batch_num: 65\n",
      "Loss of test set: 0.43236950039863586 at epoch: 4 and batch_num: 66\n",
      "Loss of test set: 0.46966439485549927 at epoch: 4 and batch_num: 67\n",
      "Loss of test set: 0.2672135829925537 at epoch: 4 and batch_num: 68\n",
      "Loss of test set: 0.3934905230998993 at epoch: 4 and batch_num: 69\n",
      "Loss of test set: 0.4115411937236786 at epoch: 4 and batch_num: 70\n",
      "Loss of test set: 0.23380383849143982 at epoch: 4 and batch_num: 71\n",
      "Loss of test set: 0.5590915679931641 at epoch: 4 and batch_num: 72\n",
      "Loss of test set: 0.3575412333011627 at epoch: 4 and batch_num: 73\n",
      "Loss of test set: 0.3602059483528137 at epoch: 4 and batch_num: 74\n",
      "Loss of test set: 0.4202292561531067 at epoch: 4 and batch_num: 75\n",
      "Loss of test set: 0.40581613779067993 at epoch: 4 and batch_num: 76\n",
      "Loss of test set: 0.24968433380126953 at epoch: 4 and batch_num: 77\n",
      "Loss of test set: 0.3646525740623474 at epoch: 4 and batch_num: 78\n",
      "Loss of test set: 0.48533201217651367 at epoch: 4 and batch_num: 79\n",
      "Loss of test set: 0.5700440406799316 at epoch: 4 and batch_num: 80\n",
      "Loss of test set: 0.6321291923522949 at epoch: 4 and batch_num: 81\n",
      "Loss of test set: 0.37471917271614075 at epoch: 4 and batch_num: 82\n",
      "Loss of test set: 0.2947869598865509 at epoch: 4 and batch_num: 83\n",
      "Loss of test set: 0.46701520681381226 at epoch: 4 and batch_num: 84\n",
      "Loss of test set: 0.4532521367073059 at epoch: 4 and batch_num: 85\n",
      "Loss of test set: 0.35616087913513184 at epoch: 4 and batch_num: 86\n",
      "Loss of test set: 0.3456602096557617 at epoch: 4 and batch_num: 87\n",
      "Loss of test set: 0.49092578887939453 at epoch: 4 and batch_num: 88\n",
      "Loss of test set: 0.48488882184028625 at epoch: 4 and batch_num: 89\n",
      "Loss of test set: 0.468802809715271 at epoch: 4 and batch_num: 90\n",
      "Loss of test set: 0.4317166209220886 at epoch: 4 and batch_num: 91\n",
      "Loss of test set: 0.4994431734085083 at epoch: 4 and batch_num: 92\n",
      "Loss of test set: 0.38255852460861206 at epoch: 4 and batch_num: 93\n",
      "Loss of test set: 0.5267083644866943 at epoch: 4 and batch_num: 94\n",
      "Loss of test set: 0.27746185660362244 at epoch: 4 and batch_num: 95\n",
      "Loss of test set: 0.33004504442214966 at epoch: 4 and batch_num: 96\n",
      "Loss of test set: 0.7308607697486877 at epoch: 4 and batch_num: 97\n",
      "Loss of test set: 0.4470716416835785 at epoch: 4 and batch_num: 98\n",
      "Loss of test set: 0.2960544228553772 at epoch: 4 and batch_num: 99\n",
      "Loss of test set: 0.4177064895629883 at epoch: 4 and batch_num: 100\n",
      "Loss of test set: 0.42996639013290405 at epoch: 4 and batch_num: 101\n",
      "Loss of test set: 0.4710865318775177 at epoch: 4 and batch_num: 102\n",
      "Loss of test set: 0.32538658380508423 at epoch: 4 and batch_num: 103\n",
      "Loss of test set: 0.302517831325531 at epoch: 4 and batch_num: 104\n",
      "Loss of test set: 0.474513441324234 at epoch: 4 and batch_num: 105\n",
      "Loss of test set: 0.40574243664741516 at epoch: 4 and batch_num: 106\n",
      "Loss of test set: 0.3123447299003601 at epoch: 4 and batch_num: 107\n",
      "Loss of test set: 0.28352290391921997 at epoch: 4 and batch_num: 108\n",
      "Loss of test set: 0.32667919993400574 at epoch: 4 and batch_num: 109\n",
      "Loss of test set: 0.3251727819442749 at epoch: 4 and batch_num: 110\n",
      "Loss of test set: 0.36238574981689453 at epoch: 4 and batch_num: 111\n",
      "Loss of test set: 0.26129937171936035 at epoch: 4 and batch_num: 112\n",
      "Loss of test set: 0.4024641215801239 at epoch: 4 and batch_num: 113\n",
      "Loss of test set: 0.39652198553085327 at epoch: 4 and batch_num: 114\n",
      "Loss of test set: 0.2966333031654358 at epoch: 4 and batch_num: 115\n",
      "Loss of test set: 0.2329791635274887 at epoch: 4 and batch_num: 116\n",
      "Loss of test set: 0.4391840696334839 at epoch: 4 and batch_num: 117\n",
      "Loss of test set: 0.4748866856098175 at epoch: 4 and batch_num: 118\n",
      "Loss of test set: 0.4711676239967346 at epoch: 4 and batch_num: 119\n",
      "Loss of test set: 0.3426714539527893 at epoch: 4 and batch_num: 120\n",
      "Loss of test set: 0.5048418641090393 at epoch: 4 and batch_num: 121\n",
      "Loss of test set: 0.45402437448501587 at epoch: 4 and batch_num: 122\n",
      "Loss of test set: 0.3982428312301636 at epoch: 4 and batch_num: 123\n",
      "Loss of test set: 0.3906475901603699 at epoch: 4 and batch_num: 124\n",
      "Loss of test set: 0.4150122404098511 at epoch: 4 and batch_num: 125\n",
      "Loss of test set: 0.5690882205963135 at epoch: 4 and batch_num: 126\n",
      "Loss of test set: 0.593268096446991 at epoch: 4 and batch_num: 127\n",
      "Loss of test set: 0.5384445190429688 at epoch: 4 and batch_num: 128\n",
      "Loss of test set: 0.33298414945602417 at epoch: 4 and batch_num: 129\n",
      "Loss of test set: 0.3172250986099243 at epoch: 4 and batch_num: 130\n",
      "Loss of test set: 0.6328310966491699 at epoch: 4 and batch_num: 131\n",
      "Loss of test set: 0.42781543731689453 at epoch: 4 and batch_num: 132\n",
      "Loss of test set: 0.5364692211151123 at epoch: 4 and batch_num: 133\n",
      "Loss of test set: 0.38063403964042664 at epoch: 4 and batch_num: 134\n",
      "Loss of test set: 0.3134348392486572 at epoch: 4 and batch_num: 135\n",
      "Loss of test set: 0.3196966052055359 at epoch: 4 and batch_num: 136\n",
      "Loss of test set: 0.243000328540802 at epoch: 4 and batch_num: 137\n",
      "Loss of test set: 0.3514941930770874 at epoch: 4 and batch_num: 138\n",
      "Loss of test set: 0.32549071311950684 at epoch: 4 and batch_num: 139\n",
      "Loss of test set: 0.35695958137512207 at epoch: 4 and batch_num: 140\n",
      "Loss of test set: 0.2375902682542801 at epoch: 4 and batch_num: 141\n",
      "Loss of test set: 0.37993180751800537 at epoch: 4 and batch_num: 142\n",
      "Loss of test set: 0.5132657289505005 at epoch: 4 and batch_num: 143\n",
      "Loss of test set: 0.34783655405044556 at epoch: 4 and batch_num: 144\n",
      "Loss of test set: 0.1972217559814453 at epoch: 4 and batch_num: 145\n",
      "Loss of test set: 0.3199443817138672 at epoch: 4 and batch_num: 146\n",
      "Loss of test set: 0.3622620403766632 at epoch: 4 and batch_num: 147\n",
      "Loss of test set: 0.4535456597805023 at epoch: 4 and batch_num: 148\n",
      "Loss of test set: 0.4878067970275879 at epoch: 4 and batch_num: 149\n",
      "Loss of test set: 0.3494551181793213 at epoch: 4 and batch_num: 150\n",
      "Loss of test set: 0.25842034816741943 at epoch: 4 and batch_num: 151\n",
      "Loss of test set: 0.4904733896255493 at epoch: 4 and batch_num: 152\n",
      "Loss of test set: 0.34721046686172485 at epoch: 4 and batch_num: 153\n",
      "Loss of test set: 0.2766973674297333 at epoch: 4 and batch_num: 154\n",
      "Loss of test set: 0.6521037817001343 at epoch: 4 and batch_num: 155\n",
      "Loss of test set: 0.44448304176330566 at epoch: 4 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8573\n",
      "Loss of train set: 0.3644084930419922 at epoch: 5 and batch_num: 0\n",
      "Loss of train set: 0.38712990283966064 at epoch: 5 and batch_num: 1\n",
      "Loss of train set: 0.667648434638977 at epoch: 5 and batch_num: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.28548362851142883 at epoch: 5 and batch_num: 3\n",
      "Loss of train set: 0.31970155239105225 at epoch: 5 and batch_num: 4\n",
      "Loss of train set: 0.3043757975101471 at epoch: 5 and batch_num: 5\n",
      "Loss of train set: 0.30439653992652893 at epoch: 5 and batch_num: 6\n",
      "Loss of train set: 0.36177968978881836 at epoch: 5 and batch_num: 7\n",
      "Loss of train set: 0.4256604313850403 at epoch: 5 and batch_num: 8\n",
      "Loss of train set: 0.3460397720336914 at epoch: 5 and batch_num: 9\n",
      "Loss of train set: 0.4416459798812866 at epoch: 5 and batch_num: 10\n",
      "Loss of train set: 0.3141334056854248 at epoch: 5 and batch_num: 11\n",
      "Loss of train set: 0.36127591133117676 at epoch: 5 and batch_num: 12\n",
      "Loss of train set: 0.2894209027290344 at epoch: 5 and batch_num: 13\n",
      "Loss of train set: 0.2907959520816803 at epoch: 5 and batch_num: 14\n",
      "Loss of train set: 0.5610299706459045 at epoch: 5 and batch_num: 15\n",
      "Loss of train set: 0.47757449746131897 at epoch: 5 and batch_num: 16\n",
      "Loss of train set: 0.5731956958770752 at epoch: 5 and batch_num: 17\n",
      "Loss of train set: 0.3414524793624878 at epoch: 5 and batch_num: 18\n",
      "Loss of train set: 0.1556853950023651 at epoch: 5 and batch_num: 19\n",
      "Loss of train set: 0.5375878810882568 at epoch: 5 and batch_num: 20\n",
      "Loss of train set: 0.31853777170181274 at epoch: 5 and batch_num: 21\n",
      "Loss of train set: 0.27182745933532715 at epoch: 5 and batch_num: 22\n",
      "Loss of train set: 0.31024765968322754 at epoch: 5 and batch_num: 23\n",
      "Loss of train set: 0.3635859787464142 at epoch: 5 and batch_num: 24\n",
      "Loss of train set: 0.3425471782684326 at epoch: 5 and batch_num: 25\n",
      "Loss of train set: 0.26109591126441956 at epoch: 5 and batch_num: 26\n",
      "Loss of train set: 0.4740428626537323 at epoch: 5 and batch_num: 27\n",
      "Loss of train set: 0.48116669058799744 at epoch: 5 and batch_num: 28\n",
      "Loss of train set: 0.4160865247249603 at epoch: 5 and batch_num: 29\n",
      "Loss of train set: 0.3987008035182953 at epoch: 5 and batch_num: 30\n",
      "Loss of train set: 0.3033981919288635 at epoch: 5 and batch_num: 31\n",
      "Loss of train set: 0.29504358768463135 at epoch: 5 and batch_num: 32\n",
      "Loss of train set: 0.2406403124332428 at epoch: 5 and batch_num: 33\n",
      "Loss of train set: 0.3530391454696655 at epoch: 5 and batch_num: 34\n",
      "Loss of train set: 0.379610151052475 at epoch: 5 and batch_num: 35\n",
      "Loss of train set: 0.314503014087677 at epoch: 5 and batch_num: 36\n",
      "Loss of train set: 0.2930949926376343 at epoch: 5 and batch_num: 37\n",
      "Loss of train set: 0.23848740756511688 at epoch: 5 and batch_num: 38\n",
      "Loss of train set: 0.22472162544727325 at epoch: 5 and batch_num: 39\n",
      "Loss of train set: 0.27500075101852417 at epoch: 5 and batch_num: 40\n",
      "Loss of train set: 0.42982083559036255 at epoch: 5 and batch_num: 41\n",
      "Loss of train set: 0.31589168310165405 at epoch: 5 and batch_num: 42\n",
      "Loss of train set: 0.31557226181030273 at epoch: 5 and batch_num: 43\n",
      "Loss of train set: 0.3116984963417053 at epoch: 5 and batch_num: 44\n",
      "Loss of train set: 0.2904970943927765 at epoch: 5 and batch_num: 45\n",
      "Loss of train set: 0.27266669273376465 at epoch: 5 and batch_num: 46\n",
      "Loss of train set: 0.3237425982952118 at epoch: 5 and batch_num: 47\n",
      "Loss of train set: 0.3072669506072998 at epoch: 5 and batch_num: 48\n",
      "Loss of train set: 0.31243473291397095 at epoch: 5 and batch_num: 49\n",
      "Loss of train set: 0.3147117793560028 at epoch: 5 and batch_num: 50\n",
      "Loss of train set: 0.3343493938446045 at epoch: 5 and batch_num: 51\n",
      "Loss of train set: 0.2684410810470581 at epoch: 5 and batch_num: 52\n",
      "Loss of train set: 0.30991289019584656 at epoch: 5 and batch_num: 53\n",
      "Loss of train set: 0.287869930267334 at epoch: 5 and batch_num: 54\n",
      "Loss of train set: 0.3129842281341553 at epoch: 5 and batch_num: 55\n",
      "Loss of train set: 0.42762914299964905 at epoch: 5 and batch_num: 56\n",
      "Loss of train set: 0.5072953104972839 at epoch: 5 and batch_num: 57\n",
      "Loss of train set: 0.46661627292633057 at epoch: 5 and batch_num: 58\n",
      "Loss of train set: 0.43456965684890747 at epoch: 5 and batch_num: 59\n",
      "Loss of train set: 0.3890150189399719 at epoch: 5 and batch_num: 60\n",
      "Loss of train set: 0.368644654750824 at epoch: 5 and batch_num: 61\n",
      "Loss of train set: 0.29135018587112427 at epoch: 5 and batch_num: 62\n",
      "Loss of train set: 0.22883018851280212 at epoch: 5 and batch_num: 63\n",
      "Loss of train set: 0.430581271648407 at epoch: 5 and batch_num: 64\n",
      "Loss of train set: 0.23542791604995728 at epoch: 5 and batch_num: 65\n",
      "Loss of train set: 0.3490840196609497 at epoch: 5 and batch_num: 66\n",
      "Loss of train set: 0.3729979395866394 at epoch: 5 and batch_num: 67\n",
      "Loss of train set: 0.23488545417785645 at epoch: 5 and batch_num: 68\n",
      "Loss of train set: 0.2700282335281372 at epoch: 5 and batch_num: 69\n",
      "Loss of train set: 0.3405200242996216 at epoch: 5 and batch_num: 70\n",
      "Loss of train set: 0.2927153706550598 at epoch: 5 and batch_num: 71\n",
      "Loss of train set: 0.40347492694854736 at epoch: 5 and batch_num: 72\n",
      "Loss of train set: 0.28079235553741455 at epoch: 5 and batch_num: 73\n",
      "Loss of train set: 0.2534501254558563 at epoch: 5 and batch_num: 74\n",
      "Loss of train set: 0.3572215139865875 at epoch: 5 and batch_num: 75\n",
      "Loss of train set: 0.4347646236419678 at epoch: 5 and batch_num: 76\n",
      "Loss of train set: 0.4694446325302124 at epoch: 5 and batch_num: 77\n",
      "Loss of train set: 0.28670793771743774 at epoch: 5 and batch_num: 78\n",
      "Loss of train set: 0.21549521386623383 at epoch: 5 and batch_num: 79\n",
      "Loss of train set: 0.249955952167511 at epoch: 5 and batch_num: 80\n",
      "Loss of train set: 0.3278229534626007 at epoch: 5 and batch_num: 81\n",
      "Loss of train set: 0.505568265914917 at epoch: 5 and batch_num: 82\n",
      "Loss of train set: 0.20180508494377136 at epoch: 5 and batch_num: 83\n",
      "Loss of train set: 0.3464682698249817 at epoch: 5 and batch_num: 84\n",
      "Loss of train set: 0.37264442443847656 at epoch: 5 and batch_num: 85\n",
      "Loss of train set: 0.5057072043418884 at epoch: 5 and batch_num: 86\n",
      "Loss of train set: 0.2492828369140625 at epoch: 5 and batch_num: 87\n",
      "Loss of train set: 0.48992955684661865 at epoch: 5 and batch_num: 88\n",
      "Loss of train set: 0.3626590371131897 at epoch: 5 and batch_num: 89\n",
      "Loss of train set: 0.3591771125793457 at epoch: 5 and batch_num: 90\n",
      "Loss of train set: 0.34164348244667053 at epoch: 5 and batch_num: 91\n",
      "Loss of train set: 0.2428915947675705 at epoch: 5 and batch_num: 92\n",
      "Loss of train set: 0.390349805355072 at epoch: 5 and batch_num: 93\n",
      "Loss of train set: 0.37828293442726135 at epoch: 5 and batch_num: 94\n",
      "Loss of train set: 0.36627814173698425 at epoch: 5 and batch_num: 95\n",
      "Loss of train set: 0.4211987555027008 at epoch: 5 and batch_num: 96\n",
      "Loss of train set: 0.2731972336769104 at epoch: 5 and batch_num: 97\n",
      "Loss of train set: 0.27696311473846436 at epoch: 5 and batch_num: 98\n",
      "Loss of train set: 0.19277983903884888 at epoch: 5 and batch_num: 99\n",
      "Loss of train set: 0.3809044361114502 at epoch: 5 and batch_num: 100\n",
      "Loss of train set: 0.43222713470458984 at epoch: 5 and batch_num: 101\n",
      "Loss of train set: 0.25728946924209595 at epoch: 5 and batch_num: 102\n",
      "Loss of train set: 0.3276577591896057 at epoch: 5 and batch_num: 103\n",
      "Loss of train set: 0.32707715034484863 at epoch: 5 and batch_num: 104\n",
      "Loss of train set: 0.3492550253868103 at epoch: 5 and batch_num: 105\n",
      "Loss of train set: 0.26780784130096436 at epoch: 5 and batch_num: 106\n",
      "Loss of train set: 0.33160340785980225 at epoch: 5 and batch_num: 107\n",
      "Loss of train set: 0.31814053654670715 at epoch: 5 and batch_num: 108\n",
      "Loss of train set: 0.3429666757583618 at epoch: 5 and batch_num: 109\n",
      "Loss of train set: 0.23984523117542267 at epoch: 5 and batch_num: 110\n",
      "Loss of train set: 0.28492826223373413 at epoch: 5 and batch_num: 111\n",
      "Loss of train set: 0.3018339276313782 at epoch: 5 and batch_num: 112\n",
      "Loss of train set: 0.24654299020767212 at epoch: 5 and batch_num: 113\n",
      "Loss of train set: 0.6680973768234253 at epoch: 5 and batch_num: 114\n",
      "Loss of train set: 0.47029805183410645 at epoch: 5 and batch_num: 115\n",
      "Loss of train set: 0.26807159185409546 at epoch: 5 and batch_num: 116\n",
      "Loss of train set: 0.48068299889564514 at epoch: 5 and batch_num: 117\n",
      "Loss of train set: 0.39995354413986206 at epoch: 5 and batch_num: 118\n",
      "Loss of train set: 0.4195173382759094 at epoch: 5 and batch_num: 119\n",
      "Loss of train set: 0.34660935401916504 at epoch: 5 and batch_num: 120\n",
      "Loss of train set: 0.32486066222190857 at epoch: 5 and batch_num: 121\n",
      "Loss of train set: 0.19387759268283844 at epoch: 5 and batch_num: 122\n",
      "Loss of train set: 0.4111214876174927 at epoch: 5 and batch_num: 123\n",
      "Loss of train set: 0.42953792214393616 at epoch: 5 and batch_num: 124\n",
      "Loss of train set: 0.42896291613578796 at epoch: 5 and batch_num: 125\n",
      "Loss of train set: 0.3245052695274353 at epoch: 5 and batch_num: 126\n",
      "Loss of train set: 0.25060611963272095 at epoch: 5 and batch_num: 127\n",
      "Loss of train set: 0.2733690142631531 at epoch: 5 and batch_num: 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.4539678394794464 at epoch: 5 and batch_num: 129\n",
      "Loss of train set: 0.2782912254333496 at epoch: 5 and batch_num: 130\n",
      "Loss of train set: 0.47954803705215454 at epoch: 5 and batch_num: 131\n",
      "Loss of train set: 0.3811373710632324 at epoch: 5 and batch_num: 132\n",
      "Loss of train set: 0.37424540519714355 at epoch: 5 and batch_num: 133\n",
      "Loss of train set: 0.2495921552181244 at epoch: 5 and batch_num: 134\n",
      "Loss of train set: 0.4161294102668762 at epoch: 5 and batch_num: 135\n",
      "Loss of train set: 0.4022901952266693 at epoch: 5 and batch_num: 136\n",
      "Loss of train set: 0.3069436550140381 at epoch: 5 and batch_num: 137\n",
      "Loss of train set: 0.33802133798599243 at epoch: 5 and batch_num: 138\n",
      "Loss of train set: 0.21047323942184448 at epoch: 5 and batch_num: 139\n",
      "Loss of train set: 0.29154080152511597 at epoch: 5 and batch_num: 140\n",
      "Loss of train set: 0.3833158016204834 at epoch: 5 and batch_num: 141\n",
      "Loss of train set: 0.46498721837997437 at epoch: 5 and batch_num: 142\n",
      "Loss of train set: 0.25727254152297974 at epoch: 5 and batch_num: 143\n",
      "Loss of train set: 0.29757407307624817 at epoch: 5 and batch_num: 144\n",
      "Loss of train set: 0.399799108505249 at epoch: 5 and batch_num: 145\n",
      "Loss of train set: 0.2645617723464966 at epoch: 5 and batch_num: 146\n",
      "Loss of train set: 0.5367766618728638 at epoch: 5 and batch_num: 147\n",
      "Loss of train set: 0.33785688877105713 at epoch: 5 and batch_num: 148\n",
      "Loss of train set: 0.2521461248397827 at epoch: 5 and batch_num: 149\n",
      "Loss of train set: 0.1853642612695694 at epoch: 5 and batch_num: 150\n",
      "Loss of train set: 0.666004478931427 at epoch: 5 and batch_num: 151\n",
      "Loss of train set: 0.37476739287376404 at epoch: 5 and batch_num: 152\n",
      "Loss of train set: 0.5269261598587036 at epoch: 5 and batch_num: 153\n",
      "Loss of train set: 0.4156985878944397 at epoch: 5 and batch_num: 154\n",
      "Loss of train set: 0.33362019062042236 at epoch: 5 and batch_num: 155\n",
      "Loss of train set: 0.297860324382782 at epoch: 5 and batch_num: 156\n",
      "Loss of train set: 0.2701478600502014 at epoch: 5 and batch_num: 157\n",
      "Loss of train set: 0.41094547510147095 at epoch: 5 and batch_num: 158\n",
      "Loss of train set: 0.5088322162628174 at epoch: 5 and batch_num: 159\n",
      "Loss of train set: 0.3792022466659546 at epoch: 5 and batch_num: 160\n",
      "Loss of train set: 0.48028141260147095 at epoch: 5 and batch_num: 161\n",
      "Loss of train set: 0.2320300042629242 at epoch: 5 and batch_num: 162\n",
      "Loss of train set: 0.2409396916627884 at epoch: 5 and batch_num: 163\n",
      "Loss of train set: 0.2971760630607605 at epoch: 5 and batch_num: 164\n",
      "Loss of train set: 0.25433313846588135 at epoch: 5 and batch_num: 165\n",
      "Loss of train set: 0.4023364782333374 at epoch: 5 and batch_num: 166\n",
      "Loss of train set: 0.2752792239189148 at epoch: 5 and batch_num: 167\n",
      "Loss of train set: 0.4293884038925171 at epoch: 5 and batch_num: 168\n",
      "Loss of train set: 0.35767674446105957 at epoch: 5 and batch_num: 169\n",
      "Loss of train set: 0.3670673966407776 at epoch: 5 and batch_num: 170\n",
      "Loss of train set: 0.4398810863494873 at epoch: 5 and batch_num: 171\n",
      "Loss of train set: 0.2512073814868927 at epoch: 5 and batch_num: 172\n",
      "Loss of train set: 0.3331814408302307 at epoch: 5 and batch_num: 173\n",
      "Loss of train set: 0.5000743865966797 at epoch: 5 and batch_num: 174\n",
      "Loss of train set: 0.4154345989227295 at epoch: 5 and batch_num: 175\n",
      "Loss of train set: 0.3376869857311249 at epoch: 5 and batch_num: 176\n",
      "Loss of train set: 0.493842750787735 at epoch: 5 and batch_num: 177\n",
      "Loss of train set: 0.4754358232021332 at epoch: 5 and batch_num: 178\n",
      "Loss of train set: 0.3250642418861389 at epoch: 5 and batch_num: 179\n",
      "Loss of train set: 0.21827572584152222 at epoch: 5 and batch_num: 180\n",
      "Loss of train set: 0.3381200432777405 at epoch: 5 and batch_num: 181\n",
      "Loss of train set: 0.24592839181423187 at epoch: 5 and batch_num: 182\n",
      "Loss of train set: 0.4932403564453125 at epoch: 5 and batch_num: 183\n",
      "Loss of train set: 0.2799359858036041 at epoch: 5 and batch_num: 184\n",
      "Loss of train set: 0.40443694591522217 at epoch: 5 and batch_num: 185\n",
      "Loss of train set: 0.3870753049850464 at epoch: 5 and batch_num: 186\n",
      "Loss of train set: 0.4693150222301483 at epoch: 5 and batch_num: 187\n",
      "Loss of train set: 0.3292767107486725 at epoch: 5 and batch_num: 188\n",
      "Loss of train set: 0.641345202922821 at epoch: 5 and batch_num: 189\n",
      "Loss of train set: 0.3771030604839325 at epoch: 5 and batch_num: 190\n",
      "Loss of train set: 0.3155883550643921 at epoch: 5 and batch_num: 191\n",
      "Loss of train set: 0.41621989011764526 at epoch: 5 and batch_num: 192\n",
      "Loss of train set: 0.41200530529022217 at epoch: 5 and batch_num: 193\n",
      "Loss of train set: 0.4817582964897156 at epoch: 5 and batch_num: 194\n",
      "Loss of train set: 0.3414008617401123 at epoch: 5 and batch_num: 195\n",
      "Loss of train set: 0.3118085265159607 at epoch: 5 and batch_num: 196\n",
      "Loss of train set: 0.3109239935874939 at epoch: 5 and batch_num: 197\n",
      "Loss of train set: 0.5089118480682373 at epoch: 5 and batch_num: 198\n",
      "Loss of train set: 0.42355233430862427 at epoch: 5 and batch_num: 199\n",
      "Loss of train set: 0.3959727883338928 at epoch: 5 and batch_num: 200\n",
      "Loss of train set: 0.2794128656387329 at epoch: 5 and batch_num: 201\n",
      "Loss of train set: 0.30093884468078613 at epoch: 5 and batch_num: 202\n",
      "Loss of train set: 0.3856256902217865 at epoch: 5 and batch_num: 203\n",
      "Loss of train set: 0.43623965978622437 at epoch: 5 and batch_num: 204\n",
      "Loss of train set: 0.23425008356571198 at epoch: 5 and batch_num: 205\n",
      "Loss of train set: 0.32103121280670166 at epoch: 5 and batch_num: 206\n",
      "Loss of train set: 0.22085799276828766 at epoch: 5 and batch_num: 207\n",
      "Loss of train set: 0.3480468988418579 at epoch: 5 and batch_num: 208\n",
      "Loss of train set: 0.40054744482040405 at epoch: 5 and batch_num: 209\n",
      "Loss of train set: 0.37657052278518677 at epoch: 5 and batch_num: 210\n",
      "Loss of train set: 0.30525439977645874 at epoch: 5 and batch_num: 211\n",
      "Loss of train set: 0.38632744550704956 at epoch: 5 and batch_num: 212\n",
      "Loss of train set: 0.3919524848461151 at epoch: 5 and batch_num: 213\n",
      "Loss of train set: 0.4675188660621643 at epoch: 5 and batch_num: 214\n",
      "Loss of train set: 0.32912537455558777 at epoch: 5 and batch_num: 215\n",
      "Loss of train set: 0.3898162245750427 at epoch: 5 and batch_num: 216\n",
      "Loss of train set: 0.38125917315483093 at epoch: 5 and batch_num: 217\n",
      "Loss of train set: 0.529548168182373 at epoch: 5 and batch_num: 218\n",
      "Loss of train set: 0.47393494844436646 at epoch: 5 and batch_num: 219\n",
      "Loss of train set: 0.36056917905807495 at epoch: 5 and batch_num: 220\n",
      "Loss of train set: 0.28908807039260864 at epoch: 5 and batch_num: 221\n",
      "Loss of train set: 0.31958818435668945 at epoch: 5 and batch_num: 222\n",
      "Loss of train set: 0.4878788888454437 at epoch: 5 and batch_num: 223\n",
      "Loss of train set: 0.29838621616363525 at epoch: 5 and batch_num: 224\n",
      "Loss of train set: 0.6938825845718384 at epoch: 5 and batch_num: 225\n",
      "Loss of train set: 0.34056878089904785 at epoch: 5 and batch_num: 226\n",
      "Loss of train set: 0.48908936977386475 at epoch: 5 and batch_num: 227\n",
      "Loss of train set: 0.2496011108160019 at epoch: 5 and batch_num: 228\n",
      "Loss of train set: 0.4955478608608246 at epoch: 5 and batch_num: 229\n",
      "Loss of train set: 0.3011358976364136 at epoch: 5 and batch_num: 230\n",
      "Loss of train set: 0.3697078227996826 at epoch: 5 and batch_num: 231\n",
      "Loss of train set: 0.47666144371032715 at epoch: 5 and batch_num: 232\n",
      "Loss of train set: 0.35454481840133667 at epoch: 5 and batch_num: 233\n",
      "Loss of train set: 0.32169026136398315 at epoch: 5 and batch_num: 234\n",
      "Loss of train set: 0.4362712502479553 at epoch: 5 and batch_num: 235\n",
      "Loss of train set: 0.33456674218177795 at epoch: 5 and batch_num: 236\n",
      "Loss of train set: 0.25234368443489075 at epoch: 5 and batch_num: 237\n",
      "Loss of train set: 0.20229944586753845 at epoch: 5 and batch_num: 238\n",
      "Loss of train set: 0.4155416190624237 at epoch: 5 and batch_num: 239\n",
      "Loss of train set: 0.4068000316619873 at epoch: 5 and batch_num: 240\n",
      "Loss of train set: 0.3655719757080078 at epoch: 5 and batch_num: 241\n",
      "Loss of train set: 0.37965720891952515 at epoch: 5 and batch_num: 242\n",
      "Loss of train set: 0.5014936327934265 at epoch: 5 and batch_num: 243\n",
      "Loss of train set: 0.24733448028564453 at epoch: 5 and batch_num: 244\n",
      "Loss of train set: 0.37817609310150146 at epoch: 5 and batch_num: 245\n",
      "Loss of train set: 0.5615320205688477 at epoch: 5 and batch_num: 246\n",
      "Loss of train set: 0.2148403376340866 at epoch: 5 and batch_num: 247\n",
      "Loss of train set: 0.19982734322547913 at epoch: 5 and batch_num: 248\n",
      "Loss of train set: 0.4044261574745178 at epoch: 5 and batch_num: 249\n",
      "Loss of train set: 0.26346391439437866 at epoch: 5 and batch_num: 250\n",
      "Loss of train set: 0.3701910972595215 at epoch: 5 and batch_num: 251\n",
      "Loss of train set: 0.2767392694950104 at epoch: 5 and batch_num: 252\n",
      "Loss of train set: 0.24009959399700165 at epoch: 5 and batch_num: 253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.33960461616516113 at epoch: 5 and batch_num: 254\n",
      "Loss of train set: 0.38209205865859985 at epoch: 5 and batch_num: 255\n",
      "Loss of train set: 0.463973730802536 at epoch: 5 and batch_num: 256\n",
      "Loss of train set: 0.3089860677719116 at epoch: 5 and batch_num: 257\n",
      "Loss of train set: 0.2329672873020172 at epoch: 5 and batch_num: 258\n",
      "Loss of train set: 0.26139169931411743 at epoch: 5 and batch_num: 259\n",
      "Loss of train set: 0.48581305146217346 at epoch: 5 and batch_num: 260\n",
      "Loss of train set: 0.21265289187431335 at epoch: 5 and batch_num: 261\n",
      "Loss of train set: 0.442771315574646 at epoch: 5 and batch_num: 262\n",
      "Loss of train set: 0.3430158495903015 at epoch: 5 and batch_num: 263\n",
      "Loss of train set: 0.24879685044288635 at epoch: 5 and batch_num: 264\n",
      "Loss of train set: 0.4767700433731079 at epoch: 5 and batch_num: 265\n",
      "Loss of train set: 0.41981184482574463 at epoch: 5 and batch_num: 266\n",
      "Loss of train set: 0.4708656668663025 at epoch: 5 and batch_num: 267\n",
      "Loss of train set: 0.33615249395370483 at epoch: 5 and batch_num: 268\n",
      "Loss of train set: 0.2966412901878357 at epoch: 5 and batch_num: 269\n",
      "Loss of train set: 0.36012890934944153 at epoch: 5 and batch_num: 270\n",
      "Loss of train set: 0.48318231105804443 at epoch: 5 and batch_num: 271\n",
      "Loss of train set: 0.20732063055038452 at epoch: 5 and batch_num: 272\n",
      "Loss of train set: 0.32568979263305664 at epoch: 5 and batch_num: 273\n",
      "Loss of train set: 0.38946837186813354 at epoch: 5 and batch_num: 274\n",
      "Loss of train set: 0.46900755167007446 at epoch: 5 and batch_num: 275\n",
      "Loss of train set: 0.2745254635810852 at epoch: 5 and batch_num: 276\n",
      "Loss of train set: 0.26686587929725647 at epoch: 5 and batch_num: 277\n",
      "Loss of train set: 0.48282039165496826 at epoch: 5 and batch_num: 278\n",
      "Loss of train set: 0.21714627742767334 at epoch: 5 and batch_num: 279\n",
      "Loss of train set: 0.3151019513607025 at epoch: 5 and batch_num: 280\n",
      "Loss of train set: 0.4295094609260559 at epoch: 5 and batch_num: 281\n",
      "Loss of train set: 0.4033568799495697 at epoch: 5 and batch_num: 282\n",
      "Loss of train set: 0.3357681632041931 at epoch: 5 and batch_num: 283\n",
      "Loss of train set: 0.24344313144683838 at epoch: 5 and batch_num: 284\n",
      "Loss of train set: 0.44155555963516235 at epoch: 5 and batch_num: 285\n",
      "Loss of train set: 0.2334829419851303 at epoch: 5 and batch_num: 286\n",
      "Loss of train set: 0.3361915647983551 at epoch: 5 and batch_num: 287\n",
      "Loss of train set: 0.47847020626068115 at epoch: 5 and batch_num: 288\n",
      "Loss of train set: 0.29889509081840515 at epoch: 5 and batch_num: 289\n",
      "Loss of train set: 0.37322914600372314 at epoch: 5 and batch_num: 290\n",
      "Loss of train set: 0.27929532527923584 at epoch: 5 and batch_num: 291\n",
      "Loss of train set: 0.538424015045166 at epoch: 5 and batch_num: 292\n",
      "Loss of train set: 0.5191129446029663 at epoch: 5 and batch_num: 293\n",
      "Loss of train set: 0.2796580195426941 at epoch: 5 and batch_num: 294\n",
      "Loss of train set: 0.30051547288894653 at epoch: 5 and batch_num: 295\n",
      "Loss of train set: 0.3367263674736023 at epoch: 5 and batch_num: 296\n",
      "Loss of train set: 0.44611287117004395 at epoch: 5 and batch_num: 297\n",
      "Loss of train set: 0.34313321113586426 at epoch: 5 and batch_num: 298\n",
      "Loss of train set: 0.2867072820663452 at epoch: 5 and batch_num: 299\n",
      "Loss of train set: 0.5128936171531677 at epoch: 5 and batch_num: 300\n",
      "Loss of train set: 0.3639473617076874 at epoch: 5 and batch_num: 301\n",
      "Loss of train set: 0.32368868589401245 at epoch: 5 and batch_num: 302\n",
      "Loss of train set: 0.35438746213912964 at epoch: 5 and batch_num: 303\n",
      "Loss of train set: 0.6100351810455322 at epoch: 5 and batch_num: 304\n",
      "Loss of train set: 0.2823548913002014 at epoch: 5 and batch_num: 305\n",
      "Loss of train set: 0.2957732081413269 at epoch: 5 and batch_num: 306\n",
      "Loss of train set: 0.3566284477710724 at epoch: 5 and batch_num: 307\n",
      "Loss of train set: 0.3965018391609192 at epoch: 5 and batch_num: 308\n",
      "Loss of train set: 0.5312627553939819 at epoch: 5 and batch_num: 309\n",
      "Loss of train set: 0.24410973489284515 at epoch: 5 and batch_num: 310\n",
      "Loss of train set: 0.3753504753112793 at epoch: 5 and batch_num: 311\n",
      "Loss of train set: 0.4981967806816101 at epoch: 5 and batch_num: 312\n",
      "Loss of train set: 0.39708417654037476 at epoch: 5 and batch_num: 313\n",
      "Loss of train set: 0.18595939874649048 at epoch: 5 and batch_num: 314\n",
      "Loss of train set: 0.27744531631469727 at epoch: 5 and batch_num: 315\n",
      "Loss of train set: 0.4363382160663605 at epoch: 5 and batch_num: 316\n",
      "Loss of train set: 0.4391578137874603 at epoch: 5 and batch_num: 317\n",
      "Loss of train set: 0.5173550248146057 at epoch: 5 and batch_num: 318\n",
      "Loss of train set: 0.23618891835212708 at epoch: 5 and batch_num: 319\n",
      "Loss of train set: 0.3142264485359192 at epoch: 5 and batch_num: 320\n",
      "Loss of train set: 0.4481387734413147 at epoch: 5 and batch_num: 321\n",
      "Loss of train set: 0.22396442294120789 at epoch: 5 and batch_num: 322\n",
      "Loss of train set: 0.45024168491363525 at epoch: 5 and batch_num: 323\n",
      "Loss of train set: 0.264788955450058 at epoch: 5 and batch_num: 324\n",
      "Loss of train set: 0.48239225149154663 at epoch: 5 and batch_num: 325\n",
      "Loss of train set: 0.32620036602020264 at epoch: 5 and batch_num: 326\n",
      "Loss of train set: 0.4461019039154053 at epoch: 5 and batch_num: 327\n",
      "Loss of train set: 0.2637331485748291 at epoch: 5 and batch_num: 328\n",
      "Loss of train set: 0.25953570008277893 at epoch: 5 and batch_num: 329\n",
      "Loss of train set: 0.34718379378318787 at epoch: 5 and batch_num: 330\n",
      "Loss of train set: 0.33628758788108826 at epoch: 5 and batch_num: 331\n",
      "Loss of train set: 0.38332539796829224 at epoch: 5 and batch_num: 332\n",
      "Loss of train set: 0.37180110812187195 at epoch: 5 and batch_num: 333\n",
      "Loss of train set: 0.31039923429489136 at epoch: 5 and batch_num: 334\n",
      "Loss of train set: 0.3936612010002136 at epoch: 5 and batch_num: 335\n",
      "Loss of train set: 0.3534666895866394 at epoch: 5 and batch_num: 336\n",
      "Loss of train set: 0.33782368898391724 at epoch: 5 and batch_num: 337\n",
      "Loss of train set: 0.2849918007850647 at epoch: 5 and batch_num: 338\n",
      "Loss of train set: 0.3690350651741028 at epoch: 5 and batch_num: 339\n",
      "Loss of train set: 0.2673117518424988 at epoch: 5 and batch_num: 340\n",
      "Loss of train set: 0.3481309413909912 at epoch: 5 and batch_num: 341\n",
      "Loss of train set: 0.39264392852783203 at epoch: 5 and batch_num: 342\n",
      "Loss of train set: 0.3967071771621704 at epoch: 5 and batch_num: 343\n",
      "Loss of train set: 0.4991540312767029 at epoch: 5 and batch_num: 344\n",
      "Loss of train set: 0.3054302930831909 at epoch: 5 and batch_num: 345\n",
      "Loss of train set: 0.29404738545417786 at epoch: 5 and batch_num: 346\n",
      "Loss of train set: 0.5995506644248962 at epoch: 5 and batch_num: 347\n",
      "Loss of train set: 0.49282240867614746 at epoch: 5 and batch_num: 348\n",
      "Loss of train set: 0.3836134672164917 at epoch: 5 and batch_num: 349\n",
      "Loss of train set: 0.2775013744831085 at epoch: 5 and batch_num: 350\n",
      "Loss of train set: 0.3438873887062073 at epoch: 5 and batch_num: 351\n",
      "Loss of train set: 0.25802624225616455 at epoch: 5 and batch_num: 352\n",
      "Loss of train set: 0.27899980545043945 at epoch: 5 and batch_num: 353\n",
      "Loss of train set: 0.24086207151412964 at epoch: 5 and batch_num: 354\n",
      "Loss of train set: 0.33878403902053833 at epoch: 5 and batch_num: 355\n",
      "Loss of train set: 0.3369475305080414 at epoch: 5 and batch_num: 356\n",
      "Loss of train set: 0.2588481307029724 at epoch: 5 and batch_num: 357\n",
      "Loss of train set: 0.4507812261581421 at epoch: 5 and batch_num: 358\n",
      "Loss of train set: 0.30178308486938477 at epoch: 5 and batch_num: 359\n",
      "Loss of train set: 0.2849370837211609 at epoch: 5 and batch_num: 360\n",
      "Loss of train set: 0.25401848554611206 at epoch: 5 and batch_num: 361\n",
      "Loss of train set: 0.20227526128292084 at epoch: 5 and batch_num: 362\n",
      "Loss of train set: 0.3770509958267212 at epoch: 5 and batch_num: 363\n",
      "Loss of train set: 0.3846864700317383 at epoch: 5 and batch_num: 364\n",
      "Loss of train set: 0.32118165493011475 at epoch: 5 and batch_num: 365\n",
      "Loss of train set: 0.312229722738266 at epoch: 5 and batch_num: 366\n",
      "Loss of train set: 0.28608930110931396 at epoch: 5 and batch_num: 367\n",
      "Loss of train set: 0.36616015434265137 at epoch: 5 and batch_num: 368\n",
      "Loss of train set: 0.26328912377357483 at epoch: 5 and batch_num: 369\n",
      "Loss of train set: 0.2318306416273117 at epoch: 5 and batch_num: 370\n",
      "Loss of train set: 0.43422016501426697 at epoch: 5 and batch_num: 371\n",
      "Loss of train set: 0.40666478872299194 at epoch: 5 and batch_num: 372\n",
      "Loss of train set: 0.3907272517681122 at epoch: 5 and batch_num: 373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3814237117767334 at epoch: 5 and batch_num: 374\n",
      "Loss of train set: 0.2881056070327759 at epoch: 5 and batch_num: 375\n",
      "Loss of train set: 0.2054227590560913 at epoch: 5 and batch_num: 376\n",
      "Loss of train set: 0.37669312953948975 at epoch: 5 and batch_num: 377\n",
      "Loss of train set: 0.2789471745491028 at epoch: 5 and batch_num: 378\n",
      "Loss of train set: 0.2930487394332886 at epoch: 5 and batch_num: 379\n",
      "Loss of train set: 0.28952839970588684 at epoch: 5 and batch_num: 380\n",
      "Loss of train set: 0.3918379545211792 at epoch: 5 and batch_num: 381\n",
      "Loss of train set: 0.2759205102920532 at epoch: 5 and batch_num: 382\n",
      "Loss of train set: 0.23006394505500793 at epoch: 5 and batch_num: 383\n",
      "Loss of train set: 0.4870188236236572 at epoch: 5 and batch_num: 384\n",
      "Loss of train set: 0.43105924129486084 at epoch: 5 and batch_num: 385\n",
      "Loss of train set: 0.2679617404937744 at epoch: 5 and batch_num: 386\n",
      "Loss of train set: 0.39683687686920166 at epoch: 5 and batch_num: 387\n",
      "Loss of train set: 0.2959538698196411 at epoch: 5 and batch_num: 388\n",
      "Loss of train set: 0.3872445225715637 at epoch: 5 and batch_num: 389\n",
      "Loss of train set: 0.2585330605506897 at epoch: 5 and batch_num: 390\n",
      "Loss of train set: 0.36632293462753296 at epoch: 5 and batch_num: 391\n",
      "Loss of train set: 0.3949546217918396 at epoch: 5 and batch_num: 392\n",
      "Loss of train set: 0.6656314134597778 at epoch: 5 and batch_num: 393\n",
      "Loss of train set: 0.32971471548080444 at epoch: 5 and batch_num: 394\n",
      "Loss of train set: 0.3005898594856262 at epoch: 5 and batch_num: 395\n",
      "Loss of train set: 0.2018147110939026 at epoch: 5 and batch_num: 396\n",
      "Loss of train set: 0.25973251461982727 at epoch: 5 and batch_num: 397\n",
      "Loss of train set: 0.2669096291065216 at epoch: 5 and batch_num: 398\n",
      "Loss of train set: 0.32726749777793884 at epoch: 5 and batch_num: 399\n",
      "Loss of train set: 0.3510276675224304 at epoch: 5 and batch_num: 400\n",
      "Loss of train set: 0.3943389356136322 at epoch: 5 and batch_num: 401\n",
      "Loss of train set: 0.4539424777030945 at epoch: 5 and batch_num: 402\n",
      "Loss of train set: 0.4040381908416748 at epoch: 5 and batch_num: 403\n",
      "Loss of train set: 0.5542139410972595 at epoch: 5 and batch_num: 404\n",
      "Loss of train set: 0.29071444272994995 at epoch: 5 and batch_num: 405\n",
      "Loss of train set: 0.21555212140083313 at epoch: 5 and batch_num: 406\n",
      "Loss of train set: 0.2733241617679596 at epoch: 5 and batch_num: 407\n",
      "Loss of train set: 0.26996150612831116 at epoch: 5 and batch_num: 408\n",
      "Loss of train set: 0.2475089579820633 at epoch: 5 and batch_num: 409\n",
      "Loss of train set: 0.2496481090784073 at epoch: 5 and batch_num: 410\n",
      "Loss of train set: 0.2796856760978699 at epoch: 5 and batch_num: 411\n",
      "Loss of train set: 0.2626415491104126 at epoch: 5 and batch_num: 412\n",
      "Loss of train set: 0.3293425738811493 at epoch: 5 and batch_num: 413\n",
      "Loss of train set: 0.21834182739257812 at epoch: 5 and batch_num: 414\n",
      "Loss of train set: 0.42341887950897217 at epoch: 5 and batch_num: 415\n",
      "Loss of train set: 0.45582306385040283 at epoch: 5 and batch_num: 416\n",
      "Loss of train set: 0.30585700273513794 at epoch: 5 and batch_num: 417\n",
      "Loss of train set: 0.41990983486175537 at epoch: 5 and batch_num: 418\n",
      "Loss of train set: 0.42656150460243225 at epoch: 5 and batch_num: 419\n",
      "Loss of train set: 0.24814282357692719 at epoch: 5 and batch_num: 420\n",
      "Loss of train set: 0.4823315441608429 at epoch: 5 and batch_num: 421\n",
      "Loss of train set: 0.19211187958717346 at epoch: 5 and batch_num: 422\n",
      "Loss of train set: 0.37766867876052856 at epoch: 5 and batch_num: 423\n",
      "Loss of train set: 0.43692708015441895 at epoch: 5 and batch_num: 424\n",
      "Loss of train set: 0.37290388345718384 at epoch: 5 and batch_num: 425\n",
      "Loss of train set: 0.43080568313598633 at epoch: 5 and batch_num: 426\n",
      "Loss of train set: 0.3207734525203705 at epoch: 5 and batch_num: 427\n",
      "Loss of train set: 0.365172803401947 at epoch: 5 and batch_num: 428\n",
      "Loss of train set: 0.32261162996292114 at epoch: 5 and batch_num: 429\n",
      "Loss of train set: 0.3857923746109009 at epoch: 5 and batch_num: 430\n",
      "Loss of train set: 0.3355610966682434 at epoch: 5 and batch_num: 431\n",
      "Loss of train set: 0.414141982793808 at epoch: 5 and batch_num: 432\n",
      "Loss of train set: 0.3676528036594391 at epoch: 5 and batch_num: 433\n",
      "Loss of train set: 0.49230459332466125 at epoch: 5 and batch_num: 434\n",
      "Loss of train set: 0.39332103729248047 at epoch: 5 and batch_num: 435\n",
      "Loss of train set: 0.22743374109268188 at epoch: 5 and batch_num: 436\n",
      "Loss of train set: 0.4067462980747223 at epoch: 5 and batch_num: 437\n",
      "Loss of train set: 0.23606891930103302 at epoch: 5 and batch_num: 438\n",
      "Loss of train set: 0.5671748518943787 at epoch: 5 and batch_num: 439\n",
      "Loss of train set: 0.22241981327533722 at epoch: 5 and batch_num: 440\n",
      "Loss of train set: 0.35822880268096924 at epoch: 5 and batch_num: 441\n",
      "Loss of train set: 0.3649917244911194 at epoch: 5 and batch_num: 442\n",
      "Loss of train set: 0.35910674929618835 at epoch: 5 and batch_num: 443\n",
      "Loss of train set: 0.5454380512237549 at epoch: 5 and batch_num: 444\n",
      "Loss of train set: 0.4849201440811157 at epoch: 5 and batch_num: 445\n",
      "Loss of train set: 0.2399282455444336 at epoch: 5 and batch_num: 446\n",
      "Loss of train set: 0.273944228887558 at epoch: 5 and batch_num: 447\n",
      "Loss of train set: 0.1936545968055725 at epoch: 5 and batch_num: 448\n",
      "Loss of train set: 0.40215176343917847 at epoch: 5 and batch_num: 449\n",
      "Loss of train set: 0.29591161012649536 at epoch: 5 and batch_num: 450\n",
      "Loss of train set: 0.4230841398239136 at epoch: 5 and batch_num: 451\n",
      "Loss of train set: 0.17276978492736816 at epoch: 5 and batch_num: 452\n",
      "Loss of train set: 0.3870152235031128 at epoch: 5 and batch_num: 453\n",
      "Loss of train set: 0.2813620865345001 at epoch: 5 and batch_num: 454\n",
      "Loss of train set: 0.3829554617404938 at epoch: 5 and batch_num: 455\n",
      "Loss of train set: 0.3023107647895813 at epoch: 5 and batch_num: 456\n",
      "Loss of train set: 0.5151238441467285 at epoch: 5 and batch_num: 457\n",
      "Loss of train set: 0.3333427608013153 at epoch: 5 and batch_num: 458\n",
      "Loss of train set: 0.30477210879325867 at epoch: 5 and batch_num: 459\n",
      "Loss of train set: 0.42151975631713867 at epoch: 5 and batch_num: 460\n",
      "Loss of train set: 0.3179336488246918 at epoch: 5 and batch_num: 461\n",
      "Loss of train set: 0.44250577688217163 at epoch: 5 and batch_num: 462\n",
      "Loss of train set: 0.21741701662540436 at epoch: 5 and batch_num: 463\n",
      "Loss of train set: 0.2932139039039612 at epoch: 5 and batch_num: 464\n",
      "Loss of train set: 0.479240745306015 at epoch: 5 and batch_num: 465\n",
      "Loss of train set: 0.31517815589904785 at epoch: 5 and batch_num: 466\n",
      "Loss of train set: 0.5759122371673584 at epoch: 5 and batch_num: 467\n",
      "Loss of train set: 0.24334828555583954 at epoch: 5 and batch_num: 468\n",
      "Loss of train set: 0.38408225774765015 at epoch: 5 and batch_num: 469\n",
      "Loss of train set: 0.30360034108161926 at epoch: 5 and batch_num: 470\n",
      "Loss of train set: 0.4823310971260071 at epoch: 5 and batch_num: 471\n",
      "Loss of train set: 0.19858485460281372 at epoch: 5 and batch_num: 472\n",
      "Loss of train set: 0.32714369893074036 at epoch: 5 and batch_num: 473\n",
      "Loss of train set: 0.37929975986480713 at epoch: 5 and batch_num: 474\n",
      "Loss of train set: 0.5647532939910889 at epoch: 5 and batch_num: 475\n",
      "Loss of train set: 0.46529722213745117 at epoch: 5 and batch_num: 476\n",
      "Loss of train set: 0.4258815050125122 at epoch: 5 and batch_num: 477\n",
      "Loss of train set: 0.34689146280288696 at epoch: 5 and batch_num: 478\n",
      "Loss of train set: 0.47647005319595337 at epoch: 5 and batch_num: 479\n",
      "Loss of train set: 0.2817266583442688 at epoch: 5 and batch_num: 480\n",
      "Loss of train set: 0.38289982080459595 at epoch: 5 and batch_num: 481\n",
      "Loss of train set: 0.24702748656272888 at epoch: 5 and batch_num: 482\n",
      "Loss of train set: 0.18205159902572632 at epoch: 5 and batch_num: 483\n",
      "Loss of train set: 0.27008694410324097 at epoch: 5 and batch_num: 484\n",
      "Loss of train set: 0.431329607963562 at epoch: 5 and batch_num: 485\n",
      "Loss of train set: 0.3759281635284424 at epoch: 5 and batch_num: 486\n",
      "Loss of train set: 0.29540228843688965 at epoch: 5 and batch_num: 487\n",
      "Loss of train set: 0.2922450304031372 at epoch: 5 and batch_num: 488\n",
      "Loss of train set: 0.3119310140609741 at epoch: 5 and batch_num: 489\n",
      "Loss of train set: 0.2997991740703583 at epoch: 5 and batch_num: 490\n",
      "Loss of train set: 0.48820266127586365 at epoch: 5 and batch_num: 491\n",
      "Loss of train set: 0.3076006770133972 at epoch: 5 and batch_num: 492\n",
      "Loss of train set: 0.23043884336948395 at epoch: 5 and batch_num: 493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.24946241080760956 at epoch: 5 and batch_num: 494\n",
      "Loss of train set: 0.3003358840942383 at epoch: 5 and batch_num: 495\n",
      "Loss of train set: 0.381176233291626 at epoch: 5 and batch_num: 496\n",
      "Loss of train set: 0.5145374536514282 at epoch: 5 and batch_num: 497\n",
      "Loss of train set: 0.15520069003105164 at epoch: 5 and batch_num: 498\n",
      "Loss of train set: 0.3530886769294739 at epoch: 5 and batch_num: 499\n",
      "Loss of train set: 0.2794909179210663 at epoch: 5 and batch_num: 500\n",
      "Loss of train set: 0.23377639055252075 at epoch: 5 and batch_num: 501\n",
      "Loss of train set: 0.34339529275894165 at epoch: 5 and batch_num: 502\n",
      "Loss of train set: 0.25338584184646606 at epoch: 5 and batch_num: 503\n",
      "Loss of train set: 0.33038148283958435 at epoch: 5 and batch_num: 504\n",
      "Loss of train set: 0.4234309494495392 at epoch: 5 and batch_num: 505\n",
      "Loss of train set: 0.28101688623428345 at epoch: 5 and batch_num: 506\n",
      "Loss of train set: 0.37487632036209106 at epoch: 5 and batch_num: 507\n",
      "Loss of train set: 0.23527196049690247 at epoch: 5 and batch_num: 508\n",
      "Loss of train set: 0.35393738746643066 at epoch: 5 and batch_num: 509\n",
      "Loss of train set: 0.426281213760376 at epoch: 5 and batch_num: 510\n",
      "Loss of train set: 0.47117429971694946 at epoch: 5 and batch_num: 511\n",
      "Loss of train set: 0.3828679621219635 at epoch: 5 and batch_num: 512\n",
      "Loss of train set: 0.3289949893951416 at epoch: 5 and batch_num: 513\n",
      "Loss of train set: 0.413300096988678 at epoch: 5 and batch_num: 514\n",
      "Loss of train set: 0.40484440326690674 at epoch: 5 and batch_num: 515\n",
      "Loss of train set: 0.30772456526756287 at epoch: 5 and batch_num: 516\n",
      "Loss of train set: 0.4693668484687805 at epoch: 5 and batch_num: 517\n",
      "Loss of train set: 0.4449087083339691 at epoch: 5 and batch_num: 518\n",
      "Loss of train set: 0.25060537457466125 at epoch: 5 and batch_num: 519\n",
      "Loss of train set: 0.4144831895828247 at epoch: 5 and batch_num: 520\n",
      "Loss of train set: 0.3603155016899109 at epoch: 5 and batch_num: 521\n",
      "Loss of train set: 0.43761125206947327 at epoch: 5 and batch_num: 522\n",
      "Loss of train set: 0.3909921646118164 at epoch: 5 and batch_num: 523\n",
      "Loss of train set: 0.26916736364364624 at epoch: 5 and batch_num: 524\n",
      "Loss of train set: 0.18933725357055664 at epoch: 5 and batch_num: 525\n",
      "Loss of train set: 0.28746724128723145 at epoch: 5 and batch_num: 526\n",
      "Loss of train set: 0.36402037739753723 at epoch: 5 and batch_num: 527\n",
      "Loss of train set: 0.36542850732803345 at epoch: 5 and batch_num: 528\n",
      "Loss of train set: 0.34704485535621643 at epoch: 5 and batch_num: 529\n",
      "Loss of train set: 0.3391592502593994 at epoch: 5 and batch_num: 530\n",
      "Loss of train set: 0.2673030197620392 at epoch: 5 and batch_num: 531\n",
      "Loss of train set: 0.29478925466537476 at epoch: 5 and batch_num: 532\n",
      "Loss of train set: 0.23531115055084229 at epoch: 5 and batch_num: 533\n",
      "Loss of train set: 0.3327234387397766 at epoch: 5 and batch_num: 534\n",
      "Loss of train set: 0.24386127293109894 at epoch: 5 and batch_num: 535\n",
      "Loss of train set: 0.3525410294532776 at epoch: 5 and batch_num: 536\n",
      "Loss of train set: 0.2981894016265869 at epoch: 5 and batch_num: 537\n",
      "Loss of train set: 0.2644796669483185 at epoch: 5 and batch_num: 538\n",
      "Loss of train set: 0.37191295623779297 at epoch: 5 and batch_num: 539\n",
      "Loss of train set: 0.23497238755226135 at epoch: 5 and batch_num: 540\n",
      "Loss of train set: 0.4876733124256134 at epoch: 5 and batch_num: 541\n",
      "Loss of train set: 0.3387313485145569 at epoch: 5 and batch_num: 542\n",
      "Loss of train set: 0.346920907497406 at epoch: 5 and batch_num: 543\n",
      "Loss of train set: 0.31984788179397583 at epoch: 5 and batch_num: 544\n",
      "Loss of train set: 0.3692283630371094 at epoch: 5 and batch_num: 545\n",
      "Loss of train set: 0.21387583017349243 at epoch: 5 and batch_num: 546\n",
      "Loss of train set: 0.504045844078064 at epoch: 5 and batch_num: 547\n",
      "Loss of train set: 0.3948386311531067 at epoch: 5 and batch_num: 548\n",
      "Loss of train set: 0.2954484820365906 at epoch: 5 and batch_num: 549\n",
      "Loss of train set: 0.46492838859558105 at epoch: 5 and batch_num: 550\n",
      "Loss of train set: 0.5493140816688538 at epoch: 5 and batch_num: 551\n",
      "Loss of train set: 0.3203094005584717 at epoch: 5 and batch_num: 552\n",
      "Loss of train set: 0.49554306268692017 at epoch: 5 and batch_num: 553\n",
      "Loss of train set: 0.22241359949111938 at epoch: 5 and batch_num: 554\n",
      "Loss of train set: 0.3630463480949402 at epoch: 5 and batch_num: 555\n",
      "Loss of train set: 0.45166346430778503 at epoch: 5 and batch_num: 556\n",
      "Loss of train set: 0.49594181776046753 at epoch: 5 and batch_num: 557\n",
      "Loss of train set: 0.17171449959278107 at epoch: 5 and batch_num: 558\n",
      "Loss of train set: 0.3207363188266754 at epoch: 5 and batch_num: 559\n",
      "Loss of train set: 0.4827367067337036 at epoch: 5 and batch_num: 560\n",
      "Loss of train set: 0.5078925490379333 at epoch: 5 and batch_num: 561\n",
      "Loss of train set: 0.47847163677215576 at epoch: 5 and batch_num: 562\n",
      "Loss of train set: 0.41743171215057373 at epoch: 5 and batch_num: 563\n",
      "Loss of train set: 0.2745163142681122 at epoch: 5 and batch_num: 564\n",
      "Loss of train set: 0.6700596809387207 at epoch: 5 and batch_num: 565\n",
      "Loss of train set: 0.30859482288360596 at epoch: 5 and batch_num: 566\n",
      "Loss of train set: 0.27607083320617676 at epoch: 5 and batch_num: 567\n",
      "Loss of train set: 0.2787698805332184 at epoch: 5 and batch_num: 568\n",
      "Loss of train set: 0.32889553904533386 at epoch: 5 and batch_num: 569\n",
      "Loss of train set: 0.3750377595424652 at epoch: 5 and batch_num: 570\n",
      "Loss of train set: 0.2273588627576828 at epoch: 5 and batch_num: 571\n",
      "Loss of train set: 0.3139193058013916 at epoch: 5 and batch_num: 572\n",
      "Loss of train set: 0.3763394355773926 at epoch: 5 and batch_num: 573\n",
      "Loss of train set: 0.2824919819831848 at epoch: 5 and batch_num: 574\n",
      "Loss of train set: 0.2899418771266937 at epoch: 5 and batch_num: 575\n",
      "Loss of train set: 0.26010265946388245 at epoch: 5 and batch_num: 576\n",
      "Loss of train set: 0.32251131534576416 at epoch: 5 and batch_num: 577\n",
      "Loss of train set: 0.4728901982307434 at epoch: 5 and batch_num: 578\n",
      "Loss of train set: 0.5115649700164795 at epoch: 5 and batch_num: 579\n",
      "Loss of train set: 0.39100635051727295 at epoch: 5 and batch_num: 580\n",
      "Loss of train set: 0.3793764114379883 at epoch: 5 and batch_num: 581\n",
      "Loss of train set: 0.23305527865886688 at epoch: 5 and batch_num: 582\n",
      "Loss of train set: 0.2775627076625824 at epoch: 5 and batch_num: 583\n",
      "Loss of train set: 0.2702278196811676 at epoch: 5 and batch_num: 584\n",
      "Loss of train set: 0.29539185762405396 at epoch: 5 and batch_num: 585\n",
      "Loss of train set: 0.29857343435287476 at epoch: 5 and batch_num: 586\n",
      "Loss of train set: 0.2888476252555847 at epoch: 5 and batch_num: 587\n",
      "Loss of train set: 0.18745040893554688 at epoch: 5 and batch_num: 588\n",
      "Loss of train set: 0.22770074009895325 at epoch: 5 and batch_num: 589\n",
      "Loss of train set: 0.5140635967254639 at epoch: 5 and batch_num: 590\n",
      "Loss of train set: 0.3958614468574524 at epoch: 5 and batch_num: 591\n",
      "Loss of train set: 0.30055609345436096 at epoch: 5 and batch_num: 592\n",
      "Loss of train set: 0.2995847761631012 at epoch: 5 and batch_num: 593\n",
      "Loss of train set: 0.2556910514831543 at epoch: 5 and batch_num: 594\n",
      "Loss of train set: 0.3215101957321167 at epoch: 5 and batch_num: 595\n",
      "Loss of train set: 0.3783602714538574 at epoch: 5 and batch_num: 596\n",
      "Loss of train set: 0.46470966935157776 at epoch: 5 and batch_num: 597\n",
      "Loss of train set: 0.4498077630996704 at epoch: 5 and batch_num: 598\n",
      "Loss of train set: 0.5243459939956665 at epoch: 5 and batch_num: 599\n",
      "Loss of train set: 0.31744202971458435 at epoch: 5 and batch_num: 600\n",
      "Loss of train set: 0.22290648519992828 at epoch: 5 and batch_num: 601\n",
      "Loss of train set: 0.4010433554649353 at epoch: 5 and batch_num: 602\n",
      "Loss of train set: 0.34010785818099976 at epoch: 5 and batch_num: 603\n",
      "Loss of train set: 0.31579330563545227 at epoch: 5 and batch_num: 604\n",
      "Loss of train set: 0.3471389412879944 at epoch: 5 and batch_num: 605\n",
      "Loss of train set: 0.443441778421402 at epoch: 5 and batch_num: 606\n",
      "Loss of train set: 0.2686350345611572 at epoch: 5 and batch_num: 607\n",
      "Loss of train set: 0.29874327778816223 at epoch: 5 and batch_num: 608\n",
      "Loss of train set: 0.25728338956832886 at epoch: 5 and batch_num: 609\n",
      "Loss of train set: 0.3451511859893799 at epoch: 5 and batch_num: 610\n",
      "Loss of train set: 0.39942705631256104 at epoch: 5 and batch_num: 611\n",
      "Loss of train set: 0.20322877168655396 at epoch: 5 and batch_num: 612\n",
      "Loss of train set: 0.2876863479614258 at epoch: 5 and batch_num: 613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.34189796447753906 at epoch: 5 and batch_num: 614\n",
      "Loss of train set: 0.3001869320869446 at epoch: 5 and batch_num: 615\n",
      "Loss of train set: 0.4983699321746826 at epoch: 5 and batch_num: 616\n",
      "Loss of train set: 0.27841052412986755 at epoch: 5 and batch_num: 617\n",
      "Loss of train set: 0.28486719727516174 at epoch: 5 and batch_num: 618\n",
      "Loss of train set: 0.22768676280975342 at epoch: 5 and batch_num: 619\n",
      "Loss of train set: 0.31730329990386963 at epoch: 5 and batch_num: 620\n",
      "Loss of train set: 0.3195241689682007 at epoch: 5 and batch_num: 621\n",
      "Loss of train set: 0.2859334647655487 at epoch: 5 and batch_num: 622\n",
      "Loss of train set: 0.31466707587242126 at epoch: 5 and batch_num: 623\n",
      "Loss of train set: 0.20400750637054443 at epoch: 5 and batch_num: 624\n",
      "Loss of train set: 0.3192867338657379 at epoch: 5 and batch_num: 625\n",
      "Loss of train set: 0.3049853444099426 at epoch: 5 and batch_num: 626\n",
      "Loss of train set: 0.5279408097267151 at epoch: 5 and batch_num: 627\n",
      "Loss of train set: 0.3699023723602295 at epoch: 5 and batch_num: 628\n",
      "Loss of train set: 0.6882186532020569 at epoch: 5 and batch_num: 629\n",
      "Loss of train set: 0.4262542724609375 at epoch: 5 and batch_num: 630\n",
      "Loss of train set: 0.21312779188156128 at epoch: 5 and batch_num: 631\n",
      "Loss of train set: 0.15003442764282227 at epoch: 5 and batch_num: 632\n",
      "Loss of train set: 0.3045646548271179 at epoch: 5 and batch_num: 633\n",
      "Loss of train set: 0.276178240776062 at epoch: 5 and batch_num: 634\n",
      "Loss of train set: 0.4138748347759247 at epoch: 5 and batch_num: 635\n",
      "Loss of train set: 0.18738828599452972 at epoch: 5 and batch_num: 636\n",
      "Loss of train set: 0.34041696786880493 at epoch: 5 and batch_num: 637\n",
      "Loss of train set: 0.4371771216392517 at epoch: 5 and batch_num: 638\n",
      "Loss of train set: 0.22076483070850372 at epoch: 5 and batch_num: 639\n",
      "Loss of train set: 0.2870936393737793 at epoch: 5 and batch_num: 640\n",
      "Loss of train set: 0.3579880893230438 at epoch: 5 and batch_num: 641\n",
      "Loss of train set: 0.2152964174747467 at epoch: 5 and batch_num: 642\n",
      "Loss of train set: 0.3742637634277344 at epoch: 5 and batch_num: 643\n",
      "Loss of train set: 0.326665461063385 at epoch: 5 and batch_num: 644\n",
      "Loss of train set: 0.5282160043716431 at epoch: 5 and batch_num: 645\n",
      "Loss of train set: 0.38964909315109253 at epoch: 5 and batch_num: 646\n",
      "Loss of train set: 0.5689639449119568 at epoch: 5 and batch_num: 647\n",
      "Loss of train set: 0.3419843912124634 at epoch: 5 and batch_num: 648\n",
      "Loss of train set: 0.3014634847640991 at epoch: 5 and batch_num: 649\n",
      "Loss of train set: 0.3425690531730652 at epoch: 5 and batch_num: 650\n",
      "Loss of train set: 0.26912325620651245 at epoch: 5 and batch_num: 651\n",
      "Loss of train set: 0.45469021797180176 at epoch: 5 and batch_num: 652\n",
      "Loss of train set: 0.2449086755514145 at epoch: 5 and batch_num: 653\n",
      "Loss of train set: 0.3322601914405823 at epoch: 5 and batch_num: 654\n",
      "Loss of train set: 0.26485762000083923 at epoch: 5 and batch_num: 655\n",
      "Loss of train set: 0.32596835494041443 at epoch: 5 and batch_num: 656\n",
      "Loss of train set: 0.2916771173477173 at epoch: 5 and batch_num: 657\n",
      "Loss of train set: 0.2753010392189026 at epoch: 5 and batch_num: 658\n",
      "Loss of train set: 0.2860388159751892 at epoch: 5 and batch_num: 659\n",
      "Loss of train set: 0.43980297446250916 at epoch: 5 and batch_num: 660\n",
      "Loss of train set: 0.47153496742248535 at epoch: 5 and batch_num: 661\n",
      "Loss of train set: 0.336531400680542 at epoch: 5 and batch_num: 662\n",
      "Loss of train set: 0.2064441740512848 at epoch: 5 and batch_num: 663\n",
      "Loss of train set: 0.4319719672203064 at epoch: 5 and batch_num: 664\n",
      "Loss of train set: 0.47028255462646484 at epoch: 5 and batch_num: 665\n",
      "Loss of train set: 0.43317484855651855 at epoch: 5 and batch_num: 666\n",
      "Loss of train set: 0.266262412071228 at epoch: 5 and batch_num: 667\n",
      "Loss of train set: 0.33745232224464417 at epoch: 5 and batch_num: 668\n",
      "Loss of train set: 0.36193811893463135 at epoch: 5 and batch_num: 669\n",
      "Loss of train set: 0.34526288509368896 at epoch: 5 and batch_num: 670\n",
      "Loss of train set: 0.39198145270347595 at epoch: 5 and batch_num: 671\n",
      "Loss of train set: 0.2708330452442169 at epoch: 5 and batch_num: 672\n",
      "Loss of train set: 0.2944675385951996 at epoch: 5 and batch_num: 673\n",
      "Loss of train set: 0.3280808925628662 at epoch: 5 and batch_num: 674\n",
      "Loss of train set: 0.24833549559116364 at epoch: 5 and batch_num: 675\n",
      "Loss of train set: 0.37127208709716797 at epoch: 5 and batch_num: 676\n",
      "Loss of train set: 0.27579283714294434 at epoch: 5 and batch_num: 677\n",
      "Loss of train set: 0.3557679057121277 at epoch: 5 and batch_num: 678\n",
      "Loss of train set: 0.11829991638660431 at epoch: 5 and batch_num: 679\n",
      "Loss of train set: 0.47812819480895996 at epoch: 5 and batch_num: 680\n",
      "Loss of train set: 0.2342360019683838 at epoch: 5 and batch_num: 681\n",
      "Loss of train set: 0.29804813861846924 at epoch: 5 and batch_num: 682\n",
      "Loss of train set: 0.44906026124954224 at epoch: 5 and batch_num: 683\n",
      "Loss of train set: 0.5393779277801514 at epoch: 5 and batch_num: 684\n",
      "Loss of train set: 0.3944494128227234 at epoch: 5 and batch_num: 685\n",
      "Loss of train set: 0.2673052251338959 at epoch: 5 and batch_num: 686\n",
      "Loss of train set: 0.3470686078071594 at epoch: 5 and batch_num: 687\n",
      "Loss of train set: 0.37032395601272583 at epoch: 5 and batch_num: 688\n",
      "Loss of train set: 0.32498037815093994 at epoch: 5 and batch_num: 689\n",
      "Loss of train set: 0.2508656978607178 at epoch: 5 and batch_num: 690\n",
      "Loss of train set: 0.45491406321525574 at epoch: 5 and batch_num: 691\n",
      "Loss of train set: 0.4510626494884491 at epoch: 5 and batch_num: 692\n",
      "Loss of train set: 0.3618488907814026 at epoch: 5 and batch_num: 693\n",
      "Loss of train set: 0.293536901473999 at epoch: 5 and batch_num: 694\n",
      "Loss of train set: 0.21504023671150208 at epoch: 5 and batch_num: 695\n",
      "Loss of train set: 0.5022659301757812 at epoch: 5 and batch_num: 696\n",
      "Loss of train set: 0.27462252974510193 at epoch: 5 and batch_num: 697\n",
      "Loss of train set: 0.18027371168136597 at epoch: 5 and batch_num: 698\n",
      "Loss of train set: 0.2199932038784027 at epoch: 5 and batch_num: 699\n",
      "Loss of train set: 0.13896197080612183 at epoch: 5 and batch_num: 700\n",
      "Loss of train set: 0.6295638680458069 at epoch: 5 and batch_num: 701\n",
      "Loss of train set: 0.26987579464912415 at epoch: 5 and batch_num: 702\n",
      "Loss of train set: 0.21490870416164398 at epoch: 5 and batch_num: 703\n",
      "Loss of train set: 0.2818675637245178 at epoch: 5 and batch_num: 704\n",
      "Loss of train set: 0.45175012946128845 at epoch: 5 and batch_num: 705\n",
      "Loss of train set: 0.5045836567878723 at epoch: 5 and batch_num: 706\n",
      "Loss of train set: 0.4316514730453491 at epoch: 5 and batch_num: 707\n",
      "Loss of train set: 0.26878833770751953 at epoch: 5 and batch_num: 708\n",
      "Loss of train set: 0.252329558134079 at epoch: 5 and batch_num: 709\n",
      "Loss of train set: 0.4432417154312134 at epoch: 5 and batch_num: 710\n",
      "Loss of train set: 0.28743162751197815 at epoch: 5 and batch_num: 711\n",
      "Loss of train set: 0.45117026567459106 at epoch: 5 and batch_num: 712\n",
      "Loss of train set: 0.37361249327659607 at epoch: 5 and batch_num: 713\n",
      "Loss of train set: 0.2573801279067993 at epoch: 5 and batch_num: 714\n",
      "Loss of train set: 0.539398729801178 at epoch: 5 and batch_num: 715\n",
      "Loss of train set: 0.3057596683502197 at epoch: 5 and batch_num: 716\n",
      "Loss of train set: 0.3432334065437317 at epoch: 5 and batch_num: 717\n",
      "Loss of train set: 0.3399142026901245 at epoch: 5 and batch_num: 718\n",
      "Loss of train set: 0.29014337062835693 at epoch: 5 and batch_num: 719\n",
      "Loss of train set: 0.336404025554657 at epoch: 5 and batch_num: 720\n",
      "Loss of train set: 0.34372302889823914 at epoch: 5 and batch_num: 721\n",
      "Loss of train set: 0.3443569839000702 at epoch: 5 and batch_num: 722\n",
      "Loss of train set: 0.22729140520095825 at epoch: 5 and batch_num: 723\n",
      "Loss of train set: 0.3606749475002289 at epoch: 5 and batch_num: 724\n",
      "Loss of train set: 0.271835595369339 at epoch: 5 and batch_num: 725\n",
      "Loss of train set: 0.3996724784374237 at epoch: 5 and batch_num: 726\n",
      "Loss of train set: 0.20776768028736115 at epoch: 5 and batch_num: 727\n",
      "Loss of train set: 0.31760451197624207 at epoch: 5 and batch_num: 728\n",
      "Loss of train set: 0.3957805335521698 at epoch: 5 and batch_num: 729\n",
      "Loss of train set: 0.39052820205688477 at epoch: 5 and batch_num: 730\n",
      "Loss of train set: 0.41301286220550537 at epoch: 5 and batch_num: 731\n",
      "Loss of train set: 0.22986239194869995 at epoch: 5 and batch_num: 732\n",
      "Loss of train set: 0.30723389983177185 at epoch: 5 and batch_num: 733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3098326623439789 at epoch: 5 and batch_num: 734\n",
      "Loss of train set: 0.4056606888771057 at epoch: 5 and batch_num: 735\n",
      "Loss of train set: 0.3035254180431366 at epoch: 5 and batch_num: 736\n",
      "Loss of train set: 0.31676071882247925 at epoch: 5 and batch_num: 737\n",
      "Loss of train set: 0.29489728808403015 at epoch: 5 and batch_num: 738\n",
      "Loss of train set: 0.24328690767288208 at epoch: 5 and batch_num: 739\n",
      "Loss of train set: 0.41775503754615784 at epoch: 5 and batch_num: 740\n",
      "Loss of train set: 0.5756316184997559 at epoch: 5 and batch_num: 741\n",
      "Loss of train set: 0.2571820616722107 at epoch: 5 and batch_num: 742\n",
      "Loss of train set: 0.4363974332809448 at epoch: 5 and batch_num: 743\n",
      "Loss of train set: 0.3378002941608429 at epoch: 5 and batch_num: 744\n",
      "Loss of train set: 0.2817978858947754 at epoch: 5 and batch_num: 745\n",
      "Loss of train set: 0.4113946557044983 at epoch: 5 and batch_num: 746\n",
      "Loss of train set: 0.38717490434646606 at epoch: 5 and batch_num: 747\n",
      "Loss of train set: 0.39557772874832153 at epoch: 5 and batch_num: 748\n",
      "Loss of train set: 0.3451545834541321 at epoch: 5 and batch_num: 749\n",
      "Loss of train set: 0.3107486963272095 at epoch: 5 and batch_num: 750\n",
      "Loss of train set: 0.2592424750328064 at epoch: 5 and batch_num: 751\n",
      "Loss of train set: 0.2951558232307434 at epoch: 5 and batch_num: 752\n",
      "Loss of train set: 0.21997478604316711 at epoch: 5 and batch_num: 753\n",
      "Loss of train set: 0.2728593945503235 at epoch: 5 and batch_num: 754\n",
      "Loss of train set: 0.39053401350975037 at epoch: 5 and batch_num: 755\n",
      "Loss of train set: 0.43187403678894043 at epoch: 5 and batch_num: 756\n",
      "Loss of train set: 0.4415869116783142 at epoch: 5 and batch_num: 757\n",
      "Loss of train set: 0.44550377130508423 at epoch: 5 and batch_num: 758\n",
      "Loss of train set: 0.5536885261535645 at epoch: 5 and batch_num: 759\n",
      "Loss of train set: 0.2550848722457886 at epoch: 5 and batch_num: 760\n",
      "Loss of train set: 0.3896525502204895 at epoch: 5 and batch_num: 761\n",
      "Loss of train set: 0.3849145770072937 at epoch: 5 and batch_num: 762\n",
      "Loss of train set: 0.368511825799942 at epoch: 5 and batch_num: 763\n",
      "Loss of train set: 0.30147844552993774 at epoch: 5 and batch_num: 764\n",
      "Loss of train set: 0.25405874848365784 at epoch: 5 and batch_num: 765\n",
      "Loss of train set: 0.4634611904621124 at epoch: 5 and batch_num: 766\n",
      "Loss of train set: 0.2387535274028778 at epoch: 5 and batch_num: 767\n",
      "Loss of train set: 0.29126042127609253 at epoch: 5 and batch_num: 768\n",
      "Loss of train set: 0.3170623779296875 at epoch: 5 and batch_num: 769\n",
      "Loss of train set: 0.400787353515625 at epoch: 5 and batch_num: 770\n",
      "Loss of train set: 0.353903591632843 at epoch: 5 and batch_num: 771\n",
      "Loss of train set: 0.4715772867202759 at epoch: 5 and batch_num: 772\n",
      "Loss of train set: 0.3442903161048889 at epoch: 5 and batch_num: 773\n",
      "Loss of train set: 0.36244720220565796 at epoch: 5 and batch_num: 774\n",
      "Loss of train set: 0.33574163913726807 at epoch: 5 and batch_num: 775\n",
      "Loss of train set: 0.22140789031982422 at epoch: 5 and batch_num: 776\n",
      "Loss of train set: 0.43288761377334595 at epoch: 5 and batch_num: 777\n",
      "Loss of train set: 0.3002351224422455 at epoch: 5 and batch_num: 778\n",
      "Loss of train set: 0.2959669530391693 at epoch: 5 and batch_num: 779\n",
      "Loss of train set: 0.5912794470787048 at epoch: 5 and batch_num: 780\n",
      "Loss of train set: 0.28020691871643066 at epoch: 5 and batch_num: 781\n",
      "Loss of train set: 0.49494993686676025 at epoch: 5 and batch_num: 782\n",
      "Loss of train set: 0.2577666640281677 at epoch: 5 and batch_num: 783\n",
      "Loss of train set: 0.19156122207641602 at epoch: 5 and batch_num: 784\n",
      "Loss of train set: 0.4539235830307007 at epoch: 5 and batch_num: 785\n",
      "Loss of train set: 0.2849111557006836 at epoch: 5 and batch_num: 786\n",
      "Loss of train set: 0.31567108631134033 at epoch: 5 and batch_num: 787\n",
      "Loss of train set: 0.47187376022338867 at epoch: 5 and batch_num: 788\n",
      "Loss of train set: 0.39652687311172485 at epoch: 5 and batch_num: 789\n",
      "Loss of train set: 0.21452395617961884 at epoch: 5 and batch_num: 790\n",
      "Loss of train set: 0.22219718992710114 at epoch: 5 and batch_num: 791\n",
      "Loss of train set: 0.3602890372276306 at epoch: 5 and batch_num: 792\n",
      "Loss of train set: 0.22937415540218353 at epoch: 5 and batch_num: 793\n",
      "Loss of train set: 0.3221512734889984 at epoch: 5 and batch_num: 794\n",
      "Loss of train set: 0.3145787715911865 at epoch: 5 and batch_num: 795\n",
      "Loss of train set: 0.4223560690879822 at epoch: 5 and batch_num: 796\n",
      "Loss of train set: 0.2823110520839691 at epoch: 5 and batch_num: 797\n",
      "Loss of train set: 0.2393430918455124 at epoch: 5 and batch_num: 798\n",
      "Loss of train set: 0.21318365633487701 at epoch: 5 and batch_num: 799\n",
      "Loss of train set: 0.36329132318496704 at epoch: 5 and batch_num: 800\n",
      "Loss of train set: 0.2855967879295349 at epoch: 5 and batch_num: 801\n",
      "Loss of train set: 0.26507192850112915 at epoch: 5 and batch_num: 802\n",
      "Loss of train set: 0.45413586497306824 at epoch: 5 and batch_num: 803\n",
      "Loss of train set: 0.2554689943790436 at epoch: 5 and batch_num: 804\n",
      "Loss of train set: 0.20838654041290283 at epoch: 5 and batch_num: 805\n",
      "Loss of train set: 0.14543524384498596 at epoch: 5 and batch_num: 806\n",
      "Loss of train set: 0.3218163847923279 at epoch: 5 and batch_num: 807\n",
      "Loss of train set: 0.26857683062553406 at epoch: 5 and batch_num: 808\n",
      "Loss of train set: 0.24675153195858002 at epoch: 5 and batch_num: 809\n",
      "Loss of train set: 0.2606213092803955 at epoch: 5 and batch_num: 810\n",
      "Loss of train set: 0.4332337975502014 at epoch: 5 and batch_num: 811\n",
      "Loss of train set: 0.2908736765384674 at epoch: 5 and batch_num: 812\n",
      "Loss of train set: 0.541450023651123 at epoch: 5 and batch_num: 813\n",
      "Loss of train set: 0.42969852685928345 at epoch: 5 and batch_num: 814\n",
      "Loss of train set: 0.14364942908287048 at epoch: 5 and batch_num: 815\n",
      "Loss of train set: 0.35710927844047546 at epoch: 5 and batch_num: 816\n",
      "Loss of train set: 0.3755315840244293 at epoch: 5 and batch_num: 817\n",
      "Loss of train set: 0.267670214176178 at epoch: 5 and batch_num: 818\n",
      "Loss of train set: 0.3157803416252136 at epoch: 5 and batch_num: 819\n",
      "Loss of train set: 0.4461480379104614 at epoch: 5 and batch_num: 820\n",
      "Loss of train set: 0.27876847982406616 at epoch: 5 and batch_num: 821\n",
      "Loss of train set: 0.38883835077285767 at epoch: 5 and batch_num: 822\n",
      "Loss of train set: 0.330718457698822 at epoch: 5 and batch_num: 823\n",
      "Loss of train set: 0.3103512227535248 at epoch: 5 and batch_num: 824\n",
      "Loss of train set: 0.34642326831817627 at epoch: 5 and batch_num: 825\n",
      "Loss of train set: 0.30063119530677795 at epoch: 5 and batch_num: 826\n",
      "Loss of train set: 0.28928467631340027 at epoch: 5 and batch_num: 827\n",
      "Loss of train set: 0.22314366698265076 at epoch: 5 and batch_num: 828\n",
      "Loss of train set: 0.4482467472553253 at epoch: 5 and batch_num: 829\n",
      "Loss of train set: 0.22419001162052155 at epoch: 5 and batch_num: 830\n",
      "Loss of train set: 0.30658209323883057 at epoch: 5 and batch_num: 831\n",
      "Loss of train set: 0.14363501965999603 at epoch: 5 and batch_num: 832\n",
      "Loss of train set: 0.29175522923469543 at epoch: 5 and batch_num: 833\n",
      "Loss of train set: 0.3432552218437195 at epoch: 5 and batch_num: 834\n",
      "Loss of train set: 0.4667741060256958 at epoch: 5 and batch_num: 835\n",
      "Loss of train set: 0.4122092127799988 at epoch: 5 and batch_num: 836\n",
      "Loss of train set: 0.5164317488670349 at epoch: 5 and batch_num: 837\n",
      "Loss of train set: 0.4768163859844208 at epoch: 5 and batch_num: 838\n",
      "Loss of train set: 0.25611478090286255 at epoch: 5 and batch_num: 839\n",
      "Loss of train set: 0.28727149963378906 at epoch: 5 and batch_num: 840\n",
      "Loss of train set: 0.38519367575645447 at epoch: 5 and batch_num: 841\n",
      "Loss of train set: 0.36382555961608887 at epoch: 5 and batch_num: 842\n",
      "Loss of train set: 0.4070996046066284 at epoch: 5 and batch_num: 843\n",
      "Loss of train set: 0.35441142320632935 at epoch: 5 and batch_num: 844\n",
      "Loss of train set: 0.26483628153800964 at epoch: 5 and batch_num: 845\n",
      "Loss of train set: 0.5305407643318176 at epoch: 5 and batch_num: 846\n",
      "Loss of train set: 0.24279770255088806 at epoch: 5 and batch_num: 847\n",
      "Loss of train set: 0.4027352035045624 at epoch: 5 and batch_num: 848\n",
      "Loss of train set: 0.31373393535614014 at epoch: 5 and batch_num: 849\n",
      "Loss of train set: 0.771242618560791 at epoch: 5 and batch_num: 850\n",
      "Loss of train set: 0.2322230339050293 at epoch: 5 and batch_num: 851\n",
      "Loss of train set: 0.49563440680503845 at epoch: 5 and batch_num: 852\n",
      "Loss of train set: 0.31694433093070984 at epoch: 5 and batch_num: 853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.24620190262794495 at epoch: 5 and batch_num: 854\n",
      "Loss of train set: 0.20538195967674255 at epoch: 5 and batch_num: 855\n",
      "Loss of train set: 0.4952852725982666 at epoch: 5 and batch_num: 856\n",
      "Loss of train set: 0.41250091791152954 at epoch: 5 and batch_num: 857\n",
      "Loss of train set: 0.5334160923957825 at epoch: 5 and batch_num: 858\n",
      "Loss of train set: 0.28137171268463135 at epoch: 5 and batch_num: 859\n",
      "Loss of train set: 0.279624879360199 at epoch: 5 and batch_num: 860\n",
      "Loss of train set: 0.18782079219818115 at epoch: 5 and batch_num: 861\n",
      "Loss of train set: 0.19555798172950745 at epoch: 5 and batch_num: 862\n",
      "Loss of train set: 0.3136337399482727 at epoch: 5 and batch_num: 863\n",
      "Loss of train set: 0.2842535674571991 at epoch: 5 and batch_num: 864\n",
      "Loss of train set: 0.38472023606300354 at epoch: 5 and batch_num: 865\n",
      "Loss of train set: 0.4093184173107147 at epoch: 5 and batch_num: 866\n",
      "Loss of train set: 0.3363884687423706 at epoch: 5 and batch_num: 867\n",
      "Loss of train set: 0.1998249590396881 at epoch: 5 and batch_num: 868\n",
      "Loss of train set: 0.48976802825927734 at epoch: 5 and batch_num: 869\n",
      "Loss of train set: 0.5243024230003357 at epoch: 5 and batch_num: 870\n",
      "Loss of train set: 0.4293191432952881 at epoch: 5 and batch_num: 871\n",
      "Loss of train set: 0.4065631031990051 at epoch: 5 and batch_num: 872\n",
      "Loss of train set: 0.3006437420845032 at epoch: 5 and batch_num: 873\n",
      "Loss of train set: 0.34323373436927795 at epoch: 5 and batch_num: 874\n",
      "Loss of train set: 0.380575567483902 at epoch: 5 and batch_num: 875\n",
      "Loss of train set: 0.3525393605232239 at epoch: 5 and batch_num: 876\n",
      "Loss of train set: 0.32848119735717773 at epoch: 5 and batch_num: 877\n",
      "Loss of train set: 0.3441264033317566 at epoch: 5 and batch_num: 878\n",
      "Loss of train set: 0.2935151755809784 at epoch: 5 and batch_num: 879\n",
      "Loss of train set: 0.2240852415561676 at epoch: 5 and batch_num: 880\n",
      "Loss of train set: 0.2318105697631836 at epoch: 5 and batch_num: 881\n",
      "Loss of train set: 0.2917291224002838 at epoch: 5 and batch_num: 882\n",
      "Loss of train set: 0.2518925666809082 at epoch: 5 and batch_num: 883\n",
      "Loss of train set: 0.3129391074180603 at epoch: 5 and batch_num: 884\n",
      "Loss of train set: 0.4518759250640869 at epoch: 5 and batch_num: 885\n",
      "Loss of train set: 0.1967879831790924 at epoch: 5 and batch_num: 886\n",
      "Loss of train set: 0.33189916610717773 at epoch: 5 and batch_num: 887\n",
      "Loss of train set: 0.29597121477127075 at epoch: 5 and batch_num: 888\n",
      "Loss of train set: 0.3321375548839569 at epoch: 5 and batch_num: 889\n",
      "Loss of train set: 0.26034533977508545 at epoch: 5 and batch_num: 890\n",
      "Loss of train set: 0.3002322018146515 at epoch: 5 and batch_num: 891\n",
      "Loss of train set: 0.4666000306606293 at epoch: 5 and batch_num: 892\n",
      "Loss of train set: 0.4014892280101776 at epoch: 5 and batch_num: 893\n",
      "Loss of train set: 0.17961814999580383 at epoch: 5 and batch_num: 894\n",
      "Loss of train set: 0.23930856585502625 at epoch: 5 and batch_num: 895\n",
      "Loss of train set: 0.3290960192680359 at epoch: 5 and batch_num: 896\n",
      "Loss of train set: 0.4121477007865906 at epoch: 5 and batch_num: 897\n",
      "Loss of train set: 0.24243462085723877 at epoch: 5 and batch_num: 898\n",
      "Loss of train set: 0.30564454197883606 at epoch: 5 and batch_num: 899\n",
      "Loss of train set: 0.35511523485183716 at epoch: 5 and batch_num: 900\n",
      "Loss of train set: 0.33065733313560486 at epoch: 5 and batch_num: 901\n",
      "Loss of train set: 0.4247673749923706 at epoch: 5 and batch_num: 902\n",
      "Loss of train set: 0.4123225510120392 at epoch: 5 and batch_num: 903\n",
      "Loss of train set: 0.2775024175643921 at epoch: 5 and batch_num: 904\n",
      "Loss of train set: 0.23274566233158112 at epoch: 5 and batch_num: 905\n",
      "Loss of train set: 0.3544391095638275 at epoch: 5 and batch_num: 906\n",
      "Loss of train set: 0.21191546320915222 at epoch: 5 and batch_num: 907\n",
      "Loss of train set: 0.16487911343574524 at epoch: 5 and batch_num: 908\n",
      "Loss of train set: 0.26252326369285583 at epoch: 5 and batch_num: 909\n",
      "Loss of train set: 0.33208900690078735 at epoch: 5 and batch_num: 910\n",
      "Loss of train set: 0.34345996379852295 at epoch: 5 and batch_num: 911\n",
      "Loss of train set: 0.31208351254463196 at epoch: 5 and batch_num: 912\n",
      "Loss of train set: 0.8249644041061401 at epoch: 5 and batch_num: 913\n",
      "Loss of train set: 0.2460411936044693 at epoch: 5 and batch_num: 914\n",
      "Loss of train set: 0.2448495626449585 at epoch: 5 and batch_num: 915\n",
      "Loss of train set: 0.2156638652086258 at epoch: 5 and batch_num: 916\n",
      "Loss of train set: 0.21402955055236816 at epoch: 5 and batch_num: 917\n",
      "Loss of train set: 0.23109886050224304 at epoch: 5 and batch_num: 918\n",
      "Loss of train set: 0.237838476896286 at epoch: 5 and batch_num: 919\n",
      "Loss of train set: 0.2911701500415802 at epoch: 5 and batch_num: 920\n",
      "Loss of train set: 0.28135064244270325 at epoch: 5 and batch_num: 921\n",
      "Loss of train set: 0.32211044430732727 at epoch: 5 and batch_num: 922\n",
      "Loss of train set: 0.29952648282051086 at epoch: 5 and batch_num: 923\n",
      "Loss of train set: 0.33090418577194214 at epoch: 5 and batch_num: 924\n",
      "Loss of train set: 0.3070968687534332 at epoch: 5 and batch_num: 925\n",
      "Loss of train set: 0.37707823514938354 at epoch: 5 and batch_num: 926\n",
      "Loss of train set: 0.2740773558616638 at epoch: 5 and batch_num: 927\n",
      "Loss of train set: 0.32205837965011597 at epoch: 5 and batch_num: 928\n",
      "Loss of train set: 0.31502678990364075 at epoch: 5 and batch_num: 929\n",
      "Loss of train set: 0.3212101459503174 at epoch: 5 and batch_num: 930\n",
      "Loss of train set: 0.37333524227142334 at epoch: 5 and batch_num: 931\n",
      "Loss of train set: 0.35104697942733765 at epoch: 5 and batch_num: 932\n",
      "Loss of train set: 0.3975338339805603 at epoch: 5 and batch_num: 933\n",
      "Loss of train set: 0.4652405381202698 at epoch: 5 and batch_num: 934\n",
      "Loss of train set: 0.32874351739883423 at epoch: 5 and batch_num: 935\n",
      "Loss of train set: 0.32002055644989014 at epoch: 5 and batch_num: 936\n",
      "Loss of train set: 0.39008110761642456 at epoch: 5 and batch_num: 937\n",
      "Accuracy of train set: 0.8768\n",
      "Loss of test set: 0.4746871888637543 at epoch: 5 and batch_num: 0\n",
      "Loss of test set: 0.4583965837955475 at epoch: 5 and batch_num: 1\n",
      "Loss of test set: 0.43512704968452454 at epoch: 5 and batch_num: 2\n",
      "Loss of test set: 0.3032519221305847 at epoch: 5 and batch_num: 3\n",
      "Loss of test set: 0.4847363829612732 at epoch: 5 and batch_num: 4\n",
      "Loss of test set: 0.49745404720306396 at epoch: 5 and batch_num: 5\n",
      "Loss of test set: 0.4223524034023285 at epoch: 5 and batch_num: 6\n",
      "Loss of test set: 0.3715437054634094 at epoch: 5 and batch_num: 7\n",
      "Loss of test set: 0.3885621428489685 at epoch: 5 and batch_num: 8\n",
      "Loss of test set: 0.3396177887916565 at epoch: 5 and batch_num: 9\n",
      "Loss of test set: 0.46199890971183777 at epoch: 5 and batch_num: 10\n",
      "Loss of test set: 0.5341823101043701 at epoch: 5 and batch_num: 11\n",
      "Loss of test set: 0.3885308504104614 at epoch: 5 and batch_num: 12\n",
      "Loss of test set: 0.5870319604873657 at epoch: 5 and batch_num: 13\n",
      "Loss of test set: 0.444909930229187 at epoch: 5 and batch_num: 14\n",
      "Loss of test set: 0.4245217442512512 at epoch: 5 and batch_num: 15\n",
      "Loss of test set: 0.2827228009700775 at epoch: 5 and batch_num: 16\n",
      "Loss of test set: 0.4260067343711853 at epoch: 5 and batch_num: 17\n",
      "Loss of test set: 0.4753476083278656 at epoch: 5 and batch_num: 18\n",
      "Loss of test set: 0.2889595627784729 at epoch: 5 and batch_num: 19\n",
      "Loss of test set: 0.46173596382141113 at epoch: 5 and batch_num: 20\n",
      "Loss of test set: 0.5075072050094604 at epoch: 5 and batch_num: 21\n",
      "Loss of test set: 0.6697136163711548 at epoch: 5 and batch_num: 22\n",
      "Loss of test set: 0.6796379685401917 at epoch: 5 and batch_num: 23\n",
      "Loss of test set: 0.4024861752986908 at epoch: 5 and batch_num: 24\n",
      "Loss of test set: 0.49852508306503296 at epoch: 5 and batch_num: 25\n",
      "Loss of test set: 0.3544793128967285 at epoch: 5 and batch_num: 26\n",
      "Loss of test set: 0.34723275899887085 at epoch: 5 and batch_num: 27\n",
      "Loss of test set: 0.42741718888282776 at epoch: 5 and batch_num: 28\n",
      "Loss of test set: 0.33545440435409546 at epoch: 5 and batch_num: 29\n",
      "Loss of test set: 0.26705819368362427 at epoch: 5 and batch_num: 30\n",
      "Loss of test set: 0.47957727313041687 at epoch: 5 and batch_num: 31\n",
      "Loss of test set: 0.44243571162223816 at epoch: 5 and batch_num: 32\n",
      "Loss of test set: 0.6021815538406372 at epoch: 5 and batch_num: 33\n",
      "Loss of test set: 0.43942612409591675 at epoch: 5 and batch_num: 34\n",
      "Loss of test set: 0.5234829187393188 at epoch: 5 and batch_num: 35\n",
      "Loss of test set: 0.6688668727874756 at epoch: 5 and batch_num: 36\n",
      "Loss of test set: 0.2570652365684509 at epoch: 5 and batch_num: 37\n",
      "Loss of test set: 0.19249466061592102 at epoch: 5 and batch_num: 38\n",
      "Loss of test set: 0.3088793158531189 at epoch: 5 and batch_num: 39\n",
      "Loss of test set: 0.24365395307540894 at epoch: 5 and batch_num: 40\n",
      "Loss of test set: 0.5475552678108215 at epoch: 5 and batch_num: 41\n",
      "Loss of test set: 0.3791348934173584 at epoch: 5 and batch_num: 42\n",
      "Loss of test set: 0.41482096910476685 at epoch: 5 and batch_num: 43\n",
      "Loss of test set: 0.30716168880462646 at epoch: 5 and batch_num: 44\n",
      "Loss of test set: 0.30361664295196533 at epoch: 5 and batch_num: 45\n",
      "Loss of test set: 0.5068204402923584 at epoch: 5 and batch_num: 46\n",
      "Loss of test set: 0.5232784748077393 at epoch: 5 and batch_num: 47\n",
      "Loss of test set: 0.26793602108955383 at epoch: 5 and batch_num: 48\n",
      "Loss of test set: 0.5567276477813721 at epoch: 5 and batch_num: 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.5576777458190918 at epoch: 5 and batch_num: 50\n",
      "Loss of test set: 0.30565863847732544 at epoch: 5 and batch_num: 51\n",
      "Loss of test set: 0.489368736743927 at epoch: 5 and batch_num: 52\n",
      "Loss of test set: 0.26674115657806396 at epoch: 5 and batch_num: 53\n",
      "Loss of test set: 0.5531885623931885 at epoch: 5 and batch_num: 54\n",
      "Loss of test set: 0.4372890889644623 at epoch: 5 and batch_num: 55\n",
      "Loss of test set: 0.4099896550178528 at epoch: 5 and batch_num: 56\n",
      "Loss of test set: 0.4509613811969757 at epoch: 5 and batch_num: 57\n",
      "Loss of test set: 0.3765270709991455 at epoch: 5 and batch_num: 58\n",
      "Loss of test set: 0.3888360261917114 at epoch: 5 and batch_num: 59\n",
      "Loss of test set: 0.5240737795829773 at epoch: 5 and batch_num: 60\n",
      "Loss of test set: 0.3657699227333069 at epoch: 5 and batch_num: 61\n",
      "Loss of test set: 0.5069942474365234 at epoch: 5 and batch_num: 62\n",
      "Loss of test set: 0.5623688697814941 at epoch: 5 and batch_num: 63\n",
      "Loss of test set: 0.4245522618293762 at epoch: 5 and batch_num: 64\n",
      "Loss of test set: 0.5549533367156982 at epoch: 5 and batch_num: 65\n",
      "Loss of test set: 0.43468260765075684 at epoch: 5 and batch_num: 66\n",
      "Loss of test set: 0.31081104278564453 at epoch: 5 and batch_num: 67\n",
      "Loss of test set: 0.5102244019508362 at epoch: 5 and batch_num: 68\n",
      "Loss of test set: 0.44341957569122314 at epoch: 5 and batch_num: 69\n",
      "Loss of test set: 0.365511417388916 at epoch: 5 and batch_num: 70\n",
      "Loss of test set: 0.4446522295475006 at epoch: 5 and batch_num: 71\n",
      "Loss of test set: 0.5036753416061401 at epoch: 5 and batch_num: 72\n",
      "Loss of test set: 0.4678020477294922 at epoch: 5 and batch_num: 73\n",
      "Loss of test set: 0.42888951301574707 at epoch: 5 and batch_num: 74\n",
      "Loss of test set: 0.38489121198654175 at epoch: 5 and batch_num: 75\n",
      "Loss of test set: 0.6699331998825073 at epoch: 5 and batch_num: 76\n",
      "Loss of test set: 0.4633851647377014 at epoch: 5 and batch_num: 77\n",
      "Loss of test set: 0.30532392859458923 at epoch: 5 and batch_num: 78\n",
      "Loss of test set: 0.32561665773391724 at epoch: 5 and batch_num: 79\n",
      "Loss of test set: 0.5869227647781372 at epoch: 5 and batch_num: 80\n",
      "Loss of test set: 0.6987202167510986 at epoch: 5 and batch_num: 81\n",
      "Loss of test set: 0.4391011595726013 at epoch: 5 and batch_num: 82\n",
      "Loss of test set: 0.5099695920944214 at epoch: 5 and batch_num: 83\n",
      "Loss of test set: 0.43520697951316833 at epoch: 5 and batch_num: 84\n",
      "Loss of test set: 0.45046812295913696 at epoch: 5 and batch_num: 85\n",
      "Loss of test set: 0.437019944190979 at epoch: 5 and batch_num: 86\n",
      "Loss of test set: 0.39887842535972595 at epoch: 5 and batch_num: 87\n",
      "Loss of test set: 0.6251521706581116 at epoch: 5 and batch_num: 88\n",
      "Loss of test set: 0.44295939803123474 at epoch: 5 and batch_num: 89\n",
      "Loss of test set: 0.47760793566703796 at epoch: 5 and batch_num: 90\n",
      "Loss of test set: 0.3633042573928833 at epoch: 5 and batch_num: 91\n",
      "Loss of test set: 0.5002743005752563 at epoch: 5 and batch_num: 92\n",
      "Loss of test set: 0.2669748067855835 at epoch: 5 and batch_num: 93\n",
      "Loss of test set: 0.45840293169021606 at epoch: 5 and batch_num: 94\n",
      "Loss of test set: 0.4014907479286194 at epoch: 5 and batch_num: 95\n",
      "Loss of test set: 0.4432103931903839 at epoch: 5 and batch_num: 96\n",
      "Loss of test set: 0.6049131155014038 at epoch: 5 and batch_num: 97\n",
      "Loss of test set: 0.388516902923584 at epoch: 5 and batch_num: 98\n",
      "Loss of test set: 0.3664190471172333 at epoch: 5 and batch_num: 99\n",
      "Loss of test set: 0.5183053612709045 at epoch: 5 and batch_num: 100\n",
      "Loss of test set: 0.36257320642471313 at epoch: 5 and batch_num: 101\n",
      "Loss of test set: 0.18415319919586182 at epoch: 5 and batch_num: 102\n",
      "Loss of test set: 0.40797075629234314 at epoch: 5 and batch_num: 103\n",
      "Loss of test set: 0.38851481676101685 at epoch: 5 and batch_num: 104\n",
      "Loss of test set: 0.3292620778083801 at epoch: 5 and batch_num: 105\n",
      "Loss of test set: 0.664709746837616 at epoch: 5 and batch_num: 106\n",
      "Loss of test set: 0.5465905070304871 at epoch: 5 and batch_num: 107\n",
      "Loss of test set: 0.35672813653945923 at epoch: 5 and batch_num: 108\n",
      "Loss of test set: 0.3927248418331146 at epoch: 5 and batch_num: 109\n",
      "Loss of test set: 0.4333910644054413 at epoch: 5 and batch_num: 110\n",
      "Loss of test set: 0.39698657393455505 at epoch: 5 and batch_num: 111\n",
      "Loss of test set: 0.3186300992965698 at epoch: 5 and batch_num: 112\n",
      "Loss of test set: 0.38568180799484253 at epoch: 5 and batch_num: 113\n",
      "Loss of test set: 0.43479251861572266 at epoch: 5 and batch_num: 114\n",
      "Loss of test set: 0.49906742572784424 at epoch: 5 and batch_num: 115\n",
      "Loss of test set: 0.43635085225105286 at epoch: 5 and batch_num: 116\n",
      "Loss of test set: 0.4063568115234375 at epoch: 5 and batch_num: 117\n",
      "Loss of test set: 0.4371757507324219 at epoch: 5 and batch_num: 118\n",
      "Loss of test set: 0.3983422517776489 at epoch: 5 and batch_num: 119\n",
      "Loss of test set: 0.6817207336425781 at epoch: 5 and batch_num: 120\n",
      "Loss of test set: 0.18909011781215668 at epoch: 5 and batch_num: 121\n",
      "Loss of test set: 0.4229269027709961 at epoch: 5 and batch_num: 122\n",
      "Loss of test set: 0.3745929002761841 at epoch: 5 and batch_num: 123\n",
      "Loss of test set: 0.29155534505844116 at epoch: 5 and batch_num: 124\n",
      "Loss of test set: 0.43947529792785645 at epoch: 5 and batch_num: 125\n",
      "Loss of test set: 0.48184266686439514 at epoch: 5 and batch_num: 126\n",
      "Loss of test set: 0.382539302110672 at epoch: 5 and batch_num: 127\n",
      "Loss of test set: 0.4387381374835968 at epoch: 5 and batch_num: 128\n",
      "Loss of test set: 0.2930869460105896 at epoch: 5 and batch_num: 129\n",
      "Loss of test set: 0.361471951007843 at epoch: 5 and batch_num: 130\n",
      "Loss of test set: 0.3578462600708008 at epoch: 5 and batch_num: 131\n",
      "Loss of test set: 0.5632818937301636 at epoch: 5 and batch_num: 132\n",
      "Loss of test set: 0.3969743251800537 at epoch: 5 and batch_num: 133\n",
      "Loss of test set: 0.4932090640068054 at epoch: 5 and batch_num: 134\n",
      "Loss of test set: 0.43613332509994507 at epoch: 5 and batch_num: 135\n",
      "Loss of test set: 0.6754179000854492 at epoch: 5 and batch_num: 136\n",
      "Loss of test set: 0.5741559267044067 at epoch: 5 and batch_num: 137\n",
      "Loss of test set: 0.519282341003418 at epoch: 5 and batch_num: 138\n",
      "Loss of test set: 0.41271328926086426 at epoch: 5 and batch_num: 139\n",
      "Loss of test set: 0.41300418972969055 at epoch: 5 and batch_num: 140\n",
      "Loss of test set: 0.5214592218399048 at epoch: 5 and batch_num: 141\n",
      "Loss of test set: 0.36354345083236694 at epoch: 5 and batch_num: 142\n",
      "Loss of test set: 0.49969014525413513 at epoch: 5 and batch_num: 143\n",
      "Loss of test set: 0.3127891421318054 at epoch: 5 and batch_num: 144\n",
      "Loss of test set: 0.2722132205963135 at epoch: 5 and batch_num: 145\n",
      "Loss of test set: 0.46002036333084106 at epoch: 5 and batch_num: 146\n",
      "Loss of test set: 0.37420541048049927 at epoch: 5 and batch_num: 147\n",
      "Loss of test set: 0.2805848717689514 at epoch: 5 and batch_num: 148\n",
      "Loss of test set: 0.44968467950820923 at epoch: 5 and batch_num: 149\n",
      "Loss of test set: 0.42075949907302856 at epoch: 5 and batch_num: 150\n",
      "Loss of test set: 0.372035950422287 at epoch: 5 and batch_num: 151\n",
      "Loss of test set: 0.3828222155570984 at epoch: 5 and batch_num: 152\n",
      "Loss of test set: 0.5856297016143799 at epoch: 5 and batch_num: 153\n",
      "Loss of test set: 0.27404648065567017 at epoch: 5 and batch_num: 154\n",
      "Loss of test set: 0.3755933940410614 at epoch: 5 and batch_num: 155\n",
      "Loss of test set: 0.4323161542415619 at epoch: 5 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8433\n",
      "Loss of train set: 0.23380987346172333 at epoch: 6 and batch_num: 0\n",
      "Loss of train set: 0.3141380846500397 at epoch: 6 and batch_num: 1\n",
      "Loss of train set: 0.2906195819377899 at epoch: 6 and batch_num: 2\n",
      "Loss of train set: 0.480872243642807 at epoch: 6 and batch_num: 3\n",
      "Loss of train set: 0.22740308940410614 at epoch: 6 and batch_num: 4\n",
      "Loss of train set: 0.2098630666732788 at epoch: 6 and batch_num: 5\n",
      "Loss of train set: 0.38020479679107666 at epoch: 6 and batch_num: 6\n",
      "Loss of train set: 0.3337944746017456 at epoch: 6 and batch_num: 7\n",
      "Loss of train set: 0.26551762223243713 at epoch: 6 and batch_num: 8\n",
      "Loss of train set: 0.40000927448272705 at epoch: 6 and batch_num: 9\n",
      "Loss of train set: 0.16098709404468536 at epoch: 6 and batch_num: 10\n",
      "Loss of train set: 0.2896244525909424 at epoch: 6 and batch_num: 11\n",
      "Loss of train set: 0.3985225558280945 at epoch: 6 and batch_num: 12\n",
      "Loss of train set: 0.34379738569259644 at epoch: 6 and batch_num: 13\n",
      "Loss of train set: 0.4493561387062073 at epoch: 6 and batch_num: 14\n",
      "Loss of train set: 0.23611614108085632 at epoch: 6 and batch_num: 15\n",
      "Loss of train set: 0.3120408058166504 at epoch: 6 and batch_num: 16\n",
      "Loss of train set: 0.5539977550506592 at epoch: 6 and batch_num: 17\n",
      "Loss of train set: 0.2931123375892639 at epoch: 6 and batch_num: 18\n",
      "Loss of train set: 0.2825976014137268 at epoch: 6 and batch_num: 19\n",
      "Loss of train set: 0.33978569507598877 at epoch: 6 and batch_num: 20\n",
      "Loss of train set: 0.19903701543807983 at epoch: 6 and batch_num: 21\n",
      "Loss of train set: 0.267269492149353 at epoch: 6 and batch_num: 22\n",
      "Loss of train set: 0.37175658345222473 at epoch: 6 and batch_num: 23\n",
      "Loss of train set: 0.3531016707420349 at epoch: 6 and batch_num: 24\n",
      "Loss of train set: 0.18124085664749146 at epoch: 6 and batch_num: 25\n",
      "Loss of train set: 0.35124146938323975 at epoch: 6 and batch_num: 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.28097671270370483 at epoch: 6 and batch_num: 27\n",
      "Loss of train set: 0.4836598038673401 at epoch: 6 and batch_num: 28\n",
      "Loss of train set: 0.3466724157333374 at epoch: 6 and batch_num: 29\n",
      "Loss of train set: 0.3038210868835449 at epoch: 6 and batch_num: 30\n",
      "Loss of train set: 0.2379949986934662 at epoch: 6 and batch_num: 31\n",
      "Loss of train set: 0.20784495770931244 at epoch: 6 and batch_num: 32\n",
      "Loss of train set: 0.5723549723625183 at epoch: 6 and batch_num: 33\n",
      "Loss of train set: 0.23812299966812134 at epoch: 6 and batch_num: 34\n",
      "Loss of train set: 0.34631583094596863 at epoch: 6 and batch_num: 35\n",
      "Loss of train set: 0.2986729145050049 at epoch: 6 and batch_num: 36\n",
      "Loss of train set: 0.35653942823410034 at epoch: 6 and batch_num: 37\n",
      "Loss of train set: 0.43279144167900085 at epoch: 6 and batch_num: 38\n",
      "Loss of train set: 0.25398585200309753 at epoch: 6 and batch_num: 39\n",
      "Loss of train set: 0.29332613945007324 at epoch: 6 and batch_num: 40\n",
      "Loss of train set: 0.19130034744739532 at epoch: 6 and batch_num: 41\n",
      "Loss of train set: 0.2772173285484314 at epoch: 6 and batch_num: 42\n",
      "Loss of train set: 0.32496193051338196 at epoch: 6 and batch_num: 43\n",
      "Loss of train set: 0.35868456959724426 at epoch: 6 and batch_num: 44\n",
      "Loss of train set: 0.3212628960609436 at epoch: 6 and batch_num: 45\n",
      "Loss of train set: 0.37838423252105713 at epoch: 6 and batch_num: 46\n",
      "Loss of train set: 0.43017351627349854 at epoch: 6 and batch_num: 47\n",
      "Loss of train set: 0.3236926794052124 at epoch: 6 and batch_num: 48\n",
      "Loss of train set: 0.3170340061187744 at epoch: 6 and batch_num: 49\n",
      "Loss of train set: 0.28847572207450867 at epoch: 6 and batch_num: 50\n",
      "Loss of train set: 0.3738107681274414 at epoch: 6 and batch_num: 51\n",
      "Loss of train set: 0.18262241780757904 at epoch: 6 and batch_num: 52\n",
      "Loss of train set: 0.3615109920501709 at epoch: 6 and batch_num: 53\n",
      "Loss of train set: 0.2980222702026367 at epoch: 6 and batch_num: 54\n",
      "Loss of train set: 0.26514503359794617 at epoch: 6 and batch_num: 55\n",
      "Loss of train set: 0.48047876358032227 at epoch: 6 and batch_num: 56\n",
      "Loss of train set: 0.3886104226112366 at epoch: 6 and batch_num: 57\n",
      "Loss of train set: 0.5495033264160156 at epoch: 6 and batch_num: 58\n",
      "Loss of train set: 0.19744744896888733 at epoch: 6 and batch_num: 59\n",
      "Loss of train set: 0.26950803399086 at epoch: 6 and batch_num: 60\n",
      "Loss of train set: 0.3455548882484436 at epoch: 6 and batch_num: 61\n",
      "Loss of train set: 0.419675350189209 at epoch: 6 and batch_num: 62\n",
      "Loss of train set: 0.5849990844726562 at epoch: 6 and batch_num: 63\n",
      "Loss of train set: 0.28102731704711914 at epoch: 6 and batch_num: 64\n",
      "Loss of train set: 0.6534501314163208 at epoch: 6 and batch_num: 65\n",
      "Loss of train set: 0.2865365743637085 at epoch: 6 and batch_num: 66\n",
      "Loss of train set: 0.28892815113067627 at epoch: 6 and batch_num: 67\n",
      "Loss of train set: 0.25193050503730774 at epoch: 6 and batch_num: 68\n",
      "Loss of train set: 0.3908219337463379 at epoch: 6 and batch_num: 69\n",
      "Loss of train set: 0.2097874879837036 at epoch: 6 and batch_num: 70\n",
      "Loss of train set: 0.28508567810058594 at epoch: 6 and batch_num: 71\n",
      "Loss of train set: 0.3660246729850769 at epoch: 6 and batch_num: 72\n",
      "Loss of train set: 0.46539586782455444 at epoch: 6 and batch_num: 73\n",
      "Loss of train set: 0.29538530111312866 at epoch: 6 and batch_num: 74\n",
      "Loss of train set: 0.30112555623054504 at epoch: 6 and batch_num: 75\n",
      "Loss of train set: 0.27362966537475586 at epoch: 6 and batch_num: 76\n",
      "Loss of train set: 0.2957187294960022 at epoch: 6 and batch_num: 77\n",
      "Loss of train set: 0.33326786756515503 at epoch: 6 and batch_num: 78\n",
      "Loss of train set: 0.2740844190120697 at epoch: 6 and batch_num: 79\n",
      "Loss of train set: 0.4242427349090576 at epoch: 6 and batch_num: 80\n",
      "Loss of train set: 0.30234262347221375 at epoch: 6 and batch_num: 81\n",
      "Loss of train set: 0.19747477769851685 at epoch: 6 and batch_num: 82\n",
      "Loss of train set: 0.23988667130470276 at epoch: 6 and batch_num: 83\n",
      "Loss of train set: 0.4115314483642578 at epoch: 6 and batch_num: 84\n",
      "Loss of train set: 0.3255944848060608 at epoch: 6 and batch_num: 85\n",
      "Loss of train set: 0.2861751616001129 at epoch: 6 and batch_num: 86\n",
      "Loss of train set: 0.2650788128376007 at epoch: 6 and batch_num: 87\n",
      "Loss of train set: 0.3098926544189453 at epoch: 6 and batch_num: 88\n",
      "Loss of train set: 0.2575395703315735 at epoch: 6 and batch_num: 89\n",
      "Loss of train set: 0.3552727997303009 at epoch: 6 and batch_num: 90\n",
      "Loss of train set: 0.3369700014591217 at epoch: 6 and batch_num: 91\n",
      "Loss of train set: 0.5432881116867065 at epoch: 6 and batch_num: 92\n",
      "Loss of train set: 0.23610186576843262 at epoch: 6 and batch_num: 93\n",
      "Loss of train set: 0.27347883582115173 at epoch: 6 and batch_num: 94\n",
      "Loss of train set: 0.2502356469631195 at epoch: 6 and batch_num: 95\n",
      "Loss of train set: 0.23395970463752747 at epoch: 6 and batch_num: 96\n",
      "Loss of train set: 0.26042816042900085 at epoch: 6 and batch_num: 97\n",
      "Loss of train set: 0.1992557942867279 at epoch: 6 and batch_num: 98\n",
      "Loss of train set: 0.21872322261333466 at epoch: 6 and batch_num: 99\n",
      "Loss of train set: 0.3378112316131592 at epoch: 6 and batch_num: 100\n",
      "Loss of train set: 0.41526010632514954 at epoch: 6 and batch_num: 101\n",
      "Loss of train set: 0.36534789204597473 at epoch: 6 and batch_num: 102\n",
      "Loss of train set: 0.311977744102478 at epoch: 6 and batch_num: 103\n",
      "Loss of train set: 0.552585780620575 at epoch: 6 and batch_num: 104\n",
      "Loss of train set: 0.47416961193084717 at epoch: 6 and batch_num: 105\n",
      "Loss of train set: 0.3042175769805908 at epoch: 6 and batch_num: 106\n",
      "Loss of train set: 0.3593335747718811 at epoch: 6 and batch_num: 107\n",
      "Loss of train set: 0.5052201151847839 at epoch: 6 and batch_num: 108\n",
      "Loss of train set: 0.22028377652168274 at epoch: 6 and batch_num: 109\n",
      "Loss of train set: 0.5015087127685547 at epoch: 6 and batch_num: 110\n",
      "Loss of train set: 0.2975085377693176 at epoch: 6 and batch_num: 111\n",
      "Loss of train set: 0.08973193168640137 at epoch: 6 and batch_num: 112\n",
      "Loss of train set: 0.3869035840034485 at epoch: 6 and batch_num: 113\n",
      "Loss of train set: 0.4234669804573059 at epoch: 6 and batch_num: 114\n",
      "Loss of train set: 0.46238434314727783 at epoch: 6 and batch_num: 115\n",
      "Loss of train set: 0.23647010326385498 at epoch: 6 and batch_num: 116\n",
      "Loss of train set: 0.2767128348350525 at epoch: 6 and batch_num: 117\n",
      "Loss of train set: 0.19672158360481262 at epoch: 6 and batch_num: 118\n",
      "Loss of train set: 0.27745333313941956 at epoch: 6 and batch_num: 119\n",
      "Loss of train set: 0.30081167817115784 at epoch: 6 and batch_num: 120\n",
      "Loss of train set: 0.23607227206230164 at epoch: 6 and batch_num: 121\n",
      "Loss of train set: 0.28880763053894043 at epoch: 6 and batch_num: 122\n",
      "Loss of train set: 0.20727351307868958 at epoch: 6 and batch_num: 123\n",
      "Loss of train set: 0.5119552612304688 at epoch: 6 and batch_num: 124\n",
      "Loss of train set: 0.40335196256637573 at epoch: 6 and batch_num: 125\n",
      "Loss of train set: 0.3470815420150757 at epoch: 6 and batch_num: 126\n",
      "Loss of train set: 0.253909707069397 at epoch: 6 and batch_num: 127\n",
      "Loss of train set: 0.28458070755004883 at epoch: 6 and batch_num: 128\n",
      "Loss of train set: 0.3569895625114441 at epoch: 6 and batch_num: 129\n",
      "Loss of train set: 0.3735305070877075 at epoch: 6 and batch_num: 130\n",
      "Loss of train set: 0.3631311058998108 at epoch: 6 and batch_num: 131\n",
      "Loss of train set: 0.20049703121185303 at epoch: 6 and batch_num: 132\n",
      "Loss of train set: 0.4005516767501831 at epoch: 6 and batch_num: 133\n",
      "Loss of train set: 0.2894889712333679 at epoch: 6 and batch_num: 134\n",
      "Loss of train set: 0.3391212821006775 at epoch: 6 and batch_num: 135\n",
      "Loss of train set: 0.4354063868522644 at epoch: 6 and batch_num: 136\n",
      "Loss of train set: 0.40178653597831726 at epoch: 6 and batch_num: 137\n",
      "Loss of train set: 0.35895997285842896 at epoch: 6 and batch_num: 138\n",
      "Loss of train set: 0.2671204209327698 at epoch: 6 and batch_num: 139\n",
      "Loss of train set: 0.31640756130218506 at epoch: 6 and batch_num: 140\n",
      "Loss of train set: 0.46595498919487 at epoch: 6 and batch_num: 141\n",
      "Loss of train set: 0.24344313144683838 at epoch: 6 and batch_num: 142\n",
      "Loss of train set: 0.3364044725894928 at epoch: 6 and batch_num: 143\n",
      "Loss of train set: 0.5136467218399048 at epoch: 6 and batch_num: 144\n",
      "Loss of train set: 0.3383055031299591 at epoch: 6 and batch_num: 145\n",
      "Loss of train set: 0.3395843505859375 at epoch: 6 and batch_num: 146\n",
      "Loss of train set: 0.24105429649353027 at epoch: 6 and batch_num: 147\n",
      "Loss of train set: 0.3987143039703369 at epoch: 6 and batch_num: 148\n",
      "Loss of train set: 0.42582520842552185 at epoch: 6 and batch_num: 149\n",
      "Loss of train set: 0.39710861444473267 at epoch: 6 and batch_num: 150\n",
      "Loss of train set: 0.40111953020095825 at epoch: 6 and batch_num: 151\n",
      "Loss of train set: 0.3764708638191223 at epoch: 6 and batch_num: 152\n",
      "Loss of train set: 0.28326964378356934 at epoch: 6 and batch_num: 153\n",
      "Loss of train set: 0.2722153067588806 at epoch: 6 and batch_num: 154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.4841029644012451 at epoch: 6 and batch_num: 155\n",
      "Loss of train set: 0.48754793405532837 at epoch: 6 and batch_num: 156\n",
      "Loss of train set: 0.32841604948043823 at epoch: 6 and batch_num: 157\n",
      "Loss of train set: 0.2710016071796417 at epoch: 6 and batch_num: 158\n",
      "Loss of train set: 0.40095967054367065 at epoch: 6 and batch_num: 159\n",
      "Loss of train set: 0.37411952018737793 at epoch: 6 and batch_num: 160\n",
      "Loss of train set: 0.30351927876472473 at epoch: 6 and batch_num: 161\n",
      "Loss of train set: 0.3583544194698334 at epoch: 6 and batch_num: 162\n",
      "Loss of train set: 0.360988974571228 at epoch: 6 and batch_num: 163\n",
      "Loss of train set: 0.47315847873687744 at epoch: 6 and batch_num: 164\n",
      "Loss of train set: 0.35865518450737 at epoch: 6 and batch_num: 165\n",
      "Loss of train set: 0.49030107259750366 at epoch: 6 and batch_num: 166\n",
      "Loss of train set: 0.5632325410842896 at epoch: 6 and batch_num: 167\n",
      "Loss of train set: 0.39808547496795654 at epoch: 6 and batch_num: 168\n",
      "Loss of train set: 0.1946496069431305 at epoch: 6 and batch_num: 169\n",
      "Loss of train set: 0.20776614546775818 at epoch: 6 and batch_num: 170\n",
      "Loss of train set: 0.4034882187843323 at epoch: 6 and batch_num: 171\n",
      "Loss of train set: 0.45428019762039185 at epoch: 6 and batch_num: 172\n",
      "Loss of train set: 0.4532572031021118 at epoch: 6 and batch_num: 173\n",
      "Loss of train set: 0.32141369581222534 at epoch: 6 and batch_num: 174\n",
      "Loss of train set: 0.2408488690853119 at epoch: 6 and batch_num: 175\n",
      "Loss of train set: 0.3116055130958557 at epoch: 6 and batch_num: 176\n",
      "Loss of train set: 0.3080669641494751 at epoch: 6 and batch_num: 177\n",
      "Loss of train set: 0.33016344904899597 at epoch: 6 and batch_num: 178\n",
      "Loss of train set: 0.5189056992530823 at epoch: 6 and batch_num: 179\n",
      "Loss of train set: 0.49872681498527527 at epoch: 6 and batch_num: 180\n",
      "Loss of train set: 0.3162519335746765 at epoch: 6 and batch_num: 181\n",
      "Loss of train set: 0.22442850470542908 at epoch: 6 and batch_num: 182\n",
      "Loss of train set: 0.2601788640022278 at epoch: 6 and batch_num: 183\n",
      "Loss of train set: 0.35514816641807556 at epoch: 6 and batch_num: 184\n",
      "Loss of train set: 0.41822874546051025 at epoch: 6 and batch_num: 185\n",
      "Loss of train set: 0.3318932056427002 at epoch: 6 and batch_num: 186\n",
      "Loss of train set: 0.25713083148002625 at epoch: 6 and batch_num: 187\n",
      "Loss of train set: 0.3568178415298462 at epoch: 6 and batch_num: 188\n",
      "Loss of train set: 0.31777626276016235 at epoch: 6 and batch_num: 189\n",
      "Loss of train set: 0.25964969396591187 at epoch: 6 and batch_num: 190\n",
      "Loss of train set: 0.40322935581207275 at epoch: 6 and batch_num: 191\n",
      "Loss of train set: 0.43627113103866577 at epoch: 6 and batch_num: 192\n",
      "Loss of train set: 0.44928377866744995 at epoch: 6 and batch_num: 193\n",
      "Loss of train set: 0.22431664168834686 at epoch: 6 and batch_num: 194\n",
      "Loss of train set: 0.38392192125320435 at epoch: 6 and batch_num: 195\n",
      "Loss of train set: 0.1722611039876938 at epoch: 6 and batch_num: 196\n",
      "Loss of train set: 0.5209553837776184 at epoch: 6 and batch_num: 197\n",
      "Loss of train set: 0.3029020428657532 at epoch: 6 and batch_num: 198\n",
      "Loss of train set: 0.2453027218580246 at epoch: 6 and batch_num: 199\n",
      "Loss of train set: 0.5464510321617126 at epoch: 6 and batch_num: 200\n",
      "Loss of train set: 0.4556545615196228 at epoch: 6 and batch_num: 201\n",
      "Loss of train set: 0.42883527278900146 at epoch: 6 and batch_num: 202\n",
      "Loss of train set: 0.26268255710601807 at epoch: 6 and batch_num: 203\n",
      "Loss of train set: 0.22420784831047058 at epoch: 6 and batch_num: 204\n",
      "Loss of train set: 0.3125659227371216 at epoch: 6 and batch_num: 205\n",
      "Loss of train set: 0.237361341714859 at epoch: 6 and batch_num: 206\n",
      "Loss of train set: 0.5327292680740356 at epoch: 6 and batch_num: 207\n",
      "Loss of train set: 0.1664455384016037 at epoch: 6 and batch_num: 208\n",
      "Loss of train set: 0.41242775321006775 at epoch: 6 and batch_num: 209\n",
      "Loss of train set: 0.2837662994861603 at epoch: 6 and batch_num: 210\n",
      "Loss of train set: 0.3356557786464691 at epoch: 6 and batch_num: 211\n",
      "Loss of train set: 0.24896268546581268 at epoch: 6 and batch_num: 212\n",
      "Loss of train set: 0.4526565670967102 at epoch: 6 and batch_num: 213\n",
      "Loss of train set: 0.37601906061172485 at epoch: 6 and batch_num: 214\n",
      "Loss of train set: 0.26141321659088135 at epoch: 6 and batch_num: 215\n",
      "Loss of train set: 0.3862544596195221 at epoch: 6 and batch_num: 216\n",
      "Loss of train set: 0.22257785499095917 at epoch: 6 and batch_num: 217\n",
      "Loss of train set: 0.33075612783432007 at epoch: 6 and batch_num: 218\n",
      "Loss of train set: 0.3329513967037201 at epoch: 6 and batch_num: 219\n",
      "Loss of train set: 0.2324429750442505 at epoch: 6 and batch_num: 220\n",
      "Loss of train set: 0.27791881561279297 at epoch: 6 and batch_num: 221\n",
      "Loss of train set: 0.4162726104259491 at epoch: 6 and batch_num: 222\n",
      "Loss of train set: 0.457177996635437 at epoch: 6 and batch_num: 223\n",
      "Loss of train set: 0.45494478940963745 at epoch: 6 and batch_num: 224\n",
      "Loss of train set: 0.26969659328460693 at epoch: 6 and batch_num: 225\n",
      "Loss of train set: 0.30883076786994934 at epoch: 6 and batch_num: 226\n",
      "Loss of train set: 0.34699881076812744 at epoch: 6 and batch_num: 227\n",
      "Loss of train set: 0.2935081720352173 at epoch: 6 and batch_num: 228\n",
      "Loss of train set: 0.5834877490997314 at epoch: 6 and batch_num: 229\n",
      "Loss of train set: 0.2972809672355652 at epoch: 6 and batch_num: 230\n",
      "Loss of train set: 0.42563849687576294 at epoch: 6 and batch_num: 231\n",
      "Loss of train set: 0.5319803357124329 at epoch: 6 and batch_num: 232\n",
      "Loss of train set: 0.24379120767116547 at epoch: 6 and batch_num: 233\n",
      "Loss of train set: 0.22362732887268066 at epoch: 6 and batch_num: 234\n",
      "Loss of train set: 0.3783143162727356 at epoch: 6 and batch_num: 235\n",
      "Loss of train set: 0.18916001915931702 at epoch: 6 and batch_num: 236\n",
      "Loss of train set: 0.4282686412334442 at epoch: 6 and batch_num: 237\n",
      "Loss of train set: 0.5621461868286133 at epoch: 6 and batch_num: 238\n",
      "Loss of train set: 0.2599729299545288 at epoch: 6 and batch_num: 239\n",
      "Loss of train set: 0.5603730082511902 at epoch: 6 and batch_num: 240\n",
      "Loss of train set: 0.24389302730560303 at epoch: 6 and batch_num: 241\n",
      "Loss of train set: 0.3226364552974701 at epoch: 6 and batch_num: 242\n",
      "Loss of train set: 0.4683469533920288 at epoch: 6 and batch_num: 243\n",
      "Loss of train set: 0.44591638445854187 at epoch: 6 and batch_num: 244\n",
      "Loss of train set: 0.4264294505119324 at epoch: 6 and batch_num: 245\n",
      "Loss of train set: 0.4636154770851135 at epoch: 6 and batch_num: 246\n",
      "Loss of train set: 0.46388429403305054 at epoch: 6 and batch_num: 247\n",
      "Loss of train set: 0.20581021904945374 at epoch: 6 and batch_num: 248\n",
      "Loss of train set: 0.27110040187835693 at epoch: 6 and batch_num: 249\n",
      "Loss of train set: 0.2962499260902405 at epoch: 6 and batch_num: 250\n",
      "Loss of train set: 0.31172657012939453 at epoch: 6 and batch_num: 251\n",
      "Loss of train set: 0.3347199559211731 at epoch: 6 and batch_num: 252\n",
      "Loss of train set: 0.4319024085998535 at epoch: 6 and batch_num: 253\n",
      "Loss of train set: 0.24523614346981049 at epoch: 6 and batch_num: 254\n",
      "Loss of train set: 0.29717424511909485 at epoch: 6 and batch_num: 255\n",
      "Loss of train set: 0.34856978058815 at epoch: 6 and batch_num: 256\n",
      "Loss of train set: 0.2593037784099579 at epoch: 6 and batch_num: 257\n",
      "Loss of train set: 0.4250364899635315 at epoch: 6 and batch_num: 258\n",
      "Loss of train set: 0.2639368176460266 at epoch: 6 and batch_num: 259\n",
      "Loss of train set: 0.17903640866279602 at epoch: 6 and batch_num: 260\n",
      "Loss of train set: 0.5445842742919922 at epoch: 6 and batch_num: 261\n",
      "Loss of train set: 0.2638748586177826 at epoch: 6 and batch_num: 262\n",
      "Loss of train set: 0.5574620962142944 at epoch: 6 and batch_num: 263\n",
      "Loss of train set: 0.4043763279914856 at epoch: 6 and batch_num: 264\n",
      "Loss of train set: 0.3905554413795471 at epoch: 6 and batch_num: 265\n",
      "Loss of train set: 0.3496721684932709 at epoch: 6 and batch_num: 266\n",
      "Loss of train set: 0.2954835593700409 at epoch: 6 and batch_num: 267\n",
      "Loss of train set: 0.3604341149330139 at epoch: 6 and batch_num: 268\n",
      "Loss of train set: 0.4398961067199707 at epoch: 6 and batch_num: 269\n",
      "Loss of train set: 0.24768328666687012 at epoch: 6 and batch_num: 270\n",
      "Loss of train set: 0.21969416737556458 at epoch: 6 and batch_num: 271\n",
      "Loss of train set: 0.2953830659389496 at epoch: 6 and batch_num: 272\n",
      "Loss of train set: 0.22254818677902222 at epoch: 6 and batch_num: 273\n",
      "Loss of train set: 0.21467092633247375 at epoch: 6 and batch_num: 274\n",
      "Loss of train set: 0.4484763741493225 at epoch: 6 and batch_num: 275\n",
      "Loss of train set: 0.3827715516090393 at epoch: 6 and batch_num: 276\n",
      "Loss of train set: 0.39106953144073486 at epoch: 6 and batch_num: 277\n",
      "Loss of train set: 0.33883094787597656 at epoch: 6 and batch_num: 278\n",
      "Loss of train set: 0.21980611979961395 at epoch: 6 and batch_num: 279\n",
      "Loss of train set: 0.2798875570297241 at epoch: 6 and batch_num: 280\n",
      "Loss of train set: 0.4120115637779236 at epoch: 6 and batch_num: 281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3989173173904419 at epoch: 6 and batch_num: 282\n",
      "Loss of train set: 0.4272894263267517 at epoch: 6 and batch_num: 283\n",
      "Loss of train set: 0.2692386507987976 at epoch: 6 and batch_num: 284\n",
      "Loss of train set: 0.2001195251941681 at epoch: 6 and batch_num: 285\n",
      "Loss of train set: 0.5963003635406494 at epoch: 6 and batch_num: 286\n",
      "Loss of train set: 0.42211857438087463 at epoch: 6 and batch_num: 287\n",
      "Loss of train set: 0.3950737714767456 at epoch: 6 and batch_num: 288\n",
      "Loss of train set: 0.42838165163993835 at epoch: 6 and batch_num: 289\n",
      "Loss of train set: 0.3071001172065735 at epoch: 6 and batch_num: 290\n",
      "Loss of train set: 0.26588189601898193 at epoch: 6 and batch_num: 291\n",
      "Loss of train set: 0.2550235390663147 at epoch: 6 and batch_num: 292\n",
      "Loss of train set: 0.3258826732635498 at epoch: 6 and batch_num: 293\n",
      "Loss of train set: 0.21302539110183716 at epoch: 6 and batch_num: 294\n",
      "Loss of train set: 0.3687855899333954 at epoch: 6 and batch_num: 295\n",
      "Loss of train set: 0.2382332980632782 at epoch: 6 and batch_num: 296\n",
      "Loss of train set: 0.48980912566185 at epoch: 6 and batch_num: 297\n",
      "Loss of train set: 0.24958334863185883 at epoch: 6 and batch_num: 298\n",
      "Loss of train set: 0.3711158335208893 at epoch: 6 and batch_num: 299\n",
      "Loss of train set: 0.39133530855178833 at epoch: 6 and batch_num: 300\n",
      "Loss of train set: 0.3374006748199463 at epoch: 6 and batch_num: 301\n",
      "Loss of train set: 0.32045620679855347 at epoch: 6 and batch_num: 302\n",
      "Loss of train set: 0.2898949980735779 at epoch: 6 and batch_num: 303\n",
      "Loss of train set: 0.21280978620052338 at epoch: 6 and batch_num: 304\n",
      "Loss of train set: 0.32376691699028015 at epoch: 6 and batch_num: 305\n",
      "Loss of train set: 0.2694956064224243 at epoch: 6 and batch_num: 306\n",
      "Loss of train set: 0.43052828311920166 at epoch: 6 and batch_num: 307\n",
      "Loss of train set: 0.2883465588092804 at epoch: 6 and batch_num: 308\n",
      "Loss of train set: 0.35616737604141235 at epoch: 6 and batch_num: 309\n",
      "Loss of train set: 0.31724289059638977 at epoch: 6 and batch_num: 310\n",
      "Loss of train set: 0.20848038792610168 at epoch: 6 and batch_num: 311\n",
      "Loss of train set: 0.22277787327766418 at epoch: 6 and batch_num: 312\n",
      "Loss of train set: 0.3553836941719055 at epoch: 6 and batch_num: 313\n",
      "Loss of train set: 0.17795810103416443 at epoch: 6 and batch_num: 314\n",
      "Loss of train set: 0.2794784605503082 at epoch: 6 and batch_num: 315\n",
      "Loss of train set: 0.36900269985198975 at epoch: 6 and batch_num: 316\n",
      "Loss of train set: 0.18118561804294586 at epoch: 6 and batch_num: 317\n",
      "Loss of train set: 0.32293152809143066 at epoch: 6 and batch_num: 318\n",
      "Loss of train set: 0.5288792848587036 at epoch: 6 and batch_num: 319\n",
      "Loss of train set: 0.2784234285354614 at epoch: 6 and batch_num: 320\n",
      "Loss of train set: 0.3084450960159302 at epoch: 6 and batch_num: 321\n",
      "Loss of train set: 0.37117159366607666 at epoch: 6 and batch_num: 322\n",
      "Loss of train set: 0.3409182131290436 at epoch: 6 and batch_num: 323\n",
      "Loss of train set: 0.4697772264480591 at epoch: 6 and batch_num: 324\n",
      "Loss of train set: 0.24453584849834442 at epoch: 6 and batch_num: 325\n",
      "Loss of train set: 0.3575913906097412 at epoch: 6 and batch_num: 326\n",
      "Loss of train set: 0.3234415650367737 at epoch: 6 and batch_num: 327\n",
      "Loss of train set: 0.482014536857605 at epoch: 6 and batch_num: 328\n",
      "Loss of train set: 0.38914772868156433 at epoch: 6 and batch_num: 329\n",
      "Loss of train set: 0.16398732364177704 at epoch: 6 and batch_num: 330\n",
      "Loss of train set: 0.37762996554374695 at epoch: 6 and batch_num: 331\n",
      "Loss of train set: 0.3375375270843506 at epoch: 6 and batch_num: 332\n",
      "Loss of train set: 0.3796674609184265 at epoch: 6 and batch_num: 333\n",
      "Loss of train set: 0.2579514980316162 at epoch: 6 and batch_num: 334\n",
      "Loss of train set: 0.2993479371070862 at epoch: 6 and batch_num: 335\n",
      "Loss of train set: 0.4089905619621277 at epoch: 6 and batch_num: 336\n",
      "Loss of train set: 0.31973716616630554 at epoch: 6 and batch_num: 337\n",
      "Loss of train set: 0.24330875277519226 at epoch: 6 and batch_num: 338\n",
      "Loss of train set: 0.4523429870605469 at epoch: 6 and batch_num: 339\n",
      "Loss of train set: 0.22602978348731995 at epoch: 6 and batch_num: 340\n",
      "Loss of train set: 0.400245726108551 at epoch: 6 and batch_num: 341\n",
      "Loss of train set: 0.3515336513519287 at epoch: 6 and batch_num: 342\n",
      "Loss of train set: 0.17402419447898865 at epoch: 6 and batch_num: 343\n",
      "Loss of train set: 0.2605128884315491 at epoch: 6 and batch_num: 344\n",
      "Loss of train set: 0.34517401456832886 at epoch: 6 and batch_num: 345\n",
      "Loss of train set: 0.4162328541278839 at epoch: 6 and batch_num: 346\n",
      "Loss of train set: 0.41123735904693604 at epoch: 6 and batch_num: 347\n",
      "Loss of train set: 0.22916799783706665 at epoch: 6 and batch_num: 348\n",
      "Loss of train set: 0.3115862011909485 at epoch: 6 and batch_num: 349\n",
      "Loss of train set: 0.2627411186695099 at epoch: 6 and batch_num: 350\n",
      "Loss of train set: 0.3394017219543457 at epoch: 6 and batch_num: 351\n",
      "Loss of train set: 0.3430342674255371 at epoch: 6 and batch_num: 352\n",
      "Loss of train set: 0.3943520486354828 at epoch: 6 and batch_num: 353\n",
      "Loss of train set: 0.2506638467311859 at epoch: 6 and batch_num: 354\n",
      "Loss of train set: 0.19172291457653046 at epoch: 6 and batch_num: 355\n",
      "Loss of train set: 0.29921090602874756 at epoch: 6 and batch_num: 356\n",
      "Loss of train set: 0.2995787262916565 at epoch: 6 and batch_num: 357\n",
      "Loss of train set: 0.43837079405784607 at epoch: 6 and batch_num: 358\n",
      "Loss of train set: 0.3499721884727478 at epoch: 6 and batch_num: 359\n",
      "Loss of train set: 0.25871384143829346 at epoch: 6 and batch_num: 360\n",
      "Loss of train set: 0.4611063003540039 at epoch: 6 and batch_num: 361\n",
      "Loss of train set: 0.24493727087974548 at epoch: 6 and batch_num: 362\n",
      "Loss of train set: 0.3682693541049957 at epoch: 6 and batch_num: 363\n",
      "Loss of train set: 0.3310745358467102 at epoch: 6 and batch_num: 364\n",
      "Loss of train set: 0.429633766412735 at epoch: 6 and batch_num: 365\n",
      "Loss of train set: 0.30182579159736633 at epoch: 6 and batch_num: 366\n",
      "Loss of train set: 0.34806162118911743 at epoch: 6 and batch_num: 367\n",
      "Loss of train set: 0.28368741273880005 at epoch: 6 and batch_num: 368\n",
      "Loss of train set: 0.23911872506141663 at epoch: 6 and batch_num: 369\n",
      "Loss of train set: 0.30089837312698364 at epoch: 6 and batch_num: 370\n",
      "Loss of train set: 0.3694610595703125 at epoch: 6 and batch_num: 371\n",
      "Loss of train set: 0.2121838629245758 at epoch: 6 and batch_num: 372\n",
      "Loss of train set: 0.4750880002975464 at epoch: 6 and batch_num: 373\n",
      "Loss of train set: 0.32152917981147766 at epoch: 6 and batch_num: 374\n",
      "Loss of train set: 0.30627527832984924 at epoch: 6 and batch_num: 375\n",
      "Loss of train set: 0.2968120574951172 at epoch: 6 and batch_num: 376\n",
      "Loss of train set: 0.24060966074466705 at epoch: 6 and batch_num: 377\n",
      "Loss of train set: 0.3748849928379059 at epoch: 6 and batch_num: 378\n",
      "Loss of train set: 0.4169478416442871 at epoch: 6 and batch_num: 379\n",
      "Loss of train set: 0.276807576417923 at epoch: 6 and batch_num: 380\n",
      "Loss of train set: 0.2994663715362549 at epoch: 6 and batch_num: 381\n",
      "Loss of train set: 0.4049221873283386 at epoch: 6 and batch_num: 382\n",
      "Loss of train set: 0.4452919363975525 at epoch: 6 and batch_num: 383\n",
      "Loss of train set: 0.2984659969806671 at epoch: 6 and batch_num: 384\n",
      "Loss of train set: 0.37188076972961426 at epoch: 6 and batch_num: 385\n",
      "Loss of train set: 0.20829357206821442 at epoch: 6 and batch_num: 386\n",
      "Loss of train set: 0.2664460837841034 at epoch: 6 and batch_num: 387\n",
      "Loss of train set: 0.5264787673950195 at epoch: 6 and batch_num: 388\n",
      "Loss of train set: 0.3259609043598175 at epoch: 6 and batch_num: 389\n",
      "Loss of train set: 0.3906988203525543 at epoch: 6 and batch_num: 390\n",
      "Loss of train set: 0.3275599181652069 at epoch: 6 and batch_num: 391\n",
      "Loss of train set: 0.39959824085235596 at epoch: 6 and batch_num: 392\n",
      "Loss of train set: 0.25211548805236816 at epoch: 6 and batch_num: 393\n",
      "Loss of train set: 0.3883656859397888 at epoch: 6 and batch_num: 394\n",
      "Loss of train set: 0.42075473070144653 at epoch: 6 and batch_num: 395\n",
      "Loss of train set: 0.3910316824913025 at epoch: 6 and batch_num: 396\n",
      "Loss of train set: 0.16358071565628052 at epoch: 6 and batch_num: 397\n",
      "Loss of train set: 0.26405709981918335 at epoch: 6 and batch_num: 398\n",
      "Loss of train set: 0.39825761318206787 at epoch: 6 and batch_num: 399\n",
      "Loss of train set: 0.43341198563575745 at epoch: 6 and batch_num: 400\n",
      "Loss of train set: 0.43997204303741455 at epoch: 6 and batch_num: 401\n",
      "Loss of train set: 0.6594635248184204 at epoch: 6 and batch_num: 402\n",
      "Loss of train set: 0.4159923195838928 at epoch: 6 and batch_num: 403\n",
      "Loss of train set: 0.43967974185943604 at epoch: 6 and batch_num: 404\n",
      "Loss of train set: 0.2888849973678589 at epoch: 6 and batch_num: 405\n",
      "Loss of train set: 0.31016218662261963 at epoch: 6 and batch_num: 406\n",
      "Loss of train set: 0.5000958442687988 at epoch: 6 and batch_num: 407\n",
      "Loss of train set: 0.37888699769973755 at epoch: 6 and batch_num: 408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.35266345739364624 at epoch: 6 and batch_num: 409\n",
      "Loss of train set: 0.4707275927066803 at epoch: 6 and batch_num: 410\n",
      "Loss of train set: 0.2577055096626282 at epoch: 6 and batch_num: 411\n",
      "Loss of train set: 0.2609555721282959 at epoch: 6 and batch_num: 412\n",
      "Loss of train set: 0.297885537147522 at epoch: 6 and batch_num: 413\n",
      "Loss of train set: 0.43728235363960266 at epoch: 6 and batch_num: 414\n",
      "Loss of train set: 0.3256770670413971 at epoch: 6 and batch_num: 415\n",
      "Loss of train set: 0.41026371717453003 at epoch: 6 and batch_num: 416\n",
      "Loss of train set: 0.38970619440078735 at epoch: 6 and batch_num: 417\n",
      "Loss of train set: 0.3530130982398987 at epoch: 6 and batch_num: 418\n",
      "Loss of train set: 0.2166384756565094 at epoch: 6 and batch_num: 419\n",
      "Loss of train set: 0.5587801337242126 at epoch: 6 and batch_num: 420\n",
      "Loss of train set: 0.48829352855682373 at epoch: 6 and batch_num: 421\n",
      "Loss of train set: 0.37615832686424255 at epoch: 6 and batch_num: 422\n",
      "Loss of train set: 0.2605576813220978 at epoch: 6 and batch_num: 423\n",
      "Loss of train set: 0.1867227554321289 at epoch: 6 and batch_num: 424\n",
      "Loss of train set: 0.3000321388244629 at epoch: 6 and batch_num: 425\n",
      "Loss of train set: 0.4413144588470459 at epoch: 6 and batch_num: 426\n",
      "Loss of train set: 0.4566580653190613 at epoch: 6 and batch_num: 427\n",
      "Loss of train set: 0.19534757733345032 at epoch: 6 and batch_num: 428\n",
      "Loss of train set: 0.4943097233772278 at epoch: 6 and batch_num: 429\n",
      "Loss of train set: 0.34843629598617554 at epoch: 6 and batch_num: 430\n",
      "Loss of train set: 0.2507707476615906 at epoch: 6 and batch_num: 431\n",
      "Loss of train set: 0.2771901488304138 at epoch: 6 and batch_num: 432\n",
      "Loss of train set: 0.3730728030204773 at epoch: 6 and batch_num: 433\n",
      "Loss of train set: 0.36278337240219116 at epoch: 6 and batch_num: 434\n",
      "Loss of train set: 0.2859715521335602 at epoch: 6 and batch_num: 435\n",
      "Loss of train set: 0.32697245478630066 at epoch: 6 and batch_num: 436\n",
      "Loss of train set: 0.33085811138153076 at epoch: 6 and batch_num: 437\n",
      "Loss of train set: 0.4531782865524292 at epoch: 6 and batch_num: 438\n",
      "Loss of train set: 0.5860772132873535 at epoch: 6 and batch_num: 439\n",
      "Loss of train set: 0.37276554107666016 at epoch: 6 and batch_num: 440\n",
      "Loss of train set: 0.36773067712783813 at epoch: 6 and batch_num: 441\n",
      "Loss of train set: 0.4307273030281067 at epoch: 6 and batch_num: 442\n",
      "Loss of train set: 0.5464471578598022 at epoch: 6 and batch_num: 443\n",
      "Loss of train set: 0.336138516664505 at epoch: 6 and batch_num: 444\n",
      "Loss of train set: 0.33620864152908325 at epoch: 6 and batch_num: 445\n",
      "Loss of train set: 0.32062166929244995 at epoch: 6 and batch_num: 446\n",
      "Loss of train set: 0.36082109808921814 at epoch: 6 and batch_num: 447\n",
      "Loss of train set: 0.3371514081954956 at epoch: 6 and batch_num: 448\n",
      "Loss of train set: 0.2899612784385681 at epoch: 6 and batch_num: 449\n",
      "Loss of train set: 0.29022854566574097 at epoch: 6 and batch_num: 450\n",
      "Loss of train set: 0.1580309271812439 at epoch: 6 and batch_num: 451\n",
      "Loss of train set: 0.32738691568374634 at epoch: 6 and batch_num: 452\n",
      "Loss of train set: 0.2076685130596161 at epoch: 6 and batch_num: 453\n",
      "Loss of train set: 0.35879501700401306 at epoch: 6 and batch_num: 454\n",
      "Loss of train set: 0.2620701193809509 at epoch: 6 and batch_num: 455\n",
      "Loss of train set: 0.28914958238601685 at epoch: 6 and batch_num: 456\n",
      "Loss of train set: 0.5054459571838379 at epoch: 6 and batch_num: 457\n",
      "Loss of train set: 0.38197940587997437 at epoch: 6 and batch_num: 458\n",
      "Loss of train set: 0.4072091281414032 at epoch: 6 and batch_num: 459\n",
      "Loss of train set: 0.25889694690704346 at epoch: 6 and batch_num: 460\n",
      "Loss of train set: 0.24613715708255768 at epoch: 6 and batch_num: 461\n",
      "Loss of train set: 0.31314918398857117 at epoch: 6 and batch_num: 462\n",
      "Loss of train set: 0.2170533537864685 at epoch: 6 and batch_num: 463\n",
      "Loss of train set: 0.302839070558548 at epoch: 6 and batch_num: 464\n",
      "Loss of train set: 0.343312531709671 at epoch: 6 and batch_num: 465\n",
      "Loss of train set: 0.31144124269485474 at epoch: 6 and batch_num: 466\n",
      "Loss of train set: 0.26256027817726135 at epoch: 6 and batch_num: 467\n",
      "Loss of train set: 0.3908286988735199 at epoch: 6 and batch_num: 468\n",
      "Loss of train set: 0.2119758278131485 at epoch: 6 and batch_num: 469\n",
      "Loss of train set: 0.23578715324401855 at epoch: 6 and batch_num: 470\n",
      "Loss of train set: 0.2902148962020874 at epoch: 6 and batch_num: 471\n",
      "Loss of train set: 0.28502410650253296 at epoch: 6 and batch_num: 472\n",
      "Loss of train set: 0.3647170066833496 at epoch: 6 and batch_num: 473\n",
      "Loss of train set: 0.3511192798614502 at epoch: 6 and batch_num: 474\n",
      "Loss of train set: 0.2675589621067047 at epoch: 6 and batch_num: 475\n",
      "Loss of train set: 0.2893938422203064 at epoch: 6 and batch_num: 476\n",
      "Loss of train set: 0.2712959051132202 at epoch: 6 and batch_num: 477\n",
      "Loss of train set: 0.4299386143684387 at epoch: 6 and batch_num: 478\n",
      "Loss of train set: 0.4138184189796448 at epoch: 6 and batch_num: 479\n",
      "Loss of train set: 0.5695260167121887 at epoch: 6 and batch_num: 480\n",
      "Loss of train set: 0.35318315029144287 at epoch: 6 and batch_num: 481\n",
      "Loss of train set: 0.33400392532348633 at epoch: 6 and batch_num: 482\n",
      "Loss of train set: 0.1857912540435791 at epoch: 6 and batch_num: 483\n",
      "Loss of train set: 0.23146654665470123 at epoch: 6 and batch_num: 484\n",
      "Loss of train set: 0.3287734091281891 at epoch: 6 and batch_num: 485\n",
      "Loss of train set: 0.5055088996887207 at epoch: 6 and batch_num: 486\n",
      "Loss of train set: 0.3697334825992584 at epoch: 6 and batch_num: 487\n",
      "Loss of train set: 0.39984026551246643 at epoch: 6 and batch_num: 488\n",
      "Loss of train set: 0.23944979906082153 at epoch: 6 and batch_num: 489\n",
      "Loss of train set: 0.3283178210258484 at epoch: 6 and batch_num: 490\n",
      "Loss of train set: 0.3370840549468994 at epoch: 6 and batch_num: 491\n",
      "Loss of train set: 0.28906893730163574 at epoch: 6 and batch_num: 492\n",
      "Loss of train set: 0.2811988294124603 at epoch: 6 and batch_num: 493\n",
      "Loss of train set: 0.41497835516929626 at epoch: 6 and batch_num: 494\n",
      "Loss of train set: 0.21911534667015076 at epoch: 6 and batch_num: 495\n",
      "Loss of train set: 0.3354640007019043 at epoch: 6 and batch_num: 496\n",
      "Loss of train set: 0.2317061424255371 at epoch: 6 and batch_num: 497\n",
      "Loss of train set: 0.2644721269607544 at epoch: 6 and batch_num: 498\n",
      "Loss of train set: 0.46619656682014465 at epoch: 6 and batch_num: 499\n",
      "Loss of train set: 0.3876410126686096 at epoch: 6 and batch_num: 500\n",
      "Loss of train set: 0.3030858039855957 at epoch: 6 and batch_num: 501\n",
      "Loss of train set: 0.3243701159954071 at epoch: 6 and batch_num: 502\n",
      "Loss of train set: 0.4098963141441345 at epoch: 6 and batch_num: 503\n",
      "Loss of train set: 0.23310543596744537 at epoch: 6 and batch_num: 504\n",
      "Loss of train set: 0.3387347459793091 at epoch: 6 and batch_num: 505\n",
      "Loss of train set: 0.2658421993255615 at epoch: 6 and batch_num: 506\n",
      "Loss of train set: 0.43704771995544434 at epoch: 6 and batch_num: 507\n",
      "Loss of train set: 0.22007378935813904 at epoch: 6 and batch_num: 508\n",
      "Loss of train set: 0.17389219999313354 at epoch: 6 and batch_num: 509\n",
      "Loss of train set: 0.37077784538269043 at epoch: 6 and batch_num: 510\n",
      "Loss of train set: 0.37765228748321533 at epoch: 6 and batch_num: 511\n",
      "Loss of train set: 0.2263728529214859 at epoch: 6 and batch_num: 512\n",
      "Loss of train set: 0.34811609983444214 at epoch: 6 and batch_num: 513\n",
      "Loss of train set: 0.3227602243423462 at epoch: 6 and batch_num: 514\n",
      "Loss of train set: 0.23630312085151672 at epoch: 6 and batch_num: 515\n",
      "Loss of train set: 0.32008612155914307 at epoch: 6 and batch_num: 516\n",
      "Loss of train set: 0.3182952105998993 at epoch: 6 and batch_num: 517\n",
      "Loss of train set: 0.4908706545829773 at epoch: 6 and batch_num: 518\n",
      "Loss of train set: 0.28703242540359497 at epoch: 6 and batch_num: 519\n",
      "Loss of train set: 0.3789251744747162 at epoch: 6 and batch_num: 520\n",
      "Loss of train set: 0.26435545086860657 at epoch: 6 and batch_num: 521\n",
      "Loss of train set: 0.33278560638427734 at epoch: 6 and batch_num: 522\n",
      "Loss of train set: 0.28972291946411133 at epoch: 6 and batch_num: 523\n",
      "Loss of train set: 0.19777944684028625 at epoch: 6 and batch_num: 524\n",
      "Loss of train set: 0.4053767919540405 at epoch: 6 and batch_num: 525\n",
      "Loss of train set: 0.3517519235610962 at epoch: 6 and batch_num: 526\n",
      "Loss of train set: 0.2667055130004883 at epoch: 6 and batch_num: 527\n",
      "Loss of train set: 0.3783741295337677 at epoch: 6 and batch_num: 528\n",
      "Loss of train set: 0.24941860139369965 at epoch: 6 and batch_num: 529\n",
      "Loss of train set: 0.40598541498184204 at epoch: 6 and batch_num: 530\n",
      "Loss of train set: 0.5179960131645203 at epoch: 6 and batch_num: 531\n",
      "Loss of train set: 0.2996845245361328 at epoch: 6 and batch_num: 532\n",
      "Loss of train set: 0.279734343290329 at epoch: 6 and batch_num: 533\n",
      "Loss of train set: 0.5017573833465576 at epoch: 6 and batch_num: 534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3658723533153534 at epoch: 6 and batch_num: 535\n",
      "Loss of train set: 0.497703492641449 at epoch: 6 and batch_num: 536\n",
      "Loss of train set: 0.37231701612472534 at epoch: 6 and batch_num: 537\n",
      "Loss of train set: 0.6030462384223938 at epoch: 6 and batch_num: 538\n",
      "Loss of train set: 0.3860340118408203 at epoch: 6 and batch_num: 539\n",
      "Loss of train set: 0.32311779260635376 at epoch: 6 and batch_num: 540\n",
      "Loss of train set: 0.2717098295688629 at epoch: 6 and batch_num: 541\n",
      "Loss of train set: 0.5589700937271118 at epoch: 6 and batch_num: 542\n",
      "Loss of train set: 0.22632110118865967 at epoch: 6 and batch_num: 543\n",
      "Loss of train set: 0.24990953505039215 at epoch: 6 and batch_num: 544\n",
      "Loss of train set: 0.40651360154151917 at epoch: 6 and batch_num: 545\n",
      "Loss of train set: 0.3303065896034241 at epoch: 6 and batch_num: 546\n",
      "Loss of train set: 0.22280752658843994 at epoch: 6 and batch_num: 547\n",
      "Loss of train set: 0.31954333186149597 at epoch: 6 and batch_num: 548\n",
      "Loss of train set: 0.3269103467464447 at epoch: 6 and batch_num: 549\n",
      "Loss of train set: 0.3055359423160553 at epoch: 6 and batch_num: 550\n",
      "Loss of train set: 0.24227093160152435 at epoch: 6 and batch_num: 551\n",
      "Loss of train set: 0.43350422382354736 at epoch: 6 and batch_num: 552\n",
      "Loss of train set: 0.5903381705284119 at epoch: 6 and batch_num: 553\n",
      "Loss of train set: 0.18982437252998352 at epoch: 6 and batch_num: 554\n",
      "Loss of train set: 0.36374709010124207 at epoch: 6 and batch_num: 555\n",
      "Loss of train set: 0.4532766342163086 at epoch: 6 and batch_num: 556\n",
      "Loss of train set: 0.38109099864959717 at epoch: 6 and batch_num: 557\n",
      "Loss of train set: 0.24765115976333618 at epoch: 6 and batch_num: 558\n",
      "Loss of train set: 0.27566203474998474 at epoch: 6 and batch_num: 559\n",
      "Loss of train set: 0.31125491857528687 at epoch: 6 and batch_num: 560\n",
      "Loss of train set: 0.26249960064888 at epoch: 6 and batch_num: 561\n",
      "Loss of train set: 0.19313278794288635 at epoch: 6 and batch_num: 562\n",
      "Loss of train set: 0.2382967472076416 at epoch: 6 and batch_num: 563\n",
      "Loss of train set: 0.26442450284957886 at epoch: 6 and batch_num: 564\n",
      "Loss of train set: 0.36652934551239014 at epoch: 6 and batch_num: 565\n",
      "Loss of train set: 0.429862380027771 at epoch: 6 and batch_num: 566\n",
      "Loss of train set: 0.32313334941864014 at epoch: 6 and batch_num: 567\n",
      "Loss of train set: 0.43331241607666016 at epoch: 6 and batch_num: 568\n",
      "Loss of train set: 0.3996436893939972 at epoch: 6 and batch_num: 569\n",
      "Loss of train set: 0.23462767899036407 at epoch: 6 and batch_num: 570\n",
      "Loss of train set: 0.37492185831069946 at epoch: 6 and batch_num: 571\n",
      "Loss of train set: 0.3110560178756714 at epoch: 6 and batch_num: 572\n",
      "Loss of train set: 0.30560797452926636 at epoch: 6 and batch_num: 573\n",
      "Loss of train set: 0.29628923535346985 at epoch: 6 and batch_num: 574\n",
      "Loss of train set: 0.411465048789978 at epoch: 6 and batch_num: 575\n",
      "Loss of train set: 0.28232628107070923 at epoch: 6 and batch_num: 576\n",
      "Loss of train set: 0.3447023630142212 at epoch: 6 and batch_num: 577\n",
      "Loss of train set: 0.22038215398788452 at epoch: 6 and batch_num: 578\n",
      "Loss of train set: 0.2695712447166443 at epoch: 6 and batch_num: 579\n",
      "Loss of train set: 0.24447360634803772 at epoch: 6 and batch_num: 580\n",
      "Loss of train set: 0.3879014551639557 at epoch: 6 and batch_num: 581\n",
      "Loss of train set: 0.3940947651863098 at epoch: 6 and batch_num: 582\n",
      "Loss of train set: 0.2185039222240448 at epoch: 6 and batch_num: 583\n",
      "Loss of train set: 0.45360231399536133 at epoch: 6 and batch_num: 584\n",
      "Loss of train set: 0.30952364206314087 at epoch: 6 and batch_num: 585\n",
      "Loss of train set: 0.4191266894340515 at epoch: 6 and batch_num: 586\n",
      "Loss of train set: 0.44186002016067505 at epoch: 6 and batch_num: 587\n",
      "Loss of train set: 0.4382588267326355 at epoch: 6 and batch_num: 588\n",
      "Loss of train set: 0.3857979476451874 at epoch: 6 and batch_num: 589\n",
      "Loss of train set: 0.34305626153945923 at epoch: 6 and batch_num: 590\n",
      "Loss of train set: 0.5126504302024841 at epoch: 6 and batch_num: 591\n",
      "Loss of train set: 0.3812875747680664 at epoch: 6 and batch_num: 592\n",
      "Loss of train set: 0.3368569612503052 at epoch: 6 and batch_num: 593\n",
      "Loss of train set: 0.5568393468856812 at epoch: 6 and batch_num: 594\n",
      "Loss of train set: 0.5672981142997742 at epoch: 6 and batch_num: 595\n",
      "Loss of train set: 0.49045640230178833 at epoch: 6 and batch_num: 596\n",
      "Loss of train set: 0.5025836825370789 at epoch: 6 and batch_num: 597\n",
      "Loss of train set: 0.35583508014678955 at epoch: 6 and batch_num: 598\n",
      "Loss of train set: 0.4082522392272949 at epoch: 6 and batch_num: 599\n",
      "Loss of train set: 0.5534857511520386 at epoch: 6 and batch_num: 600\n",
      "Loss of train set: 0.4469207525253296 at epoch: 6 and batch_num: 601\n",
      "Loss of train set: 0.3291889727115631 at epoch: 6 and batch_num: 602\n",
      "Loss of train set: 0.37499070167541504 at epoch: 6 and batch_num: 603\n",
      "Loss of train set: 0.2021915465593338 at epoch: 6 and batch_num: 604\n",
      "Loss of train set: 0.5017135143280029 at epoch: 6 and batch_num: 605\n",
      "Loss of train set: 0.2991543412208557 at epoch: 6 and batch_num: 606\n",
      "Loss of train set: 0.3057694435119629 at epoch: 6 and batch_num: 607\n",
      "Loss of train set: 0.32908186316490173 at epoch: 6 and batch_num: 608\n",
      "Loss of train set: 0.3992134630680084 at epoch: 6 and batch_num: 609\n",
      "Loss of train set: 0.31211188435554504 at epoch: 6 and batch_num: 610\n",
      "Loss of train set: 0.2738361060619354 at epoch: 6 and batch_num: 611\n",
      "Loss of train set: 0.4582866430282593 at epoch: 6 and batch_num: 612\n",
      "Loss of train set: 0.20361927151679993 at epoch: 6 and batch_num: 613\n",
      "Loss of train set: 0.4455639719963074 at epoch: 6 and batch_num: 614\n",
      "Loss of train set: 0.41374990344047546 at epoch: 6 and batch_num: 615\n",
      "Loss of train set: 0.3392574191093445 at epoch: 6 and batch_num: 616\n",
      "Loss of train set: 0.6037219762802124 at epoch: 6 and batch_num: 617\n",
      "Loss of train set: 0.3710584044456482 at epoch: 6 and batch_num: 618\n",
      "Loss of train set: 0.21713173389434814 at epoch: 6 and batch_num: 619\n",
      "Loss of train set: 0.2669471502304077 at epoch: 6 and batch_num: 620\n",
      "Loss of train set: 0.27916979789733887 at epoch: 6 and batch_num: 621\n",
      "Loss of train set: 0.20214879512786865 at epoch: 6 and batch_num: 622\n",
      "Loss of train set: 0.5823497176170349 at epoch: 6 and batch_num: 623\n",
      "Loss of train set: 0.4501979947090149 at epoch: 6 and batch_num: 624\n",
      "Loss of train set: 0.3245992660522461 at epoch: 6 and batch_num: 625\n",
      "Loss of train set: 0.1795252561569214 at epoch: 6 and batch_num: 626\n",
      "Loss of train set: 0.309211790561676 at epoch: 6 and batch_num: 627\n",
      "Loss of train set: 0.35387328267097473 at epoch: 6 and batch_num: 628\n",
      "Loss of train set: 0.2102961540222168 at epoch: 6 and batch_num: 629\n",
      "Loss of train set: 0.40935054421424866 at epoch: 6 and batch_num: 630\n",
      "Loss of train set: 0.28281155228614807 at epoch: 6 and batch_num: 631\n",
      "Loss of train set: 0.25398528575897217 at epoch: 6 and batch_num: 632\n",
      "Loss of train set: 0.2530800402164459 at epoch: 6 and batch_num: 633\n",
      "Loss of train set: 0.45741981267929077 at epoch: 6 and batch_num: 634\n",
      "Loss of train set: 0.33317410945892334 at epoch: 6 and batch_num: 635\n",
      "Loss of train set: 0.4456949234008789 at epoch: 6 and batch_num: 636\n",
      "Loss of train set: 0.3450159430503845 at epoch: 6 and batch_num: 637\n",
      "Loss of train set: 0.23657864332199097 at epoch: 6 and batch_num: 638\n",
      "Loss of train set: 0.39768174290657043 at epoch: 6 and batch_num: 639\n",
      "Loss of train set: 0.5158730745315552 at epoch: 6 and batch_num: 640\n",
      "Loss of train set: 0.36316418647766113 at epoch: 6 and batch_num: 641\n",
      "Loss of train set: 0.39641299843788147 at epoch: 6 and batch_num: 642\n",
      "Loss of train set: 0.329301655292511 at epoch: 6 and batch_num: 643\n",
      "Loss of train set: 0.4015132188796997 at epoch: 6 and batch_num: 644\n",
      "Loss of train set: 0.2642168402671814 at epoch: 6 and batch_num: 645\n",
      "Loss of train set: 0.2963447570800781 at epoch: 6 and batch_num: 646\n",
      "Loss of train set: 0.4908240735530853 at epoch: 6 and batch_num: 647\n",
      "Loss of train set: 0.3199195861816406 at epoch: 6 and batch_num: 648\n",
      "Loss of train set: 0.3682551980018616 at epoch: 6 and batch_num: 649\n",
      "Loss of train set: 0.27372369170188904 at epoch: 6 and batch_num: 650\n",
      "Loss of train set: 0.41798606514930725 at epoch: 6 and batch_num: 651\n",
      "Loss of train set: 0.353735089302063 at epoch: 6 and batch_num: 652\n",
      "Loss of train set: 0.255118727684021 at epoch: 6 and batch_num: 653\n",
      "Loss of train set: 0.28086480498313904 at epoch: 6 and batch_num: 654\n",
      "Loss of train set: 0.29456377029418945 at epoch: 6 and batch_num: 655\n",
      "Loss of train set: 0.29072263836860657 at epoch: 6 and batch_num: 656\n",
      "Loss of train set: 0.40933480858802795 at epoch: 6 and batch_num: 657\n",
      "Loss of train set: 0.39531582593917847 at epoch: 6 and batch_num: 658\n",
      "Loss of train set: 0.48599785566329956 at epoch: 6 and batch_num: 659\n",
      "Loss of train set: 0.35002362728118896 at epoch: 6 and batch_num: 660\n",
      "Loss of train set: 0.4120524525642395 at epoch: 6 and batch_num: 661\n",
      "Loss of train set: 0.4894856810569763 at epoch: 6 and batch_num: 662\n",
      "Loss of train set: 0.33090484142303467 at epoch: 6 and batch_num: 663\n",
      "Loss of train set: 0.38634365797042847 at epoch: 6 and batch_num: 664\n",
      "Loss of train set: 0.2208162248134613 at epoch: 6 and batch_num: 665\n",
      "Loss of train set: 0.32632261514663696 at epoch: 6 and batch_num: 666\n",
      "Loss of train set: 0.38993850350379944 at epoch: 6 and batch_num: 667\n",
      "Loss of train set: 0.22742778062820435 at epoch: 6 and batch_num: 668\n",
      "Loss of train set: 0.39208436012268066 at epoch: 6 and batch_num: 669\n",
      "Loss of train set: 0.4236339032649994 at epoch: 6 and batch_num: 670\n",
      "Loss of train set: 0.27604568004608154 at epoch: 6 and batch_num: 671\n",
      "Loss of train set: 0.24118682742118835 at epoch: 6 and batch_num: 672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.48505741357803345 at epoch: 6 and batch_num: 673\n",
      "Loss of train set: 0.29008749127388 at epoch: 6 and batch_num: 674\n",
      "Loss of train set: 0.38405364751815796 at epoch: 6 and batch_num: 675\n",
      "Loss of train set: 0.15745940804481506 at epoch: 6 and batch_num: 676\n",
      "Loss of train set: 0.4923520088195801 at epoch: 6 and batch_num: 677\n",
      "Loss of train set: 0.3673762083053589 at epoch: 6 and batch_num: 678\n",
      "Loss of train set: 0.26579374074935913 at epoch: 6 and batch_num: 679\n",
      "Loss of train set: 0.4091145992279053 at epoch: 6 and batch_num: 680\n",
      "Loss of train set: 0.377368688583374 at epoch: 6 and batch_num: 681\n",
      "Loss of train set: 0.24052855372428894 at epoch: 6 and batch_num: 682\n",
      "Loss of train set: 0.27260860800743103 at epoch: 6 and batch_num: 683\n",
      "Loss of train set: 0.6113495826721191 at epoch: 6 and batch_num: 684\n",
      "Loss of train set: 0.43643712997436523 at epoch: 6 and batch_num: 685\n",
      "Loss of train set: 0.33160698413848877 at epoch: 6 and batch_num: 686\n",
      "Loss of train set: 0.4809017479419708 at epoch: 6 and batch_num: 687\n",
      "Loss of train set: 0.5275464057922363 at epoch: 6 and batch_num: 688\n",
      "Loss of train set: 0.21733306348323822 at epoch: 6 and batch_num: 689\n",
      "Loss of train set: 0.30851131677627563 at epoch: 6 and batch_num: 690\n",
      "Loss of train set: 0.4771907329559326 at epoch: 6 and batch_num: 691\n",
      "Loss of train set: 0.38608765602111816 at epoch: 6 and batch_num: 692\n",
      "Loss of train set: 0.3672613501548767 at epoch: 6 and batch_num: 693\n",
      "Loss of train set: 0.5806374549865723 at epoch: 6 and batch_num: 694\n",
      "Loss of train set: 0.33519086241722107 at epoch: 6 and batch_num: 695\n",
      "Loss of train set: 0.3487848937511444 at epoch: 6 and batch_num: 696\n",
      "Loss of train set: 0.2220914661884308 at epoch: 6 and batch_num: 697\n",
      "Loss of train set: 0.4842372536659241 at epoch: 6 and batch_num: 698\n",
      "Loss of train set: 0.3049318790435791 at epoch: 6 and batch_num: 699\n",
      "Loss of train set: 0.3448260426521301 at epoch: 6 and batch_num: 700\n",
      "Loss of train set: 0.19222798943519592 at epoch: 6 and batch_num: 701\n",
      "Loss of train set: 0.26408886909484863 at epoch: 6 and batch_num: 702\n",
      "Loss of train set: 0.2658514380455017 at epoch: 6 and batch_num: 703\n",
      "Loss of train set: 0.36288297176361084 at epoch: 6 and batch_num: 704\n",
      "Loss of train set: 0.6047627925872803 at epoch: 6 and batch_num: 705\n",
      "Loss of train set: 0.3598079979419708 at epoch: 6 and batch_num: 706\n",
      "Loss of train set: 0.2722838819026947 at epoch: 6 and batch_num: 707\n",
      "Loss of train set: 0.3185186982154846 at epoch: 6 and batch_num: 708\n",
      "Loss of train set: 0.34325090050697327 at epoch: 6 and batch_num: 709\n",
      "Loss of train set: 0.3821977972984314 at epoch: 6 and batch_num: 710\n",
      "Loss of train set: 0.1587541401386261 at epoch: 6 and batch_num: 711\n",
      "Loss of train set: 0.4847322702407837 at epoch: 6 and batch_num: 712\n",
      "Loss of train set: 0.31220895051956177 at epoch: 6 and batch_num: 713\n",
      "Loss of train set: 0.29004934430122375 at epoch: 6 and batch_num: 714\n",
      "Loss of train set: 0.42862141132354736 at epoch: 6 and batch_num: 715\n",
      "Loss of train set: 0.3485450744628906 at epoch: 6 and batch_num: 716\n",
      "Loss of train set: 0.4924945533275604 at epoch: 6 and batch_num: 717\n",
      "Loss of train set: 0.36565810441970825 at epoch: 6 and batch_num: 718\n",
      "Loss of train set: 0.19582770764827728 at epoch: 6 and batch_num: 719\n",
      "Loss of train set: 0.3573601245880127 at epoch: 6 and batch_num: 720\n",
      "Loss of train set: 0.3631315529346466 at epoch: 6 and batch_num: 721\n",
      "Loss of train set: 0.23432597517967224 at epoch: 6 and batch_num: 722\n",
      "Loss of train set: 0.32699790596961975 at epoch: 6 and batch_num: 723\n",
      "Loss of train set: 0.3537173569202423 at epoch: 6 and batch_num: 724\n",
      "Loss of train set: 0.39290091395378113 at epoch: 6 and batch_num: 725\n",
      "Loss of train set: 0.3333202600479126 at epoch: 6 and batch_num: 726\n",
      "Loss of train set: 0.39534640312194824 at epoch: 6 and batch_num: 727\n",
      "Loss of train set: 0.30197232961654663 at epoch: 6 and batch_num: 728\n",
      "Loss of train set: 0.2951931059360504 at epoch: 6 and batch_num: 729\n",
      "Loss of train set: 0.4175946116447449 at epoch: 6 and batch_num: 730\n",
      "Loss of train set: 0.3015292286872864 at epoch: 6 and batch_num: 731\n",
      "Loss of train set: 0.3938750624656677 at epoch: 6 and batch_num: 732\n",
      "Loss of train set: 0.22808721661567688 at epoch: 6 and batch_num: 733\n",
      "Loss of train set: 0.33311402797698975 at epoch: 6 and batch_num: 734\n",
      "Loss of train set: 0.5258360505104065 at epoch: 6 and batch_num: 735\n",
      "Loss of train set: 0.396776020526886 at epoch: 6 and batch_num: 736\n",
      "Loss of train set: 0.282094806432724 at epoch: 6 and batch_num: 737\n",
      "Loss of train set: 0.284854531288147 at epoch: 6 and batch_num: 738\n",
      "Loss of train set: 0.39757537841796875 at epoch: 6 and batch_num: 739\n",
      "Loss of train set: 0.23184934258460999 at epoch: 6 and batch_num: 740\n",
      "Loss of train set: 0.4644460082054138 at epoch: 6 and batch_num: 741\n",
      "Loss of train set: 0.30378082394599915 at epoch: 6 and batch_num: 742\n",
      "Loss of train set: 0.3490544557571411 at epoch: 6 and batch_num: 743\n",
      "Loss of train set: 0.270609974861145 at epoch: 6 and batch_num: 744\n",
      "Loss of train set: 0.276684045791626 at epoch: 6 and batch_num: 745\n",
      "Loss of train set: 0.43367522954940796 at epoch: 6 and batch_num: 746\n",
      "Loss of train set: 0.30557018518447876 at epoch: 6 and batch_num: 747\n",
      "Loss of train set: 0.2267235666513443 at epoch: 6 and batch_num: 748\n",
      "Loss of train set: 0.3364647626876831 at epoch: 6 and batch_num: 749\n",
      "Loss of train set: 0.33539751172065735 at epoch: 6 and batch_num: 750\n",
      "Loss of train set: 0.401218056678772 at epoch: 6 and batch_num: 751\n",
      "Loss of train set: 0.16414913535118103 at epoch: 6 and batch_num: 752\n",
      "Loss of train set: 0.23125673830509186 at epoch: 6 and batch_num: 753\n",
      "Loss of train set: 0.36904773116111755 at epoch: 6 and batch_num: 754\n",
      "Loss of train set: 0.41851091384887695 at epoch: 6 and batch_num: 755\n",
      "Loss of train set: 0.2579476237297058 at epoch: 6 and batch_num: 756\n",
      "Loss of train set: 0.4174571633338928 at epoch: 6 and batch_num: 757\n",
      "Loss of train set: 0.343461275100708 at epoch: 6 and batch_num: 758\n",
      "Loss of train set: 0.47385072708129883 at epoch: 6 and batch_num: 759\n",
      "Loss of train set: 0.41598960757255554 at epoch: 6 and batch_num: 760\n",
      "Loss of train set: 0.29875293374061584 at epoch: 6 and batch_num: 761\n",
      "Loss of train set: 0.46942123770713806 at epoch: 6 and batch_num: 762\n",
      "Loss of train set: 0.5372152328491211 at epoch: 6 and batch_num: 763\n",
      "Loss of train set: 0.1973453164100647 at epoch: 6 and batch_num: 764\n",
      "Loss of train set: 0.28978946805000305 at epoch: 6 and batch_num: 765\n",
      "Loss of train set: 0.30207592248916626 at epoch: 6 and batch_num: 766\n",
      "Loss of train set: 0.311614453792572 at epoch: 6 and batch_num: 767\n",
      "Loss of train set: 0.3067074418067932 at epoch: 6 and batch_num: 768\n",
      "Loss of train set: 0.1716507077217102 at epoch: 6 and batch_num: 769\n",
      "Loss of train set: 0.4479678273200989 at epoch: 6 and batch_num: 770\n",
      "Loss of train set: 0.2948150634765625 at epoch: 6 and batch_num: 771\n",
      "Loss of train set: 0.30788034200668335 at epoch: 6 and batch_num: 772\n",
      "Loss of train set: 0.46450769901275635 at epoch: 6 and batch_num: 773\n",
      "Loss of train set: 0.5209639072418213 at epoch: 6 and batch_num: 774\n",
      "Loss of train set: 0.3959026038646698 at epoch: 6 and batch_num: 775\n",
      "Loss of train set: 0.506025493144989 at epoch: 6 and batch_num: 776\n",
      "Loss of train set: 0.30763036012649536 at epoch: 6 and batch_num: 777\n",
      "Loss of train set: 0.35266971588134766 at epoch: 6 and batch_num: 778\n",
      "Loss of train set: 0.40908920764923096 at epoch: 6 and batch_num: 779\n",
      "Loss of train set: 0.4036344289779663 at epoch: 6 and batch_num: 780\n",
      "Loss of train set: 0.22707132995128632 at epoch: 6 and batch_num: 781\n",
      "Loss of train set: 0.25785258412361145 at epoch: 6 and batch_num: 782\n",
      "Loss of train set: 0.25203174352645874 at epoch: 6 and batch_num: 783\n",
      "Loss of train set: 0.2624945044517517 at epoch: 6 and batch_num: 784\n",
      "Loss of train set: 0.26044780015945435 at epoch: 6 and batch_num: 785\n",
      "Loss of train set: 0.4371231496334076 at epoch: 6 and batch_num: 786\n",
      "Loss of train set: 0.37361180782318115 at epoch: 6 and batch_num: 787\n",
      "Loss of train set: 0.4324849247932434 at epoch: 6 and batch_num: 788\n",
      "Loss of train set: 0.2868531346321106 at epoch: 6 and batch_num: 789\n",
      "Loss of train set: 0.26273250579833984 at epoch: 6 and batch_num: 790\n",
      "Loss of train set: 0.3452422022819519 at epoch: 6 and batch_num: 791\n",
      "Loss of train set: 0.45896008610725403 at epoch: 6 and batch_num: 792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2682330310344696 at epoch: 6 and batch_num: 793\n",
      "Loss of train set: 0.22711986303329468 at epoch: 6 and batch_num: 794\n",
      "Loss of train set: 0.39929038286209106 at epoch: 6 and batch_num: 795\n",
      "Loss of train set: 0.40292811393737793 at epoch: 6 and batch_num: 796\n",
      "Loss of train set: 0.37718266248703003 at epoch: 6 and batch_num: 797\n",
      "Loss of train set: 0.41887855529785156 at epoch: 6 and batch_num: 798\n",
      "Loss of train set: 0.35433268547058105 at epoch: 6 and batch_num: 799\n",
      "Loss of train set: 0.29961517453193665 at epoch: 6 and batch_num: 800\n",
      "Loss of train set: 0.23116189241409302 at epoch: 6 and batch_num: 801\n",
      "Loss of train set: 0.34701791405677795 at epoch: 6 and batch_num: 802\n",
      "Loss of train set: 0.42748862504959106 at epoch: 6 and batch_num: 803\n",
      "Loss of train set: 0.3832538425922394 at epoch: 6 and batch_num: 804\n",
      "Loss of train set: 0.17224353551864624 at epoch: 6 and batch_num: 805\n",
      "Loss of train set: 0.44430792331695557 at epoch: 6 and batch_num: 806\n",
      "Loss of train set: 0.23205193877220154 at epoch: 6 and batch_num: 807\n",
      "Loss of train set: 0.26048189401626587 at epoch: 6 and batch_num: 808\n",
      "Loss of train set: 0.41452619433403015 at epoch: 6 and batch_num: 809\n",
      "Loss of train set: 0.19127053022384644 at epoch: 6 and batch_num: 810\n",
      "Loss of train set: 0.23412184417247772 at epoch: 6 and batch_num: 811\n",
      "Loss of train set: 0.3161425292491913 at epoch: 6 and batch_num: 812\n",
      "Loss of train set: 0.326393723487854 at epoch: 6 and batch_num: 813\n",
      "Loss of train set: 0.20254802703857422 at epoch: 6 and batch_num: 814\n",
      "Loss of train set: 0.47228509187698364 at epoch: 6 and batch_num: 815\n",
      "Loss of train set: 0.3271685242652893 at epoch: 6 and batch_num: 816\n",
      "Loss of train set: 0.28044503927230835 at epoch: 6 and batch_num: 817\n",
      "Loss of train set: 0.32085734605789185 at epoch: 6 and batch_num: 818\n",
      "Loss of train set: 0.45077183842658997 at epoch: 6 and batch_num: 819\n",
      "Loss of train set: 0.2279634177684784 at epoch: 6 and batch_num: 820\n",
      "Loss of train set: 0.4167974591255188 at epoch: 6 and batch_num: 821\n",
      "Loss of train set: 0.3578147888183594 at epoch: 6 and batch_num: 822\n",
      "Loss of train set: 0.4383232593536377 at epoch: 6 and batch_num: 823\n",
      "Loss of train set: 0.2662246823310852 at epoch: 6 and batch_num: 824\n",
      "Loss of train set: 0.23474276065826416 at epoch: 6 and batch_num: 825\n",
      "Loss of train set: 0.3823743462562561 at epoch: 6 and batch_num: 826\n",
      "Loss of train set: 0.29193660616874695 at epoch: 6 and batch_num: 827\n",
      "Loss of train set: 0.4302257299423218 at epoch: 6 and batch_num: 828\n",
      "Loss of train set: 0.41821059584617615 at epoch: 6 and batch_num: 829\n",
      "Loss of train set: 0.3415139317512512 at epoch: 6 and batch_num: 830\n",
      "Loss of train set: 0.29065901041030884 at epoch: 6 and batch_num: 831\n",
      "Loss of train set: 0.3049847483634949 at epoch: 6 and batch_num: 832\n",
      "Loss of train set: 0.3168686628341675 at epoch: 6 and batch_num: 833\n",
      "Loss of train set: 0.3754630982875824 at epoch: 6 and batch_num: 834\n",
      "Loss of train set: 0.3540923595428467 at epoch: 6 and batch_num: 835\n",
      "Loss of train set: 0.19840598106384277 at epoch: 6 and batch_num: 836\n",
      "Loss of train set: 0.3197457194328308 at epoch: 6 and batch_num: 837\n",
      "Loss of train set: 0.3535298705101013 at epoch: 6 and batch_num: 838\n",
      "Loss of train set: 0.25351548194885254 at epoch: 6 and batch_num: 839\n",
      "Loss of train set: 0.5962441563606262 at epoch: 6 and batch_num: 840\n",
      "Loss of train set: 0.2694903612136841 at epoch: 6 and batch_num: 841\n",
      "Loss of train set: 0.22103945910930634 at epoch: 6 and batch_num: 842\n",
      "Loss of train set: 0.3543710708618164 at epoch: 6 and batch_num: 843\n",
      "Loss of train set: 0.285521924495697 at epoch: 6 and batch_num: 844\n",
      "Loss of train set: 0.5392482280731201 at epoch: 6 and batch_num: 845\n",
      "Loss of train set: 0.2149164378643036 at epoch: 6 and batch_num: 846\n",
      "Loss of train set: 0.36900097131729126 at epoch: 6 and batch_num: 847\n",
      "Loss of train set: 0.4106239676475525 at epoch: 6 and batch_num: 848\n",
      "Loss of train set: 0.5612685680389404 at epoch: 6 and batch_num: 849\n",
      "Loss of train set: 0.19356262683868408 at epoch: 6 and batch_num: 850\n",
      "Loss of train set: 0.32130664587020874 at epoch: 6 and batch_num: 851\n",
      "Loss of train set: 0.2689792513847351 at epoch: 6 and batch_num: 852\n",
      "Loss of train set: 0.4621405601501465 at epoch: 6 and batch_num: 853\n",
      "Loss of train set: 0.20508357882499695 at epoch: 6 and batch_num: 854\n",
      "Loss of train set: 0.5675718188285828 at epoch: 6 and batch_num: 855\n",
      "Loss of train set: 0.5758166313171387 at epoch: 6 and batch_num: 856\n",
      "Loss of train set: 0.5047051906585693 at epoch: 6 and batch_num: 857\n",
      "Loss of train set: 0.2781810760498047 at epoch: 6 and batch_num: 858\n",
      "Loss of train set: 0.1907990425825119 at epoch: 6 and batch_num: 859\n",
      "Loss of train set: 0.3048200309276581 at epoch: 6 and batch_num: 860\n",
      "Loss of train set: 0.34541985392570496 at epoch: 6 and batch_num: 861\n",
      "Loss of train set: 0.20703889429569244 at epoch: 6 and batch_num: 862\n",
      "Loss of train set: 0.49275708198547363 at epoch: 6 and batch_num: 863\n",
      "Loss of train set: 0.46884632110595703 at epoch: 6 and batch_num: 864\n",
      "Loss of train set: 0.3177931308746338 at epoch: 6 and batch_num: 865\n",
      "Loss of train set: 0.2832235097885132 at epoch: 6 and batch_num: 866\n",
      "Loss of train set: 0.39273393154144287 at epoch: 6 and batch_num: 867\n",
      "Loss of train set: 0.3168594539165497 at epoch: 6 and batch_num: 868\n",
      "Loss of train set: 0.32356375455856323 at epoch: 6 and batch_num: 869\n",
      "Loss of train set: 0.35127580165863037 at epoch: 6 and batch_num: 870\n",
      "Loss of train set: 0.24571752548217773 at epoch: 6 and batch_num: 871\n",
      "Loss of train set: 0.41177260875701904 at epoch: 6 and batch_num: 872\n",
      "Loss of train set: 0.258115291595459 at epoch: 6 and batch_num: 873\n",
      "Loss of train set: 0.4266984462738037 at epoch: 6 and batch_num: 874\n",
      "Loss of train set: 0.35627809166908264 at epoch: 6 and batch_num: 875\n",
      "Loss of train set: 0.2544955015182495 at epoch: 6 and batch_num: 876\n",
      "Loss of train set: 0.28434786200523376 at epoch: 6 and batch_num: 877\n",
      "Loss of train set: 0.36740338802337646 at epoch: 6 and batch_num: 878\n",
      "Loss of train set: 0.2330646514892578 at epoch: 6 and batch_num: 879\n",
      "Loss of train set: 0.364044725894928 at epoch: 6 and batch_num: 880\n",
      "Loss of train set: 0.3218640089035034 at epoch: 6 and batch_num: 881\n",
      "Loss of train set: 0.34908735752105713 at epoch: 6 and batch_num: 882\n",
      "Loss of train set: 0.35439109802246094 at epoch: 6 and batch_num: 883\n",
      "Loss of train set: 0.25673049688339233 at epoch: 6 and batch_num: 884\n",
      "Loss of train set: 0.34122228622436523 at epoch: 6 and batch_num: 885\n",
      "Loss of train set: 0.2431052327156067 at epoch: 6 and batch_num: 886\n",
      "Loss of train set: 0.32784557342529297 at epoch: 6 and batch_num: 887\n",
      "Loss of train set: 0.3261423707008362 at epoch: 6 and batch_num: 888\n",
      "Loss of train set: 0.28842002153396606 at epoch: 6 and batch_num: 889\n",
      "Loss of train set: 0.3992142677307129 at epoch: 6 and batch_num: 890\n",
      "Loss of train set: 0.3194776177406311 at epoch: 6 and batch_num: 891\n",
      "Loss of train set: 0.6398777961730957 at epoch: 6 and batch_num: 892\n",
      "Loss of train set: 0.3282863199710846 at epoch: 6 and batch_num: 893\n",
      "Loss of train set: 0.3759461045265198 at epoch: 6 and batch_num: 894\n",
      "Loss of train set: 0.49456319212913513 at epoch: 6 and batch_num: 895\n",
      "Loss of train set: 0.320429265499115 at epoch: 6 and batch_num: 896\n",
      "Loss of train set: 0.4501499831676483 at epoch: 6 and batch_num: 897\n",
      "Loss of train set: 0.465453565120697 at epoch: 6 and batch_num: 898\n",
      "Loss of train set: 0.4021624028682709 at epoch: 6 and batch_num: 899\n",
      "Loss of train set: 0.6602307558059692 at epoch: 6 and batch_num: 900\n",
      "Loss of train set: 0.3122105002403259 at epoch: 6 and batch_num: 901\n",
      "Loss of train set: 0.2282504439353943 at epoch: 6 and batch_num: 902\n",
      "Loss of train set: 0.13757333159446716 at epoch: 6 and batch_num: 903\n",
      "Loss of train set: 0.24012236297130585 at epoch: 6 and batch_num: 904\n",
      "Loss of train set: 0.29876184463500977 at epoch: 6 and batch_num: 905\n",
      "Loss of train set: 0.3227830231189728 at epoch: 6 and batch_num: 906\n",
      "Loss of train set: 0.3134321868419647 at epoch: 6 and batch_num: 907\n",
      "Loss of train set: 0.28883713483810425 at epoch: 6 and batch_num: 908\n",
      "Loss of train set: 0.3943661153316498 at epoch: 6 and batch_num: 909\n",
      "Loss of train set: 0.30915364623069763 at epoch: 6 and batch_num: 910\n",
      "Loss of train set: 0.23472392559051514 at epoch: 6 and batch_num: 911\n",
      "Loss of train set: 0.41798579692840576 at epoch: 6 and batch_num: 912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3714204430580139 at epoch: 6 and batch_num: 913\n",
      "Loss of train set: 0.3935019373893738 at epoch: 6 and batch_num: 914\n",
      "Loss of train set: 0.23751476407051086 at epoch: 6 and batch_num: 915\n",
      "Loss of train set: 0.4406983256340027 at epoch: 6 and batch_num: 916\n",
      "Loss of train set: 0.3637425899505615 at epoch: 6 and batch_num: 917\n",
      "Loss of train set: 0.3838438093662262 at epoch: 6 and batch_num: 918\n",
      "Loss of train set: 0.22402380406856537 at epoch: 6 and batch_num: 919\n",
      "Loss of train set: 0.3317283093929291 at epoch: 6 and batch_num: 920\n",
      "Loss of train set: 0.3043491840362549 at epoch: 6 and batch_num: 921\n",
      "Loss of train set: 0.2608087956905365 at epoch: 6 and batch_num: 922\n",
      "Loss of train set: 0.3470718562602997 at epoch: 6 and batch_num: 923\n",
      "Loss of train set: 0.4469362795352936 at epoch: 6 and batch_num: 924\n",
      "Loss of train set: 0.2762681841850281 at epoch: 6 and batch_num: 925\n",
      "Loss of train set: 0.4301817715167999 at epoch: 6 and batch_num: 926\n",
      "Loss of train set: 0.2689105272293091 at epoch: 6 and batch_num: 927\n",
      "Loss of train set: 0.44240087270736694 at epoch: 6 and batch_num: 928\n",
      "Loss of train set: 0.47159522771835327 at epoch: 6 and batch_num: 929\n",
      "Loss of train set: 0.20150688290596008 at epoch: 6 and batch_num: 930\n",
      "Loss of train set: 0.5216002464294434 at epoch: 6 and batch_num: 931\n",
      "Loss of train set: 0.3755022883415222 at epoch: 6 and batch_num: 932\n",
      "Loss of train set: 0.41096627712249756 at epoch: 6 and batch_num: 933\n",
      "Loss of train set: 0.32786819338798523 at epoch: 6 and batch_num: 934\n",
      "Loss of train set: 0.28327083587646484 at epoch: 6 and batch_num: 935\n",
      "Loss of train set: 0.39915114641189575 at epoch: 6 and batch_num: 936\n",
      "Loss of train set: 0.22865939140319824 at epoch: 6 and batch_num: 937\n",
      "Accuracy of train set: 0.8771666666666667\n",
      "Loss of test set: 0.42918241024017334 at epoch: 6 and batch_num: 0\n",
      "Loss of test set: 0.4676907956600189 at epoch: 6 and batch_num: 1\n",
      "Loss of test set: 0.2280929982662201 at epoch: 6 and batch_num: 2\n",
      "Loss of test set: 0.3501903712749481 at epoch: 6 and batch_num: 3\n",
      "Loss of test set: 0.5025416612625122 at epoch: 6 and batch_num: 4\n",
      "Loss of test set: 0.47454655170440674 at epoch: 6 and batch_num: 5\n",
      "Loss of test set: 0.3236626982688904 at epoch: 6 and batch_num: 6\n",
      "Loss of test set: 0.3267695903778076 at epoch: 6 and batch_num: 7\n",
      "Loss of test set: 0.4512028098106384 at epoch: 6 and batch_num: 8\n",
      "Loss of test set: 0.31680381298065186 at epoch: 6 and batch_num: 9\n",
      "Loss of test set: 0.39737534523010254 at epoch: 6 and batch_num: 10\n",
      "Loss of test set: 0.4753262996673584 at epoch: 6 and batch_num: 11\n",
      "Loss of test set: 0.3427119553089142 at epoch: 6 and batch_num: 12\n",
      "Loss of test set: 0.3846924901008606 at epoch: 6 and batch_num: 13\n",
      "Loss of test set: 0.33754590153694153 at epoch: 6 and batch_num: 14\n",
      "Loss of test set: 0.3601905405521393 at epoch: 6 and batch_num: 15\n",
      "Loss of test set: 0.4111381769180298 at epoch: 6 and batch_num: 16\n",
      "Loss of test set: 0.3737560510635376 at epoch: 6 and batch_num: 17\n",
      "Loss of test set: 0.41236644983291626 at epoch: 6 and batch_num: 18\n",
      "Loss of test set: 0.5168187618255615 at epoch: 6 and batch_num: 19\n",
      "Loss of test set: 0.380910724401474 at epoch: 6 and batch_num: 20\n",
      "Loss of test set: 0.3614330291748047 at epoch: 6 and batch_num: 21\n",
      "Loss of test set: 0.24918217957019806 at epoch: 6 and batch_num: 22\n",
      "Loss of test set: 0.6845473051071167 at epoch: 6 and batch_num: 23\n",
      "Loss of test set: 0.46583208441734314 at epoch: 6 and batch_num: 24\n",
      "Loss of test set: 0.4818873405456543 at epoch: 6 and batch_num: 25\n",
      "Loss of test set: 0.3145512342453003 at epoch: 6 and batch_num: 26\n",
      "Loss of test set: 0.2864971458911896 at epoch: 6 and batch_num: 27\n",
      "Loss of test set: 0.29217979311943054 at epoch: 6 and batch_num: 28\n",
      "Loss of test set: 0.20936386287212372 at epoch: 6 and batch_num: 29\n",
      "Loss of test set: 0.2666570544242859 at epoch: 6 and batch_num: 30\n",
      "Loss of test set: 0.27680426836013794 at epoch: 6 and batch_num: 31\n",
      "Loss of test set: 0.3122798800468445 at epoch: 6 and batch_num: 32\n",
      "Loss of test set: 0.45945894718170166 at epoch: 6 and batch_num: 33\n",
      "Loss of test set: 0.44567230343818665 at epoch: 6 and batch_num: 34\n",
      "Loss of test set: 0.28122836351394653 at epoch: 6 and batch_num: 35\n",
      "Loss of test set: 0.2825208008289337 at epoch: 6 and batch_num: 36\n",
      "Loss of test set: 0.34222954511642456 at epoch: 6 and batch_num: 37\n",
      "Loss of test set: 0.3624357581138611 at epoch: 6 and batch_num: 38\n",
      "Loss of test set: 0.20466026663780212 at epoch: 6 and batch_num: 39\n",
      "Loss of test set: 0.442197322845459 at epoch: 6 and batch_num: 40\n",
      "Loss of test set: 0.3846612870693207 at epoch: 6 and batch_num: 41\n",
      "Loss of test set: 0.29856255650520325 at epoch: 6 and batch_num: 42\n",
      "Loss of test set: 0.46591973304748535 at epoch: 6 and batch_num: 43\n",
      "Loss of test set: 0.36925607919692993 at epoch: 6 and batch_num: 44\n",
      "Loss of test set: 0.2922029197216034 at epoch: 6 and batch_num: 45\n",
      "Loss of test set: 0.5749589204788208 at epoch: 6 and batch_num: 46\n",
      "Loss of test set: 0.3894177973270416 at epoch: 6 and batch_num: 47\n",
      "Loss of test set: 0.2951556444168091 at epoch: 6 and batch_num: 48\n",
      "Loss of test set: 0.3512105345726013 at epoch: 6 and batch_num: 49\n",
      "Loss of test set: 0.37195348739624023 at epoch: 6 and batch_num: 50\n",
      "Loss of test set: 0.15365101397037506 at epoch: 6 and batch_num: 51\n",
      "Loss of test set: 0.3308592140674591 at epoch: 6 and batch_num: 52\n",
      "Loss of test set: 0.5473326444625854 at epoch: 6 and batch_num: 53\n",
      "Loss of test set: 0.2971950173377991 at epoch: 6 and batch_num: 54\n",
      "Loss of test set: 0.41625386476516724 at epoch: 6 and batch_num: 55\n",
      "Loss of test set: 0.4251647889614105 at epoch: 6 and batch_num: 56\n",
      "Loss of test set: 0.4586024284362793 at epoch: 6 and batch_num: 57\n",
      "Loss of test set: 0.4074658751487732 at epoch: 6 and batch_num: 58\n",
      "Loss of test set: 0.23738448321819305 at epoch: 6 and batch_num: 59\n",
      "Loss of test set: 0.2962256669998169 at epoch: 6 and batch_num: 60\n",
      "Loss of test set: 0.39430543780326843 at epoch: 6 and batch_num: 61\n",
      "Loss of test set: 0.48487478494644165 at epoch: 6 and batch_num: 62\n",
      "Loss of test set: 0.42142486572265625 at epoch: 6 and batch_num: 63\n",
      "Loss of test set: 0.40167319774627686 at epoch: 6 and batch_num: 64\n",
      "Loss of test set: 0.48608073592185974 at epoch: 6 and batch_num: 65\n",
      "Loss of test set: 0.3862024247646332 at epoch: 6 and batch_num: 66\n",
      "Loss of test set: 0.5913408994674683 at epoch: 6 and batch_num: 67\n",
      "Loss of test set: 0.3596545457839966 at epoch: 6 and batch_num: 68\n",
      "Loss of test set: 0.4916653037071228 at epoch: 6 and batch_num: 69\n",
      "Loss of test set: 0.6632755994796753 at epoch: 6 and batch_num: 70\n",
      "Loss of test set: 0.4204137325286865 at epoch: 6 and batch_num: 71\n",
      "Loss of test set: 0.5378056764602661 at epoch: 6 and batch_num: 72\n",
      "Loss of test set: 0.17444750666618347 at epoch: 6 and batch_num: 73\n",
      "Loss of test set: 0.3353622555732727 at epoch: 6 and batch_num: 74\n",
      "Loss of test set: 0.39088717103004456 at epoch: 6 and batch_num: 75\n",
      "Loss of test set: 0.6013780832290649 at epoch: 6 and batch_num: 76\n",
      "Loss of test set: 0.45491325855255127 at epoch: 6 and batch_num: 77\n",
      "Loss of test set: 0.3338966369628906 at epoch: 6 and batch_num: 78\n",
      "Loss of test set: 0.40873637795448303 at epoch: 6 and batch_num: 79\n",
      "Loss of test set: 0.5794352293014526 at epoch: 6 and batch_num: 80\n",
      "Loss of test set: 0.3650416135787964 at epoch: 6 and batch_num: 81\n",
      "Loss of test set: 0.27366209030151367 at epoch: 6 and batch_num: 82\n",
      "Loss of test set: 0.3863908350467682 at epoch: 6 and batch_num: 83\n",
      "Loss of test set: 0.5765551328659058 at epoch: 6 and batch_num: 84\n",
      "Loss of test set: 0.3565048575401306 at epoch: 6 and batch_num: 85\n",
      "Loss of test set: 0.4008721113204956 at epoch: 6 and batch_num: 86\n",
      "Loss of test set: 0.5760959386825562 at epoch: 6 and batch_num: 87\n",
      "Loss of test set: 0.6386725306510925 at epoch: 6 and batch_num: 88\n",
      "Loss of test set: 0.4099111557006836 at epoch: 6 and batch_num: 89\n",
      "Loss of test set: 0.2467072457075119 at epoch: 6 and batch_num: 90\n",
      "Loss of test set: 0.43456846475601196 at epoch: 6 and batch_num: 91\n",
      "Loss of test set: 0.42283502221107483 at epoch: 6 and batch_num: 92\n",
      "Loss of test set: 0.27450883388519287 at epoch: 6 and batch_num: 93\n",
      "Loss of test set: 0.39680880308151245 at epoch: 6 and batch_num: 94\n",
      "Loss of test set: 0.45936334133148193 at epoch: 6 and batch_num: 95\n",
      "Loss of test set: 0.48681217432022095 at epoch: 6 and batch_num: 96\n",
      "Loss of test set: 0.46676886081695557 at epoch: 6 and batch_num: 97\n",
      "Loss of test set: 0.3135150074958801 at epoch: 6 and batch_num: 98\n",
      "Loss of test set: 0.3197813034057617 at epoch: 6 and batch_num: 99\n",
      "Loss of test set: 0.1871909201145172 at epoch: 6 and batch_num: 100\n",
      "Loss of test set: 0.39435875415802 at epoch: 6 and batch_num: 101\n",
      "Loss of test set: 0.6244103908538818 at epoch: 6 and batch_num: 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.3798237442970276 at epoch: 6 and batch_num: 103\n",
      "Loss of test set: 0.24810796976089478 at epoch: 6 and batch_num: 104\n",
      "Loss of test set: 0.5729963779449463 at epoch: 6 and batch_num: 105\n",
      "Loss of test set: 0.5545458197593689 at epoch: 6 and batch_num: 106\n",
      "Loss of test set: 0.30161815881729126 at epoch: 6 and batch_num: 107\n",
      "Loss of test set: 0.39444538950920105 at epoch: 6 and batch_num: 108\n",
      "Loss of test set: 0.30652421712875366 at epoch: 6 and batch_num: 109\n",
      "Loss of test set: 0.5406135320663452 at epoch: 6 and batch_num: 110\n",
      "Loss of test set: 0.25865527987480164 at epoch: 6 and batch_num: 111\n",
      "Loss of test set: 0.33895498514175415 at epoch: 6 and batch_num: 112\n",
      "Loss of test set: 0.38341397047042847 at epoch: 6 and batch_num: 113\n",
      "Loss of test set: 0.4046657681465149 at epoch: 6 and batch_num: 114\n",
      "Loss of test set: 0.490147203207016 at epoch: 6 and batch_num: 115\n",
      "Loss of test set: 0.3455829620361328 at epoch: 6 and batch_num: 116\n",
      "Loss of test set: 0.35513344407081604 at epoch: 6 and batch_num: 117\n",
      "Loss of test set: 0.404500812292099 at epoch: 6 and batch_num: 118\n",
      "Loss of test set: 0.43352097272872925 at epoch: 6 and batch_num: 119\n",
      "Loss of test set: 0.5013582110404968 at epoch: 6 and batch_num: 120\n",
      "Loss of test set: 0.44278189539909363 at epoch: 6 and batch_num: 121\n",
      "Loss of test set: 0.35099172592163086 at epoch: 6 and batch_num: 122\n",
      "Loss of test set: 0.25399863719940186 at epoch: 6 and batch_num: 123\n",
      "Loss of test set: 0.21910600364208221 at epoch: 6 and batch_num: 124\n",
      "Loss of test set: 0.3907015919685364 at epoch: 6 and batch_num: 125\n",
      "Loss of test set: 0.3049768805503845 at epoch: 6 and batch_num: 126\n",
      "Loss of test set: 0.3875061273574829 at epoch: 6 and batch_num: 127\n",
      "Loss of test set: 0.3946268558502197 at epoch: 6 and batch_num: 128\n",
      "Loss of test set: 0.3134526312351227 at epoch: 6 and batch_num: 129\n",
      "Loss of test set: 0.4060097336769104 at epoch: 6 and batch_num: 130\n",
      "Loss of test set: 0.2712595760822296 at epoch: 6 and batch_num: 131\n",
      "Loss of test set: 0.3771420121192932 at epoch: 6 and batch_num: 132\n",
      "Loss of test set: 0.40421587228775024 at epoch: 6 and batch_num: 133\n",
      "Loss of test set: 0.31678736209869385 at epoch: 6 and batch_num: 134\n",
      "Loss of test set: 0.31078851222991943 at epoch: 6 and batch_num: 135\n",
      "Loss of test set: 0.387154757976532 at epoch: 6 and batch_num: 136\n",
      "Loss of test set: 0.2755803167819977 at epoch: 6 and batch_num: 137\n",
      "Loss of test set: 0.28389227390289307 at epoch: 6 and batch_num: 138\n",
      "Loss of test set: 0.502034604549408 at epoch: 6 and batch_num: 139\n",
      "Loss of test set: 0.4306405484676361 at epoch: 6 and batch_num: 140\n",
      "Loss of test set: 0.3121781349182129 at epoch: 6 and batch_num: 141\n",
      "Loss of test set: 0.5912845730781555 at epoch: 6 and batch_num: 142\n",
      "Loss of test set: 0.3513818085193634 at epoch: 6 and batch_num: 143\n",
      "Loss of test set: 0.2838243246078491 at epoch: 6 and batch_num: 144\n",
      "Loss of test set: 0.5210933089256287 at epoch: 6 and batch_num: 145\n",
      "Loss of test set: 0.4702991247177124 at epoch: 6 and batch_num: 146\n",
      "Loss of test set: 0.46215781569480896 at epoch: 6 and batch_num: 147\n",
      "Loss of test set: 0.38890454173088074 at epoch: 6 and batch_num: 148\n",
      "Loss of test set: 0.25991570949554443 at epoch: 6 and batch_num: 149\n",
      "Loss of test set: 0.5730371475219727 at epoch: 6 and batch_num: 150\n",
      "Loss of test set: 0.6054704189300537 at epoch: 6 and batch_num: 151\n",
      "Loss of test set: 0.451429545879364 at epoch: 6 and batch_num: 152\n",
      "Loss of test set: 0.28957438468933105 at epoch: 6 and batch_num: 153\n",
      "Loss of test set: 0.4081454277038574 at epoch: 6 and batch_num: 154\n",
      "Loss of test set: 0.45458877086639404 at epoch: 6 and batch_num: 155\n",
      "Loss of test set: 0.4100916385650635 at epoch: 6 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8591\n",
      "Loss of train set: 0.28835025429725647 at epoch: 7 and batch_num: 0\n",
      "Loss of train set: 0.1964443325996399 at epoch: 7 and batch_num: 1\n",
      "Loss of train set: 0.29004520177841187 at epoch: 7 and batch_num: 2\n",
      "Loss of train set: 0.3826732039451599 at epoch: 7 and batch_num: 3\n",
      "Loss of train set: 0.28333157300949097 at epoch: 7 and batch_num: 4\n",
      "Loss of train set: 0.2804170846939087 at epoch: 7 and batch_num: 5\n",
      "Loss of train set: 0.270912230014801 at epoch: 7 and batch_num: 6\n",
      "Loss of train set: 0.29981642961502075 at epoch: 7 and batch_num: 7\n",
      "Loss of train set: 0.3509710133075714 at epoch: 7 and batch_num: 8\n",
      "Loss of train set: 0.3324848413467407 at epoch: 7 and batch_num: 9\n",
      "Loss of train set: 0.16756334900856018 at epoch: 7 and batch_num: 10\n",
      "Loss of train set: 0.27960559725761414 at epoch: 7 and batch_num: 11\n",
      "Loss of train set: 0.34859412908554077 at epoch: 7 and batch_num: 12\n",
      "Loss of train set: 0.4220251441001892 at epoch: 7 and batch_num: 13\n",
      "Loss of train set: 0.3606894016265869 at epoch: 7 and batch_num: 14\n",
      "Loss of train set: 0.47666236758232117 at epoch: 7 and batch_num: 15\n",
      "Loss of train set: 0.3346153199672699 at epoch: 7 and batch_num: 16\n",
      "Loss of train set: 0.2750036120414734 at epoch: 7 and batch_num: 17\n",
      "Loss of train set: 0.3187040686607361 at epoch: 7 and batch_num: 18\n",
      "Loss of train set: 0.20673128962516785 at epoch: 7 and batch_num: 19\n",
      "Loss of train set: 0.2308388352394104 at epoch: 7 and batch_num: 20\n",
      "Loss of train set: 0.5382785201072693 at epoch: 7 and batch_num: 21\n",
      "Loss of train set: 0.29125452041625977 at epoch: 7 and batch_num: 22\n",
      "Loss of train set: 0.3476438522338867 at epoch: 7 and batch_num: 23\n",
      "Loss of train set: 0.24385099112987518 at epoch: 7 and batch_num: 24\n",
      "Loss of train set: 0.28826701641082764 at epoch: 7 and batch_num: 25\n",
      "Loss of train set: 0.37112268805503845 at epoch: 7 and batch_num: 26\n",
      "Loss of train set: 0.4982491731643677 at epoch: 7 and batch_num: 27\n",
      "Loss of train set: 0.5029577612876892 at epoch: 7 and batch_num: 28\n",
      "Loss of train set: 0.49195605516433716 at epoch: 7 and batch_num: 29\n",
      "Loss of train set: 0.41328397393226624 at epoch: 7 and batch_num: 30\n",
      "Loss of train set: 0.383088618516922 at epoch: 7 and batch_num: 31\n",
      "Loss of train set: 0.19964155554771423 at epoch: 7 and batch_num: 32\n",
      "Loss of train set: 0.24130117893218994 at epoch: 7 and batch_num: 33\n",
      "Loss of train set: 0.2852943241596222 at epoch: 7 and batch_num: 34\n",
      "Loss of train set: 0.38092711567878723 at epoch: 7 and batch_num: 35\n",
      "Loss of train set: 0.2770102322101593 at epoch: 7 and batch_num: 36\n",
      "Loss of train set: 0.31761133670806885 at epoch: 7 and batch_num: 37\n",
      "Loss of train set: 0.2620360255241394 at epoch: 7 and batch_num: 38\n",
      "Loss of train set: 0.38464754819869995 at epoch: 7 and batch_num: 39\n",
      "Loss of train set: 0.3680441379547119 at epoch: 7 and batch_num: 40\n",
      "Loss of train set: 0.2676616311073303 at epoch: 7 and batch_num: 41\n",
      "Loss of train set: 0.36896342039108276 at epoch: 7 and batch_num: 42\n",
      "Loss of train set: 0.4099715054035187 at epoch: 7 and batch_num: 43\n",
      "Loss of train set: 0.36238640546798706 at epoch: 7 and batch_num: 44\n",
      "Loss of train set: 0.2959895431995392 at epoch: 7 and batch_num: 45\n",
      "Loss of train set: 0.4131850600242615 at epoch: 7 and batch_num: 46\n",
      "Loss of train set: 0.1702595055103302 at epoch: 7 and batch_num: 47\n",
      "Loss of train set: 0.553735613822937 at epoch: 7 and batch_num: 48\n",
      "Loss of train set: 0.45919567346572876 at epoch: 7 and batch_num: 49\n",
      "Loss of train set: 0.25576987862586975 at epoch: 7 and batch_num: 50\n",
      "Loss of train set: 0.6856095790863037 at epoch: 7 and batch_num: 51\n",
      "Loss of train set: 0.3298099637031555 at epoch: 7 and batch_num: 52\n",
      "Loss of train set: 0.42183375358581543 at epoch: 7 and batch_num: 53\n",
      "Loss of train set: 0.2548462152481079 at epoch: 7 and batch_num: 54\n",
      "Loss of train set: 0.4211454689502716 at epoch: 7 and batch_num: 55\n",
      "Loss of train set: 0.24116751551628113 at epoch: 7 and batch_num: 56\n",
      "Loss of train set: 0.3213239908218384 at epoch: 7 and batch_num: 57\n",
      "Loss of train set: 0.34100478887557983 at epoch: 7 and batch_num: 58\n",
      "Loss of train set: 0.2478022575378418 at epoch: 7 and batch_num: 59\n",
      "Loss of train set: 0.3054867684841156 at epoch: 7 and batch_num: 60\n",
      "Loss of train set: 0.4061506390571594 at epoch: 7 and batch_num: 61\n",
      "Loss of train set: 0.3365315794944763 at epoch: 7 and batch_num: 62\n",
      "Loss of train set: 0.3177642524242401 at epoch: 7 and batch_num: 63\n",
      "Loss of train set: 0.3091926574707031 at epoch: 7 and batch_num: 64\n",
      "Loss of train set: 0.4484459161758423 at epoch: 7 and batch_num: 65\n",
      "Loss of train set: 0.2778435945510864 at epoch: 7 and batch_num: 66\n",
      "Loss of train set: 0.36490410566329956 at epoch: 7 and batch_num: 67\n",
      "Loss of train set: 0.18247869610786438 at epoch: 7 and batch_num: 68\n",
      "Loss of train set: 0.3219418525695801 at epoch: 7 and batch_num: 69\n",
      "Loss of train set: 0.20525158941745758 at epoch: 7 and batch_num: 70\n",
      "Loss of train set: 0.4157334864139557 at epoch: 7 and batch_num: 71\n",
      "Loss of train set: 0.3581346869468689 at epoch: 7 and batch_num: 72\n",
      "Loss of train set: 0.3225768804550171 at epoch: 7 and batch_num: 73\n",
      "Loss of train set: 0.43380221724510193 at epoch: 7 and batch_num: 74\n",
      "Loss of train set: 0.40590518712997437 at epoch: 7 and batch_num: 75\n",
      "Loss of train set: 0.41236767172813416 at epoch: 7 and batch_num: 76\n",
      "Loss of train set: 0.4256264567375183 at epoch: 7 and batch_num: 77\n",
      "Loss of train set: 0.29917383193969727 at epoch: 7 and batch_num: 78\n",
      "Loss of train set: 0.24145254492759705 at epoch: 7 and batch_num: 79\n",
      "Loss of train set: 0.28293538093566895 at epoch: 7 and batch_num: 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3760216236114502 at epoch: 7 and batch_num: 81\n",
      "Loss of train set: 0.37164121866226196 at epoch: 7 and batch_num: 82\n",
      "Loss of train set: 0.21096408367156982 at epoch: 7 and batch_num: 83\n",
      "Loss of train set: 0.26310449838638306 at epoch: 7 and batch_num: 84\n",
      "Loss of train set: 0.29119592905044556 at epoch: 7 and batch_num: 85\n",
      "Loss of train set: 0.17233671247959137 at epoch: 7 and batch_num: 86\n",
      "Loss of train set: 0.2623205780982971 at epoch: 7 and batch_num: 87\n",
      "Loss of train set: 0.3893979787826538 at epoch: 7 and batch_num: 88\n",
      "Loss of train set: 0.22940143942832947 at epoch: 7 and batch_num: 89\n",
      "Loss of train set: 0.31205928325653076 at epoch: 7 and batch_num: 90\n",
      "Loss of train set: 0.2235686182975769 at epoch: 7 and batch_num: 91\n",
      "Loss of train set: 0.2613048553466797 at epoch: 7 and batch_num: 92\n",
      "Loss of train set: 0.4117978513240814 at epoch: 7 and batch_num: 93\n",
      "Loss of train set: 0.41087472438812256 at epoch: 7 and batch_num: 94\n",
      "Loss of train set: 0.30525121092796326 at epoch: 7 and batch_num: 95\n",
      "Loss of train set: 0.330079585313797 at epoch: 7 and batch_num: 96\n",
      "Loss of train set: 0.34844130277633667 at epoch: 7 and batch_num: 97\n",
      "Loss of train set: 0.2416752427816391 at epoch: 7 and batch_num: 98\n",
      "Loss of train set: 0.259463906288147 at epoch: 7 and batch_num: 99\n",
      "Loss of train set: 0.26883840560913086 at epoch: 7 and batch_num: 100\n",
      "Loss of train set: 0.4649844169616699 at epoch: 7 and batch_num: 101\n",
      "Loss of train set: 0.2446640133857727 at epoch: 7 and batch_num: 102\n",
      "Loss of train set: 0.24765770137310028 at epoch: 7 and batch_num: 103\n",
      "Loss of train set: 0.24292127788066864 at epoch: 7 and batch_num: 104\n",
      "Loss of train set: 0.21448200941085815 at epoch: 7 and batch_num: 105\n",
      "Loss of train set: 0.34902942180633545 at epoch: 7 and batch_num: 106\n",
      "Loss of train set: 0.3556273281574249 at epoch: 7 and batch_num: 107\n",
      "Loss of train set: 0.26505815982818604 at epoch: 7 and batch_num: 108\n",
      "Loss of train set: 0.3462364673614502 at epoch: 7 and batch_num: 109\n",
      "Loss of train set: 0.2013096958398819 at epoch: 7 and batch_num: 110\n",
      "Loss of train set: 0.5090063214302063 at epoch: 7 and batch_num: 111\n",
      "Loss of train set: 0.21725904941558838 at epoch: 7 and batch_num: 112\n",
      "Loss of train set: 0.22128768265247345 at epoch: 7 and batch_num: 113\n",
      "Loss of train set: 0.256375789642334 at epoch: 7 and batch_num: 114\n",
      "Loss of train set: 0.47915107011795044 at epoch: 7 and batch_num: 115\n",
      "Loss of train set: 0.5367326140403748 at epoch: 7 and batch_num: 116\n",
      "Loss of train set: 0.2742944061756134 at epoch: 7 and batch_num: 117\n",
      "Loss of train set: 0.2795504033565521 at epoch: 7 and batch_num: 118\n",
      "Loss of train set: 0.3840829133987427 at epoch: 7 and batch_num: 119\n",
      "Loss of train set: 0.21084928512573242 at epoch: 7 and batch_num: 120\n",
      "Loss of train set: 0.2868819236755371 at epoch: 7 and batch_num: 121\n",
      "Loss of train set: 0.30240970849990845 at epoch: 7 and batch_num: 122\n",
      "Loss of train set: 0.31434157490730286 at epoch: 7 and batch_num: 123\n",
      "Loss of train set: 0.24206703901290894 at epoch: 7 and batch_num: 124\n",
      "Loss of train set: 0.22921407222747803 at epoch: 7 and batch_num: 125\n",
      "Loss of train set: 0.27869775891304016 at epoch: 7 and batch_num: 126\n",
      "Loss of train set: 0.3170695900917053 at epoch: 7 and batch_num: 127\n",
      "Loss of train set: 0.27132996916770935 at epoch: 7 and batch_num: 128\n",
      "Loss of train set: 0.3911307454109192 at epoch: 7 and batch_num: 129\n",
      "Loss of train set: 0.23935487866401672 at epoch: 7 and batch_num: 130\n",
      "Loss of train set: 0.3248462677001953 at epoch: 7 and batch_num: 131\n",
      "Loss of train set: 0.2901204824447632 at epoch: 7 and batch_num: 132\n",
      "Loss of train set: 0.22872117161750793 at epoch: 7 and batch_num: 133\n",
      "Loss of train set: 0.17962265014648438 at epoch: 7 and batch_num: 134\n",
      "Loss of train set: 0.3111492395401001 at epoch: 7 and batch_num: 135\n",
      "Loss of train set: 0.33518344163894653 at epoch: 7 and batch_num: 136\n",
      "Loss of train set: 0.31702935695648193 at epoch: 7 and batch_num: 137\n",
      "Loss of train set: 0.22642678022384644 at epoch: 7 and batch_num: 138\n",
      "Loss of train set: 0.40549933910369873 at epoch: 7 and batch_num: 139\n",
      "Loss of train set: 0.21783483028411865 at epoch: 7 and batch_num: 140\n",
      "Loss of train set: 0.3869946300983429 at epoch: 7 and batch_num: 141\n",
      "Loss of train set: 0.12379393726587296 at epoch: 7 and batch_num: 142\n",
      "Loss of train set: 0.39143800735473633 at epoch: 7 and batch_num: 143\n",
      "Loss of train set: 0.4060867428779602 at epoch: 7 and batch_num: 144\n",
      "Loss of train set: 0.21800605952739716 at epoch: 7 and batch_num: 145\n",
      "Loss of train set: 0.3654470145702362 at epoch: 7 and batch_num: 146\n",
      "Loss of train set: 0.43407630920410156 at epoch: 7 and batch_num: 147\n",
      "Loss of train set: 0.2438623458147049 at epoch: 7 and batch_num: 148\n",
      "Loss of train set: 0.3388545513153076 at epoch: 7 and batch_num: 149\n",
      "Loss of train set: 0.19132426381111145 at epoch: 7 and batch_num: 150\n",
      "Loss of train set: 0.2816413938999176 at epoch: 7 and batch_num: 151\n",
      "Loss of train set: 0.31257039308547974 at epoch: 7 and batch_num: 152\n",
      "Loss of train set: 0.27043840289115906 at epoch: 7 and batch_num: 153\n",
      "Loss of train set: 0.32014739513397217 at epoch: 7 and batch_num: 154\n",
      "Loss of train set: 0.4454277753829956 at epoch: 7 and batch_num: 155\n",
      "Loss of train set: 0.2825254797935486 at epoch: 7 and batch_num: 156\n",
      "Loss of train set: 0.4082266092300415 at epoch: 7 and batch_num: 157\n",
      "Loss of train set: 0.46152663230895996 at epoch: 7 and batch_num: 158\n",
      "Loss of train set: 0.2812497615814209 at epoch: 7 and batch_num: 159\n",
      "Loss of train set: 0.4050995707511902 at epoch: 7 and batch_num: 160\n",
      "Loss of train set: 0.3618490993976593 at epoch: 7 and batch_num: 161\n",
      "Loss of train set: 0.2452215850353241 at epoch: 7 and batch_num: 162\n",
      "Loss of train set: 0.3602835237979889 at epoch: 7 and batch_num: 163\n",
      "Loss of train set: 0.322242796421051 at epoch: 7 and batch_num: 164\n",
      "Loss of train set: 0.2533000707626343 at epoch: 7 and batch_num: 165\n",
      "Loss of train set: 0.28428325057029724 at epoch: 7 and batch_num: 166\n",
      "Loss of train set: 0.3544137477874756 at epoch: 7 and batch_num: 167\n",
      "Loss of train set: 0.3623563349246979 at epoch: 7 and batch_num: 168\n",
      "Loss of train set: 0.4739120602607727 at epoch: 7 and batch_num: 169\n",
      "Loss of train set: 0.3470221757888794 at epoch: 7 and batch_num: 170\n",
      "Loss of train set: 0.2614651322364807 at epoch: 7 and batch_num: 171\n",
      "Loss of train set: 0.28802764415740967 at epoch: 7 and batch_num: 172\n",
      "Loss of train set: 0.4893454909324646 at epoch: 7 and batch_num: 173\n",
      "Loss of train set: 0.17658990621566772 at epoch: 7 and batch_num: 174\n",
      "Loss of train set: 0.5052510499954224 at epoch: 7 and batch_num: 175\n",
      "Loss of train set: 0.2844700813293457 at epoch: 7 and batch_num: 176\n",
      "Loss of train set: 0.3565811514854431 at epoch: 7 and batch_num: 177\n",
      "Loss of train set: 0.312630295753479 at epoch: 7 and batch_num: 178\n",
      "Loss of train set: 0.2849799394607544 at epoch: 7 and batch_num: 179\n",
      "Loss of train set: 0.3324629068374634 at epoch: 7 and batch_num: 180\n",
      "Loss of train set: 0.21499772369861603 at epoch: 7 and batch_num: 181\n",
      "Loss of train set: 0.30157122015953064 at epoch: 7 and batch_num: 182\n",
      "Loss of train set: 0.3553137183189392 at epoch: 7 and batch_num: 183\n",
      "Loss of train set: 0.33660614490509033 at epoch: 7 and batch_num: 184\n",
      "Loss of train set: 0.4628848433494568 at epoch: 7 and batch_num: 185\n",
      "Loss of train set: 0.18555331230163574 at epoch: 7 and batch_num: 186\n",
      "Loss of train set: 0.5164496898651123 at epoch: 7 and batch_num: 187\n",
      "Loss of train set: 0.42069363594055176 at epoch: 7 and batch_num: 188\n",
      "Loss of train set: 0.2748965620994568 at epoch: 7 and batch_num: 189\n",
      "Loss of train set: 0.42866602540016174 at epoch: 7 and batch_num: 190\n",
      "Loss of train set: 0.45454370975494385 at epoch: 7 and batch_num: 191\n",
      "Loss of train set: 0.436165988445282 at epoch: 7 and batch_num: 192\n",
      "Loss of train set: 0.40304285287857056 at epoch: 7 and batch_num: 193\n",
      "Loss of train set: 0.5147755146026611 at epoch: 7 and batch_num: 194\n",
      "Loss of train set: 0.44146013259887695 at epoch: 7 and batch_num: 195\n",
      "Loss of train set: 0.29942283034324646 at epoch: 7 and batch_num: 196\n",
      "Loss of train set: 0.4049198627471924 at epoch: 7 and batch_num: 197\n",
      "Loss of train set: 0.4072688817977905 at epoch: 7 and batch_num: 198\n",
      "Loss of train set: 0.4462439715862274 at epoch: 7 and batch_num: 199\n",
      "Loss of train set: 0.2184194028377533 at epoch: 7 and batch_num: 200\n",
      "Loss of train set: 0.2871946096420288 at epoch: 7 and batch_num: 201\n",
      "Loss of train set: 0.4145772457122803 at epoch: 7 and batch_num: 202\n",
      "Loss of train set: 0.22528502345085144 at epoch: 7 and batch_num: 203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.31883689761161804 at epoch: 7 and batch_num: 204\n",
      "Loss of train set: 0.399485319852829 at epoch: 7 and batch_num: 205\n",
      "Loss of train set: 0.25864946842193604 at epoch: 7 and batch_num: 206\n",
      "Loss of train set: 0.36289045214653015 at epoch: 7 and batch_num: 207\n",
      "Loss of train set: 0.23436683416366577 at epoch: 7 and batch_num: 208\n",
      "Loss of train set: 0.45306965708732605 at epoch: 7 and batch_num: 209\n",
      "Loss of train set: 0.3423577547073364 at epoch: 7 and batch_num: 210\n",
      "Loss of train set: 0.23823946714401245 at epoch: 7 and batch_num: 211\n",
      "Loss of train set: 0.355516254901886 at epoch: 7 and batch_num: 212\n",
      "Loss of train set: 0.2763599455356598 at epoch: 7 and batch_num: 213\n",
      "Loss of train set: 0.20875950157642365 at epoch: 7 and batch_num: 214\n",
      "Loss of train set: 0.4268336296081543 at epoch: 7 and batch_num: 215\n",
      "Loss of train set: 0.29037123918533325 at epoch: 7 and batch_num: 216\n",
      "Loss of train set: 0.4131734371185303 at epoch: 7 and batch_num: 217\n",
      "Loss of train set: 0.4200844168663025 at epoch: 7 and batch_num: 218\n",
      "Loss of train set: 0.1562095284461975 at epoch: 7 and batch_num: 219\n",
      "Loss of train set: 0.2172095626592636 at epoch: 7 and batch_num: 220\n",
      "Loss of train set: 0.3776938021183014 at epoch: 7 and batch_num: 221\n",
      "Loss of train set: 0.5348740816116333 at epoch: 7 and batch_num: 222\n",
      "Loss of train set: 0.3705487847328186 at epoch: 7 and batch_num: 223\n",
      "Loss of train set: 0.31963783502578735 at epoch: 7 and batch_num: 224\n",
      "Loss of train set: 0.36779001355171204 at epoch: 7 and batch_num: 225\n",
      "Loss of train set: 0.2733060419559479 at epoch: 7 and batch_num: 226\n",
      "Loss of train set: 0.5652660727500916 at epoch: 7 and batch_num: 227\n",
      "Loss of train set: 0.2591034770011902 at epoch: 7 and batch_num: 228\n",
      "Loss of train set: 0.2135126292705536 at epoch: 7 and batch_num: 229\n",
      "Loss of train set: 0.39313551783561707 at epoch: 7 and batch_num: 230\n",
      "Loss of train set: 0.28986093401908875 at epoch: 7 and batch_num: 231\n",
      "Loss of train set: 0.35710030794143677 at epoch: 7 and batch_num: 232\n",
      "Loss of train set: 0.20562049746513367 at epoch: 7 and batch_num: 233\n",
      "Loss of train set: 0.35983365774154663 at epoch: 7 and batch_num: 234\n",
      "Loss of train set: 0.2996670603752136 at epoch: 7 and batch_num: 235\n",
      "Loss of train set: 0.3084588050842285 at epoch: 7 and batch_num: 236\n",
      "Loss of train set: 0.23658911883831024 at epoch: 7 and batch_num: 237\n",
      "Loss of train set: 0.27013450860977173 at epoch: 7 and batch_num: 238\n",
      "Loss of train set: 0.21435636281967163 at epoch: 7 and batch_num: 239\n",
      "Loss of train set: 0.36067771911621094 at epoch: 7 and batch_num: 240\n",
      "Loss of train set: 0.3010827898979187 at epoch: 7 and batch_num: 241\n",
      "Loss of train set: 0.394870400428772 at epoch: 7 and batch_num: 242\n",
      "Loss of train set: 0.41043126583099365 at epoch: 7 and batch_num: 243\n",
      "Loss of train set: 0.24718071520328522 at epoch: 7 and batch_num: 244\n",
      "Loss of train set: 0.2729983627796173 at epoch: 7 and batch_num: 245\n",
      "Loss of train set: 0.284831702709198 at epoch: 7 and batch_num: 246\n",
      "Loss of train set: 0.18114106357097626 at epoch: 7 and batch_num: 247\n",
      "Loss of train set: 0.46143877506256104 at epoch: 7 and batch_num: 248\n",
      "Loss of train set: 0.38042566180229187 at epoch: 7 and batch_num: 249\n",
      "Loss of train set: 0.3591763973236084 at epoch: 7 and batch_num: 250\n",
      "Loss of train set: 0.4470057487487793 at epoch: 7 and batch_num: 251\n",
      "Loss of train set: 0.3301433324813843 at epoch: 7 and batch_num: 252\n",
      "Loss of train set: 0.386039674282074 at epoch: 7 and batch_num: 253\n",
      "Loss of train set: 0.32962822914123535 at epoch: 7 and batch_num: 254\n",
      "Loss of train set: 0.2992295026779175 at epoch: 7 and batch_num: 255\n",
      "Loss of train set: 0.3198561370372772 at epoch: 7 and batch_num: 256\n",
      "Loss of train set: 0.28037112951278687 at epoch: 7 and batch_num: 257\n",
      "Loss of train set: 0.3498155474662781 at epoch: 7 and batch_num: 258\n",
      "Loss of train set: 0.2403048872947693 at epoch: 7 and batch_num: 259\n",
      "Loss of train set: 0.39692893624305725 at epoch: 7 and batch_num: 260\n",
      "Loss of train set: 0.25164830684661865 at epoch: 7 and batch_num: 261\n",
      "Loss of train set: 0.2760518789291382 at epoch: 7 and batch_num: 262\n",
      "Loss of train set: 0.40468043088912964 at epoch: 7 and batch_num: 263\n",
      "Loss of train set: 0.39597123861312866 at epoch: 7 and batch_num: 264\n",
      "Loss of train set: 0.31629520654678345 at epoch: 7 and batch_num: 265\n",
      "Loss of train set: 0.34504061937332153 at epoch: 7 and batch_num: 266\n",
      "Loss of train set: 0.338756799697876 at epoch: 7 and batch_num: 267\n",
      "Loss of train set: 0.23028719425201416 at epoch: 7 and batch_num: 268\n",
      "Loss of train set: 0.4414151906967163 at epoch: 7 and batch_num: 269\n",
      "Loss of train set: 0.368303120136261 at epoch: 7 and batch_num: 270\n",
      "Loss of train set: 0.4022147059440613 at epoch: 7 and batch_num: 271\n",
      "Loss of train set: 0.2661939859390259 at epoch: 7 and batch_num: 272\n",
      "Loss of train set: 0.45863616466522217 at epoch: 7 and batch_num: 273\n",
      "Loss of train set: 0.2548029124736786 at epoch: 7 and batch_num: 274\n",
      "Loss of train set: 0.2649708390235901 at epoch: 7 and batch_num: 275\n",
      "Loss of train set: 0.7067183256149292 at epoch: 7 and batch_num: 276\n",
      "Loss of train set: 0.22269703447818756 at epoch: 7 and batch_num: 277\n",
      "Loss of train set: 0.2784031629562378 at epoch: 7 and batch_num: 278\n",
      "Loss of train set: 0.4548516571521759 at epoch: 7 and batch_num: 279\n",
      "Loss of train set: 0.2657667398452759 at epoch: 7 and batch_num: 280\n",
      "Loss of train set: 0.33982598781585693 at epoch: 7 and batch_num: 281\n",
      "Loss of train set: 0.1811395287513733 at epoch: 7 and batch_num: 282\n",
      "Loss of train set: 0.5252125859260559 at epoch: 7 and batch_num: 283\n",
      "Loss of train set: 0.2286369800567627 at epoch: 7 and batch_num: 284\n",
      "Loss of train set: 0.3560657203197479 at epoch: 7 and batch_num: 285\n",
      "Loss of train set: 0.41739383339881897 at epoch: 7 and batch_num: 286\n",
      "Loss of train set: 0.2817237377166748 at epoch: 7 and batch_num: 287\n",
      "Loss of train set: 0.13492509722709656 at epoch: 7 and batch_num: 288\n",
      "Loss of train set: 0.27491286396980286 at epoch: 7 and batch_num: 289\n",
      "Loss of train set: 0.3631245493888855 at epoch: 7 and batch_num: 290\n",
      "Loss of train set: 0.22250519692897797 at epoch: 7 and batch_num: 291\n",
      "Loss of train set: 0.1258532851934433 at epoch: 7 and batch_num: 292\n",
      "Loss of train set: 0.45878520607948303 at epoch: 7 and batch_num: 293\n",
      "Loss of train set: 0.1396920382976532 at epoch: 7 and batch_num: 294\n",
      "Loss of train set: 0.33121562004089355 at epoch: 7 and batch_num: 295\n",
      "Loss of train set: 0.44414854049682617 at epoch: 7 and batch_num: 296\n",
      "Loss of train set: 0.2880391478538513 at epoch: 7 and batch_num: 297\n",
      "Loss of train set: 0.31389230489730835 at epoch: 7 and batch_num: 298\n",
      "Loss of train set: 0.336439847946167 at epoch: 7 and batch_num: 299\n",
      "Loss of train set: 0.24596858024597168 at epoch: 7 and batch_num: 300\n",
      "Loss of train set: 0.27898427844047546 at epoch: 7 and batch_num: 301\n",
      "Loss of train set: 0.26772788166999817 at epoch: 7 and batch_num: 302\n",
      "Loss of train set: 0.37876182794570923 at epoch: 7 and batch_num: 303\n",
      "Loss of train set: 0.3009941875934601 at epoch: 7 and batch_num: 304\n",
      "Loss of train set: 0.3763282895088196 at epoch: 7 and batch_num: 305\n",
      "Loss of train set: 0.4109157621860504 at epoch: 7 and batch_num: 306\n",
      "Loss of train set: 0.3221321702003479 at epoch: 7 and batch_num: 307\n",
      "Loss of train set: 0.2703457176685333 at epoch: 7 and batch_num: 308\n",
      "Loss of train set: 0.3383370637893677 at epoch: 7 and batch_num: 309\n",
      "Loss of train set: 0.2840338349342346 at epoch: 7 and batch_num: 310\n",
      "Loss of train set: 0.4606943726539612 at epoch: 7 and batch_num: 311\n",
      "Loss of train set: 0.32076123356819153 at epoch: 7 and batch_num: 312\n",
      "Loss of train set: 0.36086100339889526 at epoch: 7 and batch_num: 313\n",
      "Loss of train set: 0.2659873366355896 at epoch: 7 and batch_num: 314\n",
      "Loss of train set: 0.414717435836792 at epoch: 7 and batch_num: 315\n",
      "Loss of train set: 0.38028183579444885 at epoch: 7 and batch_num: 316\n",
      "Loss of train set: 0.48245272040367126 at epoch: 7 and batch_num: 317\n",
      "Loss of train set: 0.29959189891815186 at epoch: 7 and batch_num: 318\n",
      "Loss of train set: 0.3122047185897827 at epoch: 7 and batch_num: 319\n",
      "Loss of train set: 0.37821805477142334 at epoch: 7 and batch_num: 320\n",
      "Loss of train set: 0.4474080204963684 at epoch: 7 and batch_num: 321\n",
      "Loss of train set: 0.25291740894317627 at epoch: 7 and batch_num: 322\n",
      "Loss of train set: 0.3794211745262146 at epoch: 7 and batch_num: 323\n",
      "Loss of train set: 0.3449915647506714 at epoch: 7 and batch_num: 324\n",
      "Loss of train set: 0.36607950925827026 at epoch: 7 and batch_num: 325\n",
      "Loss of train set: 0.3503122925758362 at epoch: 7 and batch_num: 326\n",
      "Loss of train set: 0.14146924018859863 at epoch: 7 and batch_num: 327\n",
      "Loss of train set: 0.3806302547454834 at epoch: 7 and batch_num: 328\n",
      "Loss of train set: 0.3861309885978699 at epoch: 7 and batch_num: 329\n",
      "Loss of train set: 0.1373831033706665 at epoch: 7 and batch_num: 330\n",
      "Loss of train set: 0.3986325263977051 at epoch: 7 and batch_num: 331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.22456064820289612 at epoch: 7 and batch_num: 332\n",
      "Loss of train set: 0.316756933927536 at epoch: 7 and batch_num: 333\n",
      "Loss of train set: 0.40990036725997925 at epoch: 7 and batch_num: 334\n",
      "Loss of train set: 0.309450626373291 at epoch: 7 and batch_num: 335\n",
      "Loss of train set: 0.29312562942504883 at epoch: 7 and batch_num: 336\n",
      "Loss of train set: 0.4047669768333435 at epoch: 7 and batch_num: 337\n",
      "Loss of train set: 0.3727654218673706 at epoch: 7 and batch_num: 338\n",
      "Loss of train set: 0.4352824091911316 at epoch: 7 and batch_num: 339\n",
      "Loss of train set: 0.31604474782943726 at epoch: 7 and batch_num: 340\n",
      "Loss of train set: 0.3714630603790283 at epoch: 7 and batch_num: 341\n",
      "Loss of train set: 0.31009095907211304 at epoch: 7 and batch_num: 342\n",
      "Loss of train set: 0.46222013235092163 at epoch: 7 and batch_num: 343\n",
      "Loss of train set: 0.25024986267089844 at epoch: 7 and batch_num: 344\n",
      "Loss of train set: 0.3167904019355774 at epoch: 7 and batch_num: 345\n",
      "Loss of train set: 0.45473018288612366 at epoch: 7 and batch_num: 346\n",
      "Loss of train set: 0.38701924681663513 at epoch: 7 and batch_num: 347\n",
      "Loss of train set: 0.18263530731201172 at epoch: 7 and batch_num: 348\n",
      "Loss of train set: 0.27725523710250854 at epoch: 7 and batch_num: 349\n",
      "Loss of train set: 0.3076642155647278 at epoch: 7 and batch_num: 350\n",
      "Loss of train set: 0.26617398858070374 at epoch: 7 and batch_num: 351\n",
      "Loss of train set: 0.18714386224746704 at epoch: 7 and batch_num: 352\n",
      "Loss of train set: 0.31445199251174927 at epoch: 7 and batch_num: 353\n",
      "Loss of train set: 0.4902319312095642 at epoch: 7 and batch_num: 354\n",
      "Loss of train set: 0.37633511424064636 at epoch: 7 and batch_num: 355\n",
      "Loss of train set: 0.31915563344955444 at epoch: 7 and batch_num: 356\n",
      "Loss of train set: 0.23496118187904358 at epoch: 7 and batch_num: 357\n",
      "Loss of train set: 0.30530789494514465 at epoch: 7 and batch_num: 358\n",
      "Loss of train set: 0.4357421398162842 at epoch: 7 and batch_num: 359\n",
      "Loss of train set: 0.37856531143188477 at epoch: 7 and batch_num: 360\n",
      "Loss of train set: 0.49946779012680054 at epoch: 7 and batch_num: 361\n",
      "Loss of train set: 0.3060145676136017 at epoch: 7 and batch_num: 362\n",
      "Loss of train set: 0.21180786192417145 at epoch: 7 and batch_num: 363\n",
      "Loss of train set: 0.295215368270874 at epoch: 7 and batch_num: 364\n",
      "Loss of train set: 0.319561630487442 at epoch: 7 and batch_num: 365\n",
      "Loss of train set: 0.39727669954299927 at epoch: 7 and batch_num: 366\n",
      "Loss of train set: 0.20723062753677368 at epoch: 7 and batch_num: 367\n",
      "Loss of train set: 0.26334071159362793 at epoch: 7 and batch_num: 368\n",
      "Loss of train set: 0.24409013986587524 at epoch: 7 and batch_num: 369\n",
      "Loss of train set: 0.4970431625843048 at epoch: 7 and batch_num: 370\n",
      "Loss of train set: 0.3296767771244049 at epoch: 7 and batch_num: 371\n",
      "Loss of train set: 0.3319355845451355 at epoch: 7 and batch_num: 372\n",
      "Loss of train set: 0.30474162101745605 at epoch: 7 and batch_num: 373\n",
      "Loss of train set: 0.2312292754650116 at epoch: 7 and batch_num: 374\n",
      "Loss of train set: 0.3998267352581024 at epoch: 7 and batch_num: 375\n",
      "Loss of train set: 0.25030213594436646 at epoch: 7 and batch_num: 376\n",
      "Loss of train set: 0.2815946042537689 at epoch: 7 and batch_num: 377\n",
      "Loss of train set: 0.26790231466293335 at epoch: 7 and batch_num: 378\n",
      "Loss of train set: 0.22795332968235016 at epoch: 7 and batch_num: 379\n",
      "Loss of train set: 0.25545910000801086 at epoch: 7 and batch_num: 380\n",
      "Loss of train set: 0.18677324056625366 at epoch: 7 and batch_num: 381\n",
      "Loss of train set: 0.31703895330429077 at epoch: 7 and batch_num: 382\n",
      "Loss of train set: 0.41148772835731506 at epoch: 7 and batch_num: 383\n",
      "Loss of train set: 0.5711343288421631 at epoch: 7 and batch_num: 384\n",
      "Loss of train set: 0.3834991157054901 at epoch: 7 and batch_num: 385\n",
      "Loss of train set: 0.310268759727478 at epoch: 7 and batch_num: 386\n",
      "Loss of train set: 0.37957990169525146 at epoch: 7 and batch_num: 387\n",
      "Loss of train set: 0.4373948872089386 at epoch: 7 and batch_num: 388\n",
      "Loss of train set: 0.5312132835388184 at epoch: 7 and batch_num: 389\n",
      "Loss of train set: 0.3406013548374176 at epoch: 7 and batch_num: 390\n",
      "Loss of train set: 0.16063712537288666 at epoch: 7 and batch_num: 391\n",
      "Loss of train set: 0.3304186165332794 at epoch: 7 and batch_num: 392\n",
      "Loss of train set: 0.37984955310821533 at epoch: 7 and batch_num: 393\n",
      "Loss of train set: 0.3878761827945709 at epoch: 7 and batch_num: 394\n",
      "Loss of train set: 0.4519799053668976 at epoch: 7 and batch_num: 395\n",
      "Loss of train set: 0.17182451486587524 at epoch: 7 and batch_num: 396\n",
      "Loss of train set: 0.48689740896224976 at epoch: 7 and batch_num: 397\n",
      "Loss of train set: 0.5953011512756348 at epoch: 7 and batch_num: 398\n",
      "Loss of train set: 0.47399771213531494 at epoch: 7 and batch_num: 399\n",
      "Loss of train set: 0.39266908168792725 at epoch: 7 and batch_num: 400\n",
      "Loss of train set: 0.2159007489681244 at epoch: 7 and batch_num: 401\n",
      "Loss of train set: 0.3427332937717438 at epoch: 7 and batch_num: 402\n",
      "Loss of train set: 0.3361879289150238 at epoch: 7 and batch_num: 403\n",
      "Loss of train set: 0.41800516843795776 at epoch: 7 and batch_num: 404\n",
      "Loss of train set: 0.3380296230316162 at epoch: 7 and batch_num: 405\n",
      "Loss of train set: 0.1906200349330902 at epoch: 7 and batch_num: 406\n",
      "Loss of train set: 0.26013436913490295 at epoch: 7 and batch_num: 407\n",
      "Loss of train set: 0.4644993841648102 at epoch: 7 and batch_num: 408\n",
      "Loss of train set: 0.3711664378643036 at epoch: 7 and batch_num: 409\n",
      "Loss of train set: 0.2774543762207031 at epoch: 7 and batch_num: 410\n",
      "Loss of train set: 0.2222292423248291 at epoch: 7 and batch_num: 411\n",
      "Loss of train set: 0.4564107656478882 at epoch: 7 and batch_num: 412\n",
      "Loss of train set: 0.2172274887561798 at epoch: 7 and batch_num: 413\n",
      "Loss of train set: 0.47606873512268066 at epoch: 7 and batch_num: 414\n",
      "Loss of train set: 0.21026673913002014 at epoch: 7 and batch_num: 415\n",
      "Loss of train set: 0.2966988980770111 at epoch: 7 and batch_num: 416\n",
      "Loss of train set: 0.4229087233543396 at epoch: 7 and batch_num: 417\n",
      "Loss of train set: 0.5138193368911743 at epoch: 7 and batch_num: 418\n",
      "Loss of train set: 0.4643159508705139 at epoch: 7 and batch_num: 419\n",
      "Loss of train set: 0.243413507938385 at epoch: 7 and batch_num: 420\n",
      "Loss of train set: 0.3257005512714386 at epoch: 7 and batch_num: 421\n",
      "Loss of train set: 0.4230194091796875 at epoch: 7 and batch_num: 422\n",
      "Loss of train set: 0.08915629982948303 at epoch: 7 and batch_num: 423\n",
      "Loss of train set: 0.3335224390029907 at epoch: 7 and batch_num: 424\n",
      "Loss of train set: 0.4163391590118408 at epoch: 7 and batch_num: 425\n",
      "Loss of train set: 0.4191041886806488 at epoch: 7 and batch_num: 426\n",
      "Loss of train set: 0.4489917755126953 at epoch: 7 and batch_num: 427\n",
      "Loss of train set: 0.3517243266105652 at epoch: 7 and batch_num: 428\n",
      "Loss of train set: 0.32509881258010864 at epoch: 7 and batch_num: 429\n",
      "Loss of train set: 0.31271135807037354 at epoch: 7 and batch_num: 430\n",
      "Loss of train set: 0.3078721761703491 at epoch: 7 and batch_num: 431\n",
      "Loss of train set: 0.21074232459068298 at epoch: 7 and batch_num: 432\n",
      "Loss of train set: 0.3012029528617859 at epoch: 7 and batch_num: 433\n",
      "Loss of train set: 0.2716082036495209 at epoch: 7 and batch_num: 434\n",
      "Loss of train set: 0.3115551471710205 at epoch: 7 and batch_num: 435\n",
      "Loss of train set: 0.2907031774520874 at epoch: 7 and batch_num: 436\n",
      "Loss of train set: 0.24855343997478485 at epoch: 7 and batch_num: 437\n",
      "Loss of train set: 0.32050174474716187 at epoch: 7 and batch_num: 438\n",
      "Loss of train set: 0.31343066692352295 at epoch: 7 and batch_num: 439\n",
      "Loss of train set: 0.4279741048812866 at epoch: 7 and batch_num: 440\n",
      "Loss of train set: 0.34802165627479553 at epoch: 7 and batch_num: 441\n",
      "Loss of train set: 0.4414985179901123 at epoch: 7 and batch_num: 442\n",
      "Loss of train set: 0.3559568524360657 at epoch: 7 and batch_num: 443\n",
      "Loss of train set: 0.3410409986972809 at epoch: 7 and batch_num: 444\n",
      "Loss of train set: 0.30132997035980225 at epoch: 7 and batch_num: 445\n",
      "Loss of train set: 0.369060218334198 at epoch: 7 and batch_num: 446\n",
      "Loss of train set: 0.32639968395233154 at epoch: 7 and batch_num: 447\n",
      "Loss of train set: 0.40001189708709717 at epoch: 7 and batch_num: 448\n",
      "Loss of train set: 0.4254888892173767 at epoch: 7 and batch_num: 449\n",
      "Loss of train set: 0.2784814238548279 at epoch: 7 and batch_num: 450\n",
      "Loss of train set: 0.34228694438934326 at epoch: 7 and batch_num: 451\n",
      "Loss of train set: 0.3749347925186157 at epoch: 7 and batch_num: 452\n",
      "Loss of train set: 0.26345041394233704 at epoch: 7 and batch_num: 453\n",
      "Loss of train set: 0.46231722831726074 at epoch: 7 and batch_num: 454\n",
      "Loss of train set: 0.19735752046108246 at epoch: 7 and batch_num: 455\n",
      "Loss of train set: 0.3196820616722107 at epoch: 7 and batch_num: 456\n",
      "Loss of train set: 0.35902899503707886 at epoch: 7 and batch_num: 457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.31609678268432617 at epoch: 7 and batch_num: 458\n",
      "Loss of train set: 0.398451030254364 at epoch: 7 and batch_num: 459\n",
      "Loss of train set: 0.23678019642829895 at epoch: 7 and batch_num: 460\n",
      "Loss of train set: 0.37725111842155457 at epoch: 7 and batch_num: 461\n",
      "Loss of train set: 0.3169666528701782 at epoch: 7 and batch_num: 462\n",
      "Loss of train set: 0.4486877918243408 at epoch: 7 and batch_num: 463\n",
      "Loss of train set: 0.4281085133552551 at epoch: 7 and batch_num: 464\n",
      "Loss of train set: 0.38490933179855347 at epoch: 7 and batch_num: 465\n",
      "Loss of train set: 0.2065260112285614 at epoch: 7 and batch_num: 466\n",
      "Loss of train set: 0.27716299891471863 at epoch: 7 and batch_num: 467\n",
      "Loss of train set: 0.263996422290802 at epoch: 7 and batch_num: 468\n",
      "Loss of train set: 0.22865858674049377 at epoch: 7 and batch_num: 469\n",
      "Loss of train set: 0.41916680335998535 at epoch: 7 and batch_num: 470\n",
      "Loss of train set: 0.33906787633895874 at epoch: 7 and batch_num: 471\n",
      "Loss of train set: 0.49355828762054443 at epoch: 7 and batch_num: 472\n",
      "Loss of train set: 0.1749097853899002 at epoch: 7 and batch_num: 473\n",
      "Loss of train set: 0.465135395526886 at epoch: 7 and batch_num: 474\n",
      "Loss of train set: 0.4429595470428467 at epoch: 7 and batch_num: 475\n",
      "Loss of train set: 0.3399885594844818 at epoch: 7 and batch_num: 476\n",
      "Loss of train set: 0.36714816093444824 at epoch: 7 and batch_num: 477\n",
      "Loss of train set: 0.4124830365180969 at epoch: 7 and batch_num: 478\n",
      "Loss of train set: 0.2745360732078552 at epoch: 7 and batch_num: 479\n",
      "Loss of train set: 0.3076440989971161 at epoch: 7 and batch_num: 480\n",
      "Loss of train set: 0.1719825565814972 at epoch: 7 and batch_num: 481\n",
      "Loss of train set: 0.44752633571624756 at epoch: 7 and batch_num: 482\n",
      "Loss of train set: 0.44323229789733887 at epoch: 7 and batch_num: 483\n",
      "Loss of train set: 0.2417893409729004 at epoch: 7 and batch_num: 484\n",
      "Loss of train set: 0.3983438313007355 at epoch: 7 and batch_num: 485\n",
      "Loss of train set: 0.29617345333099365 at epoch: 7 and batch_num: 486\n",
      "Loss of train set: 0.26876965165138245 at epoch: 7 and batch_num: 487\n",
      "Loss of train set: 0.3394984006881714 at epoch: 7 and batch_num: 488\n",
      "Loss of train set: 0.28269290924072266 at epoch: 7 and batch_num: 489\n",
      "Loss of train set: 0.3870234787464142 at epoch: 7 and batch_num: 490\n",
      "Loss of train set: 0.21011608839035034 at epoch: 7 and batch_num: 491\n",
      "Loss of train set: 0.35410669445991516 at epoch: 7 and batch_num: 492\n",
      "Loss of train set: 0.3050902485847473 at epoch: 7 and batch_num: 493\n",
      "Loss of train set: 0.4915524423122406 at epoch: 7 and batch_num: 494\n",
      "Loss of train set: 0.258974552154541 at epoch: 7 and batch_num: 495\n",
      "Loss of train set: 0.4941762685775757 at epoch: 7 and batch_num: 496\n",
      "Loss of train set: 0.3106996417045593 at epoch: 7 and batch_num: 497\n",
      "Loss of train set: 0.505707859992981 at epoch: 7 and batch_num: 498\n",
      "Loss of train set: 0.3327576816082001 at epoch: 7 and batch_num: 499\n",
      "Loss of train set: 0.3619111180305481 at epoch: 7 and batch_num: 500\n",
      "Loss of train set: 0.29666975140571594 at epoch: 7 and batch_num: 501\n",
      "Loss of train set: 0.38281917572021484 at epoch: 7 and batch_num: 502\n",
      "Loss of train set: 0.32543468475341797 at epoch: 7 and batch_num: 503\n",
      "Loss of train set: 0.3163079619407654 at epoch: 7 and batch_num: 504\n",
      "Loss of train set: 0.40962907671928406 at epoch: 7 and batch_num: 505\n",
      "Loss of train set: 0.15057721734046936 at epoch: 7 and batch_num: 506\n",
      "Loss of train set: 0.36742329597473145 at epoch: 7 and batch_num: 507\n",
      "Loss of train set: 0.315752774477005 at epoch: 7 and batch_num: 508\n",
      "Loss of train set: 0.421784907579422 at epoch: 7 and batch_num: 509\n",
      "Loss of train set: 0.713036298751831 at epoch: 7 and batch_num: 510\n",
      "Loss of train set: 0.291751891374588 at epoch: 7 and batch_num: 511\n",
      "Loss of train set: 0.4466308355331421 at epoch: 7 and batch_num: 512\n",
      "Loss of train set: 0.28407132625579834 at epoch: 7 and batch_num: 513\n",
      "Loss of train set: 0.31536176800727844 at epoch: 7 and batch_num: 514\n",
      "Loss of train set: 0.21796944737434387 at epoch: 7 and batch_num: 515\n",
      "Loss of train set: 0.28261518478393555 at epoch: 7 and batch_num: 516\n",
      "Loss of train set: 0.2285277247428894 at epoch: 7 and batch_num: 517\n",
      "Loss of train set: 0.3432958126068115 at epoch: 7 and batch_num: 518\n",
      "Loss of train set: 0.3682813048362732 at epoch: 7 and batch_num: 519\n",
      "Loss of train set: 0.5154186487197876 at epoch: 7 and batch_num: 520\n",
      "Loss of train set: 0.5639799237251282 at epoch: 7 and batch_num: 521\n",
      "Loss of train set: 0.32185372710227966 at epoch: 7 and batch_num: 522\n",
      "Loss of train set: 0.17861613631248474 at epoch: 7 and batch_num: 523\n",
      "Loss of train set: 0.2738487124443054 at epoch: 7 and batch_num: 524\n",
      "Loss of train set: 0.3197219967842102 at epoch: 7 and batch_num: 525\n",
      "Loss of train set: 0.20421072840690613 at epoch: 7 and batch_num: 526\n",
      "Loss of train set: 0.36864757537841797 at epoch: 7 and batch_num: 527\n",
      "Loss of train set: 0.35903048515319824 at epoch: 7 and batch_num: 528\n",
      "Loss of train set: 0.10742692649364471 at epoch: 7 and batch_num: 529\n",
      "Loss of train set: 0.3844394087791443 at epoch: 7 and batch_num: 530\n",
      "Loss of train set: 0.2614929676055908 at epoch: 7 and batch_num: 531\n",
      "Loss of train set: 0.4549905061721802 at epoch: 7 and batch_num: 532\n",
      "Loss of train set: 0.24105265736579895 at epoch: 7 and batch_num: 533\n",
      "Loss of train set: 0.3293595612049103 at epoch: 7 and batch_num: 534\n",
      "Loss of train set: 0.21082346141338348 at epoch: 7 and batch_num: 535\n",
      "Loss of train set: 0.23045490682125092 at epoch: 7 and batch_num: 536\n",
      "Loss of train set: 0.44024908542633057 at epoch: 7 and batch_num: 537\n",
      "Loss of train set: 0.31932106614112854 at epoch: 7 and batch_num: 538\n",
      "Loss of train set: 0.43693065643310547 at epoch: 7 and batch_num: 539\n",
      "Loss of train set: 0.2916479706764221 at epoch: 7 and batch_num: 540\n",
      "Loss of train set: 0.31698188185691833 at epoch: 7 and batch_num: 541\n",
      "Loss of train set: 0.45394375920295715 at epoch: 7 and batch_num: 542\n",
      "Loss of train set: 0.39662307500839233 at epoch: 7 and batch_num: 543\n",
      "Loss of train set: 0.27418118715286255 at epoch: 7 and batch_num: 544\n",
      "Loss of train set: 0.37324196100234985 at epoch: 7 and batch_num: 545\n",
      "Loss of train set: 0.2816479504108429 at epoch: 7 and batch_num: 546\n",
      "Loss of train set: 0.45109084248542786 at epoch: 7 and batch_num: 547\n",
      "Loss of train set: 0.3583872318267822 at epoch: 7 and batch_num: 548\n",
      "Loss of train set: 0.1675972044467926 at epoch: 7 and batch_num: 549\n",
      "Loss of train set: 0.3736639618873596 at epoch: 7 and batch_num: 550\n",
      "Loss of train set: 0.4210134744644165 at epoch: 7 and batch_num: 551\n",
      "Loss of train set: 0.23926633596420288 at epoch: 7 and batch_num: 552\n",
      "Loss of train set: 0.5807897448539734 at epoch: 7 and batch_num: 553\n",
      "Loss of train set: 0.251476913690567 at epoch: 7 and batch_num: 554\n",
      "Loss of train set: 0.3043309450149536 at epoch: 7 and batch_num: 555\n",
      "Loss of train set: 0.2856266498565674 at epoch: 7 and batch_num: 556\n",
      "Loss of train set: 0.28629255294799805 at epoch: 7 and batch_num: 557\n",
      "Loss of train set: 0.28079840540885925 at epoch: 7 and batch_num: 558\n",
      "Loss of train set: 0.31439992785453796 at epoch: 7 and batch_num: 559\n",
      "Loss of train set: 0.35495907068252563 at epoch: 7 and batch_num: 560\n",
      "Loss of train set: 0.5129016637802124 at epoch: 7 and batch_num: 561\n",
      "Loss of train set: 0.39523783326148987 at epoch: 7 and batch_num: 562\n",
      "Loss of train set: 0.39668840169906616 at epoch: 7 and batch_num: 563\n",
      "Loss of train set: 0.498363733291626 at epoch: 7 and batch_num: 564\n",
      "Loss of train set: 0.24382595717906952 at epoch: 7 and batch_num: 565\n",
      "Loss of train set: 0.3165042996406555 at epoch: 7 and batch_num: 566\n",
      "Loss of train set: 0.3279280662536621 at epoch: 7 and batch_num: 567\n",
      "Loss of train set: 0.24157068133354187 at epoch: 7 and batch_num: 568\n",
      "Loss of train set: 0.4329581558704376 at epoch: 7 and batch_num: 569\n",
      "Loss of train set: 0.6188045144081116 at epoch: 7 and batch_num: 570\n",
      "Loss of train set: 0.30821970105171204 at epoch: 7 and batch_num: 571\n",
      "Loss of train set: 0.30141711235046387 at epoch: 7 and batch_num: 572\n",
      "Loss of train set: 0.5244027376174927 at epoch: 7 and batch_num: 573\n",
      "Loss of train set: 0.3195951581001282 at epoch: 7 and batch_num: 574\n",
      "Loss of train set: 0.3546140193939209 at epoch: 7 and batch_num: 575\n",
      "Loss of train set: 0.32534971833229065 at epoch: 7 and batch_num: 576\n",
      "Loss of train set: 0.18121156096458435 at epoch: 7 and batch_num: 577\n",
      "Loss of train set: 0.41367876529693604 at epoch: 7 and batch_num: 578\n",
      "Loss of train set: 0.19142305850982666 at epoch: 7 and batch_num: 579\n",
      "Loss of train set: 0.444990873336792 at epoch: 7 and batch_num: 580\n",
      "Loss of train set: 0.34050893783569336 at epoch: 7 and batch_num: 581\n",
      "Loss of train set: 0.3109687566757202 at epoch: 7 and batch_num: 582\n",
      "Loss of train set: 0.14291630685329437 at epoch: 7 and batch_num: 583\n",
      "Loss of train set: 0.31131067872047424 at epoch: 7 and batch_num: 584\n",
      "Loss of train set: 0.24903710186481476 at epoch: 7 and batch_num: 585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2829669117927551 at epoch: 7 and batch_num: 586\n",
      "Loss of train set: 0.20347625017166138 at epoch: 7 and batch_num: 587\n",
      "Loss of train set: 0.26288217306137085 at epoch: 7 and batch_num: 588\n",
      "Loss of train set: 0.3917752504348755 at epoch: 7 and batch_num: 589\n",
      "Loss of train set: 0.448141872882843 at epoch: 7 and batch_num: 590\n",
      "Loss of train set: 0.38164791464805603 at epoch: 7 and batch_num: 591\n",
      "Loss of train set: 0.28848254680633545 at epoch: 7 and batch_num: 592\n",
      "Loss of train set: 0.19517570734024048 at epoch: 7 and batch_num: 593\n",
      "Loss of train set: 0.37704336643218994 at epoch: 7 and batch_num: 594\n",
      "Loss of train set: 0.29030489921569824 at epoch: 7 and batch_num: 595\n",
      "Loss of train set: 0.3922470211982727 at epoch: 7 and batch_num: 596\n",
      "Loss of train set: 0.2852627635002136 at epoch: 7 and batch_num: 597\n",
      "Loss of train set: 0.27365851402282715 at epoch: 7 and batch_num: 598\n",
      "Loss of train set: 0.16360709071159363 at epoch: 7 and batch_num: 599\n",
      "Loss of train set: 0.30836063623428345 at epoch: 7 and batch_num: 600\n",
      "Loss of train set: 0.39860936999320984 at epoch: 7 and batch_num: 601\n",
      "Loss of train set: 0.2453286349773407 at epoch: 7 and batch_num: 602\n",
      "Loss of train set: 0.5762354135513306 at epoch: 7 and batch_num: 603\n",
      "Loss of train set: 0.25069040060043335 at epoch: 7 and batch_num: 604\n",
      "Loss of train set: 0.25085127353668213 at epoch: 7 and batch_num: 605\n",
      "Loss of train set: 0.4121401309967041 at epoch: 7 and batch_num: 606\n",
      "Loss of train set: 0.28027692437171936 at epoch: 7 and batch_num: 607\n",
      "Loss of train set: 0.4876251220703125 at epoch: 7 and batch_num: 608\n",
      "Loss of train set: 0.2062523066997528 at epoch: 7 and batch_num: 609\n",
      "Loss of train set: 0.2603985071182251 at epoch: 7 and batch_num: 610\n",
      "Loss of train set: 0.5815229415893555 at epoch: 7 and batch_num: 611\n",
      "Loss of train set: 0.41529619693756104 at epoch: 7 and batch_num: 612\n",
      "Loss of train set: 0.3658398687839508 at epoch: 7 and batch_num: 613\n",
      "Loss of train set: 0.2693561315536499 at epoch: 7 and batch_num: 614\n",
      "Loss of train set: 0.2070116102695465 at epoch: 7 and batch_num: 615\n",
      "Loss of train set: 0.22187380492687225 at epoch: 7 and batch_num: 616\n",
      "Loss of train set: 0.4261886477470398 at epoch: 7 and batch_num: 617\n",
      "Loss of train set: 0.3596954643726349 at epoch: 7 and batch_num: 618\n",
      "Loss of train set: 0.4265061318874359 at epoch: 7 and batch_num: 619\n",
      "Loss of train set: 0.3599596321582794 at epoch: 7 and batch_num: 620\n",
      "Loss of train set: 0.5434043407440186 at epoch: 7 and batch_num: 621\n",
      "Loss of train set: 0.2382783740758896 at epoch: 7 and batch_num: 622\n",
      "Loss of train set: 0.2969801425933838 at epoch: 7 and batch_num: 623\n",
      "Loss of train set: 0.3810107707977295 at epoch: 7 and batch_num: 624\n",
      "Loss of train set: 0.4875931143760681 at epoch: 7 and batch_num: 625\n",
      "Loss of train set: 0.32379841804504395 at epoch: 7 and batch_num: 626\n",
      "Loss of train set: 0.517418384552002 at epoch: 7 and batch_num: 627\n",
      "Loss of train set: 0.5332753658294678 at epoch: 7 and batch_num: 628\n",
      "Loss of train set: 0.5090285539627075 at epoch: 7 and batch_num: 629\n",
      "Loss of train set: 0.42691463232040405 at epoch: 7 and batch_num: 630\n",
      "Loss of train set: 0.26347216963768005 at epoch: 7 and batch_num: 631\n",
      "Loss of train set: 0.20571327209472656 at epoch: 7 and batch_num: 632\n",
      "Loss of train set: 0.19156008958816528 at epoch: 7 and batch_num: 633\n",
      "Loss of train set: 0.5959917902946472 at epoch: 7 and batch_num: 634\n",
      "Loss of train set: 0.31375330686569214 at epoch: 7 and batch_num: 635\n",
      "Loss of train set: 0.31894493103027344 at epoch: 7 and batch_num: 636\n",
      "Loss of train set: 0.42714744806289673 at epoch: 7 and batch_num: 637\n",
      "Loss of train set: 0.3486815392971039 at epoch: 7 and batch_num: 638\n",
      "Loss of train set: 0.3890729248523712 at epoch: 7 and batch_num: 639\n",
      "Loss of train set: 0.4072938561439514 at epoch: 7 and batch_num: 640\n",
      "Loss of train set: 0.5079972743988037 at epoch: 7 and batch_num: 641\n",
      "Loss of train set: 0.19185121357440948 at epoch: 7 and batch_num: 642\n",
      "Loss of train set: 0.35663074254989624 at epoch: 7 and batch_num: 643\n",
      "Loss of train set: 0.17709705233573914 at epoch: 7 and batch_num: 644\n",
      "Loss of train set: 0.3348565697669983 at epoch: 7 and batch_num: 645\n",
      "Loss of train set: 0.32826241850852966 at epoch: 7 and batch_num: 646\n",
      "Loss of train set: 0.2764589190483093 at epoch: 7 and batch_num: 647\n",
      "Loss of train set: 0.45965689420700073 at epoch: 7 and batch_num: 648\n",
      "Loss of train set: 0.43112993240356445 at epoch: 7 and batch_num: 649\n",
      "Loss of train set: 0.37074044346809387 at epoch: 7 and batch_num: 650\n",
      "Loss of train set: 0.29756760597229004 at epoch: 7 and batch_num: 651\n",
      "Loss of train set: 0.3153032958507538 at epoch: 7 and batch_num: 652\n",
      "Loss of train set: 0.4722999334335327 at epoch: 7 and batch_num: 653\n",
      "Loss of train set: 0.31298568844795227 at epoch: 7 and batch_num: 654\n",
      "Loss of train set: 0.37976956367492676 at epoch: 7 and batch_num: 655\n",
      "Loss of train set: 0.4069938659667969 at epoch: 7 and batch_num: 656\n",
      "Loss of train set: 0.34754741191864014 at epoch: 7 and batch_num: 657\n",
      "Loss of train set: 0.3658057153224945 at epoch: 7 and batch_num: 658\n",
      "Loss of train set: 0.3558380901813507 at epoch: 7 and batch_num: 659\n",
      "Loss of train set: 0.18680763244628906 at epoch: 7 and batch_num: 660\n",
      "Loss of train set: 0.31960564851760864 at epoch: 7 and batch_num: 661\n",
      "Loss of train set: 0.3252093195915222 at epoch: 7 and batch_num: 662\n",
      "Loss of train set: 0.3219492435455322 at epoch: 7 and batch_num: 663\n",
      "Loss of train set: 0.4717022776603699 at epoch: 7 and batch_num: 664\n",
      "Loss of train set: 0.2573530077934265 at epoch: 7 and batch_num: 665\n",
      "Loss of train set: 0.2979254126548767 at epoch: 7 and batch_num: 666\n",
      "Loss of train set: 0.38417983055114746 at epoch: 7 and batch_num: 667\n",
      "Loss of train set: 0.27416205406188965 at epoch: 7 and batch_num: 668\n",
      "Loss of train set: 0.3757244646549225 at epoch: 7 and batch_num: 669\n",
      "Loss of train set: 0.33691835403442383 at epoch: 7 and batch_num: 670\n",
      "Loss of train set: 0.2802692651748657 at epoch: 7 and batch_num: 671\n",
      "Loss of train set: 0.1993754506111145 at epoch: 7 and batch_num: 672\n",
      "Loss of train set: 0.3821747601032257 at epoch: 7 and batch_num: 673\n",
      "Loss of train set: 0.24464279413223267 at epoch: 7 and batch_num: 674\n",
      "Loss of train set: 0.34989359974861145 at epoch: 7 and batch_num: 675\n",
      "Loss of train set: 0.3742425739765167 at epoch: 7 and batch_num: 676\n",
      "Loss of train set: 0.22318506240844727 at epoch: 7 and batch_num: 677\n",
      "Loss of train set: 0.4397488236427307 at epoch: 7 and batch_num: 678\n",
      "Loss of train set: 0.40782609581947327 at epoch: 7 and batch_num: 679\n",
      "Loss of train set: 0.7092154026031494 at epoch: 7 and batch_num: 680\n",
      "Loss of train set: 0.6092206239700317 at epoch: 7 and batch_num: 681\n",
      "Loss of train set: 0.3763706684112549 at epoch: 7 and batch_num: 682\n",
      "Loss of train set: 0.38345372676849365 at epoch: 7 and batch_num: 683\n",
      "Loss of train set: 0.2887951731681824 at epoch: 7 and batch_num: 684\n",
      "Loss of train set: 0.3318411111831665 at epoch: 7 and batch_num: 685\n",
      "Loss of train set: 0.5055922269821167 at epoch: 7 and batch_num: 686\n",
      "Loss of train set: 0.40394464135169983 at epoch: 7 and batch_num: 687\n",
      "Loss of train set: 0.19365090131759644 at epoch: 7 and batch_num: 688\n",
      "Loss of train set: 0.5373584032058716 at epoch: 7 and batch_num: 689\n",
      "Loss of train set: 0.2127310335636139 at epoch: 7 and batch_num: 690\n",
      "Loss of train set: 0.3668217957019806 at epoch: 7 and batch_num: 691\n",
      "Loss of train set: 0.4044116735458374 at epoch: 7 and batch_num: 692\n",
      "Loss of train set: 0.3871042728424072 at epoch: 7 and batch_num: 693\n",
      "Loss of train set: 0.3228539228439331 at epoch: 7 and batch_num: 694\n",
      "Loss of train set: 0.268825501203537 at epoch: 7 and batch_num: 695\n",
      "Loss of train set: 0.3592531979084015 at epoch: 7 and batch_num: 696\n",
      "Loss of train set: 0.2621340751647949 at epoch: 7 and batch_num: 697\n",
      "Loss of train set: 0.3712308704853058 at epoch: 7 and batch_num: 698\n",
      "Loss of train set: 0.5303176641464233 at epoch: 7 and batch_num: 699\n",
      "Loss of train set: 0.2870713472366333 at epoch: 7 and batch_num: 700\n",
      "Loss of train set: 0.49929365515708923 at epoch: 7 and batch_num: 701\n",
      "Loss of train set: 0.20700182020664215 at epoch: 7 and batch_num: 702\n",
      "Loss of train set: 0.3094448745250702 at epoch: 7 and batch_num: 703\n",
      "Loss of train set: 0.3520801067352295 at epoch: 7 and batch_num: 704\n",
      "Loss of train set: 0.36920782923698425 at epoch: 7 and batch_num: 705\n",
      "Loss of train set: 0.20046520233154297 at epoch: 7 and batch_num: 706\n",
      "Loss of train set: 0.36079978942871094 at epoch: 7 and batch_num: 707\n",
      "Loss of train set: 0.4918358027935028 at epoch: 7 and batch_num: 708\n",
      "Loss of train set: 0.565758228302002 at epoch: 7 and batch_num: 709\n",
      "Loss of train set: 0.3928139805793762 at epoch: 7 and batch_num: 710\n",
      "Loss of train set: 0.5583760738372803 at epoch: 7 and batch_num: 711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3519223928451538 at epoch: 7 and batch_num: 712\n",
      "Loss of train set: 0.2630784809589386 at epoch: 7 and batch_num: 713\n",
      "Loss of train set: 0.46372097730636597 at epoch: 7 and batch_num: 714\n",
      "Loss of train set: 0.36388733983039856 at epoch: 7 and batch_num: 715\n",
      "Loss of train set: 0.3739222288131714 at epoch: 7 and batch_num: 716\n",
      "Loss of train set: 0.28855007886886597 at epoch: 7 and batch_num: 717\n",
      "Loss of train set: 0.30772945284843445 at epoch: 7 and batch_num: 718\n",
      "Loss of train set: 0.37300461530685425 at epoch: 7 and batch_num: 719\n",
      "Loss of train set: 0.6396838426589966 at epoch: 7 and batch_num: 720\n",
      "Loss of train set: 0.47128963470458984 at epoch: 7 and batch_num: 721\n",
      "Loss of train set: 0.2649887502193451 at epoch: 7 and batch_num: 722\n",
      "Loss of train set: 0.3157910406589508 at epoch: 7 and batch_num: 723\n",
      "Loss of train set: 0.23095369338989258 at epoch: 7 and batch_num: 724\n",
      "Loss of train set: 0.30395084619522095 at epoch: 7 and batch_num: 725\n",
      "Loss of train set: 0.4842357933521271 at epoch: 7 and batch_num: 726\n",
      "Loss of train set: 0.37017136812210083 at epoch: 7 and batch_num: 727\n",
      "Loss of train set: 0.44532766938209534 at epoch: 7 and batch_num: 728\n",
      "Loss of train set: 0.3074130415916443 at epoch: 7 and batch_num: 729\n",
      "Loss of train set: 0.3191295266151428 at epoch: 7 and batch_num: 730\n",
      "Loss of train set: 0.398851215839386 at epoch: 7 and batch_num: 731\n",
      "Loss of train set: 0.43032634258270264 at epoch: 7 and batch_num: 732\n",
      "Loss of train set: 0.2143327295780182 at epoch: 7 and batch_num: 733\n",
      "Loss of train set: 0.5285136699676514 at epoch: 7 and batch_num: 734\n",
      "Loss of train set: 0.3268212080001831 at epoch: 7 and batch_num: 735\n",
      "Loss of train set: 0.342153400182724 at epoch: 7 and batch_num: 736\n",
      "Loss of train set: 0.3678845167160034 at epoch: 7 and batch_num: 737\n",
      "Loss of train set: 0.20144814252853394 at epoch: 7 and batch_num: 738\n",
      "Loss of train set: 0.18694260716438293 at epoch: 7 and batch_num: 739\n",
      "Loss of train set: 0.4195960760116577 at epoch: 7 and batch_num: 740\n",
      "Loss of train set: 0.24427427351474762 at epoch: 7 and batch_num: 741\n",
      "Loss of train set: 0.3793371319770813 at epoch: 7 and batch_num: 742\n",
      "Loss of train set: 0.24408802390098572 at epoch: 7 and batch_num: 743\n",
      "Loss of train set: 0.4301642179489136 at epoch: 7 and batch_num: 744\n",
      "Loss of train set: 0.3232382833957672 at epoch: 7 and batch_num: 745\n",
      "Loss of train set: 0.28404349088668823 at epoch: 7 and batch_num: 746\n",
      "Loss of train set: 0.2676362991333008 at epoch: 7 and batch_num: 747\n",
      "Loss of train set: 0.5470072031021118 at epoch: 7 and batch_num: 748\n",
      "Loss of train set: 0.519030749797821 at epoch: 7 and batch_num: 749\n",
      "Loss of train set: 0.33504143357276917 at epoch: 7 and batch_num: 750\n",
      "Loss of train set: 0.3109586834907532 at epoch: 7 and batch_num: 751\n",
      "Loss of train set: 0.33360445499420166 at epoch: 7 and batch_num: 752\n",
      "Loss of train set: 0.43840306997299194 at epoch: 7 and batch_num: 753\n",
      "Loss of train set: 0.48716098070144653 at epoch: 7 and batch_num: 754\n",
      "Loss of train set: 0.412575364112854 at epoch: 7 and batch_num: 755\n",
      "Loss of train set: 0.35733455419540405 at epoch: 7 and batch_num: 756\n",
      "Loss of train set: 0.21501801908016205 at epoch: 7 and batch_num: 757\n",
      "Loss of train set: 0.33135008811950684 at epoch: 7 and batch_num: 758\n",
      "Loss of train set: 0.3327666223049164 at epoch: 7 and batch_num: 759\n",
      "Loss of train set: 0.47975319623947144 at epoch: 7 and batch_num: 760\n",
      "Loss of train set: 0.1860891580581665 at epoch: 7 and batch_num: 761\n",
      "Loss of train set: 0.26148179173469543 at epoch: 7 and batch_num: 762\n",
      "Loss of train set: 0.22957377135753632 at epoch: 7 and batch_num: 763\n",
      "Loss of train set: 0.42480334639549255 at epoch: 7 and batch_num: 764\n",
      "Loss of train set: 0.328940212726593 at epoch: 7 and batch_num: 765\n",
      "Loss of train set: 0.34132054448127747 at epoch: 7 and batch_num: 766\n",
      "Loss of train set: 0.29665854573249817 at epoch: 7 and batch_num: 767\n",
      "Loss of train set: 0.41621094942092896 at epoch: 7 and batch_num: 768\n",
      "Loss of train set: 0.3607775568962097 at epoch: 7 and batch_num: 769\n",
      "Loss of train set: 0.33124840259552 at epoch: 7 and batch_num: 770\n",
      "Loss of train set: 0.26621538400650024 at epoch: 7 and batch_num: 771\n",
      "Loss of train set: 0.39099645614624023 at epoch: 7 and batch_num: 772\n",
      "Loss of train set: 0.34887468814849854 at epoch: 7 and batch_num: 773\n",
      "Loss of train set: 0.19756227731704712 at epoch: 7 and batch_num: 774\n",
      "Loss of train set: 0.2921288013458252 at epoch: 7 and batch_num: 775\n",
      "Loss of train set: 0.14325681328773499 at epoch: 7 and batch_num: 776\n",
      "Loss of train set: 0.39308813214302063 at epoch: 7 and batch_num: 777\n",
      "Loss of train set: 0.29160577058792114 at epoch: 7 and batch_num: 778\n",
      "Loss of train set: 0.3663342595100403 at epoch: 7 and batch_num: 779\n",
      "Loss of train set: 0.29524341225624084 at epoch: 7 and batch_num: 780\n",
      "Loss of train set: 0.27881139516830444 at epoch: 7 and batch_num: 781\n",
      "Loss of train set: 0.309545636177063 at epoch: 7 and batch_num: 782\n",
      "Loss of train set: 0.28901100158691406 at epoch: 7 and batch_num: 783\n",
      "Loss of train set: 0.2843376398086548 at epoch: 7 and batch_num: 784\n",
      "Loss of train set: 0.4335745871067047 at epoch: 7 and batch_num: 785\n",
      "Loss of train set: 0.4978795051574707 at epoch: 7 and batch_num: 786\n",
      "Loss of train set: 0.48005029559135437 at epoch: 7 and batch_num: 787\n",
      "Loss of train set: 0.3709227740764618 at epoch: 7 and batch_num: 788\n",
      "Loss of train set: 0.17906099557876587 at epoch: 7 and batch_num: 789\n",
      "Loss of train set: 0.3676304817199707 at epoch: 7 and batch_num: 790\n",
      "Loss of train set: 0.2547372579574585 at epoch: 7 and batch_num: 791\n",
      "Loss of train set: 0.25052496790885925 at epoch: 7 and batch_num: 792\n",
      "Loss of train set: 0.3175309896469116 at epoch: 7 and batch_num: 793\n",
      "Loss of train set: 0.19852441549301147 at epoch: 7 and batch_num: 794\n",
      "Loss of train set: 0.24389232695102692 at epoch: 7 and batch_num: 795\n",
      "Loss of train set: 0.37158316373825073 at epoch: 7 and batch_num: 796\n",
      "Loss of train set: 0.3388972282409668 at epoch: 7 and batch_num: 797\n",
      "Loss of train set: 0.23598355054855347 at epoch: 7 and batch_num: 798\n",
      "Loss of train set: 0.39639222621917725 at epoch: 7 and batch_num: 799\n",
      "Loss of train set: 0.42232048511505127 at epoch: 7 and batch_num: 800\n",
      "Loss of train set: 0.3822898864746094 at epoch: 7 and batch_num: 801\n",
      "Loss of train set: 0.18661826848983765 at epoch: 7 and batch_num: 802\n",
      "Loss of train set: 0.2645454704761505 at epoch: 7 and batch_num: 803\n",
      "Loss of train set: 0.2603934705257416 at epoch: 7 and batch_num: 804\n",
      "Loss of train set: 0.32230502367019653 at epoch: 7 and batch_num: 805\n",
      "Loss of train set: 0.2556654214859009 at epoch: 7 and batch_num: 806\n",
      "Loss of train set: 0.31118759512901306 at epoch: 7 and batch_num: 807\n",
      "Loss of train set: 0.31682348251342773 at epoch: 7 and batch_num: 808\n",
      "Loss of train set: 0.2546119689941406 at epoch: 7 and batch_num: 809\n",
      "Loss of train set: 0.4007490277290344 at epoch: 7 and batch_num: 810\n",
      "Loss of train set: 0.4353851079940796 at epoch: 7 and batch_num: 811\n",
      "Loss of train set: 0.4416080415248871 at epoch: 7 and batch_num: 812\n",
      "Loss of train set: 0.24322043359279633 at epoch: 7 and batch_num: 813\n",
      "Loss of train set: 0.3457215130329132 at epoch: 7 and batch_num: 814\n",
      "Loss of train set: 0.35702913999557495 at epoch: 7 and batch_num: 815\n",
      "Loss of train set: 0.2485922873020172 at epoch: 7 and batch_num: 816\n",
      "Loss of train set: 0.2627892792224884 at epoch: 7 and batch_num: 817\n",
      "Loss of train set: 0.3284403085708618 at epoch: 7 and batch_num: 818\n",
      "Loss of train set: 0.3296933174133301 at epoch: 7 and batch_num: 819\n",
      "Loss of train set: 0.3742135167121887 at epoch: 7 and batch_num: 820\n",
      "Loss of train set: 0.35097092390060425 at epoch: 7 and batch_num: 821\n",
      "Loss of train set: 0.2598176598548889 at epoch: 7 and batch_num: 822\n",
      "Loss of train set: 0.36042100191116333 at epoch: 7 and batch_num: 823\n",
      "Loss of train set: 0.282099187374115 at epoch: 7 and batch_num: 824\n",
      "Loss of train set: 0.23611396551132202 at epoch: 7 and batch_num: 825\n",
      "Loss of train set: 0.24428877234458923 at epoch: 7 and batch_num: 826\n",
      "Loss of train set: 0.6034061908721924 at epoch: 7 and batch_num: 827\n",
      "Loss of train set: 0.35201579332351685 at epoch: 7 and batch_num: 828\n",
      "Loss of train set: 0.2036767154932022 at epoch: 7 and batch_num: 829\n",
      "Loss of train set: 0.5168609619140625 at epoch: 7 and batch_num: 830\n",
      "Loss of train set: 0.24538058042526245 at epoch: 7 and batch_num: 831\n",
      "Loss of train set: 0.41256093978881836 at epoch: 7 and batch_num: 832\n",
      "Loss of train set: 0.3321899175643921 at epoch: 7 and batch_num: 833\n",
      "Loss of train set: 0.20360708236694336 at epoch: 7 and batch_num: 834\n",
      "Loss of train set: 0.35562586784362793 at epoch: 7 and batch_num: 835\n",
      "Loss of train set: 0.307921826839447 at epoch: 7 and batch_num: 836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.41897690296173096 at epoch: 7 and batch_num: 837\n",
      "Loss of train set: 0.3337550759315491 at epoch: 7 and batch_num: 838\n",
      "Loss of train set: 0.2662860155105591 at epoch: 7 and batch_num: 839\n",
      "Loss of train set: 0.3120241165161133 at epoch: 7 and batch_num: 840\n",
      "Loss of train set: 0.39215242862701416 at epoch: 7 and batch_num: 841\n",
      "Loss of train set: 0.371168851852417 at epoch: 7 and batch_num: 842\n",
      "Loss of train set: 0.29304027557373047 at epoch: 7 and batch_num: 843\n",
      "Loss of train set: 0.3587729036808014 at epoch: 7 and batch_num: 844\n",
      "Loss of train set: 0.4592929482460022 at epoch: 7 and batch_num: 845\n",
      "Loss of train set: 0.3712959587574005 at epoch: 7 and batch_num: 846\n",
      "Loss of train set: 0.42594847083091736 at epoch: 7 and batch_num: 847\n",
      "Loss of train set: 0.3526741862297058 at epoch: 7 and batch_num: 848\n",
      "Loss of train set: 0.34720557928085327 at epoch: 7 and batch_num: 849\n",
      "Loss of train set: 0.354079931974411 at epoch: 7 and batch_num: 850\n",
      "Loss of train set: 0.23945680260658264 at epoch: 7 and batch_num: 851\n",
      "Loss of train set: 0.29953378438949585 at epoch: 7 and batch_num: 852\n",
      "Loss of train set: 0.19218039512634277 at epoch: 7 and batch_num: 853\n",
      "Loss of train set: 0.5331844091415405 at epoch: 7 and batch_num: 854\n",
      "Loss of train set: 0.3888193964958191 at epoch: 7 and batch_num: 855\n",
      "Loss of train set: 0.2524458169937134 at epoch: 7 and batch_num: 856\n",
      "Loss of train set: 0.45652639865875244 at epoch: 7 and batch_num: 857\n",
      "Loss of train set: 0.3083457350730896 at epoch: 7 and batch_num: 858\n",
      "Loss of train set: 0.24625934660434723 at epoch: 7 and batch_num: 859\n",
      "Loss of train set: 0.28767600655555725 at epoch: 7 and batch_num: 860\n",
      "Loss of train set: 0.3618074059486389 at epoch: 7 and batch_num: 861\n",
      "Loss of train set: 0.297684907913208 at epoch: 7 and batch_num: 862\n",
      "Loss of train set: 0.20832931995391846 at epoch: 7 and batch_num: 863\n",
      "Loss of train set: 0.4627559781074524 at epoch: 7 and batch_num: 864\n",
      "Loss of train set: 0.2745864987373352 at epoch: 7 and batch_num: 865\n",
      "Loss of train set: 0.394514799118042 at epoch: 7 and batch_num: 866\n",
      "Loss of train set: 0.2901513874530792 at epoch: 7 and batch_num: 867\n",
      "Loss of train set: 0.22945156693458557 at epoch: 7 and batch_num: 868\n",
      "Loss of train set: 0.25198569893836975 at epoch: 7 and batch_num: 869\n",
      "Loss of train set: 0.24950170516967773 at epoch: 7 and batch_num: 870\n",
      "Loss of train set: 0.29891103506088257 at epoch: 7 and batch_num: 871\n",
      "Loss of train set: 0.3347618281841278 at epoch: 7 and batch_num: 872\n",
      "Loss of train set: 0.379982054233551 at epoch: 7 and batch_num: 873\n",
      "Loss of train set: 0.2825431227684021 at epoch: 7 and batch_num: 874\n",
      "Loss of train set: 0.3465253412723541 at epoch: 7 and batch_num: 875\n",
      "Loss of train set: 0.370968222618103 at epoch: 7 and batch_num: 876\n",
      "Loss of train set: 0.43244650959968567 at epoch: 7 and batch_num: 877\n",
      "Loss of train set: 0.2813044786453247 at epoch: 7 and batch_num: 878\n",
      "Loss of train set: 0.21466022729873657 at epoch: 7 and batch_num: 879\n",
      "Loss of train set: 0.36827877163887024 at epoch: 7 and batch_num: 880\n",
      "Loss of train set: 0.18699303269386292 at epoch: 7 and batch_num: 881\n",
      "Loss of train set: 0.23573996126651764 at epoch: 7 and batch_num: 882\n",
      "Loss of train set: 0.14107829332351685 at epoch: 7 and batch_num: 883\n",
      "Loss of train set: 0.32257163524627686 at epoch: 7 and batch_num: 884\n",
      "Loss of train set: 0.24203240871429443 at epoch: 7 and batch_num: 885\n",
      "Loss of train set: 0.2038470059633255 at epoch: 7 and batch_num: 886\n",
      "Loss of train set: 0.45308810472488403 at epoch: 7 and batch_num: 887\n",
      "Loss of train set: 0.4250202775001526 at epoch: 7 and batch_num: 888\n",
      "Loss of train set: 0.16687801480293274 at epoch: 7 and batch_num: 889\n",
      "Loss of train set: 0.4276503324508667 at epoch: 7 and batch_num: 890\n",
      "Loss of train set: 0.3968890905380249 at epoch: 7 and batch_num: 891\n",
      "Loss of train set: 0.29985684156417847 at epoch: 7 and batch_num: 892\n",
      "Loss of train set: 0.47704190015792847 at epoch: 7 and batch_num: 893\n",
      "Loss of train set: 0.4079586863517761 at epoch: 7 and batch_num: 894\n",
      "Loss of train set: 0.5189624428749084 at epoch: 7 and batch_num: 895\n",
      "Loss of train set: 0.2242037057876587 at epoch: 7 and batch_num: 896\n",
      "Loss of train set: 0.1879703253507614 at epoch: 7 and batch_num: 897\n",
      "Loss of train set: 0.21494147181510925 at epoch: 7 and batch_num: 898\n",
      "Loss of train set: 0.3210594356060028 at epoch: 7 and batch_num: 899\n",
      "Loss of train set: 0.20888879895210266 at epoch: 7 and batch_num: 900\n",
      "Loss of train set: 0.4282490611076355 at epoch: 7 and batch_num: 901\n",
      "Loss of train set: 0.18483158946037292 at epoch: 7 and batch_num: 902\n",
      "Loss of train set: 0.20389439165592194 at epoch: 7 and batch_num: 903\n",
      "Loss of train set: 0.2125234454870224 at epoch: 7 and batch_num: 904\n",
      "Loss of train set: 0.1766396462917328 at epoch: 7 and batch_num: 905\n",
      "Loss of train set: 0.23678217828273773 at epoch: 7 and batch_num: 906\n",
      "Loss of train set: 0.3196203112602234 at epoch: 7 and batch_num: 907\n",
      "Loss of train set: 0.24147449433803558 at epoch: 7 and batch_num: 908\n",
      "Loss of train set: 0.22900046408176422 at epoch: 7 and batch_num: 909\n",
      "Loss of train set: 0.4727354645729065 at epoch: 7 and batch_num: 910\n",
      "Loss of train set: 0.4071598947048187 at epoch: 7 and batch_num: 911\n",
      "Loss of train set: 0.4407172203063965 at epoch: 7 and batch_num: 912\n",
      "Loss of train set: 0.29717063903808594 at epoch: 7 and batch_num: 913\n",
      "Loss of train set: 0.35194435715675354 at epoch: 7 and batch_num: 914\n",
      "Loss of train set: 0.3277638554573059 at epoch: 7 and batch_num: 915\n",
      "Loss of train set: 0.3156135082244873 at epoch: 7 and batch_num: 916\n",
      "Loss of train set: 0.2804303765296936 at epoch: 7 and batch_num: 917\n",
      "Loss of train set: 0.3546176552772522 at epoch: 7 and batch_num: 918\n",
      "Loss of train set: 0.29158711433410645 at epoch: 7 and batch_num: 919\n",
      "Loss of train set: 0.41470927000045776 at epoch: 7 and batch_num: 920\n",
      "Loss of train set: 0.22080634534358978 at epoch: 7 and batch_num: 921\n",
      "Loss of train set: 0.3755848705768585 at epoch: 7 and batch_num: 922\n",
      "Loss of train set: 0.3581696152687073 at epoch: 7 and batch_num: 923\n",
      "Loss of train set: 0.331268310546875 at epoch: 7 and batch_num: 924\n",
      "Loss of train set: 0.2689046263694763 at epoch: 7 and batch_num: 925\n",
      "Loss of train set: 0.28294628858566284 at epoch: 7 and batch_num: 926\n",
      "Loss of train set: 0.3849468231201172 at epoch: 7 and batch_num: 927\n",
      "Loss of train set: 0.353789746761322 at epoch: 7 and batch_num: 928\n",
      "Loss of train set: 0.3062540590763092 at epoch: 7 and batch_num: 929\n",
      "Loss of train set: 0.4071694016456604 at epoch: 7 and batch_num: 930\n",
      "Loss of train set: 0.36043107509613037 at epoch: 7 and batch_num: 931\n",
      "Loss of train set: 0.3048716187477112 at epoch: 7 and batch_num: 932\n",
      "Loss of train set: 0.39606964588165283 at epoch: 7 and batch_num: 933\n",
      "Loss of train set: 0.2713448703289032 at epoch: 7 and batch_num: 934\n",
      "Loss of train set: 0.31216585636138916 at epoch: 7 and batch_num: 935\n",
      "Loss of train set: 0.451839804649353 at epoch: 7 and batch_num: 936\n",
      "Loss of train set: 0.5048799514770508 at epoch: 7 and batch_num: 937\n",
      "Accuracy of train set: 0.8805666666666667\n",
      "Loss of test set: 0.4423336684703827 at epoch: 7 and batch_num: 0\n",
      "Loss of test set: 0.23631517589092255 at epoch: 7 and batch_num: 1\n",
      "Loss of test set: 0.5621902942657471 at epoch: 7 and batch_num: 2\n",
      "Loss of test set: 0.22718945145606995 at epoch: 7 and batch_num: 3\n",
      "Loss of test set: 0.5086138248443604 at epoch: 7 and batch_num: 4\n",
      "Loss of test set: 0.43245068192481995 at epoch: 7 and batch_num: 5\n",
      "Loss of test set: 0.35488176345825195 at epoch: 7 and batch_num: 6\n",
      "Loss of test set: 0.379036545753479 at epoch: 7 and batch_num: 7\n",
      "Loss of test set: 0.3335951268672943 at epoch: 7 and batch_num: 8\n",
      "Loss of test set: 0.49092334508895874 at epoch: 7 and batch_num: 9\n",
      "Loss of test set: 0.3710886240005493 at epoch: 7 and batch_num: 10\n",
      "Loss of test set: 0.4901636242866516 at epoch: 7 and batch_num: 11\n",
      "Loss of test set: 0.42883288860321045 at epoch: 7 and batch_num: 12\n",
      "Loss of test set: 0.43135541677474976 at epoch: 7 and batch_num: 13\n",
      "Loss of test set: 0.3996410369873047 at epoch: 7 and batch_num: 14\n",
      "Loss of test set: 0.2140393853187561 at epoch: 7 and batch_num: 15\n",
      "Loss of test set: 0.43331801891326904 at epoch: 7 and batch_num: 16\n",
      "Loss of test set: 0.3078571557998657 at epoch: 7 and batch_num: 17\n",
      "Loss of test set: 0.4571952819824219 at epoch: 7 and batch_num: 18\n",
      "Loss of test set: 0.46379753947257996 at epoch: 7 and batch_num: 19\n",
      "Loss of test set: 0.6407846212387085 at epoch: 7 and batch_num: 20\n",
      "Loss of test set: 0.495016485452652 at epoch: 7 and batch_num: 21\n",
      "Loss of test set: 0.24874795973300934 at epoch: 7 and batch_num: 22\n",
      "Loss of test set: 0.5002481937408447 at epoch: 7 and batch_num: 23\n",
      "Loss of test set: 0.42129966616630554 at epoch: 7 and batch_num: 24\n",
      "Loss of test set: 0.3159767687320709 at epoch: 7 and batch_num: 25\n",
      "Loss of test set: 0.3827134668827057 at epoch: 7 and batch_num: 26\n",
      "Loss of test set: 0.3060418367385864 at epoch: 7 and batch_num: 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.3287507891654968 at epoch: 7 and batch_num: 28\n",
      "Loss of test set: 0.5892974138259888 at epoch: 7 and batch_num: 29\n",
      "Loss of test set: 0.5687576532363892 at epoch: 7 and batch_num: 30\n",
      "Loss of test set: 0.21634502708911896 at epoch: 7 and batch_num: 31\n",
      "Loss of test set: 0.3581183850765228 at epoch: 7 and batch_num: 32\n",
      "Loss of test set: 0.3899046778678894 at epoch: 7 and batch_num: 33\n",
      "Loss of test set: 0.3020968437194824 at epoch: 7 and batch_num: 34\n",
      "Loss of test set: 0.30048686265945435 at epoch: 7 and batch_num: 35\n",
      "Loss of test set: 0.4439881145954132 at epoch: 7 and batch_num: 36\n",
      "Loss of test set: 0.26509058475494385 at epoch: 7 and batch_num: 37\n",
      "Loss of test set: 0.48212775588035583 at epoch: 7 and batch_num: 38\n",
      "Loss of test set: 0.45081743597984314 at epoch: 7 and batch_num: 39\n",
      "Loss of test set: 0.24934837222099304 at epoch: 7 and batch_num: 40\n",
      "Loss of test set: 0.32586002349853516 at epoch: 7 and batch_num: 41\n",
      "Loss of test set: 0.21868333220481873 at epoch: 7 and batch_num: 42\n",
      "Loss of test set: 0.37148118019104004 at epoch: 7 and batch_num: 43\n",
      "Loss of test set: 0.2624780535697937 at epoch: 7 and batch_num: 44\n",
      "Loss of test set: 0.34679728746414185 at epoch: 7 and batch_num: 45\n",
      "Loss of test set: 0.42596739530563354 at epoch: 7 and batch_num: 46\n",
      "Loss of test set: 0.41482359170913696 at epoch: 7 and batch_num: 47\n",
      "Loss of test set: 0.3710506558418274 at epoch: 7 and batch_num: 48\n",
      "Loss of test set: 0.3571649491786957 at epoch: 7 and batch_num: 49\n",
      "Loss of test set: 0.4914295971393585 at epoch: 7 and batch_num: 50\n",
      "Loss of test set: 0.42421120405197144 at epoch: 7 and batch_num: 51\n",
      "Loss of test set: 0.3405393064022064 at epoch: 7 and batch_num: 52\n",
      "Loss of test set: 0.3336734473705292 at epoch: 7 and batch_num: 53\n",
      "Loss of test set: 0.3332951068878174 at epoch: 7 and batch_num: 54\n",
      "Loss of test set: 0.30234503746032715 at epoch: 7 and batch_num: 55\n",
      "Loss of test set: 0.30169638991355896 at epoch: 7 and batch_num: 56\n",
      "Loss of test set: 0.30303138494491577 at epoch: 7 and batch_num: 57\n",
      "Loss of test set: 0.4975224733352661 at epoch: 7 and batch_num: 58\n",
      "Loss of test set: 0.4947817325592041 at epoch: 7 and batch_num: 59\n",
      "Loss of test set: 0.3194788694381714 at epoch: 7 and batch_num: 60\n",
      "Loss of test set: 0.3144504427909851 at epoch: 7 and batch_num: 61\n",
      "Loss of test set: 0.38938143849372864 at epoch: 7 and batch_num: 62\n",
      "Loss of test set: 0.4804232120513916 at epoch: 7 and batch_num: 63\n",
      "Loss of test set: 0.32480818033218384 at epoch: 7 and batch_num: 64\n",
      "Loss of test set: 0.2870592474937439 at epoch: 7 and batch_num: 65\n",
      "Loss of test set: 0.41860002279281616 at epoch: 7 and batch_num: 66\n",
      "Loss of test set: 0.36892274022102356 at epoch: 7 and batch_num: 67\n",
      "Loss of test set: 0.20047855377197266 at epoch: 7 and batch_num: 68\n",
      "Loss of test set: 0.28750747442245483 at epoch: 7 and batch_num: 69\n",
      "Loss of test set: 0.3869485855102539 at epoch: 7 and batch_num: 70\n",
      "Loss of test set: 0.4090782105922699 at epoch: 7 and batch_num: 71\n",
      "Loss of test set: 0.47386521100997925 at epoch: 7 and batch_num: 72\n",
      "Loss of test set: 0.29214605689048767 at epoch: 7 and batch_num: 73\n",
      "Loss of test set: 0.3280927538871765 at epoch: 7 and batch_num: 74\n",
      "Loss of test set: 0.3915947377681732 at epoch: 7 and batch_num: 75\n",
      "Loss of test set: 0.43366050720214844 at epoch: 7 and batch_num: 76\n",
      "Loss of test set: 0.39621883630752563 at epoch: 7 and batch_num: 77\n",
      "Loss of test set: 0.46228858828544617 at epoch: 7 and batch_num: 78\n",
      "Loss of test set: 0.5302866697311401 at epoch: 7 and batch_num: 79\n",
      "Loss of test set: 0.4011126160621643 at epoch: 7 and batch_num: 80\n",
      "Loss of test set: 0.2509295344352722 at epoch: 7 and batch_num: 81\n",
      "Loss of test set: 0.3107680082321167 at epoch: 7 and batch_num: 82\n",
      "Loss of test set: 0.368275910615921 at epoch: 7 and batch_num: 83\n",
      "Loss of test set: 0.3794439435005188 at epoch: 7 and batch_num: 84\n",
      "Loss of test set: 0.5261183977127075 at epoch: 7 and batch_num: 85\n",
      "Loss of test set: 0.2753516435623169 at epoch: 7 and batch_num: 86\n",
      "Loss of test set: 0.23968447744846344 at epoch: 7 and batch_num: 87\n",
      "Loss of test set: 0.40689805150032043 at epoch: 7 and batch_num: 88\n",
      "Loss of test set: 0.4047776460647583 at epoch: 7 and batch_num: 89\n",
      "Loss of test set: 0.299787312746048 at epoch: 7 and batch_num: 90\n",
      "Loss of test set: 0.23071199655532837 at epoch: 7 and batch_num: 91\n",
      "Loss of test set: 0.41638514399528503 at epoch: 7 and batch_num: 92\n",
      "Loss of test set: 0.42234480381011963 at epoch: 7 and batch_num: 93\n",
      "Loss of test set: 0.31242650747299194 at epoch: 7 and batch_num: 94\n",
      "Loss of test set: 0.5586614608764648 at epoch: 7 and batch_num: 95\n",
      "Loss of test set: 0.3181416094303131 at epoch: 7 and batch_num: 96\n",
      "Loss of test set: 0.6700528860092163 at epoch: 7 and batch_num: 97\n",
      "Loss of test set: 0.3063424825668335 at epoch: 7 and batch_num: 98\n",
      "Loss of test set: 0.5160300135612488 at epoch: 7 and batch_num: 99\n",
      "Loss of test set: 0.2565963864326477 at epoch: 7 and batch_num: 100\n",
      "Loss of test set: 0.458019882440567 at epoch: 7 and batch_num: 101\n",
      "Loss of test set: 0.4156860113143921 at epoch: 7 and batch_num: 102\n",
      "Loss of test set: 0.5004992485046387 at epoch: 7 and batch_num: 103\n",
      "Loss of test set: 0.3903328478336334 at epoch: 7 and batch_num: 104\n",
      "Loss of test set: 0.27070966362953186 at epoch: 7 and batch_num: 105\n",
      "Loss of test set: 0.22580021619796753 at epoch: 7 and batch_num: 106\n",
      "Loss of test set: 0.26599740982055664 at epoch: 7 and batch_num: 107\n",
      "Loss of test set: 0.49532169103622437 at epoch: 7 and batch_num: 108\n",
      "Loss of test set: 0.4595540761947632 at epoch: 7 and batch_num: 109\n",
      "Loss of test set: 0.42277246713638306 at epoch: 7 and batch_num: 110\n",
      "Loss of test set: 0.2555171251296997 at epoch: 7 and batch_num: 111\n",
      "Loss of test set: 0.38296717405319214 at epoch: 7 and batch_num: 112\n",
      "Loss of test set: 0.3940202593803406 at epoch: 7 and batch_num: 113\n",
      "Loss of test set: 0.42671170830726624 at epoch: 7 and batch_num: 114\n",
      "Loss of test set: 0.40351492166519165 at epoch: 7 and batch_num: 115\n",
      "Loss of test set: 0.43019214272499084 at epoch: 7 and batch_num: 116\n",
      "Loss of test set: 0.34851962327957153 at epoch: 7 and batch_num: 117\n",
      "Loss of test set: 0.3167990446090698 at epoch: 7 and batch_num: 118\n",
      "Loss of test set: 0.42627596855163574 at epoch: 7 and batch_num: 119\n",
      "Loss of test set: 0.5211704969406128 at epoch: 7 and batch_num: 120\n",
      "Loss of test set: 0.27898406982421875 at epoch: 7 and batch_num: 121\n",
      "Loss of test set: 0.38775914907455444 at epoch: 7 and batch_num: 122\n",
      "Loss of test set: 0.5747663974761963 at epoch: 7 and batch_num: 123\n",
      "Loss of test set: 0.31536558270454407 at epoch: 7 and batch_num: 124\n",
      "Loss of test set: 0.41727858781814575 at epoch: 7 and batch_num: 125\n",
      "Loss of test set: 0.47303861379623413 at epoch: 7 and batch_num: 126\n",
      "Loss of test set: 0.49051088094711304 at epoch: 7 and batch_num: 127\n",
      "Loss of test set: 0.292880654335022 at epoch: 7 and batch_num: 128\n",
      "Loss of test set: 0.39311671257019043 at epoch: 7 and batch_num: 129\n",
      "Loss of test set: 0.3120385706424713 at epoch: 7 and batch_num: 130\n",
      "Loss of test set: 0.42177075147628784 at epoch: 7 and batch_num: 131\n",
      "Loss of test set: 0.392911434173584 at epoch: 7 and batch_num: 132\n",
      "Loss of test set: 0.5173439979553223 at epoch: 7 and batch_num: 133\n",
      "Loss of test set: 0.5593915581703186 at epoch: 7 and batch_num: 134\n",
      "Loss of test set: 0.3656136393547058 at epoch: 7 and batch_num: 135\n",
      "Loss of test set: 0.29703307151794434 at epoch: 7 and batch_num: 136\n",
      "Loss of test set: 0.5184404850006104 at epoch: 7 and batch_num: 137\n",
      "Loss of test set: 0.32261213660240173 at epoch: 7 and batch_num: 138\n",
      "Loss of test set: 0.6433910131454468 at epoch: 7 and batch_num: 139\n",
      "Loss of test set: 0.38831204175949097 at epoch: 7 and batch_num: 140\n",
      "Loss of test set: 0.19027987122535706 at epoch: 7 and batch_num: 141\n",
      "Loss of test set: 0.3484780192375183 at epoch: 7 and batch_num: 142\n",
      "Loss of test set: 0.21161502599716187 at epoch: 7 and batch_num: 143\n",
      "Loss of test set: 0.3944688141345978 at epoch: 7 and batch_num: 144\n",
      "Loss of test set: 0.3027089834213257 at epoch: 7 and batch_num: 145\n",
      "Loss of test set: 0.46683627367019653 at epoch: 7 and batch_num: 146\n",
      "Loss of test set: 0.32057660818099976 at epoch: 7 and batch_num: 147\n",
      "Loss of test set: 0.3075661063194275 at epoch: 7 and batch_num: 148\n",
      "Loss of test set: 0.36620378494262695 at epoch: 7 and batch_num: 149\n",
      "Loss of test set: 0.22252832353115082 at epoch: 7 and batch_num: 150\n",
      "Loss of test set: 0.2878079414367676 at epoch: 7 and batch_num: 151\n",
      "Loss of test set: 0.5670841932296753 at epoch: 7 and batch_num: 152\n",
      "Loss of test set: 0.3681483864784241 at epoch: 7 and batch_num: 153\n",
      "Loss of test set: 0.4458514451980591 at epoch: 7 and batch_num: 154\n",
      "Loss of test set: 0.3852035105228424 at epoch: 7 and batch_num: 155\n",
      "Loss of test set: 0.552635908126831 at epoch: 7 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8634\n",
      "Loss of train set: 0.2033248245716095 at epoch: 8 and batch_num: 0\n",
      "Loss of train set: 0.22856345772743225 at epoch: 8 and batch_num: 1\n",
      "Loss of train set: 0.47915858030319214 at epoch: 8 and batch_num: 2\n",
      "Loss of train set: 0.2421514391899109 at epoch: 8 and batch_num: 3\n",
      "Loss of train set: 0.20908895134925842 at epoch: 8 and batch_num: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.47597140073776245 at epoch: 8 and batch_num: 5\n",
      "Loss of train set: 0.3601691424846649 at epoch: 8 and batch_num: 6\n",
      "Loss of train set: 0.30447128415107727 at epoch: 8 and batch_num: 7\n",
      "Loss of train set: 0.25991934537887573 at epoch: 8 and batch_num: 8\n",
      "Loss of train set: 0.2143504023551941 at epoch: 8 and batch_num: 9\n",
      "Loss of train set: 0.2812563180923462 at epoch: 8 and batch_num: 10\n",
      "Loss of train set: 0.468336820602417 at epoch: 8 and batch_num: 11\n",
      "Loss of train set: 0.46971240639686584 at epoch: 8 and batch_num: 12\n",
      "Loss of train set: 0.41767430305480957 at epoch: 8 and batch_num: 13\n",
      "Loss of train set: 0.2852696180343628 at epoch: 8 and batch_num: 14\n",
      "Loss of train set: 0.37083899974823 at epoch: 8 and batch_num: 15\n",
      "Loss of train set: 0.2518722712993622 at epoch: 8 and batch_num: 16\n",
      "Loss of train set: 0.25729307532310486 at epoch: 8 and batch_num: 17\n",
      "Loss of train set: 0.3191802501678467 at epoch: 8 and batch_num: 18\n",
      "Loss of train set: 0.14517106115818024 at epoch: 8 and batch_num: 19\n",
      "Loss of train set: 0.158758282661438 at epoch: 8 and batch_num: 20\n",
      "Loss of train set: 0.2812458276748657 at epoch: 8 and batch_num: 21\n",
      "Loss of train set: 0.3760278522968292 at epoch: 8 and batch_num: 22\n",
      "Loss of train set: 0.23428665101528168 at epoch: 8 and batch_num: 23\n",
      "Loss of train set: 0.2929970324039459 at epoch: 8 and batch_num: 24\n",
      "Loss of train set: 0.36133140325546265 at epoch: 8 and batch_num: 25\n",
      "Loss of train set: 0.3528536856174469 at epoch: 8 and batch_num: 26\n",
      "Loss of train set: 0.30048972368240356 at epoch: 8 and batch_num: 27\n",
      "Loss of train set: 0.278077632188797 at epoch: 8 and batch_num: 28\n",
      "Loss of train set: 0.21161462366580963 at epoch: 8 and batch_num: 29\n",
      "Loss of train set: 0.26802563667297363 at epoch: 8 and batch_num: 30\n",
      "Loss of train set: 0.4270034432411194 at epoch: 8 and batch_num: 31\n",
      "Loss of train set: 0.19862541556358337 at epoch: 8 and batch_num: 32\n",
      "Loss of train set: 0.5298702120780945 at epoch: 8 and batch_num: 33\n",
      "Loss of train set: 0.48045551776885986 at epoch: 8 and batch_num: 34\n",
      "Loss of train set: 0.2493337094783783 at epoch: 8 and batch_num: 35\n",
      "Loss of train set: 0.29548028111457825 at epoch: 8 and batch_num: 36\n",
      "Loss of train set: 0.31422287225723267 at epoch: 8 and batch_num: 37\n",
      "Loss of train set: 0.3511248230934143 at epoch: 8 and batch_num: 38\n",
      "Loss of train set: 0.4622400999069214 at epoch: 8 and batch_num: 39\n",
      "Loss of train set: 0.23845088481903076 at epoch: 8 and batch_num: 40\n",
      "Loss of train set: 0.3496500253677368 at epoch: 8 and batch_num: 41\n",
      "Loss of train set: 0.2992568612098694 at epoch: 8 and batch_num: 42\n",
      "Loss of train set: 0.3246409296989441 at epoch: 8 and batch_num: 43\n",
      "Loss of train set: 0.19009041786193848 at epoch: 8 and batch_num: 44\n",
      "Loss of train set: 0.44873034954071045 at epoch: 8 and batch_num: 45\n",
      "Loss of train set: 0.4718341827392578 at epoch: 8 and batch_num: 46\n",
      "Loss of train set: 0.15372325479984283 at epoch: 8 and batch_num: 47\n",
      "Loss of train set: 0.31207823753356934 at epoch: 8 and batch_num: 48\n",
      "Loss of train set: 0.442645788192749 at epoch: 8 and batch_num: 49\n",
      "Loss of train set: 0.4476989209651947 at epoch: 8 and batch_num: 50\n",
      "Loss of train set: 0.24348273873329163 at epoch: 8 and batch_num: 51\n",
      "Loss of train set: 0.4579188823699951 at epoch: 8 and batch_num: 52\n",
      "Loss of train set: 0.5811656713485718 at epoch: 8 and batch_num: 53\n",
      "Loss of train set: 0.31628140807151794 at epoch: 8 and batch_num: 54\n",
      "Loss of train set: 0.36878424882888794 at epoch: 8 and batch_num: 55\n",
      "Loss of train set: 0.3608928620815277 at epoch: 8 and batch_num: 56\n",
      "Loss of train set: 0.19785316288471222 at epoch: 8 and batch_num: 57\n",
      "Loss of train set: 0.2939353287220001 at epoch: 8 and batch_num: 58\n",
      "Loss of train set: 0.38127508759498596 at epoch: 8 and batch_num: 59\n",
      "Loss of train set: 0.2836625576019287 at epoch: 8 and batch_num: 60\n",
      "Loss of train set: 0.3218492567539215 at epoch: 8 and batch_num: 61\n",
      "Loss of train set: 0.22689104080200195 at epoch: 8 and batch_num: 62\n",
      "Loss of train set: 0.22258272767066956 at epoch: 8 and batch_num: 63\n",
      "Loss of train set: 0.3575041890144348 at epoch: 8 and batch_num: 64\n",
      "Loss of train set: 0.36683720350265503 at epoch: 8 and batch_num: 65\n",
      "Loss of train set: 0.28873610496520996 at epoch: 8 and batch_num: 66\n",
      "Loss of train set: 0.3600982129573822 at epoch: 8 and batch_num: 67\n",
      "Loss of train set: 0.24395877122879028 at epoch: 8 and batch_num: 68\n",
      "Loss of train set: 0.5039392113685608 at epoch: 8 and batch_num: 69\n",
      "Loss of train set: 0.23132875561714172 at epoch: 8 and batch_num: 70\n",
      "Loss of train set: 0.3248361647129059 at epoch: 8 and batch_num: 71\n",
      "Loss of train set: 0.2792043089866638 at epoch: 8 and batch_num: 72\n",
      "Loss of train set: 0.32304173707962036 at epoch: 8 and batch_num: 73\n",
      "Loss of train set: 0.3122395873069763 at epoch: 8 and batch_num: 74\n",
      "Loss of train set: 0.30310970544815063 at epoch: 8 and batch_num: 75\n",
      "Loss of train set: 0.4000244438648224 at epoch: 8 and batch_num: 76\n",
      "Loss of train set: 0.21068787574768066 at epoch: 8 and batch_num: 77\n",
      "Loss of train set: 0.3999992907047272 at epoch: 8 and batch_num: 78\n",
      "Loss of train set: 0.19568222761154175 at epoch: 8 and batch_num: 79\n",
      "Loss of train set: 0.22307202219963074 at epoch: 8 and batch_num: 80\n",
      "Loss of train set: 0.24558083713054657 at epoch: 8 and batch_num: 81\n",
      "Loss of train set: 0.5084114074707031 at epoch: 8 and batch_num: 82\n",
      "Loss of train set: 0.2595670819282532 at epoch: 8 and batch_num: 83\n",
      "Loss of train set: 0.31565237045288086 at epoch: 8 and batch_num: 84\n",
      "Loss of train set: 0.3603475093841553 at epoch: 8 and batch_num: 85\n",
      "Loss of train set: 0.39892780780792236 at epoch: 8 and batch_num: 86\n",
      "Loss of train set: 0.40252694487571716 at epoch: 8 and batch_num: 87\n",
      "Loss of train set: 0.20509733259677887 at epoch: 8 and batch_num: 88\n",
      "Loss of train set: 0.21102431416511536 at epoch: 8 and batch_num: 89\n",
      "Loss of train set: 0.3497791588306427 at epoch: 8 and batch_num: 90\n",
      "Loss of train set: 0.34734511375427246 at epoch: 8 and batch_num: 91\n",
      "Loss of train set: 0.4703666865825653 at epoch: 8 and batch_num: 92\n",
      "Loss of train set: 0.3380885720252991 at epoch: 8 and batch_num: 93\n",
      "Loss of train set: 0.32346808910369873 at epoch: 8 and batch_num: 94\n",
      "Loss of train set: 0.2653414011001587 at epoch: 8 and batch_num: 95\n",
      "Loss of train set: 0.39734789729118347 at epoch: 8 and batch_num: 96\n",
      "Loss of train set: 0.35863837599754333 at epoch: 8 and batch_num: 97\n",
      "Loss of train set: 0.159341961145401 at epoch: 8 and batch_num: 98\n",
      "Loss of train set: 0.49728989601135254 at epoch: 8 and batch_num: 99\n",
      "Loss of train set: 0.13578689098358154 at epoch: 8 and batch_num: 100\n",
      "Loss of train set: 0.3134913444519043 at epoch: 8 and batch_num: 101\n",
      "Loss of train set: 0.23786088824272156 at epoch: 8 and batch_num: 102\n",
      "Loss of train set: 0.22889631986618042 at epoch: 8 and batch_num: 103\n",
      "Loss of train set: 0.3490302860736847 at epoch: 8 and batch_num: 104\n",
      "Loss of train set: 0.3678933084011078 at epoch: 8 and batch_num: 105\n",
      "Loss of train set: 0.24570924043655396 at epoch: 8 and batch_num: 106\n",
      "Loss of train set: 0.3596839904785156 at epoch: 8 and batch_num: 107\n",
      "Loss of train set: 0.29131484031677246 at epoch: 8 and batch_num: 108\n",
      "Loss of train set: 0.20362266898155212 at epoch: 8 and batch_num: 109\n",
      "Loss of train set: 0.2768574655056 at epoch: 8 and batch_num: 110\n",
      "Loss of train set: 0.23725400865077972 at epoch: 8 and batch_num: 111\n",
      "Loss of train set: 0.47581103444099426 at epoch: 8 and batch_num: 112\n",
      "Loss of train set: 0.4804941713809967 at epoch: 8 and batch_num: 113\n",
      "Loss of train set: 0.2575477361679077 at epoch: 8 and batch_num: 114\n",
      "Loss of train set: 0.4009305536746979 at epoch: 8 and batch_num: 115\n",
      "Loss of train set: 0.529883861541748 at epoch: 8 and batch_num: 116\n",
      "Loss of train set: 0.2080482840538025 at epoch: 8 and batch_num: 117\n",
      "Loss of train set: 0.35210198163986206 at epoch: 8 and batch_num: 118\n",
      "Loss of train set: 0.45413655042648315 at epoch: 8 and batch_num: 119\n",
      "Loss of train set: 0.2500813901424408 at epoch: 8 and batch_num: 120\n",
      "Loss of train set: 0.20297236740589142 at epoch: 8 and batch_num: 121\n",
      "Loss of train set: 0.37992703914642334 at epoch: 8 and batch_num: 122\n",
      "Loss of train set: 0.2293272763490677 at epoch: 8 and batch_num: 123\n",
      "Loss of train set: 0.45596837997436523 at epoch: 8 and batch_num: 124\n",
      "Loss of train set: 0.35036101937294006 at epoch: 8 and batch_num: 125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.37611448764801025 at epoch: 8 and batch_num: 126\n",
      "Loss of train set: 0.2327909767627716 at epoch: 8 and batch_num: 127\n",
      "Loss of train set: 0.14691124856472015 at epoch: 8 and batch_num: 128\n",
      "Loss of train set: 0.3792697787284851 at epoch: 8 and batch_num: 129\n",
      "Loss of train set: 0.22027981281280518 at epoch: 8 and batch_num: 130\n",
      "Loss of train set: 0.3057478964328766 at epoch: 8 and batch_num: 131\n",
      "Loss of train set: 0.32685449719429016 at epoch: 8 and batch_num: 132\n",
      "Loss of train set: 0.3002930283546448 at epoch: 8 and batch_num: 133\n",
      "Loss of train set: 0.29308342933654785 at epoch: 8 and batch_num: 134\n",
      "Loss of train set: 0.3540043830871582 at epoch: 8 and batch_num: 135\n",
      "Loss of train set: 0.42196938395500183 at epoch: 8 and batch_num: 136\n",
      "Loss of train set: 0.29773300886154175 at epoch: 8 and batch_num: 137\n",
      "Loss of train set: 0.4284646809101105 at epoch: 8 and batch_num: 138\n",
      "Loss of train set: 0.24912402033805847 at epoch: 8 and batch_num: 139\n",
      "Loss of train set: 0.2750736474990845 at epoch: 8 and batch_num: 140\n",
      "Loss of train set: 0.3043849468231201 at epoch: 8 and batch_num: 141\n",
      "Loss of train set: 0.2668185234069824 at epoch: 8 and batch_num: 142\n",
      "Loss of train set: 0.3076989948749542 at epoch: 8 and batch_num: 143\n",
      "Loss of train set: 0.357246071100235 at epoch: 8 and batch_num: 144\n",
      "Loss of train set: 0.5049160718917847 at epoch: 8 and batch_num: 145\n",
      "Loss of train set: 0.2797771096229553 at epoch: 8 and batch_num: 146\n",
      "Loss of train set: 0.3452768921852112 at epoch: 8 and batch_num: 147\n",
      "Loss of train set: 0.24326489865779877 at epoch: 8 and batch_num: 148\n",
      "Loss of train set: 0.1547819972038269 at epoch: 8 and batch_num: 149\n",
      "Loss of train set: 0.24037450551986694 at epoch: 8 and batch_num: 150\n",
      "Loss of train set: 0.3281005620956421 at epoch: 8 and batch_num: 151\n",
      "Loss of train set: 0.23712193965911865 at epoch: 8 and batch_num: 152\n",
      "Loss of train set: 0.3018379509449005 at epoch: 8 and batch_num: 153\n",
      "Loss of train set: 0.3018813729286194 at epoch: 8 and batch_num: 154\n",
      "Loss of train set: 0.3001789450645447 at epoch: 8 and batch_num: 155\n",
      "Loss of train set: 0.3179706335067749 at epoch: 8 and batch_num: 156\n",
      "Loss of train set: 0.42346543073654175 at epoch: 8 and batch_num: 157\n",
      "Loss of train set: 0.4086874723434448 at epoch: 8 and batch_num: 158\n",
      "Loss of train set: 0.23386776447296143 at epoch: 8 and batch_num: 159\n",
      "Loss of train set: 0.2680046856403351 at epoch: 8 and batch_num: 160\n",
      "Loss of train set: 0.3654661774635315 at epoch: 8 and batch_num: 161\n",
      "Loss of train set: 0.3399091064929962 at epoch: 8 and batch_num: 162\n",
      "Loss of train set: 0.2570303976535797 at epoch: 8 and batch_num: 163\n",
      "Loss of train set: 0.23888908326625824 at epoch: 8 and batch_num: 164\n",
      "Loss of train set: 0.36298519372940063 at epoch: 8 and batch_num: 165\n",
      "Loss of train set: 0.2807234823703766 at epoch: 8 and batch_num: 166\n",
      "Loss of train set: 0.3850752115249634 at epoch: 8 and batch_num: 167\n",
      "Loss of train set: 0.3056176006793976 at epoch: 8 and batch_num: 168\n",
      "Loss of train set: 0.12998108565807343 at epoch: 8 and batch_num: 169\n",
      "Loss of train set: 0.4707852303981781 at epoch: 8 and batch_num: 170\n",
      "Loss of train set: 0.40039893984794617 at epoch: 8 and batch_num: 171\n",
      "Loss of train set: 0.3576623499393463 at epoch: 8 and batch_num: 172\n",
      "Loss of train set: 0.3473874628543854 at epoch: 8 and batch_num: 173\n",
      "Loss of train set: 0.3301154375076294 at epoch: 8 and batch_num: 174\n",
      "Loss of train set: 0.25331610441207886 at epoch: 8 and batch_num: 175\n",
      "Loss of train set: 0.3880671262741089 at epoch: 8 and batch_num: 176\n",
      "Loss of train set: 0.7459861040115356 at epoch: 8 and batch_num: 177\n",
      "Loss of train set: 0.35089391469955444 at epoch: 8 and batch_num: 178\n",
      "Loss of train set: 0.3202389180660248 at epoch: 8 and batch_num: 179\n",
      "Loss of train set: 0.5881072282791138 at epoch: 8 and batch_num: 180\n",
      "Loss of train set: 0.2745841145515442 at epoch: 8 and batch_num: 181\n",
      "Loss of train set: 0.3260478377342224 at epoch: 8 and batch_num: 182\n",
      "Loss of train set: 0.2732301950454712 at epoch: 8 and batch_num: 183\n",
      "Loss of train set: 0.2564648687839508 at epoch: 8 and batch_num: 184\n",
      "Loss of train set: 0.34834620356559753 at epoch: 8 and batch_num: 185\n",
      "Loss of train set: 0.24556976556777954 at epoch: 8 and batch_num: 186\n",
      "Loss of train set: 0.4253676235675812 at epoch: 8 and batch_num: 187\n",
      "Loss of train set: 0.3016699552536011 at epoch: 8 and batch_num: 188\n",
      "Loss of train set: 0.4278257489204407 at epoch: 8 and batch_num: 189\n",
      "Loss of train set: 0.39575231075286865 at epoch: 8 and batch_num: 190\n",
      "Loss of train set: 0.3571595251560211 at epoch: 8 and batch_num: 191\n",
      "Loss of train set: 0.15083755552768707 at epoch: 8 and batch_num: 192\n",
      "Loss of train set: 0.21303024888038635 at epoch: 8 and batch_num: 193\n",
      "Loss of train set: 0.3729342520236969 at epoch: 8 and batch_num: 194\n",
      "Loss of train set: 0.42171192169189453 at epoch: 8 and batch_num: 195\n",
      "Loss of train set: 0.40108439326286316 at epoch: 8 and batch_num: 196\n",
      "Loss of train set: 0.4760251045227051 at epoch: 8 and batch_num: 197\n",
      "Loss of train set: 0.3997351825237274 at epoch: 8 and batch_num: 198\n",
      "Loss of train set: 0.4630686640739441 at epoch: 8 and batch_num: 199\n",
      "Loss of train set: 0.1980372965335846 at epoch: 8 and batch_num: 200\n",
      "Loss of train set: 0.4370726943016052 at epoch: 8 and batch_num: 201\n",
      "Loss of train set: 0.4421502947807312 at epoch: 8 and batch_num: 202\n",
      "Loss of train set: 0.3158326745033264 at epoch: 8 and batch_num: 203\n",
      "Loss of train set: 0.3518839180469513 at epoch: 8 and batch_num: 204\n",
      "Loss of train set: 0.38563841581344604 at epoch: 8 and batch_num: 205\n",
      "Loss of train set: 0.4238005578517914 at epoch: 8 and batch_num: 206\n",
      "Loss of train set: 0.42722633481025696 at epoch: 8 and batch_num: 207\n",
      "Loss of train set: 0.3711282014846802 at epoch: 8 and batch_num: 208\n",
      "Loss of train set: 0.3799021542072296 at epoch: 8 and batch_num: 209\n",
      "Loss of train set: 0.3023039996623993 at epoch: 8 and batch_num: 210\n",
      "Loss of train set: 0.35484597086906433 at epoch: 8 and batch_num: 211\n",
      "Loss of train set: 0.29182201623916626 at epoch: 8 and batch_num: 212\n",
      "Loss of train set: 0.261555016040802 at epoch: 8 and batch_num: 213\n",
      "Loss of train set: 0.18767094612121582 at epoch: 8 and batch_num: 214\n",
      "Loss of train set: 0.3661358952522278 at epoch: 8 and batch_num: 215\n",
      "Loss of train set: 0.3209284842014313 at epoch: 8 and batch_num: 216\n",
      "Loss of train set: 0.19538763165473938 at epoch: 8 and batch_num: 217\n",
      "Loss of train set: 0.4179363548755646 at epoch: 8 and batch_num: 218\n",
      "Loss of train set: 0.2443433701992035 at epoch: 8 and batch_num: 219\n",
      "Loss of train set: 0.3243459463119507 at epoch: 8 and batch_num: 220\n",
      "Loss of train set: 0.299567312002182 at epoch: 8 and batch_num: 221\n",
      "Loss of train set: 0.36298221349716187 at epoch: 8 and batch_num: 222\n",
      "Loss of train set: 0.28323280811309814 at epoch: 8 and batch_num: 223\n",
      "Loss of train set: 0.6013785004615784 at epoch: 8 and batch_num: 224\n",
      "Loss of train set: 0.360402375459671 at epoch: 8 and batch_num: 225\n",
      "Loss of train set: 0.33242562413215637 at epoch: 8 and batch_num: 226\n",
      "Loss of train set: 0.42353755235671997 at epoch: 8 and batch_num: 227\n",
      "Loss of train set: 0.24979445338249207 at epoch: 8 and batch_num: 228\n",
      "Loss of train set: 0.18342456221580505 at epoch: 8 and batch_num: 229\n",
      "Loss of train set: 0.33716273307800293 at epoch: 8 and batch_num: 230\n",
      "Loss of train set: 0.5258740186691284 at epoch: 8 and batch_num: 231\n",
      "Loss of train set: 0.4189980924129486 at epoch: 8 and batch_num: 232\n",
      "Loss of train set: 0.29803958535194397 at epoch: 8 and batch_num: 233\n",
      "Loss of train set: 0.3129177391529083 at epoch: 8 and batch_num: 234\n",
      "Loss of train set: 0.21340374648571014 at epoch: 8 and batch_num: 235\n",
      "Loss of train set: 0.3326756954193115 at epoch: 8 and batch_num: 236\n",
      "Loss of train set: 0.25779882073402405 at epoch: 8 and batch_num: 237\n",
      "Loss of train set: 0.20622125267982483 at epoch: 8 and batch_num: 238\n",
      "Loss of train set: 0.2880532145500183 at epoch: 8 and batch_num: 239\n",
      "Loss of train set: 0.2878161072731018 at epoch: 8 and batch_num: 240\n",
      "Loss of train set: 0.49093714356422424 at epoch: 8 and batch_num: 241\n",
      "Loss of train set: 0.3385610580444336 at epoch: 8 and batch_num: 242\n",
      "Loss of train set: 0.2422826886177063 at epoch: 8 and batch_num: 243\n",
      "Loss of train set: 0.18534600734710693 at epoch: 8 and batch_num: 244\n",
      "Loss of train set: 0.38150304555892944 at epoch: 8 and batch_num: 245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.5150747299194336 at epoch: 8 and batch_num: 246\n",
      "Loss of train set: 0.26957374811172485 at epoch: 8 and batch_num: 247\n",
      "Loss of train set: 0.30094584822654724 at epoch: 8 and batch_num: 248\n",
      "Loss of train set: 0.3490542471408844 at epoch: 8 and batch_num: 249\n",
      "Loss of train set: 0.36976271867752075 at epoch: 8 and batch_num: 250\n",
      "Loss of train set: 0.33738547563552856 at epoch: 8 and batch_num: 251\n",
      "Loss of train set: 0.4133372902870178 at epoch: 8 and batch_num: 252\n",
      "Loss of train set: 0.3750782608985901 at epoch: 8 and batch_num: 253\n",
      "Loss of train set: 0.31794798374176025 at epoch: 8 and batch_num: 254\n",
      "Loss of train set: 0.23923395574092865 at epoch: 8 and batch_num: 255\n",
      "Loss of train set: 0.24339011311531067 at epoch: 8 and batch_num: 256\n",
      "Loss of train set: 0.4210852384567261 at epoch: 8 and batch_num: 257\n",
      "Loss of train set: 0.43871989846229553 at epoch: 8 and batch_num: 258\n",
      "Loss of train set: 0.3213270902633667 at epoch: 8 and batch_num: 259\n",
      "Loss of train set: 0.4756367802619934 at epoch: 8 and batch_num: 260\n",
      "Loss of train set: 0.3759288191795349 at epoch: 8 and batch_num: 261\n",
      "Loss of train set: 0.2664511799812317 at epoch: 8 and batch_num: 262\n",
      "Loss of train set: 0.2863690257072449 at epoch: 8 and batch_num: 263\n",
      "Loss of train set: 0.32903191447257996 at epoch: 8 and batch_num: 264\n",
      "Loss of train set: 0.2821815013885498 at epoch: 8 and batch_num: 265\n",
      "Loss of train set: 0.32930201292037964 at epoch: 8 and batch_num: 266\n",
      "Loss of train set: 0.49215906858444214 at epoch: 8 and batch_num: 267\n",
      "Loss of train set: 0.3441337049007416 at epoch: 8 and batch_num: 268\n",
      "Loss of train set: 0.5570770502090454 at epoch: 8 and batch_num: 269\n",
      "Loss of train set: 0.29247286915779114 at epoch: 8 and batch_num: 270\n",
      "Loss of train set: 0.29531461000442505 at epoch: 8 and batch_num: 271\n",
      "Loss of train set: 0.2960759401321411 at epoch: 8 and batch_num: 272\n",
      "Loss of train set: 0.283834844827652 at epoch: 8 and batch_num: 273\n",
      "Loss of train set: 0.18430811166763306 at epoch: 8 and batch_num: 274\n",
      "Loss of train set: 0.29582250118255615 at epoch: 8 and batch_num: 275\n",
      "Loss of train set: 0.267558217048645 at epoch: 8 and batch_num: 276\n",
      "Loss of train set: 0.3882071077823639 at epoch: 8 and batch_num: 277\n",
      "Loss of train set: 0.5151951313018799 at epoch: 8 and batch_num: 278\n",
      "Loss of train set: 0.18947768211364746 at epoch: 8 and batch_num: 279\n",
      "Loss of train set: 0.24735857546329498 at epoch: 8 and batch_num: 280\n",
      "Loss of train set: 0.1870415210723877 at epoch: 8 and batch_num: 281\n",
      "Loss of train set: 0.27880898118019104 at epoch: 8 and batch_num: 282\n",
      "Loss of train set: 0.29894569516181946 at epoch: 8 and batch_num: 283\n",
      "Loss of train set: 0.34587419033050537 at epoch: 8 and batch_num: 284\n",
      "Loss of train set: 0.4339427351951599 at epoch: 8 and batch_num: 285\n",
      "Loss of train set: 0.45290499925613403 at epoch: 8 and batch_num: 286\n",
      "Loss of train set: 0.4591008126735687 at epoch: 8 and batch_num: 287\n",
      "Loss of train set: 0.2440231740474701 at epoch: 8 and batch_num: 288\n",
      "Loss of train set: 0.2937965393066406 at epoch: 8 and batch_num: 289\n",
      "Loss of train set: 0.2976856827735901 at epoch: 8 and batch_num: 290\n",
      "Loss of train set: 0.2276512086391449 at epoch: 8 and batch_num: 291\n",
      "Loss of train set: 0.3809444308280945 at epoch: 8 and batch_num: 292\n",
      "Loss of train set: 0.34591442346572876 at epoch: 8 and batch_num: 293\n",
      "Loss of train set: 0.514407753944397 at epoch: 8 and batch_num: 294\n",
      "Loss of train set: 0.18975389003753662 at epoch: 8 and batch_num: 295\n",
      "Loss of train set: 0.16291207075119019 at epoch: 8 and batch_num: 296\n",
      "Loss of train set: 0.4259461760520935 at epoch: 8 and batch_num: 297\n",
      "Loss of train set: 0.32856521010398865 at epoch: 8 and batch_num: 298\n",
      "Loss of train set: 0.41624724864959717 at epoch: 8 and batch_num: 299\n",
      "Loss of train set: 0.3200078308582306 at epoch: 8 and batch_num: 300\n",
      "Loss of train set: 0.27163976430892944 at epoch: 8 and batch_num: 301\n",
      "Loss of train set: 0.2538740336894989 at epoch: 8 and batch_num: 302\n",
      "Loss of train set: 0.2697383165359497 at epoch: 8 and batch_num: 303\n",
      "Loss of train set: 0.24722081422805786 at epoch: 8 and batch_num: 304\n",
      "Loss of train set: 0.4925486445426941 at epoch: 8 and batch_num: 305\n",
      "Loss of train set: 0.34564313292503357 at epoch: 8 and batch_num: 306\n",
      "Loss of train set: 0.32658469676971436 at epoch: 8 and batch_num: 307\n",
      "Loss of train set: 0.34190523624420166 at epoch: 8 and batch_num: 308\n",
      "Loss of train set: 0.37296515703201294 at epoch: 8 and batch_num: 309\n",
      "Loss of train set: 0.2860777676105499 at epoch: 8 and batch_num: 310\n",
      "Loss of train set: 0.4870532155036926 at epoch: 8 and batch_num: 311\n",
      "Loss of train set: 0.34203609824180603 at epoch: 8 and batch_num: 312\n",
      "Loss of train set: 0.26384279131889343 at epoch: 8 and batch_num: 313\n",
      "Loss of train set: 0.45644187927246094 at epoch: 8 and batch_num: 314\n",
      "Loss of train set: 0.24418285489082336 at epoch: 8 and batch_num: 315\n",
      "Loss of train set: 0.2741052508354187 at epoch: 8 and batch_num: 316\n",
      "Loss of train set: 0.1353643834590912 at epoch: 8 and batch_num: 317\n",
      "Loss of train set: 0.45458948612213135 at epoch: 8 and batch_num: 318\n",
      "Loss of train set: 0.2619558572769165 at epoch: 8 and batch_num: 319\n",
      "Loss of train set: 0.3697163462638855 at epoch: 8 and batch_num: 320\n",
      "Loss of train set: 0.20893073081970215 at epoch: 8 and batch_num: 321\n",
      "Loss of train set: 0.26677727699279785 at epoch: 8 and batch_num: 322\n",
      "Loss of train set: 0.3652554154396057 at epoch: 8 and batch_num: 323\n",
      "Loss of train set: 0.4340132474899292 at epoch: 8 and batch_num: 324\n",
      "Loss of train set: 0.3114358186721802 at epoch: 8 and batch_num: 325\n",
      "Loss of train set: 0.23484501242637634 at epoch: 8 and batch_num: 326\n",
      "Loss of train set: 0.24531075358390808 at epoch: 8 and batch_num: 327\n",
      "Loss of train set: 0.244736909866333 at epoch: 8 and batch_num: 328\n",
      "Loss of train set: 0.34832054376602173 at epoch: 8 and batch_num: 329\n",
      "Loss of train set: 0.18588055670261383 at epoch: 8 and batch_num: 330\n",
      "Loss of train set: 0.30919450521469116 at epoch: 8 and batch_num: 331\n",
      "Loss of train set: 0.24120643734931946 at epoch: 8 and batch_num: 332\n",
      "Loss of train set: 0.3607032895088196 at epoch: 8 and batch_num: 333\n",
      "Loss of train set: 0.22170209884643555 at epoch: 8 and batch_num: 334\n",
      "Loss of train set: 0.580125629901886 at epoch: 8 and batch_num: 335\n",
      "Loss of train set: 0.3208789825439453 at epoch: 8 and batch_num: 336\n",
      "Loss of train set: 0.38826245069503784 at epoch: 8 and batch_num: 337\n",
      "Loss of train set: 0.3939964473247528 at epoch: 8 and batch_num: 338\n",
      "Loss of train set: 0.29689404368400574 at epoch: 8 and batch_num: 339\n",
      "Loss of train set: 0.22572806477546692 at epoch: 8 and batch_num: 340\n",
      "Loss of train set: 0.37171030044555664 at epoch: 8 and batch_num: 341\n",
      "Loss of train set: 0.29032737016677856 at epoch: 8 and batch_num: 342\n",
      "Loss of train set: 0.2068747878074646 at epoch: 8 and batch_num: 343\n",
      "Loss of train set: 0.3713982105255127 at epoch: 8 and batch_num: 344\n",
      "Loss of train set: 0.33484846353530884 at epoch: 8 and batch_num: 345\n",
      "Loss of train set: 0.31618815660476685 at epoch: 8 and batch_num: 346\n",
      "Loss of train set: 0.34926456212997437 at epoch: 8 and batch_num: 347\n",
      "Loss of train set: 0.37769269943237305 at epoch: 8 and batch_num: 348\n",
      "Loss of train set: 0.39337921142578125 at epoch: 8 and batch_num: 349\n",
      "Loss of train set: 0.2542269825935364 at epoch: 8 and batch_num: 350\n",
      "Loss of train set: 0.21871930360794067 at epoch: 8 and batch_num: 351\n",
      "Loss of train set: 0.31003427505493164 at epoch: 8 and batch_num: 352\n",
      "Loss of train set: 0.3610967695713043 at epoch: 8 and batch_num: 353\n",
      "Loss of train set: 0.26965177059173584 at epoch: 8 and batch_num: 354\n",
      "Loss of train set: 0.27302560210227966 at epoch: 8 and batch_num: 355\n",
      "Loss of train set: 0.41781139373779297 at epoch: 8 and batch_num: 356\n",
      "Loss of train set: 0.4655574858188629 at epoch: 8 and batch_num: 357\n",
      "Loss of train set: 0.12922757863998413 at epoch: 8 and batch_num: 358\n",
      "Loss of train set: 0.3535749316215515 at epoch: 8 and batch_num: 359\n",
      "Loss of train set: 0.4342232346534729 at epoch: 8 and batch_num: 360\n",
      "Loss of train set: 0.28315427899360657 at epoch: 8 and batch_num: 361\n",
      "Loss of train set: 0.2755421996116638 at epoch: 8 and batch_num: 362\n",
      "Loss of train set: 0.35946011543273926 at epoch: 8 and batch_num: 363\n",
      "Loss of train set: 0.31168749928474426 at epoch: 8 and batch_num: 364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.29206880927085876 at epoch: 8 and batch_num: 365\n",
      "Loss of train set: 0.3217492699623108 at epoch: 8 and batch_num: 366\n",
      "Loss of train set: 0.3683471083641052 at epoch: 8 and batch_num: 367\n",
      "Loss of train set: 0.3580361604690552 at epoch: 8 and batch_num: 368\n",
      "Loss of train set: 0.2834290564060211 at epoch: 8 and batch_num: 369\n",
      "Loss of train set: 0.3497617840766907 at epoch: 8 and batch_num: 370\n",
      "Loss of train set: 0.3065762519836426 at epoch: 8 and batch_num: 371\n",
      "Loss of train set: 0.3407667279243469 at epoch: 8 and batch_num: 372\n",
      "Loss of train set: 0.4051424264907837 at epoch: 8 and batch_num: 373\n",
      "Loss of train set: 0.1523062288761139 at epoch: 8 and batch_num: 374\n",
      "Loss of train set: 0.3324817419052124 at epoch: 8 and batch_num: 375\n",
      "Loss of train set: 0.4483303427696228 at epoch: 8 and batch_num: 376\n",
      "Loss of train set: 0.26937153935432434 at epoch: 8 and batch_num: 377\n",
      "Loss of train set: 0.3875167965888977 at epoch: 8 and batch_num: 378\n",
      "Loss of train set: 0.26110249757766724 at epoch: 8 and batch_num: 379\n",
      "Loss of train set: 0.381952702999115 at epoch: 8 and batch_num: 380\n",
      "Loss of train set: 0.23669156432151794 at epoch: 8 and batch_num: 381\n",
      "Loss of train set: 0.2793448865413666 at epoch: 8 and batch_num: 382\n",
      "Loss of train set: 0.5158085823059082 at epoch: 8 and batch_num: 383\n",
      "Loss of train set: 0.24963673949241638 at epoch: 8 and batch_num: 384\n",
      "Loss of train set: 0.3578636348247528 at epoch: 8 and batch_num: 385\n",
      "Loss of train set: 0.2785080373287201 at epoch: 8 and batch_num: 386\n",
      "Loss of train set: 0.40228545665740967 at epoch: 8 and batch_num: 387\n",
      "Loss of train set: 0.40006327629089355 at epoch: 8 and batch_num: 388\n",
      "Loss of train set: 0.38658714294433594 at epoch: 8 and batch_num: 389\n",
      "Loss of train set: 0.257242888212204 at epoch: 8 and batch_num: 390\n",
      "Loss of train set: 0.3511962294578552 at epoch: 8 and batch_num: 391\n",
      "Loss of train set: 0.3378257155418396 at epoch: 8 and batch_num: 392\n",
      "Loss of train set: 0.28689831495285034 at epoch: 8 and batch_num: 393\n",
      "Loss of train set: 0.3686554729938507 at epoch: 8 and batch_num: 394\n",
      "Loss of train set: 0.34823334217071533 at epoch: 8 and batch_num: 395\n",
      "Loss of train set: 0.2972339987754822 at epoch: 8 and batch_num: 396\n",
      "Loss of train set: 0.44176721572875977 at epoch: 8 and batch_num: 397\n",
      "Loss of train set: 0.38746178150177 at epoch: 8 and batch_num: 398\n",
      "Loss of train set: 0.2749534249305725 at epoch: 8 and batch_num: 399\n",
      "Loss of train set: 0.26831480860710144 at epoch: 8 and batch_num: 400\n",
      "Loss of train set: 0.49597686529159546 at epoch: 8 and batch_num: 401\n",
      "Loss of train set: 0.37163692712783813 at epoch: 8 and batch_num: 402\n",
      "Loss of train set: 0.537594199180603 at epoch: 8 and batch_num: 403\n",
      "Loss of train set: 0.263464093208313 at epoch: 8 and batch_num: 404\n",
      "Loss of train set: 0.3734965920448303 at epoch: 8 and batch_num: 405\n",
      "Loss of train set: 0.5150334239006042 at epoch: 8 and batch_num: 406\n",
      "Loss of train set: 0.3615387976169586 at epoch: 8 and batch_num: 407\n",
      "Loss of train set: 0.315546452999115 at epoch: 8 and batch_num: 408\n",
      "Loss of train set: 0.3373406231403351 at epoch: 8 and batch_num: 409\n",
      "Loss of train set: 0.252644807100296 at epoch: 8 and batch_num: 410\n",
      "Loss of train set: 0.3818630874156952 at epoch: 8 and batch_num: 411\n",
      "Loss of train set: 0.2310791313648224 at epoch: 8 and batch_num: 412\n",
      "Loss of train set: 0.3925235867500305 at epoch: 8 and batch_num: 413\n",
      "Loss of train set: 0.3667726516723633 at epoch: 8 and batch_num: 414\n",
      "Loss of train set: 0.26374420523643494 at epoch: 8 and batch_num: 415\n",
      "Loss of train set: 0.28181278705596924 at epoch: 8 and batch_num: 416\n",
      "Loss of train set: 0.3634323477745056 at epoch: 8 and batch_num: 417\n",
      "Loss of train set: 0.36087530851364136 at epoch: 8 and batch_num: 418\n",
      "Loss of train set: 0.28280317783355713 at epoch: 8 and batch_num: 419\n",
      "Loss of train set: 0.6153725385665894 at epoch: 8 and batch_num: 420\n",
      "Loss of train set: 0.28572899103164673 at epoch: 8 and batch_num: 421\n",
      "Loss of train set: 0.22251558303833008 at epoch: 8 and batch_num: 422\n",
      "Loss of train set: 0.20878194272518158 at epoch: 8 and batch_num: 423\n",
      "Loss of train set: 0.29238590598106384 at epoch: 8 and batch_num: 424\n",
      "Loss of train set: 0.4240882992744446 at epoch: 8 and batch_num: 425\n",
      "Loss of train set: 0.2930941581726074 at epoch: 8 and batch_num: 426\n",
      "Loss of train set: 0.4089193344116211 at epoch: 8 and batch_num: 427\n",
      "Loss of train set: 0.24321623146533966 at epoch: 8 and batch_num: 428\n",
      "Loss of train set: 0.45280590653419495 at epoch: 8 and batch_num: 429\n",
      "Loss of train set: 0.2800048589706421 at epoch: 8 and batch_num: 430\n",
      "Loss of train set: 0.28006911277770996 at epoch: 8 and batch_num: 431\n",
      "Loss of train set: 0.42523571848869324 at epoch: 8 and batch_num: 432\n",
      "Loss of train set: 0.15750719606876373 at epoch: 8 and batch_num: 433\n",
      "Loss of train set: 0.48213493824005127 at epoch: 8 and batch_num: 434\n",
      "Loss of train set: 0.22194412350654602 at epoch: 8 and batch_num: 435\n",
      "Loss of train set: 0.3372654616832733 at epoch: 8 and batch_num: 436\n",
      "Loss of train set: 0.29250556230545044 at epoch: 8 and batch_num: 437\n",
      "Loss of train set: 0.39780473709106445 at epoch: 8 and batch_num: 438\n",
      "Loss of train set: 0.6181671619415283 at epoch: 8 and batch_num: 439\n",
      "Loss of train set: 0.2807402014732361 at epoch: 8 and batch_num: 440\n",
      "Loss of train set: 0.2703414559364319 at epoch: 8 and batch_num: 441\n",
      "Loss of train set: 0.3996320366859436 at epoch: 8 and batch_num: 442\n",
      "Loss of train set: 0.264641135931015 at epoch: 8 and batch_num: 443\n",
      "Loss of train set: 0.38064876198768616 at epoch: 8 and batch_num: 444\n",
      "Loss of train set: 0.4389162063598633 at epoch: 8 and batch_num: 445\n",
      "Loss of train set: 0.2778032422065735 at epoch: 8 and batch_num: 446\n",
      "Loss of train set: 0.34255915880203247 at epoch: 8 and batch_num: 447\n",
      "Loss of train set: 0.31717875599861145 at epoch: 8 and batch_num: 448\n",
      "Loss of train set: 0.2836703658103943 at epoch: 8 and batch_num: 449\n",
      "Loss of train set: 0.24745802581310272 at epoch: 8 and batch_num: 450\n",
      "Loss of train set: 0.3068663775920868 at epoch: 8 and batch_num: 451\n",
      "Loss of train set: 0.2583569586277008 at epoch: 8 and batch_num: 452\n",
      "Loss of train set: 0.2918092906475067 at epoch: 8 and batch_num: 453\n",
      "Loss of train set: 0.3250318765640259 at epoch: 8 and batch_num: 454\n",
      "Loss of train set: 0.3750782012939453 at epoch: 8 and batch_num: 455\n",
      "Loss of train set: 0.44366782903671265 at epoch: 8 and batch_num: 456\n",
      "Loss of train set: 0.33411699533462524 at epoch: 8 and batch_num: 457\n",
      "Loss of train set: 0.4222829341888428 at epoch: 8 and batch_num: 458\n",
      "Loss of train set: 0.5419716238975525 at epoch: 8 and batch_num: 459\n",
      "Loss of train set: 0.35139569640159607 at epoch: 8 and batch_num: 460\n",
      "Loss of train set: 0.40044116973876953 at epoch: 8 and batch_num: 461\n",
      "Loss of train set: 0.29893922805786133 at epoch: 8 and batch_num: 462\n",
      "Loss of train set: 0.3555022180080414 at epoch: 8 and batch_num: 463\n",
      "Loss of train set: 0.3531641364097595 at epoch: 8 and batch_num: 464\n",
      "Loss of train set: 0.2639993727207184 at epoch: 8 and batch_num: 465\n",
      "Loss of train set: 0.2607317268848419 at epoch: 8 and batch_num: 466\n",
      "Loss of train set: 0.28351786732673645 at epoch: 8 and batch_num: 467\n",
      "Loss of train set: 0.5084119439125061 at epoch: 8 and batch_num: 468\n",
      "Loss of train set: 0.3659045100212097 at epoch: 8 and batch_num: 469\n",
      "Loss of train set: 0.3054049015045166 at epoch: 8 and batch_num: 470\n",
      "Loss of train set: 0.24634383618831635 at epoch: 8 and batch_num: 471\n",
      "Loss of train set: 0.2702709436416626 at epoch: 8 and batch_num: 472\n",
      "Loss of train set: 0.4477783441543579 at epoch: 8 and batch_num: 473\n",
      "Loss of train set: 0.36476966738700867 at epoch: 8 and batch_num: 474\n",
      "Loss of train set: 0.33129772543907166 at epoch: 8 and batch_num: 475\n",
      "Loss of train set: 0.41828691959381104 at epoch: 8 and batch_num: 476\n",
      "Loss of train set: 0.16093480587005615 at epoch: 8 and batch_num: 477\n",
      "Loss of train set: 0.28853410482406616 at epoch: 8 and batch_num: 478\n",
      "Loss of train set: 0.349751353263855 at epoch: 8 and batch_num: 479\n",
      "Loss of train set: 0.4242548644542694 at epoch: 8 and batch_num: 480\n",
      "Loss of train set: 0.3164829611778259 at epoch: 8 and batch_num: 481\n",
      "Loss of train set: 0.23543143272399902 at epoch: 8 and batch_num: 482\n",
      "Loss of train set: 0.31995248794555664 at epoch: 8 and batch_num: 483\n",
      "Loss of train set: 0.48324453830718994 at epoch: 8 and batch_num: 484\n",
      "Loss of train set: 0.39926743507385254 at epoch: 8 and batch_num: 485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.4206847548484802 at epoch: 8 and batch_num: 486\n",
      "Loss of train set: 0.2461090385913849 at epoch: 8 and batch_num: 487\n",
      "Loss of train set: 0.5150731205940247 at epoch: 8 and batch_num: 488\n",
      "Loss of train set: 0.3842810392379761 at epoch: 8 and batch_num: 489\n",
      "Loss of train set: 0.233411967754364 at epoch: 8 and batch_num: 490\n",
      "Loss of train set: 0.2641856074333191 at epoch: 8 and batch_num: 491\n",
      "Loss of train set: 0.33194562792778015 at epoch: 8 and batch_num: 492\n",
      "Loss of train set: 0.258251816034317 at epoch: 8 and batch_num: 493\n",
      "Loss of train set: 0.27614259719848633 at epoch: 8 and batch_num: 494\n",
      "Loss of train set: 0.27099764347076416 at epoch: 8 and batch_num: 495\n",
      "Loss of train set: 0.4190291166305542 at epoch: 8 and batch_num: 496\n",
      "Loss of train set: 0.4427005350589752 at epoch: 8 and batch_num: 497\n",
      "Loss of train set: 0.3795246481895447 at epoch: 8 and batch_num: 498\n",
      "Loss of train set: 0.38189437985420227 at epoch: 8 and batch_num: 499\n",
      "Loss of train set: 0.20488761365413666 at epoch: 8 and batch_num: 500\n",
      "Loss of train set: 0.24217388033866882 at epoch: 8 and batch_num: 501\n",
      "Loss of train set: 0.4098270535469055 at epoch: 8 and batch_num: 502\n",
      "Loss of train set: 0.26156923174858093 at epoch: 8 and batch_num: 503\n",
      "Loss of train set: 0.3316194415092468 at epoch: 8 and batch_num: 504\n",
      "Loss of train set: 0.24109846353530884 at epoch: 8 and batch_num: 505\n",
      "Loss of train set: 0.4296119213104248 at epoch: 8 and batch_num: 506\n",
      "Loss of train set: 0.39308488368988037 at epoch: 8 and batch_num: 507\n",
      "Loss of train set: 0.23779219388961792 at epoch: 8 and batch_num: 508\n",
      "Loss of train set: 0.3810369372367859 at epoch: 8 and batch_num: 509\n",
      "Loss of train set: 0.23184210062026978 at epoch: 8 and batch_num: 510\n",
      "Loss of train set: 0.23962858319282532 at epoch: 8 and batch_num: 511\n",
      "Loss of train set: 0.309037446975708 at epoch: 8 and batch_num: 512\n",
      "Loss of train set: 0.37371575832366943 at epoch: 8 and batch_num: 513\n",
      "Loss of train set: 0.6333414316177368 at epoch: 8 and batch_num: 514\n",
      "Loss of train set: 0.259899377822876 at epoch: 8 and batch_num: 515\n",
      "Loss of train set: 0.421352356672287 at epoch: 8 and batch_num: 516\n",
      "Loss of train set: 0.2705722153186798 at epoch: 8 and batch_num: 517\n",
      "Loss of train set: 0.34587734937667847 at epoch: 8 and batch_num: 518\n",
      "Loss of train set: 0.4041270613670349 at epoch: 8 and batch_num: 519\n",
      "Loss of train set: 0.20710787177085876 at epoch: 8 and batch_num: 520\n",
      "Loss of train set: 0.30869224667549133 at epoch: 8 and batch_num: 521\n",
      "Loss of train set: 0.37966254353523254 at epoch: 8 and batch_num: 522\n",
      "Loss of train set: 0.42455920577049255 at epoch: 8 and batch_num: 523\n",
      "Loss of train set: 0.390808641910553 at epoch: 8 and batch_num: 524\n",
      "Loss of train set: 0.49299371242523193 at epoch: 8 and batch_num: 525\n",
      "Loss of train set: 0.16610553860664368 at epoch: 8 and batch_num: 526\n",
      "Loss of train set: 0.34183722734451294 at epoch: 8 and batch_num: 527\n",
      "Loss of train set: 0.3495970368385315 at epoch: 8 and batch_num: 528\n",
      "Loss of train set: 0.2367415726184845 at epoch: 8 and batch_num: 529\n",
      "Loss of train set: 0.27550411224365234 at epoch: 8 and batch_num: 530\n",
      "Loss of train set: 0.17478349804878235 at epoch: 8 and batch_num: 531\n",
      "Loss of train set: 0.346943199634552 at epoch: 8 and batch_num: 532\n",
      "Loss of train set: 0.20331206917762756 at epoch: 8 and batch_num: 533\n",
      "Loss of train set: 0.29242029786109924 at epoch: 8 and batch_num: 534\n",
      "Loss of train set: 0.3136833608150482 at epoch: 8 and batch_num: 535\n",
      "Loss of train set: 0.24077367782592773 at epoch: 8 and batch_num: 536\n",
      "Loss of train set: 0.283053457736969 at epoch: 8 and batch_num: 537\n",
      "Loss of train set: 0.4046989679336548 at epoch: 8 and batch_num: 538\n",
      "Loss of train set: 0.2649723291397095 at epoch: 8 and batch_num: 539\n",
      "Loss of train set: 0.32685959339141846 at epoch: 8 and batch_num: 540\n",
      "Loss of train set: 0.2639802098274231 at epoch: 8 and batch_num: 541\n",
      "Loss of train set: 0.3110005557537079 at epoch: 8 and batch_num: 542\n",
      "Loss of train set: 0.4222806990146637 at epoch: 8 and batch_num: 543\n",
      "Loss of train set: 0.2679736614227295 at epoch: 8 and batch_num: 544\n",
      "Loss of train set: 0.4170589745044708 at epoch: 8 and batch_num: 545\n",
      "Loss of train set: 0.21570183336734772 at epoch: 8 and batch_num: 546\n",
      "Loss of train set: 0.3004903793334961 at epoch: 8 and batch_num: 547\n",
      "Loss of train set: 0.34349796175956726 at epoch: 8 and batch_num: 548\n",
      "Loss of train set: 0.31836050748825073 at epoch: 8 and batch_num: 549\n",
      "Loss of train set: 0.4193163514137268 at epoch: 8 and batch_num: 550\n",
      "Loss of train set: 0.45625245571136475 at epoch: 8 and batch_num: 551\n",
      "Loss of train set: 0.3371772766113281 at epoch: 8 and batch_num: 552\n",
      "Loss of train set: 0.26209181547164917 at epoch: 8 and batch_num: 553\n",
      "Loss of train set: 0.16966113448143005 at epoch: 8 and batch_num: 554\n",
      "Loss of train set: 0.3126920461654663 at epoch: 8 and batch_num: 555\n",
      "Loss of train set: 0.2562538981437683 at epoch: 8 and batch_num: 556\n",
      "Loss of train set: 0.4178393483161926 at epoch: 8 and batch_num: 557\n",
      "Loss of train set: 0.30025890469551086 at epoch: 8 and batch_num: 558\n",
      "Loss of train set: 0.2821449041366577 at epoch: 8 and batch_num: 559\n",
      "Loss of train set: 0.339846670627594 at epoch: 8 and batch_num: 560\n",
      "Loss of train set: 0.28040173649787903 at epoch: 8 and batch_num: 561\n",
      "Loss of train set: 0.33248546719551086 at epoch: 8 and batch_num: 562\n",
      "Loss of train set: 0.33711713552474976 at epoch: 8 and batch_num: 563\n",
      "Loss of train set: 0.19061030447483063 at epoch: 8 and batch_num: 564\n",
      "Loss of train set: 0.15700113773345947 at epoch: 8 and batch_num: 565\n",
      "Loss of train set: 0.2552427053451538 at epoch: 8 and batch_num: 566\n",
      "Loss of train set: 0.2992058992385864 at epoch: 8 and batch_num: 567\n",
      "Loss of train set: 0.30790022015571594 at epoch: 8 and batch_num: 568\n",
      "Loss of train set: 0.3802291750907898 at epoch: 8 and batch_num: 569\n",
      "Loss of train set: 0.32789871096611023 at epoch: 8 and batch_num: 570\n",
      "Loss of train set: 0.48970896005630493 at epoch: 8 and batch_num: 571\n",
      "Loss of train set: 0.37074539065361023 at epoch: 8 and batch_num: 572\n",
      "Loss of train set: 0.4837878346443176 at epoch: 8 and batch_num: 573\n",
      "Loss of train set: 0.24091657996177673 at epoch: 8 and batch_num: 574\n",
      "Loss of train set: 0.30784568190574646 at epoch: 8 and batch_num: 575\n",
      "Loss of train set: 0.28270602226257324 at epoch: 8 and batch_num: 576\n",
      "Loss of train set: 0.29715317487716675 at epoch: 8 and batch_num: 577\n",
      "Loss of train set: 0.21288086473941803 at epoch: 8 and batch_num: 578\n",
      "Loss of train set: 0.14412786066532135 at epoch: 8 and batch_num: 579\n",
      "Loss of train set: 0.21037866175174713 at epoch: 8 and batch_num: 580\n",
      "Loss of train set: 0.469704270362854 at epoch: 8 and batch_num: 581\n",
      "Loss of train set: 0.4149225354194641 at epoch: 8 and batch_num: 582\n",
      "Loss of train set: 0.24842071533203125 at epoch: 8 and batch_num: 583\n",
      "Loss of train set: 0.34346041083335876 at epoch: 8 and batch_num: 584\n",
      "Loss of train set: 0.27476829290390015 at epoch: 8 and batch_num: 585\n",
      "Loss of train set: 0.19638307392597198 at epoch: 8 and batch_num: 586\n",
      "Loss of train set: 0.377320796251297 at epoch: 8 and batch_num: 587\n",
      "Loss of train set: 0.293160080909729 at epoch: 8 and batch_num: 588\n",
      "Loss of train set: 0.4508572220802307 at epoch: 8 and batch_num: 589\n",
      "Loss of train set: 0.2815745174884796 at epoch: 8 and batch_num: 590\n",
      "Loss of train set: 0.6183388233184814 at epoch: 8 and batch_num: 591\n",
      "Loss of train set: 0.4269610643386841 at epoch: 8 and batch_num: 592\n",
      "Loss of train set: 0.24634981155395508 at epoch: 8 and batch_num: 593\n",
      "Loss of train set: 0.25986555218696594 at epoch: 8 and batch_num: 594\n",
      "Loss of train set: 0.2741699814796448 at epoch: 8 and batch_num: 595\n",
      "Loss of train set: 0.30654239654541016 at epoch: 8 and batch_num: 596\n",
      "Loss of train set: 0.3292108476161957 at epoch: 8 and batch_num: 597\n",
      "Loss of train set: 0.3512866199016571 at epoch: 8 and batch_num: 598\n",
      "Loss of train set: 0.4274576008319855 at epoch: 8 and batch_num: 599\n",
      "Loss of train set: 0.35214245319366455 at epoch: 8 and batch_num: 600\n",
      "Loss of train set: 0.38957667350769043 at epoch: 8 and batch_num: 601\n",
      "Loss of train set: 0.41061121225357056 at epoch: 8 and batch_num: 602\n",
      "Loss of train set: 0.27325356006622314 at epoch: 8 and batch_num: 603\n",
      "Loss of train set: 0.24462160468101501 at epoch: 8 and batch_num: 604\n",
      "Loss of train set: 0.2658957839012146 at epoch: 8 and batch_num: 605\n",
      "Loss of train set: 0.29803991317749023 at epoch: 8 and batch_num: 606\n",
      "Loss of train set: 0.3934785723686218 at epoch: 8 and batch_num: 607\n",
      "Loss of train set: 0.46865153312683105 at epoch: 8 and batch_num: 608\n",
      "Loss of train set: 0.2905595302581787 at epoch: 8 and batch_num: 609\n",
      "Loss of train set: 0.30270910263061523 at epoch: 8 and batch_num: 610\n",
      "Loss of train set: 0.3740910589694977 at epoch: 8 and batch_num: 611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.21934032440185547 at epoch: 8 and batch_num: 612\n",
      "Loss of train set: 0.26141273975372314 at epoch: 8 and batch_num: 613\n",
      "Loss of train set: 0.24788197875022888 at epoch: 8 and batch_num: 614\n",
      "Loss of train set: 0.301155149936676 at epoch: 8 and batch_num: 615\n",
      "Loss of train set: 0.26500558853149414 at epoch: 8 and batch_num: 616\n",
      "Loss of train set: 0.29638615250587463 at epoch: 8 and batch_num: 617\n",
      "Loss of train set: 0.4936319589614868 at epoch: 8 and batch_num: 618\n",
      "Loss of train set: 0.2570657432079315 at epoch: 8 and batch_num: 619\n",
      "Loss of train set: 0.2667368948459625 at epoch: 8 and batch_num: 620\n",
      "Loss of train set: 0.35712212324142456 at epoch: 8 and batch_num: 621\n",
      "Loss of train set: 0.3695903718471527 at epoch: 8 and batch_num: 622\n",
      "Loss of train set: 0.368336021900177 at epoch: 8 and batch_num: 623\n",
      "Loss of train set: 0.512651801109314 at epoch: 8 and batch_num: 624\n",
      "Loss of train set: 0.18868908286094666 at epoch: 8 and batch_num: 625\n",
      "Loss of train set: 0.25557297468185425 at epoch: 8 and batch_num: 626\n",
      "Loss of train set: 0.3263091444969177 at epoch: 8 and batch_num: 627\n",
      "Loss of train set: 0.33933186531066895 at epoch: 8 and batch_num: 628\n",
      "Loss of train set: 0.33388495445251465 at epoch: 8 and batch_num: 629\n",
      "Loss of train set: 0.33908432722091675 at epoch: 8 and batch_num: 630\n",
      "Loss of train set: 0.36348509788513184 at epoch: 8 and batch_num: 631\n",
      "Loss of train set: 0.2893289625644684 at epoch: 8 and batch_num: 632\n",
      "Loss of train set: 0.3324471116065979 at epoch: 8 and batch_num: 633\n",
      "Loss of train set: 0.34773287177085876 at epoch: 8 and batch_num: 634\n",
      "Loss of train set: 0.2800460159778595 at epoch: 8 and batch_num: 635\n",
      "Loss of train set: 0.187259703874588 at epoch: 8 and batch_num: 636\n",
      "Loss of train set: 0.3677143156528473 at epoch: 8 and batch_num: 637\n",
      "Loss of train set: 0.20901818573474884 at epoch: 8 and batch_num: 638\n",
      "Loss of train set: 0.6429752111434937 at epoch: 8 and batch_num: 639\n",
      "Loss of train set: 0.32089298963546753 at epoch: 8 and batch_num: 640\n",
      "Loss of train set: 0.2828247547149658 at epoch: 8 and batch_num: 641\n",
      "Loss of train set: 0.32286596298217773 at epoch: 8 and batch_num: 642\n",
      "Loss of train set: 0.3691731095314026 at epoch: 8 and batch_num: 643\n",
      "Loss of train set: 0.3493064045906067 at epoch: 8 and batch_num: 644\n",
      "Loss of train set: 0.3602635860443115 at epoch: 8 and batch_num: 645\n",
      "Loss of train set: 0.36238205432891846 at epoch: 8 and batch_num: 646\n",
      "Loss of train set: 0.39062070846557617 at epoch: 8 and batch_num: 647\n",
      "Loss of train set: 0.16890624165534973 at epoch: 8 and batch_num: 648\n",
      "Loss of train set: 0.39118415117263794 at epoch: 8 and batch_num: 649\n",
      "Loss of train set: 0.3104078769683838 at epoch: 8 and batch_num: 650\n",
      "Loss of train set: 0.2628624439239502 at epoch: 8 and batch_num: 651\n",
      "Loss of train set: 0.45938217639923096 at epoch: 8 and batch_num: 652\n",
      "Loss of train set: 0.42949140071868896 at epoch: 8 and batch_num: 653\n",
      "Loss of train set: 0.2704787850379944 at epoch: 8 and batch_num: 654\n",
      "Loss of train set: 0.15347138047218323 at epoch: 8 and batch_num: 655\n",
      "Loss of train set: 0.33710023760795593 at epoch: 8 and batch_num: 656\n",
      "Loss of train set: 0.2090461105108261 at epoch: 8 and batch_num: 657\n",
      "Loss of train set: 0.23565936088562012 at epoch: 8 and batch_num: 658\n",
      "Loss of train set: 0.31520652770996094 at epoch: 8 and batch_num: 659\n",
      "Loss of train set: 0.4583170413970947 at epoch: 8 and batch_num: 660\n",
      "Loss of train set: 0.41238686442375183 at epoch: 8 and batch_num: 661\n",
      "Loss of train set: 0.24210745096206665 at epoch: 8 and batch_num: 662\n",
      "Loss of train set: 0.18314668536186218 at epoch: 8 and batch_num: 663\n",
      "Loss of train set: 0.3958023190498352 at epoch: 8 and batch_num: 664\n",
      "Loss of train set: 0.43847334384918213 at epoch: 8 and batch_num: 665\n",
      "Loss of train set: 0.5192283987998962 at epoch: 8 and batch_num: 666\n",
      "Loss of train set: 0.37734729051589966 at epoch: 8 and batch_num: 667\n",
      "Loss of train set: 0.2938295304775238 at epoch: 8 and batch_num: 668\n",
      "Loss of train set: 0.22864070534706116 at epoch: 8 and batch_num: 669\n",
      "Loss of train set: 0.3683250844478607 at epoch: 8 and batch_num: 670\n",
      "Loss of train set: 0.22055548429489136 at epoch: 8 and batch_num: 671\n",
      "Loss of train set: 0.30316853523254395 at epoch: 8 and batch_num: 672\n",
      "Loss of train set: 0.43476152420043945 at epoch: 8 and batch_num: 673\n",
      "Loss of train set: 0.26753735542297363 at epoch: 8 and batch_num: 674\n",
      "Loss of train set: 0.351057767868042 at epoch: 8 and batch_num: 675\n",
      "Loss of train set: 0.37088167667388916 at epoch: 8 and batch_num: 676\n",
      "Loss of train set: 0.21288347244262695 at epoch: 8 and batch_num: 677\n",
      "Loss of train set: 0.5095846056938171 at epoch: 8 and batch_num: 678\n",
      "Loss of train set: 0.4306734800338745 at epoch: 8 and batch_num: 679\n",
      "Loss of train set: 0.22972899675369263 at epoch: 8 and batch_num: 680\n",
      "Loss of train set: 0.2388661503791809 at epoch: 8 and batch_num: 681\n",
      "Loss of train set: 0.481017529964447 at epoch: 8 and batch_num: 682\n",
      "Loss of train set: 0.25766897201538086 at epoch: 8 and batch_num: 683\n",
      "Loss of train set: 0.3742176294326782 at epoch: 8 and batch_num: 684\n",
      "Loss of train set: 0.37372446060180664 at epoch: 8 and batch_num: 685\n",
      "Loss of train set: 0.33828121423721313 at epoch: 8 and batch_num: 686\n",
      "Loss of train set: 0.4107530117034912 at epoch: 8 and batch_num: 687\n",
      "Loss of train set: 0.40694278478622437 at epoch: 8 and batch_num: 688\n",
      "Loss of train set: 0.37529921531677246 at epoch: 8 and batch_num: 689\n",
      "Loss of train set: 0.3700142502784729 at epoch: 8 and batch_num: 690\n",
      "Loss of train set: 0.36397668719291687 at epoch: 8 and batch_num: 691\n",
      "Loss of train set: 0.3832487165927887 at epoch: 8 and batch_num: 692\n",
      "Loss of train set: 0.3630542755126953 at epoch: 8 and batch_num: 693\n",
      "Loss of train set: 0.24342253804206848 at epoch: 8 and batch_num: 694\n",
      "Loss of train set: 0.25679925084114075 at epoch: 8 and batch_num: 695\n",
      "Loss of train set: 0.4373023211956024 at epoch: 8 and batch_num: 696\n",
      "Loss of train set: 0.25925666093826294 at epoch: 8 and batch_num: 697\n",
      "Loss of train set: 0.4798782169818878 at epoch: 8 and batch_num: 698\n",
      "Loss of train set: 0.1689666360616684 at epoch: 8 and batch_num: 699\n",
      "Loss of train set: 0.3341791033744812 at epoch: 8 and batch_num: 700\n",
      "Loss of train set: 0.4202752411365509 at epoch: 8 and batch_num: 701\n",
      "Loss of train set: 0.3134559690952301 at epoch: 8 and batch_num: 702\n",
      "Loss of train set: 0.19305646419525146 at epoch: 8 and batch_num: 703\n",
      "Loss of train set: 0.29351601004600525 at epoch: 8 and batch_num: 704\n",
      "Loss of train set: 0.43337786197662354 at epoch: 8 and batch_num: 705\n",
      "Loss of train set: 0.3580203652381897 at epoch: 8 and batch_num: 706\n",
      "Loss of train set: 0.24631595611572266 at epoch: 8 and batch_num: 707\n",
      "Loss of train set: 0.5266196131706238 at epoch: 8 and batch_num: 708\n",
      "Loss of train set: 0.21965116262435913 at epoch: 8 and batch_num: 709\n",
      "Loss of train set: 0.3441235423088074 at epoch: 8 and batch_num: 710\n",
      "Loss of train set: 0.20830141007900238 at epoch: 8 and batch_num: 711\n",
      "Loss of train set: 0.25449663400650024 at epoch: 8 and batch_num: 712\n",
      "Loss of train set: 0.313202828168869 at epoch: 8 and batch_num: 713\n",
      "Loss of train set: 0.2797742187976837 at epoch: 8 and batch_num: 714\n",
      "Loss of train set: 0.368450790643692 at epoch: 8 and batch_num: 715\n",
      "Loss of train set: 0.14944960176944733 at epoch: 8 and batch_num: 716\n",
      "Loss of train set: 0.27974557876586914 at epoch: 8 and batch_num: 717\n",
      "Loss of train set: 0.38076645135879517 at epoch: 8 and batch_num: 718\n",
      "Loss of train set: 0.40077486634254456 at epoch: 8 and batch_num: 719\n",
      "Loss of train set: 0.34839317202568054 at epoch: 8 and batch_num: 720\n",
      "Loss of train set: 0.39265280961990356 at epoch: 8 and batch_num: 721\n",
      "Loss of train set: 0.2432972639799118 at epoch: 8 and batch_num: 722\n",
      "Loss of train set: 0.5076087713241577 at epoch: 8 and batch_num: 723\n",
      "Loss of train set: 0.37891069054603577 at epoch: 8 and batch_num: 724\n",
      "Loss of train set: 0.4114263653755188 at epoch: 8 and batch_num: 725\n",
      "Loss of train set: 0.3939340114593506 at epoch: 8 and batch_num: 726\n",
      "Loss of train set: 0.19494947791099548 at epoch: 8 and batch_num: 727\n",
      "Loss of train set: 0.3643510341644287 at epoch: 8 and batch_num: 728\n",
      "Loss of train set: 0.35395368933677673 at epoch: 8 and batch_num: 729\n",
      "Loss of train set: 0.4362219572067261 at epoch: 8 and batch_num: 730\n",
      "Loss of train set: 0.3013448715209961 at epoch: 8 and batch_num: 731\n",
      "Loss of train set: 0.2720531225204468 at epoch: 8 and batch_num: 732\n",
      "Loss of train set: 0.28798022866249084 at epoch: 8 and batch_num: 733\n",
      "Loss of train set: 0.3749063014984131 at epoch: 8 and batch_num: 734\n",
      "Loss of train set: 0.342620849609375 at epoch: 8 and batch_num: 735\n",
      "Loss of train set: 0.4778481125831604 at epoch: 8 and batch_num: 736\n",
      "Loss of train set: 0.19164812564849854 at epoch: 8 and batch_num: 737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.28285014629364014 at epoch: 8 and batch_num: 738\n",
      "Loss of train set: 0.4115533232688904 at epoch: 8 and batch_num: 739\n",
      "Loss of train set: 0.30247801542282104 at epoch: 8 and batch_num: 740\n",
      "Loss of train set: 0.5068860650062561 at epoch: 8 and batch_num: 741\n",
      "Loss of train set: 0.3281624913215637 at epoch: 8 and batch_num: 742\n",
      "Loss of train set: 0.4618823826313019 at epoch: 8 and batch_num: 743\n",
      "Loss of train set: 0.22613871097564697 at epoch: 8 and batch_num: 744\n",
      "Loss of train set: 0.27798646688461304 at epoch: 8 and batch_num: 745\n",
      "Loss of train set: 0.3641146421432495 at epoch: 8 and batch_num: 746\n",
      "Loss of train set: 0.4536842405796051 at epoch: 8 and batch_num: 747\n",
      "Loss of train set: 0.3942352533340454 at epoch: 8 and batch_num: 748\n",
      "Loss of train set: 0.2890549600124359 at epoch: 8 and batch_num: 749\n",
      "Loss of train set: 0.4046264588832855 at epoch: 8 and batch_num: 750\n",
      "Loss of train set: 0.35639509558677673 at epoch: 8 and batch_num: 751\n",
      "Loss of train set: 0.4097699522972107 at epoch: 8 and batch_num: 752\n",
      "Loss of train set: 0.25130903720855713 at epoch: 8 and batch_num: 753\n",
      "Loss of train set: 0.2920011878013611 at epoch: 8 and batch_num: 754\n",
      "Loss of train set: 0.3093968629837036 at epoch: 8 and batch_num: 755\n",
      "Loss of train set: 0.24485106766223907 at epoch: 8 and batch_num: 756\n",
      "Loss of train set: 0.15312866866588593 at epoch: 8 and batch_num: 757\n",
      "Loss of train set: 0.36504027247428894 at epoch: 8 and batch_num: 758\n",
      "Loss of train set: 0.3667575716972351 at epoch: 8 and batch_num: 759\n",
      "Loss of train set: 0.3722879886627197 at epoch: 8 and batch_num: 760\n",
      "Loss of train set: 0.33848023414611816 at epoch: 8 and batch_num: 761\n",
      "Loss of train set: 0.4689144492149353 at epoch: 8 and batch_num: 762\n",
      "Loss of train set: 0.5585719347000122 at epoch: 8 and batch_num: 763\n",
      "Loss of train set: 0.32986417412757874 at epoch: 8 and batch_num: 764\n",
      "Loss of train set: 0.3258832097053528 at epoch: 8 and batch_num: 765\n",
      "Loss of train set: 0.2143367975950241 at epoch: 8 and batch_num: 766\n",
      "Loss of train set: 0.23911771178245544 at epoch: 8 and batch_num: 767\n",
      "Loss of train set: 0.21813169121742249 at epoch: 8 and batch_num: 768\n",
      "Loss of train set: 0.5002206563949585 at epoch: 8 and batch_num: 769\n",
      "Loss of train set: 0.2469571977853775 at epoch: 8 and batch_num: 770\n",
      "Loss of train set: 0.42256203293800354 at epoch: 8 and batch_num: 771\n",
      "Loss of train set: 0.24995647370815277 at epoch: 8 and batch_num: 772\n",
      "Loss of train set: 0.5347616076469421 at epoch: 8 and batch_num: 773\n",
      "Loss of train set: 0.31021571159362793 at epoch: 8 and batch_num: 774\n",
      "Loss of train set: 0.330513060092926 at epoch: 8 and batch_num: 775\n",
      "Loss of train set: 0.24347034096717834 at epoch: 8 and batch_num: 776\n",
      "Loss of train set: 0.3696974813938141 at epoch: 8 and batch_num: 777\n",
      "Loss of train set: 0.42632991075515747 at epoch: 8 and batch_num: 778\n",
      "Loss of train set: 0.34133338928222656 at epoch: 8 and batch_num: 779\n",
      "Loss of train set: 0.2709953188896179 at epoch: 8 and batch_num: 780\n",
      "Loss of train set: 0.3424704372882843 at epoch: 8 and batch_num: 781\n",
      "Loss of train set: 0.3034796714782715 at epoch: 8 and batch_num: 782\n",
      "Loss of train set: 0.47115209698677063 at epoch: 8 and batch_num: 783\n",
      "Loss of train set: 0.21780629456043243 at epoch: 8 and batch_num: 784\n",
      "Loss of train set: 0.42798811197280884 at epoch: 8 and batch_num: 785\n",
      "Loss of train set: 0.2440163642168045 at epoch: 8 and batch_num: 786\n",
      "Loss of train set: 0.28368079662323 at epoch: 8 and batch_num: 787\n",
      "Loss of train set: 0.34447020292282104 at epoch: 8 and batch_num: 788\n",
      "Loss of train set: 0.3097444176673889 at epoch: 8 and batch_num: 789\n",
      "Loss of train set: 0.2973977327346802 at epoch: 8 and batch_num: 790\n",
      "Loss of train set: 0.5986847281455994 at epoch: 8 and batch_num: 791\n",
      "Loss of train set: 0.2326015830039978 at epoch: 8 and batch_num: 792\n",
      "Loss of train set: 0.45400285720825195 at epoch: 8 and batch_num: 793\n",
      "Loss of train set: 0.36835092306137085 at epoch: 8 and batch_num: 794\n",
      "Loss of train set: 0.4567415416240692 at epoch: 8 and batch_num: 795\n",
      "Loss of train set: 0.36217913031578064 at epoch: 8 and batch_num: 796\n",
      "Loss of train set: 0.22130444645881653 at epoch: 8 and batch_num: 797\n",
      "Loss of train set: 0.35157209634780884 at epoch: 8 and batch_num: 798\n",
      "Loss of train set: 0.2545221745967865 at epoch: 8 and batch_num: 799\n",
      "Loss of train set: 0.4044426679611206 at epoch: 8 and batch_num: 800\n",
      "Loss of train set: 0.16584962606430054 at epoch: 8 and batch_num: 801\n",
      "Loss of train set: 0.2940515875816345 at epoch: 8 and batch_num: 802\n",
      "Loss of train set: 0.19239266216754913 at epoch: 8 and batch_num: 803\n",
      "Loss of train set: 0.23051601648330688 at epoch: 8 and batch_num: 804\n",
      "Loss of train set: 0.3541869819164276 at epoch: 8 and batch_num: 805\n",
      "Loss of train set: 0.3193318843841553 at epoch: 8 and batch_num: 806\n",
      "Loss of train set: 0.2010396122932434 at epoch: 8 and batch_num: 807\n",
      "Loss of train set: 0.33394259214401245 at epoch: 8 and batch_num: 808\n",
      "Loss of train set: 0.4138389229774475 at epoch: 8 and batch_num: 809\n",
      "Loss of train set: 0.31664717197418213 at epoch: 8 and batch_num: 810\n",
      "Loss of train set: 0.38378268480300903 at epoch: 8 and batch_num: 811\n",
      "Loss of train set: 0.42191237211227417 at epoch: 8 and batch_num: 812\n",
      "Loss of train set: 0.3169613778591156 at epoch: 8 and batch_num: 813\n",
      "Loss of train set: 0.19835850596427917 at epoch: 8 and batch_num: 814\n",
      "Loss of train set: 0.3443308472633362 at epoch: 8 and batch_num: 815\n",
      "Loss of train set: 0.273105263710022 at epoch: 8 and batch_num: 816\n",
      "Loss of train set: 0.33808720111846924 at epoch: 8 and batch_num: 817\n",
      "Loss of train set: 0.2395341396331787 at epoch: 8 and batch_num: 818\n",
      "Loss of train set: 0.5073255896568298 at epoch: 8 and batch_num: 819\n",
      "Loss of train set: 0.44562897086143494 at epoch: 8 and batch_num: 820\n",
      "Loss of train set: 0.27570846676826477 at epoch: 8 and batch_num: 821\n",
      "Loss of train set: 0.46260693669319153 at epoch: 8 and batch_num: 822\n",
      "Loss of train set: 0.22769853472709656 at epoch: 8 and batch_num: 823\n",
      "Loss of train set: 0.30074968934059143 at epoch: 8 and batch_num: 824\n",
      "Loss of train set: 0.188131183385849 at epoch: 8 and batch_num: 825\n",
      "Loss of train set: 0.3131852149963379 at epoch: 8 and batch_num: 826\n",
      "Loss of train set: 0.24968576431274414 at epoch: 8 and batch_num: 827\n",
      "Loss of train set: 0.5333647727966309 at epoch: 8 and batch_num: 828\n",
      "Loss of train set: 0.3447954058647156 at epoch: 8 and batch_num: 829\n",
      "Loss of train set: 0.42507249116897583 at epoch: 8 and batch_num: 830\n",
      "Loss of train set: 0.4539828896522522 at epoch: 8 and batch_num: 831\n",
      "Loss of train set: 0.25740575790405273 at epoch: 8 and batch_num: 832\n",
      "Loss of train set: 0.2512587010860443 at epoch: 8 and batch_num: 833\n",
      "Loss of train set: 0.35645103454589844 at epoch: 8 and batch_num: 834\n",
      "Loss of train set: 0.27866047620773315 at epoch: 8 and batch_num: 835\n",
      "Loss of train set: 0.5451034903526306 at epoch: 8 and batch_num: 836\n",
      "Loss of train set: 0.26144587993621826 at epoch: 8 and batch_num: 837\n",
      "Loss of train set: 0.2466895580291748 at epoch: 8 and batch_num: 838\n",
      "Loss of train set: 0.3073117136955261 at epoch: 8 and batch_num: 839\n",
      "Loss of train set: 0.2041613608598709 at epoch: 8 and batch_num: 840\n",
      "Loss of train set: 0.3868767023086548 at epoch: 8 and batch_num: 841\n",
      "Loss of train set: 0.24068397283554077 at epoch: 8 and batch_num: 842\n",
      "Loss of train set: 0.3862280249595642 at epoch: 8 and batch_num: 843\n",
      "Loss of train set: 0.39415839314460754 at epoch: 8 and batch_num: 844\n",
      "Loss of train set: 0.28300392627716064 at epoch: 8 and batch_num: 845\n",
      "Loss of train set: 0.3401378095149994 at epoch: 8 and batch_num: 846\n",
      "Loss of train set: 0.440376341342926 at epoch: 8 and batch_num: 847\n",
      "Loss of train set: 0.34899067878723145 at epoch: 8 and batch_num: 848\n",
      "Loss of train set: 0.474174439907074 at epoch: 8 and batch_num: 849\n",
      "Loss of train set: 0.4663856029510498 at epoch: 8 and batch_num: 850\n",
      "Loss of train set: 0.47591879963874817 at epoch: 8 and batch_num: 851\n",
      "Loss of train set: 0.33995521068573 at epoch: 8 and batch_num: 852\n",
      "Loss of train set: 0.3200150728225708 at epoch: 8 and batch_num: 853\n",
      "Loss of train set: 0.35868972539901733 at epoch: 8 and batch_num: 854\n",
      "Loss of train set: 0.36164671182632446 at epoch: 8 and batch_num: 855\n",
      "Loss of train set: 0.3318965435028076 at epoch: 8 and batch_num: 856\n",
      "Loss of train set: 0.2858195900917053 at epoch: 8 and batch_num: 857\n",
      "Loss of train set: 0.39236950874328613 at epoch: 8 and batch_num: 858\n",
      "Loss of train set: 0.2078457474708557 at epoch: 8 and batch_num: 859\n",
      "Loss of train set: 0.4662766754627228 at epoch: 8 and batch_num: 860\n",
      "Loss of train set: 0.2310006022453308 at epoch: 8 and batch_num: 861\n",
      "Loss of train set: 0.34823089838027954 at epoch: 8 and batch_num: 862\n",
      "Loss of train set: 0.4016784429550171 at epoch: 8 and batch_num: 863\n",
      "Loss of train set: 0.38320088386535645 at epoch: 8 and batch_num: 864\n",
      "Loss of train set: 0.27675774693489075 at epoch: 8 and batch_num: 865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.32426056265830994 at epoch: 8 and batch_num: 866\n",
      "Loss of train set: 0.3911969065666199 at epoch: 8 and batch_num: 867\n",
      "Loss of train set: 0.2566627860069275 at epoch: 8 and batch_num: 868\n",
      "Loss of train set: 0.4175966680049896 at epoch: 8 and batch_num: 869\n",
      "Loss of train set: 0.275531142950058 at epoch: 8 and batch_num: 870\n",
      "Loss of train set: 0.46899357438087463 at epoch: 8 and batch_num: 871\n",
      "Loss of train set: 0.26720157265663147 at epoch: 8 and batch_num: 872\n",
      "Loss of train set: 0.6920775175094604 at epoch: 8 and batch_num: 873\n",
      "Loss of train set: 0.4353748559951782 at epoch: 8 and batch_num: 874\n",
      "Loss of train set: 0.3417065739631653 at epoch: 8 and batch_num: 875\n",
      "Loss of train set: 0.3548036813735962 at epoch: 8 and batch_num: 876\n",
      "Loss of train set: 0.5020124912261963 at epoch: 8 and batch_num: 877\n",
      "Loss of train set: 0.2780233323574066 at epoch: 8 and batch_num: 878\n",
      "Loss of train set: 0.29892072081565857 at epoch: 8 and batch_num: 879\n",
      "Loss of train set: 0.31359490752220154 at epoch: 8 and batch_num: 880\n",
      "Loss of train set: 0.27906644344329834 at epoch: 8 and batch_num: 881\n",
      "Loss of train set: 0.30367371439933777 at epoch: 8 and batch_num: 882\n",
      "Loss of train set: 0.30657291412353516 at epoch: 8 and batch_num: 883\n",
      "Loss of train set: 0.23674292862415314 at epoch: 8 and batch_num: 884\n",
      "Loss of train set: 0.5171352624893188 at epoch: 8 and batch_num: 885\n",
      "Loss of train set: 0.37804216146469116 at epoch: 8 and batch_num: 886\n",
      "Loss of train set: 0.28726306557655334 at epoch: 8 and batch_num: 887\n",
      "Loss of train set: 0.2034452259540558 at epoch: 8 and batch_num: 888\n",
      "Loss of train set: 0.3868623971939087 at epoch: 8 and batch_num: 889\n",
      "Loss of train set: 0.24720102548599243 at epoch: 8 and batch_num: 890\n",
      "Loss of train set: 0.5414450764656067 at epoch: 8 and batch_num: 891\n",
      "Loss of train set: 0.3113064765930176 at epoch: 8 and batch_num: 892\n",
      "Loss of train set: 0.5286802649497986 at epoch: 8 and batch_num: 893\n",
      "Loss of train set: 0.5444738268852234 at epoch: 8 and batch_num: 894\n",
      "Loss of train set: 0.35944461822509766 at epoch: 8 and batch_num: 895\n",
      "Loss of train set: 0.24548883736133575 at epoch: 8 and batch_num: 896\n",
      "Loss of train set: 0.3388069272041321 at epoch: 8 and batch_num: 897\n",
      "Loss of train set: 0.2570737302303314 at epoch: 8 and batch_num: 898\n",
      "Loss of train set: 0.34889155626296997 at epoch: 8 and batch_num: 899\n",
      "Loss of train set: 0.19583144783973694 at epoch: 8 and batch_num: 900\n",
      "Loss of train set: 0.21311938762664795 at epoch: 8 and batch_num: 901\n",
      "Loss of train set: 0.238189697265625 at epoch: 8 and batch_num: 902\n",
      "Loss of train set: 0.24425916373729706 at epoch: 8 and batch_num: 903\n",
      "Loss of train set: 0.2984668016433716 at epoch: 8 and batch_num: 904\n",
      "Loss of train set: 0.37946754693984985 at epoch: 8 and batch_num: 905\n",
      "Loss of train set: 0.27709531784057617 at epoch: 8 and batch_num: 906\n",
      "Loss of train set: 0.23329658806324005 at epoch: 8 and batch_num: 907\n",
      "Loss of train set: 0.35082000494003296 at epoch: 8 and batch_num: 908\n",
      "Loss of train set: 0.33040285110473633 at epoch: 8 and batch_num: 909\n",
      "Loss of train set: 0.2220924198627472 at epoch: 8 and batch_num: 910\n",
      "Loss of train set: 0.2757605314254761 at epoch: 8 and batch_num: 911\n",
      "Loss of train set: 0.3168470859527588 at epoch: 8 and batch_num: 912\n",
      "Loss of train set: 0.25526726245880127 at epoch: 8 and batch_num: 913\n",
      "Loss of train set: 0.32455974817276 at epoch: 8 and batch_num: 914\n",
      "Loss of train set: 0.19593621790409088 at epoch: 8 and batch_num: 915\n",
      "Loss of train set: 0.42156410217285156 at epoch: 8 and batch_num: 916\n",
      "Loss of train set: 0.3580062687397003 at epoch: 8 and batch_num: 917\n",
      "Loss of train set: 0.3445000946521759 at epoch: 8 and batch_num: 918\n",
      "Loss of train set: 0.3781943917274475 at epoch: 8 and batch_num: 919\n",
      "Loss of train set: 0.32909178733825684 at epoch: 8 and batch_num: 920\n",
      "Loss of train set: 0.3221193552017212 at epoch: 8 and batch_num: 921\n",
      "Loss of train set: 0.40530771017074585 at epoch: 8 and batch_num: 922\n",
      "Loss of train set: 0.32816553115844727 at epoch: 8 and batch_num: 923\n",
      "Loss of train set: 0.35128647089004517 at epoch: 8 and batch_num: 924\n",
      "Loss of train set: 0.3063238859176636 at epoch: 8 and batch_num: 925\n",
      "Loss of train set: 0.3219177722930908 at epoch: 8 and batch_num: 926\n",
      "Loss of train set: 0.36678916215896606 at epoch: 8 and batch_num: 927\n",
      "Loss of train set: 0.32970282435417175 at epoch: 8 and batch_num: 928\n",
      "Loss of train set: 0.17807206511497498 at epoch: 8 and batch_num: 929\n",
      "Loss of train set: 0.16873016953468323 at epoch: 8 and batch_num: 930\n",
      "Loss of train set: 0.3241475224494934 at epoch: 8 and batch_num: 931\n",
      "Loss of train set: 0.40205174684524536 at epoch: 8 and batch_num: 932\n",
      "Loss of train set: 0.3700506091117859 at epoch: 8 and batch_num: 933\n",
      "Loss of train set: 0.21296927332878113 at epoch: 8 and batch_num: 934\n",
      "Loss of train set: 0.3169637620449066 at epoch: 8 and batch_num: 935\n",
      "Loss of train set: 0.43917375802993774 at epoch: 8 and batch_num: 936\n",
      "Loss of train set: 0.2839035391807556 at epoch: 8 and batch_num: 937\n",
      "Accuracy of train set: 0.8823833333333333\n",
      "Loss of test set: 0.20783266425132751 at epoch: 8 and batch_num: 0\n",
      "Loss of test set: 0.45152702927589417 at epoch: 8 and batch_num: 1\n",
      "Loss of test set: 0.34418410062789917 at epoch: 8 and batch_num: 2\n",
      "Loss of test set: 0.29953598976135254 at epoch: 8 and batch_num: 3\n",
      "Loss of test set: 0.43559277057647705 at epoch: 8 and batch_num: 4\n",
      "Loss of test set: 0.18042027950286865 at epoch: 8 and batch_num: 5\n",
      "Loss of test set: 0.34308409690856934 at epoch: 8 and batch_num: 6\n",
      "Loss of test set: 0.3308098614215851 at epoch: 8 and batch_num: 7\n",
      "Loss of test set: 0.5831737518310547 at epoch: 8 and batch_num: 8\n",
      "Loss of test set: 0.4322505295276642 at epoch: 8 and batch_num: 9\n",
      "Loss of test set: 0.49052369594573975 at epoch: 8 and batch_num: 10\n",
      "Loss of test set: 0.44516521692276 at epoch: 8 and batch_num: 11\n",
      "Loss of test set: 0.3341769278049469 at epoch: 8 and batch_num: 12\n",
      "Loss of test set: 0.45480555295944214 at epoch: 8 and batch_num: 13\n",
      "Loss of test set: 0.40128403902053833 at epoch: 8 and batch_num: 14\n",
      "Loss of test set: 0.1971590220928192 at epoch: 8 and batch_num: 15\n",
      "Loss of test set: 0.45741426944732666 at epoch: 8 and batch_num: 16\n",
      "Loss of test set: 0.22483518719673157 at epoch: 8 and batch_num: 17\n",
      "Loss of test set: 0.5428148508071899 at epoch: 8 and batch_num: 18\n",
      "Loss of test set: 0.4743119478225708 at epoch: 8 and batch_num: 19\n",
      "Loss of test set: 0.48258277773857117 at epoch: 8 and batch_num: 20\n",
      "Loss of test set: 0.56290602684021 at epoch: 8 and batch_num: 21\n",
      "Loss of test set: 0.36026298999786377 at epoch: 8 and batch_num: 22\n",
      "Loss of test set: 0.2913427948951721 at epoch: 8 and batch_num: 23\n",
      "Loss of test set: 0.3768516182899475 at epoch: 8 and batch_num: 24\n",
      "Loss of test set: 0.2157498002052307 at epoch: 8 and batch_num: 25\n",
      "Loss of test set: 0.3616502583026886 at epoch: 8 and batch_num: 26\n",
      "Loss of test set: 0.4339516758918762 at epoch: 8 and batch_num: 27\n",
      "Loss of test set: 0.252415269613266 at epoch: 8 and batch_num: 28\n",
      "Loss of test set: 0.2908533215522766 at epoch: 8 and batch_num: 29\n",
      "Loss of test set: 0.4967648684978485 at epoch: 8 and batch_num: 30\n",
      "Loss of test set: 0.277263879776001 at epoch: 8 and batch_num: 31\n",
      "Loss of test set: 0.4740254282951355 at epoch: 8 and batch_num: 32\n",
      "Loss of test set: 0.45256316661834717 at epoch: 8 and batch_num: 33\n",
      "Loss of test set: 0.39774495363235474 at epoch: 8 and batch_num: 34\n",
      "Loss of test set: 0.37982338666915894 at epoch: 8 and batch_num: 35\n",
      "Loss of test set: 0.30166783928871155 at epoch: 8 and batch_num: 36\n",
      "Loss of test set: 0.4384428858757019 at epoch: 8 and batch_num: 37\n",
      "Loss of test set: 0.38139504194259644 at epoch: 8 and batch_num: 38\n",
      "Loss of test set: 0.4100912809371948 at epoch: 8 and batch_num: 39\n",
      "Loss of test set: 0.7291942238807678 at epoch: 8 and batch_num: 40\n",
      "Loss of test set: 0.39601290225982666 at epoch: 8 and batch_num: 41\n",
      "Loss of test set: 0.5603762865066528 at epoch: 8 and batch_num: 42\n",
      "Loss of test set: 0.4075758457183838 at epoch: 8 and batch_num: 43\n",
      "Loss of test set: 0.43668144941329956 at epoch: 8 and batch_num: 44\n",
      "Loss of test set: 0.522736132144928 at epoch: 8 and batch_num: 45\n",
      "Loss of test set: 0.3920864462852478 at epoch: 8 and batch_num: 46\n",
      "Loss of test set: 0.3543126583099365 at epoch: 8 and batch_num: 47\n",
      "Loss of test set: 0.2813846468925476 at epoch: 8 and batch_num: 48\n",
      "Loss of test set: 0.3849365711212158 at epoch: 8 and batch_num: 49\n",
      "Loss of test set: 0.31604382395744324 at epoch: 8 and batch_num: 50\n",
      "Loss of test set: 0.5410963296890259 at epoch: 8 and batch_num: 51\n",
      "Loss of test set: 0.4495357275009155 at epoch: 8 and batch_num: 52\n",
      "Loss of test set: 0.38308924436569214 at epoch: 8 and batch_num: 53\n",
      "Loss of test set: 0.28213635087013245 at epoch: 8 and batch_num: 54\n",
      "Loss of test set: 0.3228863775730133 at epoch: 8 and batch_num: 55\n",
      "Loss of test set: 0.4682813882827759 at epoch: 8 and batch_num: 56\n",
      "Loss of test set: 0.4008302092552185 at epoch: 8 and batch_num: 57\n",
      "Loss of test set: 0.4720115661621094 at epoch: 8 and batch_num: 58\n",
      "Loss of test set: 0.4302147328853607 at epoch: 8 and batch_num: 59\n",
      "Loss of test set: 0.32456570863723755 at epoch: 8 and batch_num: 60\n",
      "Loss of test set: 0.31655484437942505 at epoch: 8 and batch_num: 61\n",
      "Loss of test set: 0.481508731842041 at epoch: 8 and batch_num: 62\n",
      "Loss of test set: 0.47624820470809937 at epoch: 8 and batch_num: 63\n",
      "Loss of test set: 0.3340453803539276 at epoch: 8 and batch_num: 64\n",
      "Loss of test set: 0.41098612546920776 at epoch: 8 and batch_num: 65\n",
      "Loss of test set: 0.6060195565223694 at epoch: 8 and batch_num: 66\n",
      "Loss of test set: 0.41650065779685974 at epoch: 8 and batch_num: 67\n",
      "Loss of test set: 0.5890087485313416 at epoch: 8 and batch_num: 68\n",
      "Loss of test set: 0.3618309497833252 at epoch: 8 and batch_num: 69\n",
      "Loss of test set: 0.4312501549720764 at epoch: 8 and batch_num: 70\n",
      "Loss of test set: 0.5783928632736206 at epoch: 8 and batch_num: 71\n",
      "Loss of test set: 0.378824919462204 at epoch: 8 and batch_num: 72\n",
      "Loss of test set: 0.5077282786369324 at epoch: 8 and batch_num: 73\n",
      "Loss of test set: 0.8032476902008057 at epoch: 8 and batch_num: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.3918117582798004 at epoch: 8 and batch_num: 75\n",
      "Loss of test set: 0.36371028423309326 at epoch: 8 and batch_num: 76\n",
      "Loss of test set: 0.35458895564079285 at epoch: 8 and batch_num: 77\n",
      "Loss of test set: 0.2870248556137085 at epoch: 8 and batch_num: 78\n",
      "Loss of test set: 0.33714091777801514 at epoch: 8 and batch_num: 79\n",
      "Loss of test set: 0.4337151050567627 at epoch: 8 and batch_num: 80\n",
      "Loss of test set: 0.4531517028808594 at epoch: 8 and batch_num: 81\n",
      "Loss of test set: 0.2987959384918213 at epoch: 8 and batch_num: 82\n",
      "Loss of test set: 0.48908212780952454 at epoch: 8 and batch_num: 83\n",
      "Loss of test set: 0.2822708487510681 at epoch: 8 and batch_num: 84\n",
      "Loss of test set: 0.4236670732498169 at epoch: 8 and batch_num: 85\n",
      "Loss of test set: 0.5002379417419434 at epoch: 8 and batch_num: 86\n",
      "Loss of test set: 0.26339560747146606 at epoch: 8 and batch_num: 87\n",
      "Loss of test set: 0.32306763529777527 at epoch: 8 and batch_num: 88\n",
      "Loss of test set: 0.4394572377204895 at epoch: 8 and batch_num: 89\n",
      "Loss of test set: 0.3409598469734192 at epoch: 8 and batch_num: 90\n",
      "Loss of test set: 0.24489012360572815 at epoch: 8 and batch_num: 91\n",
      "Loss of test set: 0.4930088520050049 at epoch: 8 and batch_num: 92\n",
      "Loss of test set: 0.19582782685756683 at epoch: 8 and batch_num: 93\n",
      "Loss of test set: 0.4450046420097351 at epoch: 8 and batch_num: 94\n",
      "Loss of test set: 0.2437727153301239 at epoch: 8 and batch_num: 95\n",
      "Loss of test set: 0.4292052388191223 at epoch: 8 and batch_num: 96\n",
      "Loss of test set: 0.3274996280670166 at epoch: 8 and batch_num: 97\n",
      "Loss of test set: 0.4224548637866974 at epoch: 8 and batch_num: 98\n",
      "Loss of test set: 0.5703219175338745 at epoch: 8 and batch_num: 99\n",
      "Loss of test set: 0.5158082842826843 at epoch: 8 and batch_num: 100\n",
      "Loss of test set: 0.3182706832885742 at epoch: 8 and batch_num: 101\n",
      "Loss of test set: 0.5386825799942017 at epoch: 8 and batch_num: 102\n",
      "Loss of test set: 0.3359431028366089 at epoch: 8 and batch_num: 103\n",
      "Loss of test set: 0.22098679840564728 at epoch: 8 and batch_num: 104\n",
      "Loss of test set: 0.47751301527023315 at epoch: 8 and batch_num: 105\n",
      "Loss of test set: 0.33901017904281616 at epoch: 8 and batch_num: 106\n",
      "Loss of test set: 0.7329268455505371 at epoch: 8 and batch_num: 107\n",
      "Loss of test set: 0.4663497805595398 at epoch: 8 and batch_num: 108\n",
      "Loss of test set: 0.3424798250198364 at epoch: 8 and batch_num: 109\n",
      "Loss of test set: 0.3325171172618866 at epoch: 8 and batch_num: 110\n",
      "Loss of test set: 0.36523541808128357 at epoch: 8 and batch_num: 111\n",
      "Loss of test set: 0.28493139147758484 at epoch: 8 and batch_num: 112\n",
      "Loss of test set: 0.4140946865081787 at epoch: 8 and batch_num: 113\n",
      "Loss of test set: 0.2987404465675354 at epoch: 8 and batch_num: 114\n",
      "Loss of test set: 0.41548633575439453 at epoch: 8 and batch_num: 115\n",
      "Loss of test set: 0.37598514556884766 at epoch: 8 and batch_num: 116\n",
      "Loss of test set: 0.41728150844573975 at epoch: 8 and batch_num: 117\n",
      "Loss of test set: 0.30400025844573975 at epoch: 8 and batch_num: 118\n",
      "Loss of test set: 0.485768586397171 at epoch: 8 and batch_num: 119\n",
      "Loss of test set: 0.30428841710090637 at epoch: 8 and batch_num: 120\n",
      "Loss of test set: 0.4481155574321747 at epoch: 8 and batch_num: 121\n",
      "Loss of test set: 0.33453017473220825 at epoch: 8 and batch_num: 122\n",
      "Loss of test set: 0.3885752558708191 at epoch: 8 and batch_num: 123\n",
      "Loss of test set: 0.35017287731170654 at epoch: 8 and batch_num: 124\n",
      "Loss of test set: 0.4666364789009094 at epoch: 8 and batch_num: 125\n",
      "Loss of test set: 0.5829945802688599 at epoch: 8 and batch_num: 126\n",
      "Loss of test set: 0.2561822533607483 at epoch: 8 and batch_num: 127\n",
      "Loss of test set: 0.20481351017951965 at epoch: 8 and batch_num: 128\n",
      "Loss of test set: 0.3356882929801941 at epoch: 8 and batch_num: 129\n",
      "Loss of test set: 0.3646070659160614 at epoch: 8 and batch_num: 130\n",
      "Loss of test set: 0.3328172266483307 at epoch: 8 and batch_num: 131\n",
      "Loss of test set: 0.6418377161026001 at epoch: 8 and batch_num: 132\n",
      "Loss of test set: 0.4171268939971924 at epoch: 8 and batch_num: 133\n",
      "Loss of test set: 0.28336501121520996 at epoch: 8 and batch_num: 134\n",
      "Loss of test set: 0.45295771956443787 at epoch: 8 and batch_num: 135\n",
      "Loss of test set: 0.4595659077167511 at epoch: 8 and batch_num: 136\n",
      "Loss of test set: 0.3290311396121979 at epoch: 8 and batch_num: 137\n",
      "Loss of test set: 0.39523398876190186 at epoch: 8 and batch_num: 138\n",
      "Loss of test set: 0.2520756423473358 at epoch: 8 and batch_num: 139\n",
      "Loss of test set: 0.3653567433357239 at epoch: 8 and batch_num: 140\n",
      "Loss of test set: 0.2875230312347412 at epoch: 8 and batch_num: 141\n",
      "Loss of test set: 0.33305150270462036 at epoch: 8 and batch_num: 142\n",
      "Loss of test set: 0.34737497568130493 at epoch: 8 and batch_num: 143\n",
      "Loss of test set: 0.2556151747703552 at epoch: 8 and batch_num: 144\n",
      "Loss of test set: 0.3946932256221771 at epoch: 8 and batch_num: 145\n",
      "Loss of test set: 0.3519594073295593 at epoch: 8 and batch_num: 146\n",
      "Loss of test set: 0.2184707224369049 at epoch: 8 and batch_num: 147\n",
      "Loss of test set: 0.5256898403167725 at epoch: 8 and batch_num: 148\n",
      "Loss of test set: 0.4941539466381073 at epoch: 8 and batch_num: 149\n",
      "Loss of test set: 0.4488709568977356 at epoch: 8 and batch_num: 150\n",
      "Loss of test set: 0.6060686111450195 at epoch: 8 and batch_num: 151\n",
      "Loss of test set: 0.42622390389442444 at epoch: 8 and batch_num: 152\n",
      "Loss of test set: 0.3141143023967743 at epoch: 8 and batch_num: 153\n",
      "Loss of test set: 0.4333432614803314 at epoch: 8 and batch_num: 154\n",
      "Loss of test set: 0.4256075322628021 at epoch: 8 and batch_num: 155\n",
      "Loss of test set: 0.20715725421905518 at epoch: 8 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8583\n",
      "Loss of train set: 0.5458208322525024 at epoch: 9 and batch_num: 0\n",
      "Loss of train set: 0.38759592175483704 at epoch: 9 and batch_num: 1\n",
      "Loss of train set: 0.3105502724647522 at epoch: 9 and batch_num: 2\n",
      "Loss of train set: 0.22146368026733398 at epoch: 9 and batch_num: 3\n",
      "Loss of train set: 0.3397836685180664 at epoch: 9 and batch_num: 4\n",
      "Loss of train set: 0.4188229441642761 at epoch: 9 and batch_num: 5\n",
      "Loss of train set: 0.16071438789367676 at epoch: 9 and batch_num: 6\n",
      "Loss of train set: 0.28283119201660156 at epoch: 9 and batch_num: 7\n",
      "Loss of train set: 0.3624991774559021 at epoch: 9 and batch_num: 8\n",
      "Loss of train set: 0.4303586483001709 at epoch: 9 and batch_num: 9\n",
      "Loss of train set: 0.33219558000564575 at epoch: 9 and batch_num: 10\n",
      "Loss of train set: 0.3243288993835449 at epoch: 9 and batch_num: 11\n",
      "Loss of train set: 0.266448438167572 at epoch: 9 and batch_num: 12\n",
      "Loss of train set: 0.4307882785797119 at epoch: 9 and batch_num: 13\n",
      "Loss of train set: 0.24739591777324677 at epoch: 9 and batch_num: 14\n",
      "Loss of train set: 0.30648231506347656 at epoch: 9 and batch_num: 15\n",
      "Loss of train set: 0.4954665005207062 at epoch: 9 and batch_num: 16\n",
      "Loss of train set: 0.2800573706626892 at epoch: 9 and batch_num: 17\n",
      "Loss of train set: 0.26720303297042847 at epoch: 9 and batch_num: 18\n",
      "Loss of train set: 0.3352423310279846 at epoch: 9 and batch_num: 19\n",
      "Loss of train set: 0.32064783573150635 at epoch: 9 and batch_num: 20\n",
      "Loss of train set: 0.431348979473114 at epoch: 9 and batch_num: 21\n",
      "Loss of train set: 0.1726943999528885 at epoch: 9 and batch_num: 22\n",
      "Loss of train set: 0.3428661525249481 at epoch: 9 and batch_num: 23\n",
      "Loss of train set: 0.3305375277996063 at epoch: 9 and batch_num: 24\n",
      "Loss of train set: 0.42154833674430847 at epoch: 9 and batch_num: 25\n",
      "Loss of train set: 0.3156217634677887 at epoch: 9 and batch_num: 26\n",
      "Loss of train set: 0.2611929178237915 at epoch: 9 and batch_num: 27\n",
      "Loss of train set: 0.20510616898536682 at epoch: 9 and batch_num: 28\n",
      "Loss of train set: 0.37926220893859863 at epoch: 9 and batch_num: 29\n",
      "Loss of train set: 0.4558632969856262 at epoch: 9 and batch_num: 30\n",
      "Loss of train set: 0.33158212900161743 at epoch: 9 and batch_num: 31\n",
      "Loss of train set: 0.24818094074726105 at epoch: 9 and batch_num: 32\n",
      "Loss of train set: 0.3873114585876465 at epoch: 9 and batch_num: 33\n",
      "Loss of train set: 0.40088438987731934 at epoch: 9 and batch_num: 34\n",
      "Loss of train set: 0.20847110450267792 at epoch: 9 and batch_num: 35\n",
      "Loss of train set: 0.2958214282989502 at epoch: 9 and batch_num: 36\n",
      "Loss of train set: 0.21893352270126343 at epoch: 9 and batch_num: 37\n",
      "Loss of train set: 0.3355794847011566 at epoch: 9 and batch_num: 38\n",
      "Loss of train set: 0.5966271758079529 at epoch: 9 and batch_num: 39\n",
      "Loss of train set: 0.38126590847969055 at epoch: 9 and batch_num: 40\n",
      "Loss of train set: 0.24283187091350555 at epoch: 9 and batch_num: 41\n",
      "Loss of train set: 0.2976341247558594 at epoch: 9 and batch_num: 42\n",
      "Loss of train set: 0.7201177477836609 at epoch: 9 and batch_num: 43\n",
      "Loss of train set: 0.39957395195961 at epoch: 9 and batch_num: 44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.24924370646476746 at epoch: 9 and batch_num: 45\n",
      "Loss of train set: 0.19311532378196716 at epoch: 9 and batch_num: 46\n",
      "Loss of train set: 0.29673007130622864 at epoch: 9 and batch_num: 47\n",
      "Loss of train set: 0.3876727819442749 at epoch: 9 and batch_num: 48\n",
      "Loss of train set: 0.2974608540534973 at epoch: 9 and batch_num: 49\n",
      "Loss of train set: 0.41794174909591675 at epoch: 9 and batch_num: 50\n",
      "Loss of train set: 0.4877268671989441 at epoch: 9 and batch_num: 51\n",
      "Loss of train set: 0.3771466314792633 at epoch: 9 and batch_num: 52\n",
      "Loss of train set: 0.29029402136802673 at epoch: 9 and batch_num: 53\n",
      "Loss of train set: 0.32338613271713257 at epoch: 9 and batch_num: 54\n",
      "Loss of train set: 0.3353707194328308 at epoch: 9 and batch_num: 55\n",
      "Loss of train set: 0.5413758754730225 at epoch: 9 and batch_num: 56\n",
      "Loss of train set: 0.3361600935459137 at epoch: 9 and batch_num: 57\n",
      "Loss of train set: 0.18536324799060822 at epoch: 9 and batch_num: 58\n",
      "Loss of train set: 0.26131993532180786 at epoch: 9 and batch_num: 59\n",
      "Loss of train set: 0.2683427929878235 at epoch: 9 and batch_num: 60\n",
      "Loss of train set: 0.2547362446784973 at epoch: 9 and batch_num: 61\n",
      "Loss of train set: 0.3286437690258026 at epoch: 9 and batch_num: 62\n",
      "Loss of train set: 0.35090911388397217 at epoch: 9 and batch_num: 63\n",
      "Loss of train set: 0.34845170378685 at epoch: 9 and batch_num: 64\n",
      "Loss of train set: 0.2885676324367523 at epoch: 9 and batch_num: 65\n",
      "Loss of train set: 0.30959147214889526 at epoch: 9 and batch_num: 66\n",
      "Loss of train set: 0.36644041538238525 at epoch: 9 and batch_num: 67\n",
      "Loss of train set: 0.2864881157875061 at epoch: 9 and batch_num: 68\n",
      "Loss of train set: 0.21846014261245728 at epoch: 9 and batch_num: 69\n",
      "Loss of train set: 0.34140443801879883 at epoch: 9 and batch_num: 70\n",
      "Loss of train set: 0.3393288552761078 at epoch: 9 and batch_num: 71\n",
      "Loss of train set: 0.26125016808509827 at epoch: 9 and batch_num: 72\n",
      "Loss of train set: 0.2876693904399872 at epoch: 9 and batch_num: 73\n",
      "Loss of train set: 0.2614843547344208 at epoch: 9 and batch_num: 74\n",
      "Loss of train set: 0.3217635154724121 at epoch: 9 and batch_num: 75\n",
      "Loss of train set: 0.403401255607605 at epoch: 9 and batch_num: 76\n",
      "Loss of train set: 0.253205269575119 at epoch: 9 and batch_num: 77\n",
      "Loss of train set: 0.26936835050582886 at epoch: 9 and batch_num: 78\n",
      "Loss of train set: 0.21279503405094147 at epoch: 9 and batch_num: 79\n",
      "Loss of train set: 0.3713494539260864 at epoch: 9 and batch_num: 80\n",
      "Loss of train set: 0.14742764830589294 at epoch: 9 and batch_num: 81\n",
      "Loss of train set: 0.2864629328250885 at epoch: 9 and batch_num: 82\n",
      "Loss of train set: 0.20163989067077637 at epoch: 9 and batch_num: 83\n",
      "Loss of train set: 0.33369529247283936 at epoch: 9 and batch_num: 84\n",
      "Loss of train set: 0.2779410481452942 at epoch: 9 and batch_num: 85\n",
      "Loss of train set: 0.3329421877861023 at epoch: 9 and batch_num: 86\n",
      "Loss of train set: 0.3178856074810028 at epoch: 9 and batch_num: 87\n",
      "Loss of train set: 0.14900758862495422 at epoch: 9 and batch_num: 88\n",
      "Loss of train set: 0.25979602336883545 at epoch: 9 and batch_num: 89\n",
      "Loss of train set: 0.33390429615974426 at epoch: 9 and batch_num: 90\n",
      "Loss of train set: 0.30196160078048706 at epoch: 9 and batch_num: 91\n",
      "Loss of train set: 0.325899600982666 at epoch: 9 and batch_num: 92\n",
      "Loss of train set: 0.3280898630619049 at epoch: 9 and batch_num: 93\n",
      "Loss of train set: 0.33287370204925537 at epoch: 9 and batch_num: 94\n",
      "Loss of train set: 0.5972640514373779 at epoch: 9 and batch_num: 95\n",
      "Loss of train set: 0.2885196805000305 at epoch: 9 and batch_num: 96\n",
      "Loss of train set: 0.27234703302383423 at epoch: 9 and batch_num: 97\n",
      "Loss of train set: 0.5587093234062195 at epoch: 9 and batch_num: 98\n",
      "Loss of train set: 0.3264343738555908 at epoch: 9 and batch_num: 99\n",
      "Loss of train set: 0.30456870794296265 at epoch: 9 and batch_num: 100\n",
      "Loss of train set: 0.42039236426353455 at epoch: 9 and batch_num: 101\n",
      "Loss of train set: 0.34995055198669434 at epoch: 9 and batch_num: 102\n",
      "Loss of train set: 0.40483105182647705 at epoch: 9 and batch_num: 103\n",
      "Loss of train set: 0.28882884979248047 at epoch: 9 and batch_num: 104\n",
      "Loss of train set: 0.415658175945282 at epoch: 9 and batch_num: 105\n",
      "Loss of train set: 0.2755149006843567 at epoch: 9 and batch_num: 106\n",
      "Loss of train set: 0.26315972208976746 at epoch: 9 and batch_num: 107\n",
      "Loss of train set: 0.36675140261650085 at epoch: 9 and batch_num: 108\n",
      "Loss of train set: 0.38006263971328735 at epoch: 9 and batch_num: 109\n",
      "Loss of train set: 0.2178569883108139 at epoch: 9 and batch_num: 110\n",
      "Loss of train set: 0.3574769198894501 at epoch: 9 and batch_num: 111\n",
      "Loss of train set: 0.3882560133934021 at epoch: 9 and batch_num: 112\n",
      "Loss of train set: 0.304756760597229 at epoch: 9 and batch_num: 113\n",
      "Loss of train set: 0.19831174612045288 at epoch: 9 and batch_num: 114\n",
      "Loss of train set: 0.472828209400177 at epoch: 9 and batch_num: 115\n",
      "Loss of train set: 0.19590060412883759 at epoch: 9 and batch_num: 116\n",
      "Loss of train set: 0.35158562660217285 at epoch: 9 and batch_num: 117\n",
      "Loss of train set: 0.26908737421035767 at epoch: 9 and batch_num: 118\n",
      "Loss of train set: 0.3544350862503052 at epoch: 9 and batch_num: 119\n",
      "Loss of train set: 0.39159512519836426 at epoch: 9 and batch_num: 120\n",
      "Loss of train set: 0.3031820058822632 at epoch: 9 and batch_num: 121\n",
      "Loss of train set: 0.1736665517091751 at epoch: 9 and batch_num: 122\n",
      "Loss of train set: 0.36855176091194153 at epoch: 9 and batch_num: 123\n",
      "Loss of train set: 0.2762877941131592 at epoch: 9 and batch_num: 124\n",
      "Loss of train set: 0.3193944990634918 at epoch: 9 and batch_num: 125\n",
      "Loss of train set: 0.31640422344207764 at epoch: 9 and batch_num: 126\n",
      "Loss of train set: 0.4644562602043152 at epoch: 9 and batch_num: 127\n",
      "Loss of train set: 0.37252962589263916 at epoch: 9 and batch_num: 128\n",
      "Loss of train set: 0.3936291038990021 at epoch: 9 and batch_num: 129\n",
      "Loss of train set: 0.46175190806388855 at epoch: 9 and batch_num: 130\n",
      "Loss of train set: 0.23319779336452484 at epoch: 9 and batch_num: 131\n",
      "Loss of train set: 0.2358202189207077 at epoch: 9 and batch_num: 132\n",
      "Loss of train set: 0.3811427354812622 at epoch: 9 and batch_num: 133\n",
      "Loss of train set: 0.34225350618362427 at epoch: 9 and batch_num: 134\n",
      "Loss of train set: 0.18387989699840546 at epoch: 9 and batch_num: 135\n",
      "Loss of train set: 0.4492800533771515 at epoch: 9 and batch_num: 136\n",
      "Loss of train set: 0.3190361261367798 at epoch: 9 and batch_num: 137\n",
      "Loss of train set: 0.34389907121658325 at epoch: 9 and batch_num: 138\n",
      "Loss of train set: 0.24832499027252197 at epoch: 9 and batch_num: 139\n",
      "Loss of train set: 0.32769253849983215 at epoch: 9 and batch_num: 140\n",
      "Loss of train set: 0.37655216455459595 at epoch: 9 and batch_num: 141\n",
      "Loss of train set: 0.2891336977481842 at epoch: 9 and batch_num: 142\n",
      "Loss of train set: 0.20105820894241333 at epoch: 9 and batch_num: 143\n",
      "Loss of train set: 0.35647621750831604 at epoch: 9 and batch_num: 144\n",
      "Loss of train set: 0.34386205673217773 at epoch: 9 and batch_num: 145\n",
      "Loss of train set: 0.2919984757900238 at epoch: 9 and batch_num: 146\n",
      "Loss of train set: 0.34745195508003235 at epoch: 9 and batch_num: 147\n",
      "Loss of train set: 0.2495499551296234 at epoch: 9 and batch_num: 148\n",
      "Loss of train set: 0.27691102027893066 at epoch: 9 and batch_num: 149\n",
      "Loss of train set: 0.17279121279716492 at epoch: 9 and batch_num: 150\n",
      "Loss of train set: 0.29468733072280884 at epoch: 9 and batch_num: 151\n",
      "Loss of train set: 0.29668790102005005 at epoch: 9 and batch_num: 152\n",
      "Loss of train set: 0.41585594415664673 at epoch: 9 and batch_num: 153\n",
      "Loss of train set: 0.34006205201148987 at epoch: 9 and batch_num: 154\n",
      "Loss of train set: 0.4689946472644806 at epoch: 9 and batch_num: 155\n",
      "Loss of train set: 0.3890703022480011 at epoch: 9 and batch_num: 156\n",
      "Loss of train set: 0.347260057926178 at epoch: 9 and batch_num: 157\n",
      "Loss of train set: 0.18841469287872314 at epoch: 9 and batch_num: 158\n",
      "Loss of train set: 0.3784545660018921 at epoch: 9 and batch_num: 159\n",
      "Loss of train set: 0.2783507704734802 at epoch: 9 and batch_num: 160\n",
      "Loss of train set: 0.23749008774757385 at epoch: 9 and batch_num: 161\n",
      "Loss of train set: 0.400608628988266 at epoch: 9 and batch_num: 162\n",
      "Loss of train set: 0.3044108748435974 at epoch: 9 and batch_num: 163\n",
      "Loss of train set: 0.48336508870124817 at epoch: 9 and batch_num: 164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.30307435989379883 at epoch: 9 and batch_num: 165\n",
      "Loss of train set: 0.26948100328445435 at epoch: 9 and batch_num: 166\n",
      "Loss of train set: 0.2715778052806854 at epoch: 9 and batch_num: 167\n",
      "Loss of train set: 0.33754923939704895 at epoch: 9 and batch_num: 168\n",
      "Loss of train set: 0.3200910687446594 at epoch: 9 and batch_num: 169\n",
      "Loss of train set: 0.3420632481575012 at epoch: 9 and batch_num: 170\n",
      "Loss of train set: 0.16844148933887482 at epoch: 9 and batch_num: 171\n",
      "Loss of train set: 0.2384500503540039 at epoch: 9 and batch_num: 172\n",
      "Loss of train set: 0.2859768271446228 at epoch: 9 and batch_num: 173\n",
      "Loss of train set: 0.3171103000640869 at epoch: 9 and batch_num: 174\n",
      "Loss of train set: 0.19883671402931213 at epoch: 9 and batch_num: 175\n",
      "Loss of train set: 0.2648189067840576 at epoch: 9 and batch_num: 176\n",
      "Loss of train set: 0.35426628589630127 at epoch: 9 and batch_num: 177\n",
      "Loss of train set: 0.18372875452041626 at epoch: 9 and batch_num: 178\n",
      "Loss of train set: 0.22531646490097046 at epoch: 9 and batch_num: 179\n",
      "Loss of train set: 0.2964920997619629 at epoch: 9 and batch_num: 180\n",
      "Loss of train set: 0.37618255615234375 at epoch: 9 and batch_num: 181\n",
      "Loss of train set: 0.2630360722541809 at epoch: 9 and batch_num: 182\n",
      "Loss of train set: 0.2537052631378174 at epoch: 9 and batch_num: 183\n",
      "Loss of train set: 0.3535397946834564 at epoch: 9 and batch_num: 184\n",
      "Loss of train set: 0.42479389905929565 at epoch: 9 and batch_num: 185\n",
      "Loss of train set: 0.3884371221065521 at epoch: 9 and batch_num: 186\n",
      "Loss of train set: 0.3709293603897095 at epoch: 9 and batch_num: 187\n",
      "Loss of train set: 0.33215445280075073 at epoch: 9 and batch_num: 188\n",
      "Loss of train set: 0.4124302268028259 at epoch: 9 and batch_num: 189\n",
      "Loss of train set: 0.30677610635757446 at epoch: 9 and batch_num: 190\n",
      "Loss of train set: 0.2086506187915802 at epoch: 9 and batch_num: 191\n",
      "Loss of train set: 0.4676179885864258 at epoch: 9 and batch_num: 192\n",
      "Loss of train set: 0.20398515462875366 at epoch: 9 and batch_num: 193\n",
      "Loss of train set: 0.40917009115219116 at epoch: 9 and batch_num: 194\n",
      "Loss of train set: 0.2320185750722885 at epoch: 9 and batch_num: 195\n",
      "Loss of train set: 0.3388293385505676 at epoch: 9 and batch_num: 196\n",
      "Loss of train set: 0.3218340277671814 at epoch: 9 and batch_num: 197\n",
      "Loss of train set: 0.26516324281692505 at epoch: 9 and batch_num: 198\n",
      "Loss of train set: 0.4051783084869385 at epoch: 9 and batch_num: 199\n",
      "Loss of train set: 0.21348224580287933 at epoch: 9 and batch_num: 200\n",
      "Loss of train set: 0.40143701434135437 at epoch: 9 and batch_num: 201\n",
      "Loss of train set: 0.31012582778930664 at epoch: 9 and batch_num: 202\n",
      "Loss of train set: 0.2323407679796219 at epoch: 9 and batch_num: 203\n",
      "Loss of train set: 0.3699282109737396 at epoch: 9 and batch_num: 204\n",
      "Loss of train set: 0.5990729331970215 at epoch: 9 and batch_num: 205\n",
      "Loss of train set: 0.3194332420825958 at epoch: 9 and batch_num: 206\n",
      "Loss of train set: 0.4398829936981201 at epoch: 9 and batch_num: 207\n",
      "Loss of train set: 0.330344557762146 at epoch: 9 and batch_num: 208\n",
      "Loss of train set: 0.41825100779533386 at epoch: 9 and batch_num: 209\n",
      "Loss of train set: 0.2337414026260376 at epoch: 9 and batch_num: 210\n",
      "Loss of train set: 0.27808505296707153 at epoch: 9 and batch_num: 211\n",
      "Loss of train set: 0.3954503834247589 at epoch: 9 and batch_num: 212\n",
      "Loss of train set: 0.35460469126701355 at epoch: 9 and batch_num: 213\n",
      "Loss of train set: 0.2598908841609955 at epoch: 9 and batch_num: 214\n",
      "Loss of train set: 0.33662503957748413 at epoch: 9 and batch_num: 215\n",
      "Loss of train set: 0.2497600018978119 at epoch: 9 and batch_num: 216\n",
      "Loss of train set: 0.17965999245643616 at epoch: 9 and batch_num: 217\n",
      "Loss of train set: 0.28961417078971863 at epoch: 9 and batch_num: 218\n",
      "Loss of train set: 0.2785196304321289 at epoch: 9 and batch_num: 219\n",
      "Loss of train set: 0.3248128592967987 at epoch: 9 and batch_num: 220\n",
      "Loss of train set: 0.2782568633556366 at epoch: 9 and batch_num: 221\n",
      "Loss of train set: 0.5238954424858093 at epoch: 9 and batch_num: 222\n",
      "Loss of train set: 0.21318908035755157 at epoch: 9 and batch_num: 223\n",
      "Loss of train set: 0.4042804539203644 at epoch: 9 and batch_num: 224\n",
      "Loss of train set: 0.18165749311447144 at epoch: 9 and batch_num: 225\n",
      "Loss of train set: 0.2618252635002136 at epoch: 9 and batch_num: 226\n",
      "Loss of train set: 0.28765082359313965 at epoch: 9 and batch_num: 227\n",
      "Loss of train set: 0.322218656539917 at epoch: 9 and batch_num: 228\n",
      "Loss of train set: 0.20594020187854767 at epoch: 9 and batch_num: 229\n",
      "Loss of train set: 0.398497074842453 at epoch: 9 and batch_num: 230\n",
      "Loss of train set: 0.25668713450431824 at epoch: 9 and batch_num: 231\n",
      "Loss of train set: 0.30896836519241333 at epoch: 9 and batch_num: 232\n",
      "Loss of train set: 0.36157768964767456 at epoch: 9 and batch_num: 233\n",
      "Loss of train set: 0.3095688223838806 at epoch: 9 and batch_num: 234\n",
      "Loss of train set: 0.29660332202911377 at epoch: 9 and batch_num: 235\n",
      "Loss of train set: 0.2418614774942398 at epoch: 9 and batch_num: 236\n",
      "Loss of train set: 0.2969639301300049 at epoch: 9 and batch_num: 237\n",
      "Loss of train set: 0.34226906299591064 at epoch: 9 and batch_num: 238\n",
      "Loss of train set: 0.23970022797584534 at epoch: 9 and batch_num: 239\n",
      "Loss of train set: 0.32339924573898315 at epoch: 9 and batch_num: 240\n",
      "Loss of train set: 0.35152384638786316 at epoch: 9 and batch_num: 241\n",
      "Loss of train set: 0.28653353452682495 at epoch: 9 and batch_num: 242\n",
      "Loss of train set: 0.35110804438591003 at epoch: 9 and batch_num: 243\n",
      "Loss of train set: 0.3343561589717865 at epoch: 9 and batch_num: 244\n",
      "Loss of train set: 0.29647478461265564 at epoch: 9 and batch_num: 245\n",
      "Loss of train set: 0.33434581756591797 at epoch: 9 and batch_num: 246\n",
      "Loss of train set: 0.3883451223373413 at epoch: 9 and batch_num: 247\n",
      "Loss of train set: 0.3952673375606537 at epoch: 9 and batch_num: 248\n",
      "Loss of train set: 0.3236963748931885 at epoch: 9 and batch_num: 249\n",
      "Loss of train set: 0.116019606590271 at epoch: 9 and batch_num: 250\n",
      "Loss of train set: 0.24973317980766296 at epoch: 9 and batch_num: 251\n",
      "Loss of train set: 0.2724628150463104 at epoch: 9 and batch_num: 252\n",
      "Loss of train set: 0.14387637376785278 at epoch: 9 and batch_num: 253\n",
      "Loss of train set: 0.4229075014591217 at epoch: 9 and batch_num: 254\n",
      "Loss of train set: 0.1747061014175415 at epoch: 9 and batch_num: 255\n",
      "Loss of train set: 0.2955978512763977 at epoch: 9 and batch_num: 256\n",
      "Loss of train set: 0.33501917123794556 at epoch: 9 and batch_num: 257\n",
      "Loss of train set: 0.29339849948883057 at epoch: 9 and batch_num: 258\n",
      "Loss of train set: 0.3184342682361603 at epoch: 9 and batch_num: 259\n",
      "Loss of train set: 0.41879409551620483 at epoch: 9 and batch_num: 260\n",
      "Loss of train set: 0.3684113025665283 at epoch: 9 and batch_num: 261\n",
      "Loss of train set: 0.2615066170692444 at epoch: 9 and batch_num: 262\n",
      "Loss of train set: 0.24512729048728943 at epoch: 9 and batch_num: 263\n",
      "Loss of train set: 0.3134326934814453 at epoch: 9 and batch_num: 264\n",
      "Loss of train set: 0.2664565443992615 at epoch: 9 and batch_num: 265\n",
      "Loss of train set: 0.34412139654159546 at epoch: 9 and batch_num: 266\n",
      "Loss of train set: 0.4617466926574707 at epoch: 9 and batch_num: 267\n",
      "Loss of train set: 0.3246701657772064 at epoch: 9 and batch_num: 268\n",
      "Loss of train set: 0.4862212538719177 at epoch: 9 and batch_num: 269\n",
      "Loss of train set: 0.17086976766586304 at epoch: 9 and batch_num: 270\n",
      "Loss of train set: 0.20330935716629028 at epoch: 9 and batch_num: 271\n",
      "Loss of train set: 0.174448162317276 at epoch: 9 and batch_num: 272\n",
      "Loss of train set: 0.3891224265098572 at epoch: 9 and batch_num: 273\n",
      "Loss of train set: 0.32244569063186646 at epoch: 9 and batch_num: 274\n",
      "Loss of train set: 0.33896690607070923 at epoch: 9 and batch_num: 275\n",
      "Loss of train set: 0.29755696654319763 at epoch: 9 and batch_num: 276\n",
      "Loss of train set: 0.46166616678237915 at epoch: 9 and batch_num: 277\n",
      "Loss of train set: 0.4022592008113861 at epoch: 9 and batch_num: 278\n",
      "Loss of train set: 0.38670697808265686 at epoch: 9 and batch_num: 279\n",
      "Loss of train set: 0.5684857368469238 at epoch: 9 and batch_num: 280\n",
      "Loss of train set: 0.4112567901611328 at epoch: 9 and batch_num: 281\n",
      "Loss of train set: 0.36553096771240234 at epoch: 9 and batch_num: 282\n",
      "Loss of train set: 0.32018256187438965 at epoch: 9 and batch_num: 283\n",
      "Loss of train set: 0.4062526226043701 at epoch: 9 and batch_num: 284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.35782960057258606 at epoch: 9 and batch_num: 285\n",
      "Loss of train set: 0.3678656220436096 at epoch: 9 and batch_num: 286\n",
      "Loss of train set: 0.4164539575576782 at epoch: 9 and batch_num: 287\n",
      "Loss of train set: 0.23199763894081116 at epoch: 9 and batch_num: 288\n",
      "Loss of train set: 0.3448386788368225 at epoch: 9 and batch_num: 289\n",
      "Loss of train set: 0.3633168339729309 at epoch: 9 and batch_num: 290\n",
      "Loss of train set: 0.3023519515991211 at epoch: 9 and batch_num: 291\n",
      "Loss of train set: 0.40590140223503113 at epoch: 9 and batch_num: 292\n",
      "Loss of train set: 0.31184783577919006 at epoch: 9 and batch_num: 293\n",
      "Loss of train set: 0.25776243209838867 at epoch: 9 and batch_num: 294\n",
      "Loss of train set: 0.3114474415779114 at epoch: 9 and batch_num: 295\n",
      "Loss of train set: 0.299448698759079 at epoch: 9 and batch_num: 296\n",
      "Loss of train set: 0.31837818026542664 at epoch: 9 and batch_num: 297\n",
      "Loss of train set: 0.31376418471336365 at epoch: 9 and batch_num: 298\n",
      "Loss of train set: 0.3828766942024231 at epoch: 9 and batch_num: 299\n",
      "Loss of train set: 0.5146499872207642 at epoch: 9 and batch_num: 300\n",
      "Loss of train set: 0.24690113961696625 at epoch: 9 and batch_num: 301\n",
      "Loss of train set: 0.2252417504787445 at epoch: 9 and batch_num: 302\n",
      "Loss of train set: 0.3150561451911926 at epoch: 9 and batch_num: 303\n",
      "Loss of train set: 0.2617727518081665 at epoch: 9 and batch_num: 304\n",
      "Loss of train set: 0.3723151683807373 at epoch: 9 and batch_num: 305\n",
      "Loss of train set: 0.16225194931030273 at epoch: 9 and batch_num: 306\n",
      "Loss of train set: 0.19212326407432556 at epoch: 9 and batch_num: 307\n",
      "Loss of train set: 0.25647681951522827 at epoch: 9 and batch_num: 308\n",
      "Loss of train set: 0.23512396216392517 at epoch: 9 and batch_num: 309\n",
      "Loss of train set: 0.2376684993505478 at epoch: 9 and batch_num: 310\n",
      "Loss of train set: 0.3555000126361847 at epoch: 9 and batch_num: 311\n",
      "Loss of train set: 0.3128545880317688 at epoch: 9 and batch_num: 312\n",
      "Loss of train set: 0.3450303077697754 at epoch: 9 and batch_num: 313\n",
      "Loss of train set: 0.3139338493347168 at epoch: 9 and batch_num: 314\n",
      "Loss of train set: 0.28107592463493347 at epoch: 9 and batch_num: 315\n",
      "Loss of train set: 0.3492336869239807 at epoch: 9 and batch_num: 316\n",
      "Loss of train set: 0.3579052686691284 at epoch: 9 and batch_num: 317\n",
      "Loss of train set: 0.20363637804985046 at epoch: 9 and batch_num: 318\n",
      "Loss of train set: 0.2554298937320709 at epoch: 9 and batch_num: 319\n",
      "Loss of train set: 0.43011030554771423 at epoch: 9 and batch_num: 320\n",
      "Loss of train set: 0.17096973955631256 at epoch: 9 and batch_num: 321\n",
      "Loss of train set: 0.3621218800544739 at epoch: 9 and batch_num: 322\n",
      "Loss of train set: 0.32761675119400024 at epoch: 9 and batch_num: 323\n",
      "Loss of train set: 0.141399547457695 at epoch: 9 and batch_num: 324\n",
      "Loss of train set: 0.5172419548034668 at epoch: 9 and batch_num: 325\n",
      "Loss of train set: 0.41485005617141724 at epoch: 9 and batch_num: 326\n",
      "Loss of train set: 0.2631798982620239 at epoch: 9 and batch_num: 327\n",
      "Loss of train set: 0.23947882652282715 at epoch: 9 and batch_num: 328\n",
      "Loss of train set: 0.3201035261154175 at epoch: 9 and batch_num: 329\n",
      "Loss of train set: 0.3031978905200958 at epoch: 9 and batch_num: 330\n",
      "Loss of train set: 0.35959360003471375 at epoch: 9 and batch_num: 331\n",
      "Loss of train set: 0.21256816387176514 at epoch: 9 and batch_num: 332\n",
      "Loss of train set: 0.34130680561065674 at epoch: 9 and batch_num: 333\n",
      "Loss of train set: 0.39129766821861267 at epoch: 9 and batch_num: 334\n",
      "Loss of train set: 0.317882239818573 at epoch: 9 and batch_num: 335\n",
      "Loss of train set: 0.15684102475643158 at epoch: 9 and batch_num: 336\n",
      "Loss of train set: 0.26770490407943726 at epoch: 9 and batch_num: 337\n",
      "Loss of train set: 0.4004805088043213 at epoch: 9 and batch_num: 338\n",
      "Loss of train set: 0.3245643973350525 at epoch: 9 and batch_num: 339\n",
      "Loss of train set: 0.20397920906543732 at epoch: 9 and batch_num: 340\n",
      "Loss of train set: 0.630507230758667 at epoch: 9 and batch_num: 341\n",
      "Loss of train set: 0.3497470021247864 at epoch: 9 and batch_num: 342\n",
      "Loss of train set: 0.29386940598487854 at epoch: 9 and batch_num: 343\n",
      "Loss of train set: 0.23471444845199585 at epoch: 9 and batch_num: 344\n",
      "Loss of train set: 0.30168038606643677 at epoch: 9 and batch_num: 345\n",
      "Loss of train set: 0.2853093147277832 at epoch: 9 and batch_num: 346\n",
      "Loss of train set: 0.4109337627887726 at epoch: 9 and batch_num: 347\n",
      "Loss of train set: 0.28274738788604736 at epoch: 9 and batch_num: 348\n",
      "Loss of train set: 0.2936514616012573 at epoch: 9 and batch_num: 349\n",
      "Loss of train set: 0.44881632924079895 at epoch: 9 and batch_num: 350\n",
      "Loss of train set: 0.2357357144355774 at epoch: 9 and batch_num: 351\n",
      "Loss of train set: 0.3609282374382019 at epoch: 9 and batch_num: 352\n",
      "Loss of train set: 0.4834528863430023 at epoch: 9 and batch_num: 353\n",
      "Loss of train set: 0.4576895833015442 at epoch: 9 and batch_num: 354\n",
      "Loss of train set: 0.37426599860191345 at epoch: 9 and batch_num: 355\n",
      "Loss of train set: 0.2222289741039276 at epoch: 9 and batch_num: 356\n",
      "Loss of train set: 0.2995973229408264 at epoch: 9 and batch_num: 357\n",
      "Loss of train set: 0.32105740904808044 at epoch: 9 and batch_num: 358\n",
      "Loss of train set: 0.29366374015808105 at epoch: 9 and batch_num: 359\n",
      "Loss of train set: 0.22586986422538757 at epoch: 9 and batch_num: 360\n",
      "Loss of train set: 0.32461902499198914 at epoch: 9 and batch_num: 361\n",
      "Loss of train set: 0.4340344965457916 at epoch: 9 and batch_num: 362\n",
      "Loss of train set: 0.2696762681007385 at epoch: 9 and batch_num: 363\n",
      "Loss of train set: 0.27707186341285706 at epoch: 9 and batch_num: 364\n",
      "Loss of train set: 0.16268491744995117 at epoch: 9 and batch_num: 365\n",
      "Loss of train set: 0.3867271840572357 at epoch: 9 and batch_num: 366\n",
      "Loss of train set: 0.25322362780570984 at epoch: 9 and batch_num: 367\n",
      "Loss of train set: 0.2638973593711853 at epoch: 9 and batch_num: 368\n",
      "Loss of train set: 0.23764386773109436 at epoch: 9 and batch_num: 369\n",
      "Loss of train set: 0.1810794174671173 at epoch: 9 and batch_num: 370\n",
      "Loss of train set: 0.38184624910354614 at epoch: 9 and batch_num: 371\n",
      "Loss of train set: 0.2567615509033203 at epoch: 9 and batch_num: 372\n",
      "Loss of train set: 0.2542570233345032 at epoch: 9 and batch_num: 373\n",
      "Loss of train set: 0.45257753133773804 at epoch: 9 and batch_num: 374\n",
      "Loss of train set: 0.4292595386505127 at epoch: 9 and batch_num: 375\n",
      "Loss of train set: 0.3087998032569885 at epoch: 9 and batch_num: 376\n",
      "Loss of train set: 0.39510631561279297 at epoch: 9 and batch_num: 377\n",
      "Loss of train set: 0.33548879623413086 at epoch: 9 and batch_num: 378\n",
      "Loss of train set: 0.22273188829421997 at epoch: 9 and batch_num: 379\n",
      "Loss of train set: 0.39254289865493774 at epoch: 9 and batch_num: 380\n",
      "Loss of train set: 0.32165154814720154 at epoch: 9 and batch_num: 381\n",
      "Loss of train set: 0.25768208503723145 at epoch: 9 and batch_num: 382\n",
      "Loss of train set: 0.2305821031332016 at epoch: 9 and batch_num: 383\n",
      "Loss of train set: 0.29721683263778687 at epoch: 9 and batch_num: 384\n",
      "Loss of train set: 0.27495458722114563 at epoch: 9 and batch_num: 385\n",
      "Loss of train set: 0.19527122378349304 at epoch: 9 and batch_num: 386\n",
      "Loss of train set: 0.40754908323287964 at epoch: 9 and batch_num: 387\n",
      "Loss of train set: 0.35336560010910034 at epoch: 9 and batch_num: 388\n",
      "Loss of train set: 0.3400314748287201 at epoch: 9 and batch_num: 389\n",
      "Loss of train set: 0.31376075744628906 at epoch: 9 and batch_num: 390\n",
      "Loss of train set: 0.22894957661628723 at epoch: 9 and batch_num: 391\n",
      "Loss of train set: 0.38315558433532715 at epoch: 9 and batch_num: 392\n",
      "Loss of train set: 0.36948850750923157 at epoch: 9 and batch_num: 393\n",
      "Loss of train set: 0.3312077522277832 at epoch: 9 and batch_num: 394\n",
      "Loss of train set: 0.26287171244621277 at epoch: 9 and batch_num: 395\n",
      "Loss of train set: 0.5493687391281128 at epoch: 9 and batch_num: 396\n",
      "Loss of train set: 0.3480615019798279 at epoch: 9 and batch_num: 397\n",
      "Loss of train set: 0.32993680238723755 at epoch: 9 and batch_num: 398\n",
      "Loss of train set: 0.24011409282684326 at epoch: 9 and batch_num: 399\n",
      "Loss of train set: 0.16662168502807617 at epoch: 9 and batch_num: 400\n",
      "Loss of train set: 0.3138185143470764 at epoch: 9 and batch_num: 401\n",
      "Loss of train set: 0.45498958230018616 at epoch: 9 and batch_num: 402\n",
      "Loss of train set: 0.46829521656036377 at epoch: 9 and batch_num: 403\n",
      "Loss of train set: 0.41496047377586365 at epoch: 9 and batch_num: 404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.20741790533065796 at epoch: 9 and batch_num: 405\n",
      "Loss of train set: 0.2616496682167053 at epoch: 9 and batch_num: 406\n",
      "Loss of train set: 0.40576136112213135 at epoch: 9 and batch_num: 407\n",
      "Loss of train set: 0.35655897855758667 at epoch: 9 and batch_num: 408\n",
      "Loss of train set: 0.2809390425682068 at epoch: 9 and batch_num: 409\n",
      "Loss of train set: 0.26559919118881226 at epoch: 9 and batch_num: 410\n",
      "Loss of train set: 0.2871558666229248 at epoch: 9 and batch_num: 411\n",
      "Loss of train set: 0.36951905488967896 at epoch: 9 and batch_num: 412\n",
      "Loss of train set: 0.22860391438007355 at epoch: 9 and batch_num: 413\n",
      "Loss of train set: 0.5028470754623413 at epoch: 9 and batch_num: 414\n",
      "Loss of train set: 0.5780788660049438 at epoch: 9 and batch_num: 415\n",
      "Loss of train set: 0.5239991545677185 at epoch: 9 and batch_num: 416\n",
      "Loss of train set: 0.2134082317352295 at epoch: 9 and batch_num: 417\n",
      "Loss of train set: 0.1921151876449585 at epoch: 9 and batch_num: 418\n",
      "Loss of train set: 0.2780536115169525 at epoch: 9 and batch_num: 419\n",
      "Loss of train set: 0.2828144431114197 at epoch: 9 and batch_num: 420\n",
      "Loss of train set: 0.33851173520088196 at epoch: 9 and batch_num: 421\n",
      "Loss of train set: 0.3995501399040222 at epoch: 9 and batch_num: 422\n",
      "Loss of train set: 0.44770506024360657 at epoch: 9 and batch_num: 423\n",
      "Loss of train set: 0.3358776867389679 at epoch: 9 and batch_num: 424\n",
      "Loss of train set: 0.2935388684272766 at epoch: 9 and batch_num: 425\n",
      "Loss of train set: 0.412261426448822 at epoch: 9 and batch_num: 426\n",
      "Loss of train set: 0.26307955384254456 at epoch: 9 and batch_num: 427\n",
      "Loss of train set: 0.4209039807319641 at epoch: 9 and batch_num: 428\n",
      "Loss of train set: 0.46052423119544983 at epoch: 9 and batch_num: 429\n",
      "Loss of train set: 0.4326664209365845 at epoch: 9 and batch_num: 430\n",
      "Loss of train set: 0.26370108127593994 at epoch: 9 and batch_num: 431\n",
      "Loss of train set: 0.12373620271682739 at epoch: 9 and batch_num: 432\n",
      "Loss of train set: 0.3468571901321411 at epoch: 9 and batch_num: 433\n",
      "Loss of train set: 0.3604530096054077 at epoch: 9 and batch_num: 434\n",
      "Loss of train set: 0.4572547972202301 at epoch: 9 and batch_num: 435\n",
      "Loss of train set: 0.19034813344478607 at epoch: 9 and batch_num: 436\n",
      "Loss of train set: 0.3703046441078186 at epoch: 9 and batch_num: 437\n",
      "Loss of train set: 0.32743650674819946 at epoch: 9 and batch_num: 438\n",
      "Loss of train set: 0.2889312207698822 at epoch: 9 and batch_num: 439\n",
      "Loss of train set: 0.2580929398536682 at epoch: 9 and batch_num: 440\n",
      "Loss of train set: 0.27167102694511414 at epoch: 9 and batch_num: 441\n",
      "Loss of train set: 0.522764265537262 at epoch: 9 and batch_num: 442\n",
      "Loss of train set: 0.22202831506729126 at epoch: 9 and batch_num: 443\n",
      "Loss of train set: 0.36191070079803467 at epoch: 9 and batch_num: 444\n",
      "Loss of train set: 0.33785152435302734 at epoch: 9 and batch_num: 445\n",
      "Loss of train set: 0.26755863428115845 at epoch: 9 and batch_num: 446\n",
      "Loss of train set: 0.21166594326496124 at epoch: 9 and batch_num: 447\n",
      "Loss of train set: 0.4185810685157776 at epoch: 9 and batch_num: 448\n",
      "Loss of train set: 0.3915163278579712 at epoch: 9 and batch_num: 449\n",
      "Loss of train set: 0.4324272572994232 at epoch: 9 and batch_num: 450\n",
      "Loss of train set: 0.19310346245765686 at epoch: 9 and batch_num: 451\n",
      "Loss of train set: 0.29820865392684937 at epoch: 9 and batch_num: 452\n",
      "Loss of train set: 0.38644295930862427 at epoch: 9 and batch_num: 453\n",
      "Loss of train set: 0.2624939978122711 at epoch: 9 and batch_num: 454\n",
      "Loss of train set: 0.26105061173439026 at epoch: 9 and batch_num: 455\n",
      "Loss of train set: 0.2823275625705719 at epoch: 9 and batch_num: 456\n",
      "Loss of train set: 0.36112135648727417 at epoch: 9 and batch_num: 457\n",
      "Loss of train set: 0.29764842987060547 at epoch: 9 and batch_num: 458\n",
      "Loss of train set: 0.43210285902023315 at epoch: 9 and batch_num: 459\n",
      "Loss of train set: 0.3424496650695801 at epoch: 9 and batch_num: 460\n",
      "Loss of train set: 0.2822823226451874 at epoch: 9 and batch_num: 461\n",
      "Loss of train set: 0.16652072966098785 at epoch: 9 and batch_num: 462\n",
      "Loss of train set: 0.42040514945983887 at epoch: 9 and batch_num: 463\n",
      "Loss of train set: 0.2196492701768875 at epoch: 9 and batch_num: 464\n",
      "Loss of train set: 0.2983252704143524 at epoch: 9 and batch_num: 465\n",
      "Loss of train set: 0.3217772841453552 at epoch: 9 and batch_num: 466\n",
      "Loss of train set: 0.3282402753829956 at epoch: 9 and batch_num: 467\n",
      "Loss of train set: 0.2942265570163727 at epoch: 9 and batch_num: 468\n",
      "Loss of train set: 0.41577842831611633 at epoch: 9 and batch_num: 469\n",
      "Loss of train set: 0.24936243891716003 at epoch: 9 and batch_num: 470\n",
      "Loss of train set: 0.17266914248466492 at epoch: 9 and batch_num: 471\n",
      "Loss of train set: 0.18650180101394653 at epoch: 9 and batch_num: 472\n",
      "Loss of train set: 0.3615902364253998 at epoch: 9 and batch_num: 473\n",
      "Loss of train set: 0.18421101570129395 at epoch: 9 and batch_num: 474\n",
      "Loss of train set: 0.3265380859375 at epoch: 9 and batch_num: 475\n",
      "Loss of train set: 0.4120733141899109 at epoch: 9 and batch_num: 476\n",
      "Loss of train set: 0.2792544960975647 at epoch: 9 and batch_num: 477\n",
      "Loss of train set: 0.4392209053039551 at epoch: 9 and batch_num: 478\n",
      "Loss of train set: 0.3087746798992157 at epoch: 9 and batch_num: 479\n",
      "Loss of train set: 0.38111960887908936 at epoch: 9 and batch_num: 480\n",
      "Loss of train set: 0.260675311088562 at epoch: 9 and batch_num: 481\n",
      "Loss of train set: 0.26132673025131226 at epoch: 9 and batch_num: 482\n",
      "Loss of train set: 0.38097143173217773 at epoch: 9 and batch_num: 483\n",
      "Loss of train set: 0.2986682057380676 at epoch: 9 and batch_num: 484\n",
      "Loss of train set: 0.37590816617012024 at epoch: 9 and batch_num: 485\n",
      "Loss of train set: 0.2739412486553192 at epoch: 9 and batch_num: 486\n",
      "Loss of train set: 0.32199278473854065 at epoch: 9 and batch_num: 487\n",
      "Loss of train set: 0.44100069999694824 at epoch: 9 and batch_num: 488\n",
      "Loss of train set: 0.39039695262908936 at epoch: 9 and batch_num: 489\n",
      "Loss of train set: 0.33957287669181824 at epoch: 9 and batch_num: 490\n",
      "Loss of train set: 0.4994785189628601 at epoch: 9 and batch_num: 491\n",
      "Loss of train set: 0.2354957014322281 at epoch: 9 and batch_num: 492\n",
      "Loss of train set: 0.2959798574447632 at epoch: 9 and batch_num: 493\n",
      "Loss of train set: 0.4488573372364044 at epoch: 9 and batch_num: 494\n",
      "Loss of train set: 0.23892289400100708 at epoch: 9 and batch_num: 495\n",
      "Loss of train set: 0.3884108066558838 at epoch: 9 and batch_num: 496\n",
      "Loss of train set: 0.25441694259643555 at epoch: 9 and batch_num: 497\n",
      "Loss of train set: 0.22365885972976685 at epoch: 9 and batch_num: 498\n",
      "Loss of train set: 0.18637296557426453 at epoch: 9 and batch_num: 499\n",
      "Loss of train set: 0.140827476978302 at epoch: 9 and batch_num: 500\n",
      "Loss of train set: 0.2022346705198288 at epoch: 9 and batch_num: 501\n",
      "Loss of train set: 0.22896485030651093 at epoch: 9 and batch_num: 502\n",
      "Loss of train set: 0.27106159925460815 at epoch: 9 and batch_num: 503\n",
      "Loss of train set: 0.23040564358234406 at epoch: 9 and batch_num: 504\n",
      "Loss of train set: 0.16497758030891418 at epoch: 9 and batch_num: 505\n",
      "Loss of train set: 0.4935303330421448 at epoch: 9 and batch_num: 506\n",
      "Loss of train set: 0.22858299314975739 at epoch: 9 and batch_num: 507\n",
      "Loss of train set: 0.18047016859054565 at epoch: 9 and batch_num: 508\n",
      "Loss of train set: 0.3811614513397217 at epoch: 9 and batch_num: 509\n",
      "Loss of train set: 0.3087350130081177 at epoch: 9 and batch_num: 510\n",
      "Loss of train set: 0.2646471858024597 at epoch: 9 and batch_num: 511\n",
      "Loss of train set: 0.46147727966308594 at epoch: 9 and batch_num: 512\n",
      "Loss of train set: 0.2634235620498657 at epoch: 9 and batch_num: 513\n",
      "Loss of train set: 0.4973219037055969 at epoch: 9 and batch_num: 514\n",
      "Loss of train set: 0.17664431035518646 at epoch: 9 and batch_num: 515\n",
      "Loss of train set: 0.2350984811782837 at epoch: 9 and batch_num: 516\n",
      "Loss of train set: 0.45290690660476685 at epoch: 9 and batch_num: 517\n",
      "Loss of train set: 0.2798768877983093 at epoch: 9 and batch_num: 518\n",
      "Loss of train set: 0.4837445616722107 at epoch: 9 and batch_num: 519\n",
      "Loss of train set: 0.30036455392837524 at epoch: 9 and batch_num: 520\n",
      "Loss of train set: 0.16738250851631165 at epoch: 9 and batch_num: 521\n",
      "Loss of train set: 0.35486429929733276 at epoch: 9 and batch_num: 522\n",
      "Loss of train set: 0.2923818826675415 at epoch: 9 and batch_num: 523\n",
      "Loss of train set: 0.26613983511924744 at epoch: 9 and batch_num: 524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.27232927083969116 at epoch: 9 and batch_num: 525\n",
      "Loss of train set: 0.27489379048347473 at epoch: 9 and batch_num: 526\n",
      "Loss of train set: 0.4650562107563019 at epoch: 9 and batch_num: 527\n",
      "Loss of train set: 0.43487802147865295 at epoch: 9 and batch_num: 528\n",
      "Loss of train set: 0.524935245513916 at epoch: 9 and batch_num: 529\n",
      "Loss of train set: 0.19461512565612793 at epoch: 9 and batch_num: 530\n",
      "Loss of train set: 0.43747663497924805 at epoch: 9 and batch_num: 531\n",
      "Loss of train set: 0.2784276604652405 at epoch: 9 and batch_num: 532\n",
      "Loss of train set: 0.2582380175590515 at epoch: 9 and batch_num: 533\n",
      "Loss of train set: 0.32357943058013916 at epoch: 9 and batch_num: 534\n",
      "Loss of train set: 0.28358498215675354 at epoch: 9 and batch_num: 535\n",
      "Loss of train set: 0.4136294722557068 at epoch: 9 and batch_num: 536\n",
      "Loss of train set: 0.2658482491970062 at epoch: 9 and batch_num: 537\n",
      "Loss of train set: 0.19150689244270325 at epoch: 9 and batch_num: 538\n",
      "Loss of train set: 0.3660736680030823 at epoch: 9 and batch_num: 539\n",
      "Loss of train set: 0.17481158673763275 at epoch: 9 and batch_num: 540\n",
      "Loss of train set: 0.31538066267967224 at epoch: 9 and batch_num: 541\n",
      "Loss of train set: 0.37164920568466187 at epoch: 9 and batch_num: 542\n",
      "Loss of train set: 0.34958142042160034 at epoch: 9 and batch_num: 543\n",
      "Loss of train set: 0.24492418766021729 at epoch: 9 and batch_num: 544\n",
      "Loss of train set: 0.4949030876159668 at epoch: 9 and batch_num: 545\n",
      "Loss of train set: 0.34477508068084717 at epoch: 9 and batch_num: 546\n",
      "Loss of train set: 0.19619232416152954 at epoch: 9 and batch_num: 547\n",
      "Loss of train set: 0.48298606276512146 at epoch: 9 and batch_num: 548\n",
      "Loss of train set: 0.21740207076072693 at epoch: 9 and batch_num: 549\n",
      "Loss of train set: 0.19702894985675812 at epoch: 9 and batch_num: 550\n",
      "Loss of train set: 0.22512248158454895 at epoch: 9 and batch_num: 551\n",
      "Loss of train set: 0.15301422774791718 at epoch: 9 and batch_num: 552\n",
      "Loss of train set: 0.23418253660202026 at epoch: 9 and batch_num: 553\n",
      "Loss of train set: 0.32696622610092163 at epoch: 9 and batch_num: 554\n",
      "Loss of train set: 0.31031739711761475 at epoch: 9 and batch_num: 555\n",
      "Loss of train set: 0.4224678575992584 at epoch: 9 and batch_num: 556\n",
      "Loss of train set: 0.23475682735443115 at epoch: 9 and batch_num: 557\n",
      "Loss of train set: 0.2966369390487671 at epoch: 9 and batch_num: 558\n",
      "Loss of train set: 0.335534930229187 at epoch: 9 and batch_num: 559\n",
      "Loss of train set: 0.3318784832954407 at epoch: 9 and batch_num: 560\n",
      "Loss of train set: 0.19283302128314972 at epoch: 9 and batch_num: 561\n",
      "Loss of train set: 0.2934471666812897 at epoch: 9 and batch_num: 562\n",
      "Loss of train set: 0.4328653812408447 at epoch: 9 and batch_num: 563\n",
      "Loss of train set: 0.39038434624671936 at epoch: 9 and batch_num: 564\n",
      "Loss of train set: 0.4015478789806366 at epoch: 9 and batch_num: 565\n",
      "Loss of train set: 0.2298557013273239 at epoch: 9 and batch_num: 566\n",
      "Loss of train set: 0.3589310348033905 at epoch: 9 and batch_num: 567\n",
      "Loss of train set: 0.32843294739723206 at epoch: 9 and batch_num: 568\n",
      "Loss of train set: 0.4534982442855835 at epoch: 9 and batch_num: 569\n",
      "Loss of train set: 0.3244573771953583 at epoch: 9 and batch_num: 570\n",
      "Loss of train set: 0.2829549014568329 at epoch: 9 and batch_num: 571\n",
      "Loss of train set: 0.4044881761074066 at epoch: 9 and batch_num: 572\n",
      "Loss of train set: 0.20503246784210205 at epoch: 9 and batch_num: 573\n",
      "Loss of train set: 0.3484853506088257 at epoch: 9 and batch_num: 574\n",
      "Loss of train set: 0.4417485296726227 at epoch: 9 and batch_num: 575\n",
      "Loss of train set: 0.20139384269714355 at epoch: 9 and batch_num: 576\n",
      "Loss of train set: 0.24174931645393372 at epoch: 9 and batch_num: 577\n",
      "Loss of train set: 0.23746061325073242 at epoch: 9 and batch_num: 578\n",
      "Loss of train set: 0.49188336730003357 at epoch: 9 and batch_num: 579\n",
      "Loss of train set: 0.586063027381897 at epoch: 9 and batch_num: 580\n",
      "Loss of train set: 0.41340941190719604 at epoch: 9 and batch_num: 581\n",
      "Loss of train set: 0.14337186515331268 at epoch: 9 and batch_num: 582\n",
      "Loss of train set: 0.42375919222831726 at epoch: 9 and batch_num: 583\n",
      "Loss of train set: 0.3116607069969177 at epoch: 9 and batch_num: 584\n",
      "Loss of train set: 0.453316867351532 at epoch: 9 and batch_num: 585\n",
      "Loss of train set: 0.31278693675994873 at epoch: 9 and batch_num: 586\n",
      "Loss of train set: 0.5165103077888489 at epoch: 9 and batch_num: 587\n",
      "Loss of train set: 0.28745174407958984 at epoch: 9 and batch_num: 588\n",
      "Loss of train set: 0.3758290410041809 at epoch: 9 and batch_num: 589\n",
      "Loss of train set: 0.4703875780105591 at epoch: 9 and batch_num: 590\n",
      "Loss of train set: 0.2994905114173889 at epoch: 9 and batch_num: 591\n",
      "Loss of train set: 0.35004478693008423 at epoch: 9 and batch_num: 592\n",
      "Loss of train set: 0.2196909636259079 at epoch: 9 and batch_num: 593\n",
      "Loss of train set: 0.5420588850975037 at epoch: 9 and batch_num: 594\n",
      "Loss of train set: 0.28577157855033875 at epoch: 9 and batch_num: 595\n",
      "Loss of train set: 0.3576219081878662 at epoch: 9 and batch_num: 596\n",
      "Loss of train set: 0.4085226058959961 at epoch: 9 and batch_num: 597\n",
      "Loss of train set: 0.3134957551956177 at epoch: 9 and batch_num: 598\n",
      "Loss of train set: 0.41249775886535645 at epoch: 9 and batch_num: 599\n",
      "Loss of train set: 0.2910553216934204 at epoch: 9 and batch_num: 600\n",
      "Loss of train set: 0.2215416133403778 at epoch: 9 and batch_num: 601\n",
      "Loss of train set: 0.3690023422241211 at epoch: 9 and batch_num: 602\n",
      "Loss of train set: 0.43774718046188354 at epoch: 9 and batch_num: 603\n",
      "Loss of train set: 0.3169010877609253 at epoch: 9 and batch_num: 604\n",
      "Loss of train set: 0.23598814010620117 at epoch: 9 and batch_num: 605\n",
      "Loss of train set: 0.32023367285728455 at epoch: 9 and batch_num: 606\n",
      "Loss of train set: 0.36690554022789 at epoch: 9 and batch_num: 607\n",
      "Loss of train set: 0.33316951990127563 at epoch: 9 and batch_num: 608\n",
      "Loss of train set: 0.39817014336586 at epoch: 9 and batch_num: 609\n",
      "Loss of train set: 0.3552621901035309 at epoch: 9 and batch_num: 610\n",
      "Loss of train set: 0.2514600157737732 at epoch: 9 and batch_num: 611\n",
      "Loss of train set: 0.27024292945861816 at epoch: 9 and batch_num: 612\n",
      "Loss of train set: 0.37946969270706177 at epoch: 9 and batch_num: 613\n",
      "Loss of train set: 0.32615503668785095 at epoch: 9 and batch_num: 614\n",
      "Loss of train set: 0.5222888588905334 at epoch: 9 and batch_num: 615\n",
      "Loss of train set: 0.31085169315338135 at epoch: 9 and batch_num: 616\n",
      "Loss of train set: 0.355901837348938 at epoch: 9 and batch_num: 617\n",
      "Loss of train set: 0.32199567556381226 at epoch: 9 and batch_num: 618\n",
      "Loss of train set: 0.2588028013706207 at epoch: 9 and batch_num: 619\n",
      "Loss of train set: 0.12796005606651306 at epoch: 9 and batch_num: 620\n",
      "Loss of train set: 0.45441684126853943 at epoch: 9 and batch_num: 621\n",
      "Loss of train set: 0.29689568281173706 at epoch: 9 and batch_num: 622\n",
      "Loss of train set: 0.3494633436203003 at epoch: 9 and batch_num: 623\n",
      "Loss of train set: 0.21499094367027283 at epoch: 9 and batch_num: 624\n",
      "Loss of train set: 0.4693547785282135 at epoch: 9 and batch_num: 625\n",
      "Loss of train set: 0.43725478649139404 at epoch: 9 and batch_num: 626\n",
      "Loss of train set: 0.26099228858947754 at epoch: 9 and batch_num: 627\n",
      "Loss of train set: 0.2645835876464844 at epoch: 9 and batch_num: 628\n",
      "Loss of train set: 0.19425582885742188 at epoch: 9 and batch_num: 629\n",
      "Loss of train set: 0.345906525850296 at epoch: 9 and batch_num: 630\n",
      "Loss of train set: 0.42287904024124146 at epoch: 9 and batch_num: 631\n",
      "Loss of train set: 0.3810315430164337 at epoch: 9 and batch_num: 632\n",
      "Loss of train set: 0.3197798728942871 at epoch: 9 and batch_num: 633\n",
      "Loss of train set: 0.31408238410949707 at epoch: 9 and batch_num: 634\n",
      "Loss of train set: 0.2342367172241211 at epoch: 9 and batch_num: 635\n",
      "Loss of train set: 0.3231703042984009 at epoch: 9 and batch_num: 636\n",
      "Loss of train set: 0.19352120161056519 at epoch: 9 and batch_num: 637\n",
      "Loss of train set: 0.18543753027915955 at epoch: 9 and batch_num: 638\n",
      "Loss of train set: 0.3196709454059601 at epoch: 9 and batch_num: 639\n",
      "Loss of train set: 0.4493617117404938 at epoch: 9 and batch_num: 640\n",
      "Loss of train set: 0.18046855926513672 at epoch: 9 and batch_num: 641\n",
      "Loss of train set: 0.44780832529067993 at epoch: 9 and batch_num: 642\n",
      "Loss of train set: 0.21698155999183655 at epoch: 9 and batch_num: 643\n",
      "Loss of train set: 0.35385826230049133 at epoch: 9 and batch_num: 644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.23275092244148254 at epoch: 9 and batch_num: 645\n",
      "Loss of train set: 0.20391309261322021 at epoch: 9 and batch_num: 646\n",
      "Loss of train set: 0.3454526364803314 at epoch: 9 and batch_num: 647\n",
      "Loss of train set: 0.25201693177223206 at epoch: 9 and batch_num: 648\n",
      "Loss of train set: 0.2996148467063904 at epoch: 9 and batch_num: 649\n",
      "Loss of train set: 0.2291320562362671 at epoch: 9 and batch_num: 650\n",
      "Loss of train set: 0.3727113902568817 at epoch: 9 and batch_num: 651\n",
      "Loss of train set: 0.20068584382534027 at epoch: 9 and batch_num: 652\n",
      "Loss of train set: 0.2859109342098236 at epoch: 9 and batch_num: 653\n",
      "Loss of train set: 0.3735140562057495 at epoch: 9 and batch_num: 654\n",
      "Loss of train set: 0.38103413581848145 at epoch: 9 and batch_num: 655\n",
      "Loss of train set: 0.33577919006347656 at epoch: 9 and batch_num: 656\n",
      "Loss of train set: 0.5830641984939575 at epoch: 9 and batch_num: 657\n",
      "Loss of train set: 0.3515802025794983 at epoch: 9 and batch_num: 658\n",
      "Loss of train set: 0.37508681416511536 at epoch: 9 and batch_num: 659\n",
      "Loss of train set: 0.36631909012794495 at epoch: 9 and batch_num: 660\n",
      "Loss of train set: 0.2846090793609619 at epoch: 9 and batch_num: 661\n",
      "Loss of train set: 0.3889632821083069 at epoch: 9 and batch_num: 662\n",
      "Loss of train set: 0.41452962160110474 at epoch: 9 and batch_num: 663\n",
      "Loss of train set: 0.20117461681365967 at epoch: 9 and batch_num: 664\n",
      "Loss of train set: 0.4157482385635376 at epoch: 9 and batch_num: 665\n",
      "Loss of train set: 0.21717031300067902 at epoch: 9 and batch_num: 666\n",
      "Loss of train set: 0.29800674319267273 at epoch: 9 and batch_num: 667\n",
      "Loss of train set: 0.39776611328125 at epoch: 9 and batch_num: 668\n",
      "Loss of train set: 0.3143879473209381 at epoch: 9 and batch_num: 669\n",
      "Loss of train set: 0.3674220144748688 at epoch: 9 and batch_num: 670\n",
      "Loss of train set: 0.33224448561668396 at epoch: 9 and batch_num: 671\n",
      "Loss of train set: 0.25072285532951355 at epoch: 9 and batch_num: 672\n",
      "Loss of train set: 0.2646995186805725 at epoch: 9 and batch_num: 673\n",
      "Loss of train set: 0.2599280774593353 at epoch: 9 and batch_num: 674\n",
      "Loss of train set: 0.31670475006103516 at epoch: 9 and batch_num: 675\n",
      "Loss of train set: 0.34693482518196106 at epoch: 9 and batch_num: 676\n",
      "Loss of train set: 0.4094282388687134 at epoch: 9 and batch_num: 677\n",
      "Loss of train set: 0.34213680028915405 at epoch: 9 and batch_num: 678\n",
      "Loss of train set: 0.6790933609008789 at epoch: 9 and batch_num: 679\n",
      "Loss of train set: 0.396071195602417 at epoch: 9 and batch_num: 680\n",
      "Loss of train set: 0.4833819568157196 at epoch: 9 and batch_num: 681\n",
      "Loss of train set: 0.2937367856502533 at epoch: 9 and batch_num: 682\n",
      "Loss of train set: 0.3562810719013214 at epoch: 9 and batch_num: 683\n",
      "Loss of train set: 0.16088417172431946 at epoch: 9 and batch_num: 684\n",
      "Loss of train set: 0.5557093024253845 at epoch: 9 and batch_num: 685\n",
      "Loss of train set: 0.2653595507144928 at epoch: 9 and batch_num: 686\n",
      "Loss of train set: 0.4165289103984833 at epoch: 9 and batch_num: 687\n",
      "Loss of train set: 0.5749742388725281 at epoch: 9 and batch_num: 688\n",
      "Loss of train set: 0.352628231048584 at epoch: 9 and batch_num: 689\n",
      "Loss of train set: 0.3911033272743225 at epoch: 9 and batch_num: 690\n",
      "Loss of train set: 0.1971808671951294 at epoch: 9 and batch_num: 691\n",
      "Loss of train set: 0.2910965383052826 at epoch: 9 and batch_num: 692\n",
      "Loss of train set: 0.42214733362197876 at epoch: 9 and batch_num: 693\n",
      "Loss of train set: 0.3526625633239746 at epoch: 9 and batch_num: 694\n",
      "Loss of train set: 0.4993899464607239 at epoch: 9 and batch_num: 695\n",
      "Loss of train set: 0.3500838577747345 at epoch: 9 and batch_num: 696\n",
      "Loss of train set: 0.274131178855896 at epoch: 9 and batch_num: 697\n",
      "Loss of train set: 0.2505946159362793 at epoch: 9 and batch_num: 698\n",
      "Loss of train set: 0.24583399295806885 at epoch: 9 and batch_num: 699\n",
      "Loss of train set: 0.2658870816230774 at epoch: 9 and batch_num: 700\n",
      "Loss of train set: 0.4772809147834778 at epoch: 9 and batch_num: 701\n",
      "Loss of train set: 0.330020010471344 at epoch: 9 and batch_num: 702\n",
      "Loss of train set: 0.23342451453208923 at epoch: 9 and batch_num: 703\n",
      "Loss of train set: 0.5044052600860596 at epoch: 9 and batch_num: 704\n",
      "Loss of train set: 0.35923948884010315 at epoch: 9 and batch_num: 705\n",
      "Loss of train set: 0.4636768102645874 at epoch: 9 and batch_num: 706\n",
      "Loss of train set: 0.3677328824996948 at epoch: 9 and batch_num: 707\n",
      "Loss of train set: 0.3408879041671753 at epoch: 9 and batch_num: 708\n",
      "Loss of train set: 0.3865830898284912 at epoch: 9 and batch_num: 709\n",
      "Loss of train set: 0.3327176570892334 at epoch: 9 and batch_num: 710\n",
      "Loss of train set: 0.490631103515625 at epoch: 9 and batch_num: 711\n",
      "Loss of train set: 0.38532939553260803 at epoch: 9 and batch_num: 712\n",
      "Loss of train set: 0.22756130993366241 at epoch: 9 and batch_num: 713\n",
      "Loss of train set: 0.28159043192863464 at epoch: 9 and batch_num: 714\n",
      "Loss of train set: 0.2866409420967102 at epoch: 9 and batch_num: 715\n",
      "Loss of train set: 0.36741843819618225 at epoch: 9 and batch_num: 716\n",
      "Loss of train set: 0.28519129753112793 at epoch: 9 and batch_num: 717\n",
      "Loss of train set: 0.2887800931930542 at epoch: 9 and batch_num: 718\n",
      "Loss of train set: 0.24943017959594727 at epoch: 9 and batch_num: 719\n",
      "Loss of train set: 0.4060172438621521 at epoch: 9 and batch_num: 720\n",
      "Loss of train set: 0.27626949548721313 at epoch: 9 and batch_num: 721\n",
      "Loss of train set: 0.31024423241615295 at epoch: 9 and batch_num: 722\n",
      "Loss of train set: 0.3443230986595154 at epoch: 9 and batch_num: 723\n",
      "Loss of train set: 0.37317949533462524 at epoch: 9 and batch_num: 724\n",
      "Loss of train set: 0.4135720431804657 at epoch: 9 and batch_num: 725\n",
      "Loss of train set: 0.3622880280017853 at epoch: 9 and batch_num: 726\n",
      "Loss of train set: 0.4051368236541748 at epoch: 9 and batch_num: 727\n",
      "Loss of train set: 0.2669382095336914 at epoch: 9 and batch_num: 728\n",
      "Loss of train set: 0.27181145548820496 at epoch: 9 and batch_num: 729\n",
      "Loss of train set: 0.2627255916595459 at epoch: 9 and batch_num: 730\n",
      "Loss of train set: 0.2673671245574951 at epoch: 9 and batch_num: 731\n",
      "Loss of train set: 0.3389163017272949 at epoch: 9 and batch_num: 732\n",
      "Loss of train set: 0.28968000411987305 at epoch: 9 and batch_num: 733\n",
      "Loss of train set: 0.32814300060272217 at epoch: 9 and batch_num: 734\n",
      "Loss of train set: 0.25386160612106323 at epoch: 9 and batch_num: 735\n",
      "Loss of train set: 0.2786855697631836 at epoch: 9 and batch_num: 736\n",
      "Loss of train set: 0.37680768966674805 at epoch: 9 and batch_num: 737\n",
      "Loss of train set: 0.29692721366882324 at epoch: 9 and batch_num: 738\n",
      "Loss of train set: 0.39725786447525024 at epoch: 9 and batch_num: 739\n",
      "Loss of train set: 0.1691775619983673 at epoch: 9 and batch_num: 740\n",
      "Loss of train set: 0.4177020192146301 at epoch: 9 and batch_num: 741\n",
      "Loss of train set: 0.25543975830078125 at epoch: 9 and batch_num: 742\n",
      "Loss of train set: 0.51363205909729 at epoch: 9 and batch_num: 743\n",
      "Loss of train set: 0.29830384254455566 at epoch: 9 and batch_num: 744\n",
      "Loss of train set: 0.329319566488266 at epoch: 9 and batch_num: 745\n",
      "Loss of train set: 0.2124905288219452 at epoch: 9 and batch_num: 746\n",
      "Loss of train set: 0.32769596576690674 at epoch: 9 and batch_num: 747\n",
      "Loss of train set: 0.28415340185165405 at epoch: 9 and batch_num: 748\n",
      "Loss of train set: 0.37439578771591187 at epoch: 9 and batch_num: 749\n",
      "Loss of train set: 0.3620050549507141 at epoch: 9 and batch_num: 750\n",
      "Loss of train set: 0.2980423867702484 at epoch: 9 and batch_num: 751\n",
      "Loss of train set: 0.4083256423473358 at epoch: 9 and batch_num: 752\n",
      "Loss of train set: 0.30943626165390015 at epoch: 9 and batch_num: 753\n",
      "Loss of train set: 0.412891149520874 at epoch: 9 and batch_num: 754\n",
      "Loss of train set: 0.5143370032310486 at epoch: 9 and batch_num: 755\n",
      "Loss of train set: 0.3354218602180481 at epoch: 9 and batch_num: 756\n",
      "Loss of train set: 0.21080605685710907 at epoch: 9 and batch_num: 757\n",
      "Loss of train set: 0.30489182472229004 at epoch: 9 and batch_num: 758\n",
      "Loss of train set: 0.4563162326812744 at epoch: 9 and batch_num: 759\n",
      "Loss of train set: 0.3504374325275421 at epoch: 9 and batch_num: 760\n",
      "Loss of train set: 0.4796464145183563 at epoch: 9 and batch_num: 761\n",
      "Loss of train set: 0.21738308668136597 at epoch: 9 and batch_num: 762\n",
      "Loss of train set: 0.46987301111221313 at epoch: 9 and batch_num: 763\n",
      "Loss of train set: 0.29858821630477905 at epoch: 9 and batch_num: 764\n",
      "Loss of train set: 0.3571299910545349 at epoch: 9 and batch_num: 765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.38715261220932007 at epoch: 9 and batch_num: 766\n",
      "Loss of train set: 0.3223079442977905 at epoch: 9 and batch_num: 767\n",
      "Loss of train set: 0.42959561944007874 at epoch: 9 and batch_num: 768\n",
      "Loss of train set: 0.3967364430427551 at epoch: 9 and batch_num: 769\n",
      "Loss of train set: 0.31439751386642456 at epoch: 9 and batch_num: 770\n",
      "Loss of train set: 0.22864291071891785 at epoch: 9 and batch_num: 771\n",
      "Loss of train set: 0.3207763433456421 at epoch: 9 and batch_num: 772\n",
      "Loss of train set: 0.3137231469154358 at epoch: 9 and batch_num: 773\n",
      "Loss of train set: 0.33722469210624695 at epoch: 9 and batch_num: 774\n",
      "Loss of train set: 0.39373457431793213 at epoch: 9 and batch_num: 775\n",
      "Loss of train set: 0.3407891094684601 at epoch: 9 and batch_num: 776\n",
      "Loss of train set: 0.446241557598114 at epoch: 9 and batch_num: 777\n",
      "Loss of train set: 0.4525631070137024 at epoch: 9 and batch_num: 778\n",
      "Loss of train set: 0.3046819567680359 at epoch: 9 and batch_num: 779\n",
      "Loss of train set: 0.32756149768829346 at epoch: 9 and batch_num: 780\n",
      "Loss of train set: 0.34872153401374817 at epoch: 9 and batch_num: 781\n",
      "Loss of train set: 0.2942841649055481 at epoch: 9 and batch_num: 782\n",
      "Loss of train set: 0.19411976635456085 at epoch: 9 and batch_num: 783\n",
      "Loss of train set: 0.25165289640426636 at epoch: 9 and batch_num: 784\n",
      "Loss of train set: 0.40909627079963684 at epoch: 9 and batch_num: 785\n",
      "Loss of train set: 0.3389351963996887 at epoch: 9 and batch_num: 786\n",
      "Loss of train set: 0.23999032378196716 at epoch: 9 and batch_num: 787\n",
      "Loss of train set: 0.456569641828537 at epoch: 9 and batch_num: 788\n",
      "Loss of train set: 0.2610671818256378 at epoch: 9 and batch_num: 789\n",
      "Loss of train set: 0.20634160935878754 at epoch: 9 and batch_num: 790\n",
      "Loss of train set: 0.2620874047279358 at epoch: 9 and batch_num: 791\n",
      "Loss of train set: 0.24872411787509918 at epoch: 9 and batch_num: 792\n",
      "Loss of train set: 0.20542295277118683 at epoch: 9 and batch_num: 793\n",
      "Loss of train set: 0.2878837287425995 at epoch: 9 and batch_num: 794\n",
      "Loss of train set: 0.2590562105178833 at epoch: 9 and batch_num: 795\n",
      "Loss of train set: 0.4145987033843994 at epoch: 9 and batch_num: 796\n",
      "Loss of train set: 0.2982345223426819 at epoch: 9 and batch_num: 797\n",
      "Loss of train set: 0.4127001166343689 at epoch: 9 and batch_num: 798\n",
      "Loss of train set: 0.25420767068862915 at epoch: 9 and batch_num: 799\n",
      "Loss of train set: 0.2910414934158325 at epoch: 9 and batch_num: 800\n",
      "Loss of train set: 0.22455181181430817 at epoch: 9 and batch_num: 801\n",
      "Loss of train set: 0.3245730996131897 at epoch: 9 and batch_num: 802\n",
      "Loss of train set: 0.26602932810783386 at epoch: 9 and batch_num: 803\n",
      "Loss of train set: 0.2526754140853882 at epoch: 9 and batch_num: 804\n",
      "Loss of train set: 0.2526049315929413 at epoch: 9 and batch_num: 805\n",
      "Loss of train set: 0.3872644901275635 at epoch: 9 and batch_num: 806\n",
      "Loss of train set: 0.2795810103416443 at epoch: 9 and batch_num: 807\n",
      "Loss of train set: 0.3201029300689697 at epoch: 9 and batch_num: 808\n",
      "Loss of train set: 0.3118215501308441 at epoch: 9 and batch_num: 809\n",
      "Loss of train set: 0.44943860173225403 at epoch: 9 and batch_num: 810\n",
      "Loss of train set: 0.33454009890556335 at epoch: 9 and batch_num: 811\n",
      "Loss of train set: 0.4275781512260437 at epoch: 9 and batch_num: 812\n",
      "Loss of train set: 0.1791398823261261 at epoch: 9 and batch_num: 813\n",
      "Loss of train set: 0.27303048968315125 at epoch: 9 and batch_num: 814\n",
      "Loss of train set: 0.41009169816970825 at epoch: 9 and batch_num: 815\n",
      "Loss of train set: 0.44780558347702026 at epoch: 9 and batch_num: 816\n",
      "Loss of train set: 0.22132867574691772 at epoch: 9 and batch_num: 817\n",
      "Loss of train set: 0.28512781858444214 at epoch: 9 and batch_num: 818\n",
      "Loss of train set: 0.18333180248737335 at epoch: 9 and batch_num: 819\n",
      "Loss of train set: 0.4048506021499634 at epoch: 9 and batch_num: 820\n",
      "Loss of train set: 0.32992789149284363 at epoch: 9 and batch_num: 821\n",
      "Loss of train set: 0.5020944476127625 at epoch: 9 and batch_num: 822\n",
      "Loss of train set: 0.4155140519142151 at epoch: 9 and batch_num: 823\n",
      "Loss of train set: 0.3047211766242981 at epoch: 9 and batch_num: 824\n",
      "Loss of train set: 0.20358684659004211 at epoch: 9 and batch_num: 825\n",
      "Loss of train set: 0.324994832277298 at epoch: 9 and batch_num: 826\n",
      "Loss of train set: 0.3134029507637024 at epoch: 9 and batch_num: 827\n",
      "Loss of train set: 0.3200332522392273 at epoch: 9 and batch_num: 828\n",
      "Loss of train set: 0.2630915641784668 at epoch: 9 and batch_num: 829\n",
      "Loss of train set: 0.33348169922828674 at epoch: 9 and batch_num: 830\n",
      "Loss of train set: 0.41343531012535095 at epoch: 9 and batch_num: 831\n",
      "Loss of train set: 0.3171122074127197 at epoch: 9 and batch_num: 832\n",
      "Loss of train set: 0.28771889209747314 at epoch: 9 and batch_num: 833\n",
      "Loss of train set: 0.5250056982040405 at epoch: 9 and batch_num: 834\n",
      "Loss of train set: 0.18625915050506592 at epoch: 9 and batch_num: 835\n",
      "Loss of train set: 0.3393062949180603 at epoch: 9 and batch_num: 836\n",
      "Loss of train set: 0.2132778763771057 at epoch: 9 and batch_num: 837\n",
      "Loss of train set: 0.41217392683029175 at epoch: 9 and batch_num: 838\n",
      "Loss of train set: 0.45170387625694275 at epoch: 9 and batch_num: 839\n",
      "Loss of train set: 0.3515163064002991 at epoch: 9 and batch_num: 840\n",
      "Loss of train set: 0.2651442289352417 at epoch: 9 and batch_num: 841\n",
      "Loss of train set: 0.3177141845226288 at epoch: 9 and batch_num: 842\n",
      "Loss of train set: 0.16367082297801971 at epoch: 9 and batch_num: 843\n",
      "Loss of train set: 0.27471429109573364 at epoch: 9 and batch_num: 844\n",
      "Loss of train set: 0.3627394437789917 at epoch: 9 and batch_num: 845\n",
      "Loss of train set: 0.2155194878578186 at epoch: 9 and batch_num: 846\n",
      "Loss of train set: 0.34016621112823486 at epoch: 9 and batch_num: 847\n",
      "Loss of train set: 0.2855084240436554 at epoch: 9 and batch_num: 848\n",
      "Loss of train set: 0.48291635513305664 at epoch: 9 and batch_num: 849\n",
      "Loss of train set: 0.33900678157806396 at epoch: 9 and batch_num: 850\n",
      "Loss of train set: 0.5762704610824585 at epoch: 9 and batch_num: 851\n",
      "Loss of train set: 0.3656966984272003 at epoch: 9 and batch_num: 852\n",
      "Loss of train set: 0.3733556866645813 at epoch: 9 and batch_num: 853\n",
      "Loss of train set: 0.36116889119148254 at epoch: 9 and batch_num: 854\n",
      "Loss of train set: 0.2469938099384308 at epoch: 9 and batch_num: 855\n",
      "Loss of train set: 0.24439182877540588 at epoch: 9 and batch_num: 856\n",
      "Loss of train set: 0.4916020333766937 at epoch: 9 and batch_num: 857\n",
      "Loss of train set: 0.2974094748497009 at epoch: 9 and batch_num: 858\n",
      "Loss of train set: 0.3128272593021393 at epoch: 9 and batch_num: 859\n",
      "Loss of train set: 0.21331165730953217 at epoch: 9 and batch_num: 860\n",
      "Loss of train set: 0.24545298516750336 at epoch: 9 and batch_num: 861\n",
      "Loss of train set: 0.40470194816589355 at epoch: 9 and batch_num: 862\n",
      "Loss of train set: 0.26469486951828003 at epoch: 9 and batch_num: 863\n",
      "Loss of train set: 0.3454001843929291 at epoch: 9 and batch_num: 864\n",
      "Loss of train set: 0.1708393692970276 at epoch: 9 and batch_num: 865\n",
      "Loss of train set: 0.1856570988893509 at epoch: 9 and batch_num: 866\n",
      "Loss of train set: 0.38762563467025757 at epoch: 9 and batch_num: 867\n",
      "Loss of train set: 0.3672485053539276 at epoch: 9 and batch_num: 868\n",
      "Loss of train set: 0.2925184369087219 at epoch: 9 and batch_num: 869\n",
      "Loss of train set: 0.1975286304950714 at epoch: 9 and batch_num: 870\n",
      "Loss of train set: 0.263447642326355 at epoch: 9 and batch_num: 871\n",
      "Loss of train set: 0.281177282333374 at epoch: 9 and batch_num: 872\n",
      "Loss of train set: 0.31622791290283203 at epoch: 9 and batch_num: 873\n",
      "Loss of train set: 0.37211740016937256 at epoch: 9 and batch_num: 874\n",
      "Loss of train set: 0.2624205946922302 at epoch: 9 and batch_num: 875\n",
      "Loss of train set: 0.33316361904144287 at epoch: 9 and batch_num: 876\n",
      "Loss of train set: 0.3328682780265808 at epoch: 9 and batch_num: 877\n",
      "Loss of train set: 0.3723400831222534 at epoch: 9 and batch_num: 878\n",
      "Loss of train set: 0.3584323525428772 at epoch: 9 and batch_num: 879\n",
      "Loss of train set: 0.2982904016971588 at epoch: 9 and batch_num: 880\n",
      "Loss of train set: 0.22619283199310303 at epoch: 9 and batch_num: 881\n",
      "Loss of train set: 0.3108671307563782 at epoch: 9 and batch_num: 882\n",
      "Loss of train set: 0.3558339774608612 at epoch: 9 and batch_num: 883\n",
      "Loss of train set: 0.4101591110229492 at epoch: 9 and batch_num: 884\n",
      "Loss of train set: 0.3079896569252014 at epoch: 9 and batch_num: 885\n",
      "Loss of train set: 0.35299772024154663 at epoch: 9 and batch_num: 886\n",
      "Loss of train set: 0.5141571760177612 at epoch: 9 and batch_num: 887\n",
      "Loss of train set: 0.47016429901123047 at epoch: 9 and batch_num: 888\n",
      "Loss of train set: 0.3851303458213806 at epoch: 9 and batch_num: 889\n",
      "Loss of train set: 0.31082281470298767 at epoch: 9 and batch_num: 890\n",
      "Loss of train set: 0.39090588688850403 at epoch: 9 and batch_num: 891\n",
      "Loss of train set: 0.43883126974105835 at epoch: 9 and batch_num: 892\n",
      "Loss of train set: 0.35289281606674194 at epoch: 9 and batch_num: 893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.5224736332893372 at epoch: 9 and batch_num: 894\n",
      "Loss of train set: 0.4404958486557007 at epoch: 9 and batch_num: 895\n",
      "Loss of train set: 0.4156767725944519 at epoch: 9 and batch_num: 896\n",
      "Loss of train set: 0.26360559463500977 at epoch: 9 and batch_num: 897\n",
      "Loss of train set: 0.27892476320266724 at epoch: 9 and batch_num: 898\n",
      "Loss of train set: 0.28039515018463135 at epoch: 9 and batch_num: 899\n",
      "Loss of train set: 0.3890831470489502 at epoch: 9 and batch_num: 900\n",
      "Loss of train set: 0.38993000984191895 at epoch: 9 and batch_num: 901\n",
      "Loss of train set: 0.2513420283794403 at epoch: 9 and batch_num: 902\n",
      "Loss of train set: 0.3274109959602356 at epoch: 9 and batch_num: 903\n",
      "Loss of train set: 0.25639107823371887 at epoch: 9 and batch_num: 904\n",
      "Loss of train set: 0.31412866711616516 at epoch: 9 and batch_num: 905\n",
      "Loss of train set: 0.32206010818481445 at epoch: 9 and batch_num: 906\n",
      "Loss of train set: 0.26960644125938416 at epoch: 9 and batch_num: 907\n",
      "Loss of train set: 0.2378513514995575 at epoch: 9 and batch_num: 908\n",
      "Loss of train set: 0.14428038895130157 at epoch: 9 and batch_num: 909\n",
      "Loss of train set: 0.3044601082801819 at epoch: 9 and batch_num: 910\n",
      "Loss of train set: 0.32219839096069336 at epoch: 9 and batch_num: 911\n",
      "Loss of train set: 0.2921508550643921 at epoch: 9 and batch_num: 912\n",
      "Loss of train set: 0.5699597597122192 at epoch: 9 and batch_num: 913\n",
      "Loss of train set: 0.39427638053894043 at epoch: 9 and batch_num: 914\n",
      "Loss of train set: 0.5118958950042725 at epoch: 9 and batch_num: 915\n",
      "Loss of train set: 0.3338494896888733 at epoch: 9 and batch_num: 916\n",
      "Loss of train set: 0.37187835574150085 at epoch: 9 and batch_num: 917\n",
      "Loss of train set: 0.3105640709400177 at epoch: 9 and batch_num: 918\n",
      "Loss of train set: 0.2961236238479614 at epoch: 9 and batch_num: 919\n",
      "Loss of train set: 0.33595022559165955 at epoch: 9 and batch_num: 920\n",
      "Loss of train set: 0.3033137619495392 at epoch: 9 and batch_num: 921\n",
      "Loss of train set: 0.5915829539299011 at epoch: 9 and batch_num: 922\n",
      "Loss of train set: 0.2726093530654907 at epoch: 9 and batch_num: 923\n",
      "Loss of train set: 0.24121522903442383 at epoch: 9 and batch_num: 924\n",
      "Loss of train set: 0.12255927175283432 at epoch: 9 and batch_num: 925\n",
      "Loss of train set: 0.28067687153816223 at epoch: 9 and batch_num: 926\n",
      "Loss of train set: 0.4104123115539551 at epoch: 9 and batch_num: 927\n",
      "Loss of train set: 0.2882217764854431 at epoch: 9 and batch_num: 928\n",
      "Loss of train set: 0.39688315987586975 at epoch: 9 and batch_num: 929\n",
      "Loss of train set: 0.5479148626327515 at epoch: 9 and batch_num: 930\n",
      "Loss of train set: 0.4103851318359375 at epoch: 9 and batch_num: 931\n",
      "Loss of train set: 0.23901309072971344 at epoch: 9 and batch_num: 932\n",
      "Loss of train set: 0.3575081527233124 at epoch: 9 and batch_num: 933\n",
      "Loss of train set: 0.37244120240211487 at epoch: 9 and batch_num: 934\n",
      "Loss of train set: 0.37159138917922974 at epoch: 9 and batch_num: 935\n",
      "Loss of train set: 0.20194971561431885 at epoch: 9 and batch_num: 936\n",
      "Loss of train set: 0.23407292366027832 at epoch: 9 and batch_num: 937\n",
      "Accuracy of train set: 0.8837833333333334\n",
      "Loss of test set: 0.4663756489753723 at epoch: 9 and batch_num: 0\n",
      "Loss of test set: 0.3551388382911682 at epoch: 9 and batch_num: 1\n",
      "Loss of test set: 0.17841699719429016 at epoch: 9 and batch_num: 2\n",
      "Loss of test set: 0.410461962223053 at epoch: 9 and batch_num: 3\n",
      "Loss of test set: 0.3890242278575897 at epoch: 9 and batch_num: 4\n",
      "Loss of test set: 0.4230717420578003 at epoch: 9 and batch_num: 5\n",
      "Loss of test set: 0.316814124584198 at epoch: 9 and batch_num: 6\n",
      "Loss of test set: 0.4127345681190491 at epoch: 9 and batch_num: 7\n",
      "Loss of test set: 0.3220006823539734 at epoch: 9 and batch_num: 8\n",
      "Loss of test set: 0.3000141680240631 at epoch: 9 and batch_num: 9\n",
      "Loss of test set: 0.4529896378517151 at epoch: 9 and batch_num: 10\n",
      "Loss of test set: 0.42752373218536377 at epoch: 9 and batch_num: 11\n",
      "Loss of test set: 0.27946004271507263 at epoch: 9 and batch_num: 12\n",
      "Loss of test set: 0.21643272042274475 at epoch: 9 and batch_num: 13\n",
      "Loss of test set: 0.3919109106063843 at epoch: 9 and batch_num: 14\n",
      "Loss of test set: 0.5514274835586548 at epoch: 9 and batch_num: 15\n",
      "Loss of test set: 0.24145002663135529 at epoch: 9 and batch_num: 16\n",
      "Loss of test set: 0.2990197539329529 at epoch: 9 and batch_num: 17\n",
      "Loss of test set: 0.26144838333129883 at epoch: 9 and batch_num: 18\n",
      "Loss of test set: 0.38428980112075806 at epoch: 9 and batch_num: 19\n",
      "Loss of test set: 0.39826470613479614 at epoch: 9 and batch_num: 20\n",
      "Loss of test set: 0.4609943628311157 at epoch: 9 and batch_num: 21\n",
      "Loss of test set: 0.5092270970344543 at epoch: 9 and batch_num: 22\n",
      "Loss of test set: 0.3182482421398163 at epoch: 9 and batch_num: 23\n",
      "Loss of test set: 0.4517626166343689 at epoch: 9 and batch_num: 24\n",
      "Loss of test set: 0.45978736877441406 at epoch: 9 and batch_num: 25\n",
      "Loss of test set: 0.3408817648887634 at epoch: 9 and batch_num: 26\n",
      "Loss of test set: 0.43894100189208984 at epoch: 9 and batch_num: 27\n",
      "Loss of test set: 0.3349129557609558 at epoch: 9 and batch_num: 28\n",
      "Loss of test set: 0.4518556594848633 at epoch: 9 and batch_num: 29\n",
      "Loss of test set: 0.3201757073402405 at epoch: 9 and batch_num: 30\n",
      "Loss of test set: 0.443627268075943 at epoch: 9 and batch_num: 31\n",
      "Loss of test set: 0.2737296223640442 at epoch: 9 and batch_num: 32\n",
      "Loss of test set: 0.30238866806030273 at epoch: 9 and batch_num: 33\n",
      "Loss of test set: 0.3421837389469147 at epoch: 9 and batch_num: 34\n",
      "Loss of test set: 0.45448970794677734 at epoch: 9 and batch_num: 35\n",
      "Loss of test set: 0.507598876953125 at epoch: 9 and batch_num: 36\n",
      "Loss of test set: 0.2827717065811157 at epoch: 9 and batch_num: 37\n",
      "Loss of test set: 0.40143826603889465 at epoch: 9 and batch_num: 38\n",
      "Loss of test set: 0.38974127173423767 at epoch: 9 and batch_num: 39\n",
      "Loss of test set: 0.355360209941864 at epoch: 9 and batch_num: 40\n",
      "Loss of test set: 0.5442773699760437 at epoch: 9 and batch_num: 41\n",
      "Loss of test set: 0.4101092219352722 at epoch: 9 and batch_num: 42\n",
      "Loss of test set: 0.25578466057777405 at epoch: 9 and batch_num: 43\n",
      "Loss of test set: 0.2679184675216675 at epoch: 9 and batch_num: 44\n",
      "Loss of test set: 0.26399195194244385 at epoch: 9 and batch_num: 45\n",
      "Loss of test set: 0.25515881180763245 at epoch: 9 and batch_num: 46\n",
      "Loss of test set: 0.5067920684814453 at epoch: 9 and batch_num: 47\n",
      "Loss of test set: 0.3727266192436218 at epoch: 9 and batch_num: 48\n",
      "Loss of test set: 0.4019082188606262 at epoch: 9 and batch_num: 49\n",
      "Loss of test set: 0.38788527250289917 at epoch: 9 and batch_num: 50\n",
      "Loss of test set: 0.3070301413536072 at epoch: 9 and batch_num: 51\n",
      "Loss of test set: 0.262645959854126 at epoch: 9 and batch_num: 52\n",
      "Loss of test set: 0.3323098421096802 at epoch: 9 and batch_num: 53\n",
      "Loss of test set: 0.29504650831222534 at epoch: 9 and batch_num: 54\n",
      "Loss of test set: 0.4831320345401764 at epoch: 9 and batch_num: 55\n",
      "Loss of test set: 0.3386179804801941 at epoch: 9 and batch_num: 56\n",
      "Loss of test set: 0.44292151927948 at epoch: 9 and batch_num: 57\n",
      "Loss of test set: 0.24588187038898468 at epoch: 9 and batch_num: 58\n",
      "Loss of test set: 0.32750529050827026 at epoch: 9 and batch_num: 59\n",
      "Loss of test set: 0.44287392497062683 at epoch: 9 and batch_num: 60\n",
      "Loss of test set: 0.3950536847114563 at epoch: 9 and batch_num: 61\n",
      "Loss of test set: 0.46937575936317444 at epoch: 9 and batch_num: 62\n",
      "Loss of test set: 0.4349414110183716 at epoch: 9 and batch_num: 63\n",
      "Loss of test set: 0.33735713362693787 at epoch: 9 and batch_num: 64\n",
      "Loss of test set: 0.31726664304733276 at epoch: 9 and batch_num: 65\n",
      "Loss of test set: 0.5670927166938782 at epoch: 9 and batch_num: 66\n",
      "Loss of test set: 0.3238855004310608 at epoch: 9 and batch_num: 67\n",
      "Loss of test set: 0.3902755379676819 at epoch: 9 and batch_num: 68\n",
      "Loss of test set: 0.5307967662811279 at epoch: 9 and batch_num: 69\n",
      "Loss of test set: 0.2060781717300415 at epoch: 9 and batch_num: 70\n",
      "Loss of test set: 0.7257300615310669 at epoch: 9 and batch_num: 71\n",
      "Loss of test set: 0.36219537258148193 at epoch: 9 and batch_num: 72\n",
      "Loss of test set: 0.40475165843963623 at epoch: 9 and batch_num: 73\n",
      "Loss of test set: 0.36323630809783936 at epoch: 9 and batch_num: 74\n",
      "Loss of test set: 0.48212650418281555 at epoch: 9 and batch_num: 75\n",
      "Loss of test set: 0.4161108732223511 at epoch: 9 and batch_num: 76\n",
      "Loss of test set: 0.4978637993335724 at epoch: 9 and batch_num: 77\n",
      "Loss of test set: 0.405014306306839 at epoch: 9 and batch_num: 78\n",
      "Loss of test set: 0.53368079662323 at epoch: 9 and batch_num: 79\n",
      "Loss of test set: 0.3198910057544708 at epoch: 9 and batch_num: 80\n",
      "Loss of test set: 0.42010244727134705 at epoch: 9 and batch_num: 81\n",
      "Loss of test set: 0.43488937616348267 at epoch: 9 and batch_num: 82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.3571007549762726 at epoch: 9 and batch_num: 83\n",
      "Loss of test set: 0.30609580874443054 at epoch: 9 and batch_num: 84\n",
      "Loss of test set: 0.5697828531265259 at epoch: 9 and batch_num: 85\n",
      "Loss of test set: 0.42568111419677734 at epoch: 9 and batch_num: 86\n",
      "Loss of test set: 0.2599533796310425 at epoch: 9 and batch_num: 87\n",
      "Loss of test set: 0.16316485404968262 at epoch: 9 and batch_num: 88\n",
      "Loss of test set: 0.36759084463119507 at epoch: 9 and batch_num: 89\n",
      "Loss of test set: 0.3481326401233673 at epoch: 9 and batch_num: 90\n",
      "Loss of test set: 0.6175189018249512 at epoch: 9 and batch_num: 91\n",
      "Loss of test set: 0.3101915717124939 at epoch: 9 and batch_num: 92\n",
      "Loss of test set: 0.41170164942741394 at epoch: 9 and batch_num: 93\n",
      "Loss of test set: 0.31537288427352905 at epoch: 9 and batch_num: 94\n",
      "Loss of test set: 0.3915899395942688 at epoch: 9 and batch_num: 95\n",
      "Loss of test set: 0.35253769159317017 at epoch: 9 and batch_num: 96\n",
      "Loss of test set: 0.4749367833137512 at epoch: 9 and batch_num: 97\n",
      "Loss of test set: 0.4100475311279297 at epoch: 9 and batch_num: 98\n",
      "Loss of test set: 0.24395084381103516 at epoch: 9 and batch_num: 99\n",
      "Loss of test set: 0.41764503717422485 at epoch: 9 and batch_num: 100\n",
      "Loss of test set: 0.3570530414581299 at epoch: 9 and batch_num: 101\n",
      "Loss of test set: 0.49287325143814087 at epoch: 9 and batch_num: 102\n",
      "Loss of test set: 0.6138252019882202 at epoch: 9 and batch_num: 103\n",
      "Loss of test set: 0.4173709750175476 at epoch: 9 and batch_num: 104\n",
      "Loss of test set: 0.4519388675689697 at epoch: 9 and batch_num: 105\n",
      "Loss of test set: 0.23327350616455078 at epoch: 9 and batch_num: 106\n",
      "Loss of test set: 0.26538601517677307 at epoch: 9 and batch_num: 107\n",
      "Loss of test set: 0.35308516025543213 at epoch: 9 and batch_num: 108\n",
      "Loss of test set: 0.418172687292099 at epoch: 9 and batch_num: 109\n",
      "Loss of test set: 0.4366726577281952 at epoch: 9 and batch_num: 110\n",
      "Loss of test set: 0.2212834656238556 at epoch: 9 and batch_num: 111\n",
      "Loss of test set: 0.3651331663131714 at epoch: 9 and batch_num: 112\n",
      "Loss of test set: 0.3515283465385437 at epoch: 9 and batch_num: 113\n",
      "Loss of test set: 0.32651859521865845 at epoch: 9 and batch_num: 114\n",
      "Loss of test set: 0.6102710366249084 at epoch: 9 and batch_num: 115\n",
      "Loss of test set: 0.22784832119941711 at epoch: 9 and batch_num: 116\n",
      "Loss of test set: 0.4166076183319092 at epoch: 9 and batch_num: 117\n",
      "Loss of test set: 0.254173219203949 at epoch: 9 and batch_num: 118\n",
      "Loss of test set: 0.31515881419181824 at epoch: 9 and batch_num: 119\n",
      "Loss of test set: 0.22464221715927124 at epoch: 9 and batch_num: 120\n",
      "Loss of test set: 0.24954384565353394 at epoch: 9 and batch_num: 121\n",
      "Loss of test set: 0.2552882730960846 at epoch: 9 and batch_num: 122\n",
      "Loss of test set: 0.3063873052597046 at epoch: 9 and batch_num: 123\n",
      "Loss of test set: 0.19924791157245636 at epoch: 9 and batch_num: 124\n",
      "Loss of test set: 0.28185147047042847 at epoch: 9 and batch_num: 125\n",
      "Loss of test set: 0.3275963366031647 at epoch: 9 and batch_num: 126\n",
      "Loss of test set: 0.20257097482681274 at epoch: 9 and batch_num: 127\n",
      "Loss of test set: 0.5905507802963257 at epoch: 9 and batch_num: 128\n",
      "Loss of test set: 0.1004679799079895 at epoch: 9 and batch_num: 129\n",
      "Loss of test set: 0.40023425221443176 at epoch: 9 and batch_num: 130\n",
      "Loss of test set: 0.5457508563995361 at epoch: 9 and batch_num: 131\n",
      "Loss of test set: 0.3709738850593567 at epoch: 9 and batch_num: 132\n",
      "Loss of test set: 0.2959211468696594 at epoch: 9 and batch_num: 133\n",
      "Loss of test set: 0.38444632291793823 at epoch: 9 and batch_num: 134\n",
      "Loss of test set: 0.2630678117275238 at epoch: 9 and batch_num: 135\n",
      "Loss of test set: 0.28662624955177307 at epoch: 9 and batch_num: 136\n",
      "Loss of test set: 0.44233983755111694 at epoch: 9 and batch_num: 137\n",
      "Loss of test set: 0.36447805166244507 at epoch: 9 and batch_num: 138\n",
      "Loss of test set: 0.2857804596424103 at epoch: 9 and batch_num: 139\n",
      "Loss of test set: 0.5571127533912659 at epoch: 9 and batch_num: 140\n",
      "Loss of test set: 0.36873674392700195 at epoch: 9 and batch_num: 141\n",
      "Loss of test set: 0.21811224520206451 at epoch: 9 and batch_num: 142\n",
      "Loss of test set: 0.3567858934402466 at epoch: 9 and batch_num: 143\n",
      "Loss of test set: 0.46603673696517944 at epoch: 9 and batch_num: 144\n",
      "Loss of test set: 0.37786614894866943 at epoch: 9 and batch_num: 145\n",
      "Loss of test set: 0.3725505769252777 at epoch: 9 and batch_num: 146\n",
      "Loss of test set: 0.2596513032913208 at epoch: 9 and batch_num: 147\n",
      "Loss of test set: 0.44459474086761475 at epoch: 9 and batch_num: 148\n",
      "Loss of test set: 0.40925878286361694 at epoch: 9 and batch_num: 149\n",
      "Loss of test set: 0.5121741890907288 at epoch: 9 and batch_num: 150\n",
      "Loss of test set: 0.2609488368034363 at epoch: 9 and batch_num: 151\n",
      "Loss of test set: 0.17006170749664307 at epoch: 9 and batch_num: 152\n",
      "Loss of test set: 0.5245413184165955 at epoch: 9 and batch_num: 153\n",
      "Loss of test set: 0.23197244107723236 at epoch: 9 and batch_num: 154\n",
      "Loss of test set: 0.40143758058547974 at epoch: 9 and batch_num: 155\n",
      "Loss of test set: 0.2053436040878296 at epoch: 9 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8669\n",
      "Loss of train set: 0.3768330514431 at epoch: 10 and batch_num: 0\n",
      "Loss of train set: 0.4407128095626831 at epoch: 10 and batch_num: 1\n",
      "Loss of train set: 0.2315778136253357 at epoch: 10 and batch_num: 2\n",
      "Loss of train set: 0.3745661675930023 at epoch: 10 and batch_num: 3\n",
      "Loss of train set: 0.37034523487091064 at epoch: 10 and batch_num: 4\n",
      "Loss of train set: 0.447867751121521 at epoch: 10 and batch_num: 5\n",
      "Loss of train set: 0.2312730997800827 at epoch: 10 and batch_num: 6\n",
      "Loss of train set: 0.3136869966983795 at epoch: 10 and batch_num: 7\n",
      "Loss of train set: 0.23107284307479858 at epoch: 10 and batch_num: 8\n",
      "Loss of train set: 0.22623786330223083 at epoch: 10 and batch_num: 9\n",
      "Loss of train set: 0.3973807692527771 at epoch: 10 and batch_num: 10\n",
      "Loss of train set: 0.38152867555618286 at epoch: 10 and batch_num: 11\n",
      "Loss of train set: 0.3411042094230652 at epoch: 10 and batch_num: 12\n",
      "Loss of train set: 0.18024981021881104 at epoch: 10 and batch_num: 13\n",
      "Loss of train set: 0.23309387266635895 at epoch: 10 and batch_num: 14\n",
      "Loss of train set: 0.5064874887466431 at epoch: 10 and batch_num: 15\n",
      "Loss of train set: 0.4883902370929718 at epoch: 10 and batch_num: 16\n",
      "Loss of train set: 0.1870437115430832 at epoch: 10 and batch_num: 17\n",
      "Loss of train set: 0.36236071586608887 at epoch: 10 and batch_num: 18\n",
      "Loss of train set: 0.3794183135032654 at epoch: 10 and batch_num: 19\n",
      "Loss of train set: 0.20469501614570618 at epoch: 10 and batch_num: 20\n",
      "Loss of train set: 0.7182072401046753 at epoch: 10 and batch_num: 21\n",
      "Loss of train set: 0.3090505599975586 at epoch: 10 and batch_num: 22\n",
      "Loss of train set: 0.35242408514022827 at epoch: 10 and batch_num: 23\n",
      "Loss of train set: 0.366090327501297 at epoch: 10 and batch_num: 24\n",
      "Loss of train set: 0.3119211792945862 at epoch: 10 and batch_num: 25\n",
      "Loss of train set: 0.3501507341861725 at epoch: 10 and batch_num: 26\n",
      "Loss of train set: 0.2439422756433487 at epoch: 10 and batch_num: 27\n",
      "Loss of train set: 0.39522767066955566 at epoch: 10 and batch_num: 28\n",
      "Loss of train set: 0.367105633020401 at epoch: 10 and batch_num: 29\n",
      "Loss of train set: 0.314687043428421 at epoch: 10 and batch_num: 30\n",
      "Loss of train set: 0.3385270833969116 at epoch: 10 and batch_num: 31\n",
      "Loss of train set: 0.21743667125701904 at epoch: 10 and batch_num: 32\n",
      "Loss of train set: 0.2174278348684311 at epoch: 10 and batch_num: 33\n",
      "Loss of train set: 0.2569065988063812 at epoch: 10 and batch_num: 34\n",
      "Loss of train set: 0.2059268355369568 at epoch: 10 and batch_num: 35\n",
      "Loss of train set: 0.3828134536743164 at epoch: 10 and batch_num: 36\n",
      "Loss of train set: 0.5480770468711853 at epoch: 10 and batch_num: 37\n",
      "Loss of train set: 0.2667247951030731 at epoch: 10 and batch_num: 38\n",
      "Loss of train set: 0.23499590158462524 at epoch: 10 and batch_num: 39\n",
      "Loss of train set: 0.2308119684457779 at epoch: 10 and batch_num: 40\n",
      "Loss of train set: 0.656312108039856 at epoch: 10 and batch_num: 41\n",
      "Loss of train set: 0.31337791681289673 at epoch: 10 and batch_num: 42\n",
      "Loss of train set: 0.2257726788520813 at epoch: 10 and batch_num: 43\n",
      "Loss of train set: 0.449781209230423 at epoch: 10 and batch_num: 44\n",
      "Loss of train set: 0.23407742381095886 at epoch: 10 and batch_num: 45\n",
      "Loss of train set: 0.3154614567756653 at epoch: 10 and batch_num: 46\n",
      "Loss of train set: 0.23816953599452972 at epoch: 10 and batch_num: 47\n",
      "Loss of train set: 0.2068704068660736 at epoch: 10 and batch_num: 48\n",
      "Loss of train set: 0.4547152817249298 at epoch: 10 and batch_num: 49\n",
      "Loss of train set: 0.2952830195426941 at epoch: 10 and batch_num: 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.33259499073028564 at epoch: 10 and batch_num: 51\n",
      "Loss of train set: 0.19776801764965057 at epoch: 10 and batch_num: 52\n",
      "Loss of train set: 0.17191827297210693 at epoch: 10 and batch_num: 53\n",
      "Loss of train set: 0.2310125082731247 at epoch: 10 and batch_num: 54\n",
      "Loss of train set: 0.32577958703041077 at epoch: 10 and batch_num: 55\n",
      "Loss of train set: 0.3421340584754944 at epoch: 10 and batch_num: 56\n",
      "Loss of train set: 0.295391321182251 at epoch: 10 and batch_num: 57\n",
      "Loss of train set: 0.32556238770484924 at epoch: 10 and batch_num: 58\n",
      "Loss of train set: 0.39223670959472656 at epoch: 10 and batch_num: 59\n",
      "Loss of train set: 0.3610004782676697 at epoch: 10 and batch_num: 60\n",
      "Loss of train set: 0.2564947307109833 at epoch: 10 and batch_num: 61\n",
      "Loss of train set: 0.26797178387641907 at epoch: 10 and batch_num: 62\n",
      "Loss of train set: 0.23478564620018005 at epoch: 10 and batch_num: 63\n",
      "Loss of train set: 0.25429803133010864 at epoch: 10 and batch_num: 64\n",
      "Loss of train set: 0.19930317997932434 at epoch: 10 and batch_num: 65\n",
      "Loss of train set: 0.5377912521362305 at epoch: 10 and batch_num: 66\n",
      "Loss of train set: 0.25850537419319153 at epoch: 10 and batch_num: 67\n",
      "Loss of train set: 0.3655107617378235 at epoch: 10 and batch_num: 68\n",
      "Loss of train set: 0.4530993402004242 at epoch: 10 and batch_num: 69\n",
      "Loss of train set: 0.2708459496498108 at epoch: 10 and batch_num: 70\n",
      "Loss of train set: 0.36231210827827454 at epoch: 10 and batch_num: 71\n",
      "Loss of train set: 0.28021711111068726 at epoch: 10 and batch_num: 72\n",
      "Loss of train set: 0.21184784173965454 at epoch: 10 and batch_num: 73\n",
      "Loss of train set: 0.2857358455657959 at epoch: 10 and batch_num: 74\n",
      "Loss of train set: 0.190547376871109 at epoch: 10 and batch_num: 75\n",
      "Loss of train set: 0.20899833738803864 at epoch: 10 and batch_num: 76\n",
      "Loss of train set: 0.22709815204143524 at epoch: 10 and batch_num: 77\n",
      "Loss of train set: 0.3127349317073822 at epoch: 10 and batch_num: 78\n",
      "Loss of train set: 0.5090158581733704 at epoch: 10 and batch_num: 79\n",
      "Loss of train set: 0.45637935400009155 at epoch: 10 and batch_num: 80\n",
      "Loss of train set: 0.27199482917785645 at epoch: 10 and batch_num: 81\n",
      "Loss of train set: 0.4409213364124298 at epoch: 10 and batch_num: 82\n",
      "Loss of train set: 0.4288708567619324 at epoch: 10 and batch_num: 83\n",
      "Loss of train set: 0.3117518424987793 at epoch: 10 and batch_num: 84\n",
      "Loss of train set: 0.2733519971370697 at epoch: 10 and batch_num: 85\n",
      "Loss of train set: 0.36589452624320984 at epoch: 10 and batch_num: 86\n",
      "Loss of train set: 0.32703715562820435 at epoch: 10 and batch_num: 87\n",
      "Loss of train set: 0.33461427688598633 at epoch: 10 and batch_num: 88\n",
      "Loss of train set: 0.22872373461723328 at epoch: 10 and batch_num: 89\n",
      "Loss of train set: 0.25621214509010315 at epoch: 10 and batch_num: 90\n",
      "Loss of train set: 0.24665513634681702 at epoch: 10 and batch_num: 91\n",
      "Loss of train set: 0.3114800453186035 at epoch: 10 and batch_num: 92\n",
      "Loss of train set: 0.209798201918602 at epoch: 10 and batch_num: 93\n",
      "Loss of train set: 0.24039766192436218 at epoch: 10 and batch_num: 94\n",
      "Loss of train set: 0.2692359983921051 at epoch: 10 and batch_num: 95\n",
      "Loss of train set: 0.3137626051902771 at epoch: 10 and batch_num: 96\n",
      "Loss of train set: 0.3110552430152893 at epoch: 10 and batch_num: 97\n",
      "Loss of train set: 0.3024263381958008 at epoch: 10 and batch_num: 98\n",
      "Loss of train set: 0.44224441051483154 at epoch: 10 and batch_num: 99\n",
      "Loss of train set: 0.29200661182403564 at epoch: 10 and batch_num: 100\n",
      "Loss of train set: 0.28104984760284424 at epoch: 10 and batch_num: 101\n",
      "Loss of train set: 0.30122488737106323 at epoch: 10 and batch_num: 102\n",
      "Loss of train set: 0.39027172327041626 at epoch: 10 and batch_num: 103\n",
      "Loss of train set: 0.21516543626785278 at epoch: 10 and batch_num: 104\n",
      "Loss of train set: 0.18787993490695953 at epoch: 10 and batch_num: 105\n",
      "Loss of train set: 0.16199365258216858 at epoch: 10 and batch_num: 106\n",
      "Loss of train set: 0.3437903821468353 at epoch: 10 and batch_num: 107\n",
      "Loss of train set: 0.5456434488296509 at epoch: 10 and batch_num: 108\n",
      "Loss of train set: 0.3251562714576721 at epoch: 10 and batch_num: 109\n",
      "Loss of train set: 0.41457319259643555 at epoch: 10 and batch_num: 110\n",
      "Loss of train set: 0.26949554681777954 at epoch: 10 and batch_num: 111\n",
      "Loss of train set: 0.5507392883300781 at epoch: 10 and batch_num: 112\n",
      "Loss of train set: 0.3821292817592621 at epoch: 10 and batch_num: 113\n",
      "Loss of train set: 0.1671491265296936 at epoch: 10 and batch_num: 114\n",
      "Loss of train set: 0.34507015347480774 at epoch: 10 and batch_num: 115\n",
      "Loss of train set: 0.2769106924533844 at epoch: 10 and batch_num: 116\n",
      "Loss of train set: 0.3522389531135559 at epoch: 10 and batch_num: 117\n",
      "Loss of train set: 0.3614276647567749 at epoch: 10 and batch_num: 118\n",
      "Loss of train set: 0.25468796491622925 at epoch: 10 and batch_num: 119\n",
      "Loss of train set: 0.2822722792625427 at epoch: 10 and batch_num: 120\n",
      "Loss of train set: 0.2867116928100586 at epoch: 10 and batch_num: 121\n",
      "Loss of train set: 0.4873965084552765 at epoch: 10 and batch_num: 122\n",
      "Loss of train set: 0.3017905354499817 at epoch: 10 and batch_num: 123\n",
      "Loss of train set: 0.4248661994934082 at epoch: 10 and batch_num: 124\n",
      "Loss of train set: 0.3369854688644409 at epoch: 10 and batch_num: 125\n",
      "Loss of train set: 0.2785913050174713 at epoch: 10 and batch_num: 126\n",
      "Loss of train set: 0.3335113525390625 at epoch: 10 and batch_num: 127\n",
      "Loss of train set: 0.42180120944976807 at epoch: 10 and batch_num: 128\n",
      "Loss of train set: 0.3237953186035156 at epoch: 10 and batch_num: 129\n",
      "Loss of train set: 0.32442688941955566 at epoch: 10 and batch_num: 130\n",
      "Loss of train set: 0.23901869356632233 at epoch: 10 and batch_num: 131\n",
      "Loss of train set: 0.2736561596393585 at epoch: 10 and batch_num: 132\n",
      "Loss of train set: 0.2586163282394409 at epoch: 10 and batch_num: 133\n",
      "Loss of train set: 0.43373924493789673 at epoch: 10 and batch_num: 134\n",
      "Loss of train set: 0.2709801197052002 at epoch: 10 and batch_num: 135\n",
      "Loss of train set: 0.501239538192749 at epoch: 10 and batch_num: 136\n",
      "Loss of train set: 0.37917569279670715 at epoch: 10 and batch_num: 137\n",
      "Loss of train set: 0.2294563502073288 at epoch: 10 and batch_num: 138\n",
      "Loss of train set: 0.40115824341773987 at epoch: 10 and batch_num: 139\n",
      "Loss of train set: 0.338807076215744 at epoch: 10 and batch_num: 140\n",
      "Loss of train set: 0.34492984414100647 at epoch: 10 and batch_num: 141\n",
      "Loss of train set: 0.3125707507133484 at epoch: 10 and batch_num: 142\n",
      "Loss of train set: 0.32436054944992065 at epoch: 10 and batch_num: 143\n",
      "Loss of train set: 0.35104307532310486 at epoch: 10 and batch_num: 144\n",
      "Loss of train set: 0.3658267855644226 at epoch: 10 and batch_num: 145\n",
      "Loss of train set: 0.16629791259765625 at epoch: 10 and batch_num: 146\n",
      "Loss of train set: 0.4450506567955017 at epoch: 10 and batch_num: 147\n",
      "Loss of train set: 0.3652692437171936 at epoch: 10 and batch_num: 148\n",
      "Loss of train set: 0.46006470918655396 at epoch: 10 and batch_num: 149\n",
      "Loss of train set: 0.26568496227264404 at epoch: 10 and batch_num: 150\n",
      "Loss of train set: 0.3487239480018616 at epoch: 10 and batch_num: 151\n",
      "Loss of train set: 0.212782621383667 at epoch: 10 and batch_num: 152\n",
      "Loss of train set: 0.18211115896701813 at epoch: 10 and batch_num: 153\n",
      "Loss of train set: 0.31270796060562134 at epoch: 10 and batch_num: 154\n",
      "Loss of train set: 0.18851757049560547 at epoch: 10 and batch_num: 155\n",
      "Loss of train set: 0.2827569544315338 at epoch: 10 and batch_num: 156\n",
      "Loss of train set: 0.4385502338409424 at epoch: 10 and batch_num: 157\n",
      "Loss of train set: 0.4445032477378845 at epoch: 10 and batch_num: 158\n",
      "Loss of train set: 0.2883046567440033 at epoch: 10 and batch_num: 159\n",
      "Loss of train set: 0.5263028740882874 at epoch: 10 and batch_num: 160\n",
      "Loss of train set: 0.2626808285713196 at epoch: 10 and batch_num: 161\n",
      "Loss of train set: 0.4164523482322693 at epoch: 10 and batch_num: 162\n",
      "Loss of train set: 0.2337743192911148 at epoch: 10 and batch_num: 163\n",
      "Loss of train set: 0.28429871797561646 at epoch: 10 and batch_num: 164\n",
      "Loss of train set: 0.2670328617095947 at epoch: 10 and batch_num: 165\n",
      "Loss of train set: 0.21104376018047333 at epoch: 10 and batch_num: 166\n",
      "Loss of train set: 0.3750793933868408 at epoch: 10 and batch_num: 167\n",
      "Loss of train set: 0.279580295085907 at epoch: 10 and batch_num: 168\n",
      "Loss of train set: 0.2836947441101074 at epoch: 10 and batch_num: 169\n",
      "Loss of train set: 0.451507568359375 at epoch: 10 and batch_num: 170\n",
      "Loss of train set: 0.4272731840610504 at epoch: 10 and batch_num: 171\n",
      "Loss of train set: 0.30503106117248535 at epoch: 10 and batch_num: 172\n",
      "Loss of train set: 0.3253597617149353 at epoch: 10 and batch_num: 173\n",
      "Loss of train set: 0.17067137360572815 at epoch: 10 and batch_num: 174\n",
      "Loss of train set: 0.448581337928772 at epoch: 10 and batch_num: 175\n",
      "Loss of train set: 0.4093613624572754 at epoch: 10 and batch_num: 176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.33431077003479004 at epoch: 10 and batch_num: 177\n",
      "Loss of train set: 0.3604740798473358 at epoch: 10 and batch_num: 178\n",
      "Loss of train set: 0.365283727645874 at epoch: 10 and batch_num: 179\n",
      "Loss of train set: 0.24611377716064453 at epoch: 10 and batch_num: 180\n",
      "Loss of train set: 0.35298776626586914 at epoch: 10 and batch_num: 181\n",
      "Loss of train set: 0.13021869957447052 at epoch: 10 and batch_num: 182\n",
      "Loss of train set: 0.4091578722000122 at epoch: 10 and batch_num: 183\n",
      "Loss of train set: 0.3319440484046936 at epoch: 10 and batch_num: 184\n",
      "Loss of train set: 0.35723137855529785 at epoch: 10 and batch_num: 185\n",
      "Loss of train set: 0.17520469427108765 at epoch: 10 and batch_num: 186\n",
      "Loss of train set: 0.5394436120986938 at epoch: 10 and batch_num: 187\n",
      "Loss of train set: 0.27174121141433716 at epoch: 10 and batch_num: 188\n",
      "Loss of train set: 0.27544882893562317 at epoch: 10 and batch_num: 189\n",
      "Loss of train set: 0.18192297220230103 at epoch: 10 and batch_num: 190\n",
      "Loss of train set: 0.23188963532447815 at epoch: 10 and batch_num: 191\n",
      "Loss of train set: 0.217397078871727 at epoch: 10 and batch_num: 192\n",
      "Loss of train set: 0.2338196337223053 at epoch: 10 and batch_num: 193\n",
      "Loss of train set: 0.18625348806381226 at epoch: 10 and batch_num: 194\n",
      "Loss of train set: 0.2671676278114319 at epoch: 10 and batch_num: 195\n",
      "Loss of train set: 0.3937752842903137 at epoch: 10 and batch_num: 196\n",
      "Loss of train set: 0.3113500475883484 at epoch: 10 and batch_num: 197\n",
      "Loss of train set: 0.32814663648605347 at epoch: 10 and batch_num: 198\n",
      "Loss of train set: 0.3747415542602539 at epoch: 10 and batch_num: 199\n",
      "Loss of train set: 0.32139813899993896 at epoch: 10 and batch_num: 200\n",
      "Loss of train set: 0.3915731906890869 at epoch: 10 and batch_num: 201\n",
      "Loss of train set: 0.2616688013076782 at epoch: 10 and batch_num: 202\n",
      "Loss of train set: 0.3383485674858093 at epoch: 10 and batch_num: 203\n",
      "Loss of train set: 0.31609994173049927 at epoch: 10 and batch_num: 204\n",
      "Loss of train set: 0.22202444076538086 at epoch: 10 and batch_num: 205\n",
      "Loss of train set: 0.24925783276557922 at epoch: 10 and batch_num: 206\n",
      "Loss of train set: 0.24548277258872986 at epoch: 10 and batch_num: 207\n",
      "Loss of train set: 0.30062633752822876 at epoch: 10 and batch_num: 208\n",
      "Loss of train set: 0.2980937063694 at epoch: 10 and batch_num: 209\n",
      "Loss of train set: 0.41709834337234497 at epoch: 10 and batch_num: 210\n",
      "Loss of train set: 0.23618531227111816 at epoch: 10 and batch_num: 211\n",
      "Loss of train set: 0.11169935762882233 at epoch: 10 and batch_num: 212\n",
      "Loss of train set: 0.4003639221191406 at epoch: 10 and batch_num: 213\n",
      "Loss of train set: 0.3055464029312134 at epoch: 10 and batch_num: 214\n",
      "Loss of train set: 0.2413024753332138 at epoch: 10 and batch_num: 215\n",
      "Loss of train set: 0.36883100867271423 at epoch: 10 and batch_num: 216\n",
      "Loss of train set: 0.3825264871120453 at epoch: 10 and batch_num: 217\n",
      "Loss of train set: 0.4133100211620331 at epoch: 10 and batch_num: 218\n",
      "Loss of train set: 0.4579751789569855 at epoch: 10 and batch_num: 219\n",
      "Loss of train set: 0.3743874728679657 at epoch: 10 and batch_num: 220\n",
      "Loss of train set: 0.38781723380088806 at epoch: 10 and batch_num: 221\n",
      "Loss of train set: 0.3126867413520813 at epoch: 10 and batch_num: 222\n",
      "Loss of train set: 0.5367833971977234 at epoch: 10 and batch_num: 223\n",
      "Loss of train set: 0.2862078845500946 at epoch: 10 and batch_num: 224\n",
      "Loss of train set: 0.21212024986743927 at epoch: 10 and batch_num: 225\n",
      "Loss of train set: 0.2459857016801834 at epoch: 10 and batch_num: 226\n",
      "Loss of train set: 0.3186629116535187 at epoch: 10 and batch_num: 227\n",
      "Loss of train set: 0.3625767230987549 at epoch: 10 and batch_num: 228\n",
      "Loss of train set: 0.3155050575733185 at epoch: 10 and batch_num: 229\n",
      "Loss of train set: 0.3558421730995178 at epoch: 10 and batch_num: 230\n",
      "Loss of train set: 0.31736111640930176 at epoch: 10 and batch_num: 231\n",
      "Loss of train set: 0.17707151174545288 at epoch: 10 and batch_num: 232\n",
      "Loss of train set: 0.30155736207962036 at epoch: 10 and batch_num: 233\n",
      "Loss of train set: 0.3405458927154541 at epoch: 10 and batch_num: 234\n",
      "Loss of train set: 0.32349011301994324 at epoch: 10 and batch_num: 235\n",
      "Loss of train set: 0.43687957525253296 at epoch: 10 and batch_num: 236\n",
      "Loss of train set: 0.3233773112297058 at epoch: 10 and batch_num: 237\n",
      "Loss of train set: 0.31932389736175537 at epoch: 10 and batch_num: 238\n",
      "Loss of train set: 0.2826487123966217 at epoch: 10 and batch_num: 239\n",
      "Loss of train set: 0.31173545122146606 at epoch: 10 and batch_num: 240\n",
      "Loss of train set: 0.3207460641860962 at epoch: 10 and batch_num: 241\n",
      "Loss of train set: 0.308600515127182 at epoch: 10 and batch_num: 242\n",
      "Loss of train set: 0.3924710750579834 at epoch: 10 and batch_num: 243\n",
      "Loss of train set: 0.22856362164020538 at epoch: 10 and batch_num: 244\n",
      "Loss of train set: 0.24731431901454926 at epoch: 10 and batch_num: 245\n",
      "Loss of train set: 0.3093334138393402 at epoch: 10 and batch_num: 246\n",
      "Loss of train set: 0.2762502133846283 at epoch: 10 and batch_num: 247\n",
      "Loss of train set: 0.4794865846633911 at epoch: 10 and batch_num: 248\n",
      "Loss of train set: 0.260829359292984 at epoch: 10 and batch_num: 249\n",
      "Loss of train set: 0.3139994740486145 at epoch: 10 and batch_num: 250\n",
      "Loss of train set: 0.4373791217803955 at epoch: 10 and batch_num: 251\n",
      "Loss of train set: 0.42445337772369385 at epoch: 10 and batch_num: 252\n",
      "Loss of train set: 0.3200632929801941 at epoch: 10 and batch_num: 253\n",
      "Loss of train set: 0.23168522119522095 at epoch: 10 and batch_num: 254\n",
      "Loss of train set: 0.32516610622406006 at epoch: 10 and batch_num: 255\n",
      "Loss of train set: 0.47764936089515686 at epoch: 10 and batch_num: 256\n",
      "Loss of train set: 0.3845307230949402 at epoch: 10 and batch_num: 257\n",
      "Loss of train set: 0.35606735944747925 at epoch: 10 and batch_num: 258\n",
      "Loss of train set: 0.3835623562335968 at epoch: 10 and batch_num: 259\n",
      "Loss of train set: 0.27381432056427 at epoch: 10 and batch_num: 260\n",
      "Loss of train set: 0.38695603609085083 at epoch: 10 and batch_num: 261\n",
      "Loss of train set: 0.18376439809799194 at epoch: 10 and batch_num: 262\n",
      "Loss of train set: 0.3798275589942932 at epoch: 10 and batch_num: 263\n",
      "Loss of train set: 0.18674907088279724 at epoch: 10 and batch_num: 264\n",
      "Loss of train set: 0.4471958875656128 at epoch: 10 and batch_num: 265\n",
      "Loss of train set: 0.3417368233203888 at epoch: 10 and batch_num: 266\n",
      "Loss of train set: 0.32410377264022827 at epoch: 10 and batch_num: 267\n",
      "Loss of train set: 0.29340022802352905 at epoch: 10 and batch_num: 268\n",
      "Loss of train set: 0.3213263154029846 at epoch: 10 and batch_num: 269\n",
      "Loss of train set: 0.3351558446884155 at epoch: 10 and batch_num: 270\n",
      "Loss of train set: 0.3692440390586853 at epoch: 10 and batch_num: 271\n",
      "Loss of train set: 0.3409944176673889 at epoch: 10 and batch_num: 272\n",
      "Loss of train set: 0.14782369136810303 at epoch: 10 and batch_num: 273\n",
      "Loss of train set: 0.35216522216796875 at epoch: 10 and batch_num: 274\n",
      "Loss of train set: 0.3383556604385376 at epoch: 10 and batch_num: 275\n",
      "Loss of train set: 0.4419437646865845 at epoch: 10 and batch_num: 276\n",
      "Loss of train set: 0.556989312171936 at epoch: 10 and batch_num: 277\n",
      "Loss of train set: 0.27032363414764404 at epoch: 10 and batch_num: 278\n",
      "Loss of train set: 0.3644636273384094 at epoch: 10 and batch_num: 279\n",
      "Loss of train set: 0.34903484582901 at epoch: 10 and batch_num: 280\n",
      "Loss of train set: 0.34205469489097595 at epoch: 10 and batch_num: 281\n",
      "Loss of train set: 0.27911972999572754 at epoch: 10 and batch_num: 282\n",
      "Loss of train set: 0.29943177103996277 at epoch: 10 and batch_num: 283\n",
      "Loss of train set: 0.3374082148075104 at epoch: 10 and batch_num: 284\n",
      "Loss of train set: 0.18726952373981476 at epoch: 10 and batch_num: 285\n",
      "Loss of train set: 0.333053320646286 at epoch: 10 and batch_num: 286\n",
      "Loss of train set: 0.23552662134170532 at epoch: 10 and batch_num: 287\n",
      "Loss of train set: 0.3024153709411621 at epoch: 10 and batch_num: 288\n",
      "Loss of train set: 0.31195205450057983 at epoch: 10 and batch_num: 289\n",
      "Loss of train set: 0.41026630997657776 at epoch: 10 and batch_num: 290\n",
      "Loss of train set: 0.40161067247390747 at epoch: 10 and batch_num: 291\n",
      "Loss of train set: 0.2188710719347 at epoch: 10 and batch_num: 292\n",
      "Loss of train set: 0.30278265476226807 at epoch: 10 and batch_num: 293\n",
      "Loss of train set: 0.4437646269798279 at epoch: 10 and batch_num: 294\n",
      "Loss of train set: 0.22879061102867126 at epoch: 10 and batch_num: 295\n",
      "Loss of train set: 0.20872218906879425 at epoch: 10 and batch_num: 296\n",
      "Loss of train set: 0.23237821459770203 at epoch: 10 and batch_num: 297\n",
      "Loss of train set: 0.4228202700614929 at epoch: 10 and batch_num: 298\n",
      "Loss of train set: 0.24208930134773254 at epoch: 10 and batch_num: 299\n",
      "Loss of train set: 0.21942180395126343 at epoch: 10 and batch_num: 300\n",
      "Loss of train set: 0.2869265079498291 at epoch: 10 and batch_num: 301\n",
      "Loss of train set: 0.37608802318573 at epoch: 10 and batch_num: 302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.34891563653945923 at epoch: 10 and batch_num: 303\n",
      "Loss of train set: 0.503473162651062 at epoch: 10 and batch_num: 304\n",
      "Loss of train set: 0.19782650470733643 at epoch: 10 and batch_num: 305\n",
      "Loss of train set: 0.3481793701648712 at epoch: 10 and batch_num: 306\n",
      "Loss of train set: 0.44443005323410034 at epoch: 10 and batch_num: 307\n",
      "Loss of train set: 0.350816547870636 at epoch: 10 and batch_num: 308\n",
      "Loss of train set: 0.2615668475627899 at epoch: 10 and batch_num: 309\n",
      "Loss of train set: 0.2980388402938843 at epoch: 10 and batch_num: 310\n",
      "Loss of train set: 0.5118462443351746 at epoch: 10 and batch_num: 311\n",
      "Loss of train set: 0.22954726219177246 at epoch: 10 and batch_num: 312\n",
      "Loss of train set: 0.40124446153640747 at epoch: 10 and batch_num: 313\n",
      "Loss of train set: 0.4216070771217346 at epoch: 10 and batch_num: 314\n",
      "Loss of train set: 0.4122104048728943 at epoch: 10 and batch_num: 315\n",
      "Loss of train set: 0.3022908568382263 at epoch: 10 and batch_num: 316\n",
      "Loss of train set: 0.3749813437461853 at epoch: 10 and batch_num: 317\n",
      "Loss of train set: 0.21022969484329224 at epoch: 10 and batch_num: 318\n",
      "Loss of train set: 0.24953322112560272 at epoch: 10 and batch_num: 319\n",
      "Loss of train set: 0.22776632010936737 at epoch: 10 and batch_num: 320\n",
      "Loss of train set: 0.33433449268341064 at epoch: 10 and batch_num: 321\n",
      "Loss of train set: 0.34727054834365845 at epoch: 10 and batch_num: 322\n",
      "Loss of train set: 0.36671772599220276 at epoch: 10 and batch_num: 323\n",
      "Loss of train set: 0.1697562038898468 at epoch: 10 and batch_num: 324\n",
      "Loss of train set: 0.30672186613082886 at epoch: 10 and batch_num: 325\n",
      "Loss of train set: 0.29638242721557617 at epoch: 10 and batch_num: 326\n",
      "Loss of train set: 0.2780422270298004 at epoch: 10 and batch_num: 327\n",
      "Loss of train set: 0.2788986563682556 at epoch: 10 and batch_num: 328\n",
      "Loss of train set: 0.33610302209854126 at epoch: 10 and batch_num: 329\n",
      "Loss of train set: 0.28640034794807434 at epoch: 10 and batch_num: 330\n",
      "Loss of train set: 0.30015701055526733 at epoch: 10 and batch_num: 331\n",
      "Loss of train set: 0.32651859521865845 at epoch: 10 and batch_num: 332\n",
      "Loss of train set: 0.2663841247558594 at epoch: 10 and batch_num: 333\n",
      "Loss of train set: 0.17969223856925964 at epoch: 10 and batch_num: 334\n",
      "Loss of train set: 0.40547001361846924 at epoch: 10 and batch_num: 335\n",
      "Loss of train set: 0.3139720857143402 at epoch: 10 and batch_num: 336\n",
      "Loss of train set: 0.24548321962356567 at epoch: 10 and batch_num: 337\n",
      "Loss of train set: 0.42732861638069153 at epoch: 10 and batch_num: 338\n",
      "Loss of train set: 0.22038349509239197 at epoch: 10 and batch_num: 339\n",
      "Loss of train set: 0.3155108392238617 at epoch: 10 and batch_num: 340\n",
      "Loss of train set: 0.2935229539871216 at epoch: 10 and batch_num: 341\n",
      "Loss of train set: 0.35826221108436584 at epoch: 10 and batch_num: 342\n",
      "Loss of train set: 0.4168105721473694 at epoch: 10 and batch_num: 343\n",
      "Loss of train set: 0.22238551080226898 at epoch: 10 and batch_num: 344\n",
      "Loss of train set: 0.26137423515319824 at epoch: 10 and batch_num: 345\n",
      "Loss of train set: 0.38403862714767456 at epoch: 10 and batch_num: 346\n",
      "Loss of train set: 0.4468601644039154 at epoch: 10 and batch_num: 347\n",
      "Loss of train set: 0.3064306378364563 at epoch: 10 and batch_num: 348\n",
      "Loss of train set: 0.5037410259246826 at epoch: 10 and batch_num: 349\n",
      "Loss of train set: 0.39225244522094727 at epoch: 10 and batch_num: 350\n",
      "Loss of train set: 0.2884497046470642 at epoch: 10 and batch_num: 351\n",
      "Loss of train set: 0.2491406500339508 at epoch: 10 and batch_num: 352\n",
      "Loss of train set: 0.3268653154373169 at epoch: 10 and batch_num: 353\n",
      "Loss of train set: 0.34276124835014343 at epoch: 10 and batch_num: 354\n",
      "Loss of train set: 0.21554934978485107 at epoch: 10 and batch_num: 355\n",
      "Loss of train set: 0.27426877617836 at epoch: 10 and batch_num: 356\n",
      "Loss of train set: 0.3747766315937042 at epoch: 10 and batch_num: 357\n",
      "Loss of train set: 0.30577290058135986 at epoch: 10 and batch_num: 358\n",
      "Loss of train set: 0.1905907243490219 at epoch: 10 and batch_num: 359\n",
      "Loss of train set: 0.20554959774017334 at epoch: 10 and batch_num: 360\n",
      "Loss of train set: 0.3962338864803314 at epoch: 10 and batch_num: 361\n",
      "Loss of train set: 0.17076195776462555 at epoch: 10 and batch_num: 362\n",
      "Loss of train set: 0.24790462851524353 at epoch: 10 and batch_num: 363\n",
      "Loss of train set: 0.343919962644577 at epoch: 10 and batch_num: 364\n",
      "Loss of train set: 0.25686919689178467 at epoch: 10 and batch_num: 365\n",
      "Loss of train set: 0.309415340423584 at epoch: 10 and batch_num: 366\n",
      "Loss of train set: 0.35123172402381897 at epoch: 10 and batch_num: 367\n",
      "Loss of train set: 0.4851418733596802 at epoch: 10 and batch_num: 368\n",
      "Loss of train set: 0.4192746877670288 at epoch: 10 and batch_num: 369\n",
      "Loss of train set: 0.4281390309333801 at epoch: 10 and batch_num: 370\n",
      "Loss of train set: 0.25355252623558044 at epoch: 10 and batch_num: 371\n",
      "Loss of train set: 0.15065240859985352 at epoch: 10 and batch_num: 372\n",
      "Loss of train set: 0.32523712515830994 at epoch: 10 and batch_num: 373\n",
      "Loss of train set: 0.21520081162452698 at epoch: 10 and batch_num: 374\n",
      "Loss of train set: 0.12411312758922577 at epoch: 10 and batch_num: 375\n",
      "Loss of train set: 0.36415886878967285 at epoch: 10 and batch_num: 376\n",
      "Loss of train set: 0.19220399856567383 at epoch: 10 and batch_num: 377\n",
      "Loss of train set: 0.2918315827846527 at epoch: 10 and batch_num: 378\n",
      "Loss of train set: 0.27270033955574036 at epoch: 10 and batch_num: 379\n",
      "Loss of train set: 0.42264124751091003 at epoch: 10 and batch_num: 380\n",
      "Loss of train set: 0.38327109813690186 at epoch: 10 and batch_num: 381\n",
      "Loss of train set: 0.36230507493019104 at epoch: 10 and batch_num: 382\n",
      "Loss of train set: 0.29615485668182373 at epoch: 10 and batch_num: 383\n",
      "Loss of train set: 0.3102382719516754 at epoch: 10 and batch_num: 384\n",
      "Loss of train set: 0.21508345007896423 at epoch: 10 and batch_num: 385\n",
      "Loss of train set: 0.339348703622818 at epoch: 10 and batch_num: 386\n",
      "Loss of train set: 0.3114405870437622 at epoch: 10 and batch_num: 387\n",
      "Loss of train set: 0.2901424765586853 at epoch: 10 and batch_num: 388\n",
      "Loss of train set: 0.3046729266643524 at epoch: 10 and batch_num: 389\n",
      "Loss of train set: 0.2377130687236786 at epoch: 10 and batch_num: 390\n",
      "Loss of train set: 0.26774707436561584 at epoch: 10 and batch_num: 391\n",
      "Loss of train set: 0.36223164200782776 at epoch: 10 and batch_num: 392\n",
      "Loss of train set: 0.38763242959976196 at epoch: 10 and batch_num: 393\n",
      "Loss of train set: 0.44265830516815186 at epoch: 10 and batch_num: 394\n",
      "Loss of train set: 0.26432979106903076 at epoch: 10 and batch_num: 395\n",
      "Loss of train set: 0.39415833353996277 at epoch: 10 and batch_num: 396\n",
      "Loss of train set: 0.254999041557312 at epoch: 10 and batch_num: 397\n",
      "Loss of train set: 0.32691308856010437 at epoch: 10 and batch_num: 398\n",
      "Loss of train set: 0.18292337656021118 at epoch: 10 and batch_num: 399\n",
      "Loss of train set: 0.411571741104126 at epoch: 10 and batch_num: 400\n",
      "Loss of train set: 0.3483582139015198 at epoch: 10 and batch_num: 401\n",
      "Loss of train set: 0.20989957451820374 at epoch: 10 and batch_num: 402\n",
      "Loss of train set: 0.2596512734889984 at epoch: 10 and batch_num: 403\n",
      "Loss of train set: 0.24516084790229797 at epoch: 10 and batch_num: 404\n",
      "Loss of train set: 0.4646873474121094 at epoch: 10 and batch_num: 405\n",
      "Loss of train set: 0.20684760808944702 at epoch: 10 and batch_num: 406\n",
      "Loss of train set: 0.12491306662559509 at epoch: 10 and batch_num: 407\n",
      "Loss of train set: 0.25624075531959534 at epoch: 10 and batch_num: 408\n",
      "Loss of train set: 0.3593031167984009 at epoch: 10 and batch_num: 409\n",
      "Loss of train set: 0.5536174774169922 at epoch: 10 and batch_num: 410\n",
      "Loss of train set: 0.23625098168849945 at epoch: 10 and batch_num: 411\n",
      "Loss of train set: 0.14383281767368317 at epoch: 10 and batch_num: 412\n",
      "Loss of train set: 0.3026827573776245 at epoch: 10 and batch_num: 413\n",
      "Loss of train set: 0.35912084579467773 at epoch: 10 and batch_num: 414\n",
      "Loss of train set: 0.2538681626319885 at epoch: 10 and batch_num: 415\n",
      "Loss of train set: 0.4353843629360199 at epoch: 10 and batch_num: 416\n",
      "Loss of train set: 0.32867348194122314 at epoch: 10 and batch_num: 417\n",
      "Loss of train set: 0.3104516863822937 at epoch: 10 and batch_num: 418\n",
      "Loss of train set: 0.37124013900756836 at epoch: 10 and batch_num: 419\n",
      "Loss of train set: 0.28870126605033875 at epoch: 10 and batch_num: 420\n",
      "Loss of train set: 0.6715645790100098 at epoch: 10 and batch_num: 421\n",
      "Loss of train set: 0.4275233745574951 at epoch: 10 and batch_num: 422\n",
      "Loss of train set: 0.45820340514183044 at epoch: 10 and batch_num: 423\n",
      "Loss of train set: 0.3375442326068878 at epoch: 10 and batch_num: 424\n",
      "Loss of train set: 0.45492684841156006 at epoch: 10 and batch_num: 425\n",
      "Loss of train set: 0.3574260473251343 at epoch: 10 and batch_num: 426\n",
      "Loss of train set: 0.421932578086853 at epoch: 10 and batch_num: 427\n",
      "Loss of train set: 0.3951059579849243 at epoch: 10 and batch_num: 428\n",
      "Loss of train set: 0.3402460217475891 at epoch: 10 and batch_num: 429\n",
      "Loss of train set: 0.32286012172698975 at epoch: 10 and batch_num: 430\n",
      "Loss of train set: 0.48903709650039673 at epoch: 10 and batch_num: 431\n",
      "Loss of train set: 0.23562201857566833 at epoch: 10 and batch_num: 432\n",
      "Loss of train set: 0.18207073211669922 at epoch: 10 and batch_num: 433\n",
      "Loss of train set: 0.4705302119255066 at epoch: 10 and batch_num: 434\n",
      "Loss of train set: 0.49059706926345825 at epoch: 10 and batch_num: 435\n",
      "Loss of train set: 0.31022149324417114 at epoch: 10 and batch_num: 436\n",
      "Loss of train set: 0.26250314712524414 at epoch: 10 and batch_num: 437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.38011521100997925 at epoch: 10 and batch_num: 438\n",
      "Loss of train set: 0.3138793408870697 at epoch: 10 and batch_num: 439\n",
      "Loss of train set: 0.38203102350234985 at epoch: 10 and batch_num: 440\n",
      "Loss of train set: 0.5192593336105347 at epoch: 10 and batch_num: 441\n",
      "Loss of train set: 0.39843326807022095 at epoch: 10 and batch_num: 442\n",
      "Loss of train set: 0.49131354689598083 at epoch: 10 and batch_num: 443\n",
      "Loss of train set: 0.2315545380115509 at epoch: 10 and batch_num: 444\n",
      "Loss of train set: 0.5164190530776978 at epoch: 10 and batch_num: 445\n",
      "Loss of train set: 0.2828017473220825 at epoch: 10 and batch_num: 446\n",
      "Loss of train set: 0.4516682028770447 at epoch: 10 and batch_num: 447\n",
      "Loss of train set: 0.26433315873146057 at epoch: 10 and batch_num: 448\n",
      "Loss of train set: 0.36792123317718506 at epoch: 10 and batch_num: 449\n",
      "Loss of train set: 0.30200907588005066 at epoch: 10 and batch_num: 450\n",
      "Loss of train set: 0.26029279828071594 at epoch: 10 and batch_num: 451\n",
      "Loss of train set: 0.26548871397972107 at epoch: 10 and batch_num: 452\n",
      "Loss of train set: 0.61699378490448 at epoch: 10 and batch_num: 453\n",
      "Loss of train set: 0.220984548330307 at epoch: 10 and batch_num: 454\n",
      "Loss of train set: 0.3763437867164612 at epoch: 10 and batch_num: 455\n",
      "Loss of train set: 0.36768990755081177 at epoch: 10 and batch_num: 456\n",
      "Loss of train set: 0.31762057542800903 at epoch: 10 and batch_num: 457\n",
      "Loss of train set: 0.24672743678092957 at epoch: 10 and batch_num: 458\n",
      "Loss of train set: 0.3197067975997925 at epoch: 10 and batch_num: 459\n",
      "Loss of train set: 0.1602194905281067 at epoch: 10 and batch_num: 460\n",
      "Loss of train set: 0.38773512840270996 at epoch: 10 and batch_num: 461\n",
      "Loss of train set: 0.37440016865730286 at epoch: 10 and batch_num: 462\n",
      "Loss of train set: 0.4651758670806885 at epoch: 10 and batch_num: 463\n",
      "Loss of train set: 0.36764663457870483 at epoch: 10 and batch_num: 464\n",
      "Loss of train set: 0.3421522080898285 at epoch: 10 and batch_num: 465\n",
      "Loss of train set: 0.3083004355430603 at epoch: 10 and batch_num: 466\n",
      "Loss of train set: 0.3698467016220093 at epoch: 10 and batch_num: 467\n",
      "Loss of train set: 0.14377574622631073 at epoch: 10 and batch_num: 468\n",
      "Loss of train set: 0.20758351683616638 at epoch: 10 and batch_num: 469\n",
      "Loss of train set: 0.3386569321155548 at epoch: 10 and batch_num: 470\n",
      "Loss of train set: 0.3942762017250061 at epoch: 10 and batch_num: 471\n",
      "Loss of train set: 0.2932578921318054 at epoch: 10 and batch_num: 472\n",
      "Loss of train set: 0.23794399201869965 at epoch: 10 and batch_num: 473\n",
      "Loss of train set: 0.39468082785606384 at epoch: 10 and batch_num: 474\n",
      "Loss of train set: 0.1562098264694214 at epoch: 10 and batch_num: 475\n",
      "Loss of train set: 0.1885407269001007 at epoch: 10 and batch_num: 476\n",
      "Loss of train set: 0.18771465122699738 at epoch: 10 and batch_num: 477\n",
      "Loss of train set: 0.3756411671638489 at epoch: 10 and batch_num: 478\n",
      "Loss of train set: 0.4353950023651123 at epoch: 10 and batch_num: 479\n",
      "Loss of train set: 0.40526437759399414 at epoch: 10 and batch_num: 480\n",
      "Loss of train set: 0.31740763783454895 at epoch: 10 and batch_num: 481\n",
      "Loss of train set: 0.31350746750831604 at epoch: 10 and batch_num: 482\n",
      "Loss of train set: 0.4519411623477936 at epoch: 10 and batch_num: 483\n",
      "Loss of train set: 0.27287760376930237 at epoch: 10 and batch_num: 484\n",
      "Loss of train set: 0.18378520011901855 at epoch: 10 and batch_num: 485\n",
      "Loss of train set: 0.2827104330062866 at epoch: 10 and batch_num: 486\n",
      "Loss of train set: 0.39368316531181335 at epoch: 10 and batch_num: 487\n",
      "Loss of train set: 0.3739193081855774 at epoch: 10 and batch_num: 488\n",
      "Loss of train set: 0.3375503420829773 at epoch: 10 and batch_num: 489\n",
      "Loss of train set: 0.4190087914466858 at epoch: 10 and batch_num: 490\n",
      "Loss of train set: 0.31033855676651 at epoch: 10 and batch_num: 491\n",
      "Loss of train set: 0.27337270975112915 at epoch: 10 and batch_num: 492\n",
      "Loss of train set: 0.2629586458206177 at epoch: 10 and batch_num: 493\n",
      "Loss of train set: 0.2724660634994507 at epoch: 10 and batch_num: 494\n",
      "Loss of train set: 0.2068336009979248 at epoch: 10 and batch_num: 495\n",
      "Loss of train set: 0.3508015275001526 at epoch: 10 and batch_num: 496\n",
      "Loss of train set: 0.39415329694747925 at epoch: 10 and batch_num: 497\n",
      "Loss of train set: 0.2843917906284332 at epoch: 10 and batch_num: 498\n",
      "Loss of train set: 0.3272240161895752 at epoch: 10 and batch_num: 499\n",
      "Loss of train set: 0.31797564029693604 at epoch: 10 and batch_num: 500\n",
      "Loss of train set: 0.3571910858154297 at epoch: 10 and batch_num: 501\n",
      "Loss of train set: 0.2837141156196594 at epoch: 10 and batch_num: 502\n",
      "Loss of train set: 0.3120865523815155 at epoch: 10 and batch_num: 503\n",
      "Loss of train set: 0.3269874155521393 at epoch: 10 and batch_num: 504\n",
      "Loss of train set: 0.20986095070838928 at epoch: 10 and batch_num: 505\n",
      "Loss of train set: 0.315496027469635 at epoch: 10 and batch_num: 506\n",
      "Loss of train set: 0.4220418930053711 at epoch: 10 and batch_num: 507\n",
      "Loss of train set: 0.39750730991363525 at epoch: 10 and batch_num: 508\n",
      "Loss of train set: 0.21857509016990662 at epoch: 10 and batch_num: 509\n",
      "Loss of train set: 0.36404484510421753 at epoch: 10 and batch_num: 510\n",
      "Loss of train set: 0.23130059242248535 at epoch: 10 and batch_num: 511\n",
      "Loss of train set: 0.4097287952899933 at epoch: 10 and batch_num: 512\n",
      "Loss of train set: 0.42248275876045227 at epoch: 10 and batch_num: 513\n",
      "Loss of train set: 0.2610858082771301 at epoch: 10 and batch_num: 514\n",
      "Loss of train set: 0.15260466933250427 at epoch: 10 and batch_num: 515\n",
      "Loss of train set: 0.22530099749565125 at epoch: 10 and batch_num: 516\n",
      "Loss of train set: 0.22192728519439697 at epoch: 10 and batch_num: 517\n",
      "Loss of train set: 0.33508166670799255 at epoch: 10 and batch_num: 518\n",
      "Loss of train set: 0.22902776300907135 at epoch: 10 and batch_num: 519\n",
      "Loss of train set: 0.37389373779296875 at epoch: 10 and batch_num: 520\n",
      "Loss of train set: 0.3698568344116211 at epoch: 10 and batch_num: 521\n",
      "Loss of train set: 0.2648113965988159 at epoch: 10 and batch_num: 522\n",
      "Loss of train set: 0.24789631366729736 at epoch: 10 and batch_num: 523\n",
      "Loss of train set: 0.6153672933578491 at epoch: 10 and batch_num: 524\n",
      "Loss of train set: 0.337064266204834 at epoch: 10 and batch_num: 525\n",
      "Loss of train set: 0.236522376537323 at epoch: 10 and batch_num: 526\n",
      "Loss of train set: 0.35739246010780334 at epoch: 10 and batch_num: 527\n",
      "Loss of train set: 0.2104601114988327 at epoch: 10 and batch_num: 528\n",
      "Loss of train set: 0.30075719952583313 at epoch: 10 and batch_num: 529\n",
      "Loss of train set: 0.27794378995895386 at epoch: 10 and batch_num: 530\n",
      "Loss of train set: 0.24433016777038574 at epoch: 10 and batch_num: 531\n",
      "Loss of train set: 0.36010611057281494 at epoch: 10 and batch_num: 532\n",
      "Loss of train set: 0.4310201108455658 at epoch: 10 and batch_num: 533\n",
      "Loss of train set: 0.2507265508174896 at epoch: 10 and batch_num: 534\n",
      "Loss of train set: 0.19614040851593018 at epoch: 10 and batch_num: 535\n",
      "Loss of train set: 0.21172063052654266 at epoch: 10 and batch_num: 536\n",
      "Loss of train set: 0.6984121799468994 at epoch: 10 and batch_num: 537\n",
      "Loss of train set: 0.5936837196350098 at epoch: 10 and batch_num: 538\n",
      "Loss of train set: 0.37092429399490356 at epoch: 10 and batch_num: 539\n",
      "Loss of train set: 0.38309335708618164 at epoch: 10 and batch_num: 540\n",
      "Loss of train set: 0.27813154458999634 at epoch: 10 and batch_num: 541\n",
      "Loss of train set: 0.3529956340789795 at epoch: 10 and batch_num: 542\n",
      "Loss of train set: 0.1727290153503418 at epoch: 10 and batch_num: 543\n",
      "Loss of train set: 0.34644412994384766 at epoch: 10 and batch_num: 544\n",
      "Loss of train set: 0.25875869393348694 at epoch: 10 and batch_num: 545\n",
      "Loss of train set: 0.3284739851951599 at epoch: 10 and batch_num: 546\n",
      "Loss of train set: 0.3422084152698517 at epoch: 10 and batch_num: 547\n",
      "Loss of train set: 0.41245129704475403 at epoch: 10 and batch_num: 548\n",
      "Loss of train set: 0.2633873224258423 at epoch: 10 and batch_num: 549\n",
      "Loss of train set: 0.22844760119915009 at epoch: 10 and batch_num: 550\n",
      "Loss of train set: 0.3324419856071472 at epoch: 10 and batch_num: 551\n",
      "Loss of train set: 0.3693864047527313 at epoch: 10 and batch_num: 552\n",
      "Loss of train set: 0.37468454241752625 at epoch: 10 and batch_num: 553\n",
      "Loss of train set: 0.31535249948501587 at epoch: 10 and batch_num: 554\n",
      "Loss of train set: 0.515739381313324 at epoch: 10 and batch_num: 555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.22816379368305206 at epoch: 10 and batch_num: 556\n",
      "Loss of train set: 0.2710094749927521 at epoch: 10 and batch_num: 557\n",
      "Loss of train set: 0.20184950530529022 at epoch: 10 and batch_num: 558\n",
      "Loss of train set: 0.38319867849349976 at epoch: 10 and batch_num: 559\n",
      "Loss of train set: 0.3105809688568115 at epoch: 10 and batch_num: 560\n",
      "Loss of train set: 0.3079914450645447 at epoch: 10 and batch_num: 561\n",
      "Loss of train set: 0.2486289143562317 at epoch: 10 and batch_num: 562\n",
      "Loss of train set: 0.2596927285194397 at epoch: 10 and batch_num: 563\n",
      "Loss of train set: 0.1660401076078415 at epoch: 10 and batch_num: 564\n",
      "Loss of train set: 0.2928169369697571 at epoch: 10 and batch_num: 565\n",
      "Loss of train set: 0.2423928678035736 at epoch: 10 and batch_num: 566\n",
      "Loss of train set: 0.24408185482025146 at epoch: 10 and batch_num: 567\n",
      "Loss of train set: 0.28000152111053467 at epoch: 10 and batch_num: 568\n",
      "Loss of train set: 0.4289715886116028 at epoch: 10 and batch_num: 569\n",
      "Loss of train set: 0.38333413004875183 at epoch: 10 and batch_num: 570\n",
      "Loss of train set: 0.4162016808986664 at epoch: 10 and batch_num: 571\n",
      "Loss of train set: 0.3260580599308014 at epoch: 10 and batch_num: 572\n",
      "Loss of train set: 0.38948994874954224 at epoch: 10 and batch_num: 573\n",
      "Loss of train set: 0.20686396956443787 at epoch: 10 and batch_num: 574\n",
      "Loss of train set: 0.2343451976776123 at epoch: 10 and batch_num: 575\n",
      "Loss of train set: 0.4185759127140045 at epoch: 10 and batch_num: 576\n",
      "Loss of train set: 0.2079188972711563 at epoch: 10 and batch_num: 577\n",
      "Loss of train set: 0.38746771216392517 at epoch: 10 and batch_num: 578\n",
      "Loss of train set: 0.3867815434932709 at epoch: 10 and batch_num: 579\n",
      "Loss of train set: 0.41655492782592773 at epoch: 10 and batch_num: 580\n",
      "Loss of train set: 0.41745123267173767 at epoch: 10 and batch_num: 581\n",
      "Loss of train set: 0.4393509328365326 at epoch: 10 and batch_num: 582\n",
      "Loss of train set: 0.22810615599155426 at epoch: 10 and batch_num: 583\n",
      "Loss of train set: 0.3484534025192261 at epoch: 10 and batch_num: 584\n",
      "Loss of train set: 0.2782418429851532 at epoch: 10 and batch_num: 585\n",
      "Loss of train set: 0.4387362003326416 at epoch: 10 and batch_num: 586\n",
      "Loss of train set: 0.4600617587566376 at epoch: 10 and batch_num: 587\n",
      "Loss of train set: 0.199335515499115 at epoch: 10 and batch_num: 588\n",
      "Loss of train set: 0.3572924733161926 at epoch: 10 and batch_num: 589\n",
      "Loss of train set: 0.3382284641265869 at epoch: 10 and batch_num: 590\n",
      "Loss of train set: 0.3537413775920868 at epoch: 10 and batch_num: 591\n",
      "Loss of train set: 0.29742446541786194 at epoch: 10 and batch_num: 592\n",
      "Loss of train set: 0.25012534856796265 at epoch: 10 and batch_num: 593\n",
      "Loss of train set: 0.21162891387939453 at epoch: 10 and batch_num: 594\n",
      "Loss of train set: 0.5196571946144104 at epoch: 10 and batch_num: 595\n",
      "Loss of train set: 0.24785910546779633 at epoch: 10 and batch_num: 596\n",
      "Loss of train set: 0.19361871480941772 at epoch: 10 and batch_num: 597\n",
      "Loss of train set: 0.32352718710899353 at epoch: 10 and batch_num: 598\n",
      "Loss of train set: 0.3500695526599884 at epoch: 10 and batch_num: 599\n",
      "Loss of train set: 0.2333039939403534 at epoch: 10 and batch_num: 600\n",
      "Loss of train set: 0.31853777170181274 at epoch: 10 and batch_num: 601\n",
      "Loss of train set: 0.2776349186897278 at epoch: 10 and batch_num: 602\n",
      "Loss of train set: 0.3460255563259125 at epoch: 10 and batch_num: 603\n",
      "Loss of train set: 0.25569790601730347 at epoch: 10 and batch_num: 604\n",
      "Loss of train set: 0.45354902744293213 at epoch: 10 and batch_num: 605\n",
      "Loss of train set: 0.32499048113822937 at epoch: 10 and batch_num: 606\n",
      "Loss of train set: 0.2994152009487152 at epoch: 10 and batch_num: 607\n",
      "Loss of train set: 0.2066880166530609 at epoch: 10 and batch_num: 608\n",
      "Loss of train set: 0.3358626961708069 at epoch: 10 and batch_num: 609\n",
      "Loss of train set: 0.3015844225883484 at epoch: 10 and batch_num: 610\n",
      "Loss of train set: 0.33784422278404236 at epoch: 10 and batch_num: 611\n",
      "Loss of train set: 0.4844146966934204 at epoch: 10 and batch_num: 612\n",
      "Loss of train set: 0.5353597402572632 at epoch: 10 and batch_num: 613\n",
      "Loss of train set: 0.3691368103027344 at epoch: 10 and batch_num: 614\n",
      "Loss of train set: 0.311618834733963 at epoch: 10 and batch_num: 615\n",
      "Loss of train set: 0.2075364887714386 at epoch: 10 and batch_num: 616\n",
      "Loss of train set: 0.461225688457489 at epoch: 10 and batch_num: 617\n",
      "Loss of train set: 0.3178941309452057 at epoch: 10 and batch_num: 618\n",
      "Loss of train set: 0.24355976283550262 at epoch: 10 and batch_num: 619\n",
      "Loss of train set: 0.28795483708381653 at epoch: 10 and batch_num: 620\n",
      "Loss of train set: 0.302251935005188 at epoch: 10 and batch_num: 621\n",
      "Loss of train set: 0.38207298517227173 at epoch: 10 and batch_num: 622\n",
      "Loss of train set: 0.3049389123916626 at epoch: 10 and batch_num: 623\n",
      "Loss of train set: 0.2822394371032715 at epoch: 10 and batch_num: 624\n",
      "Loss of train set: 0.3860820531845093 at epoch: 10 and batch_num: 625\n",
      "Loss of train set: 0.4575120508670807 at epoch: 10 and batch_num: 626\n",
      "Loss of train set: 0.31848645210266113 at epoch: 10 and batch_num: 627\n",
      "Loss of train set: 0.41253864765167236 at epoch: 10 and batch_num: 628\n",
      "Loss of train set: 0.36748838424682617 at epoch: 10 and batch_num: 629\n",
      "Loss of train set: 0.3772070109844208 at epoch: 10 and batch_num: 630\n",
      "Loss of train set: 0.2877848148345947 at epoch: 10 and batch_num: 631\n",
      "Loss of train set: 0.36757272481918335 at epoch: 10 and batch_num: 632\n",
      "Loss of train set: 0.41265422105789185 at epoch: 10 and batch_num: 633\n",
      "Loss of train set: 0.5677649974822998 at epoch: 10 and batch_num: 634\n",
      "Loss of train set: 0.3237239122390747 at epoch: 10 and batch_num: 635\n",
      "Loss of train set: 0.20402175188064575 at epoch: 10 and batch_num: 636\n",
      "Loss of train set: 0.4900529086589813 at epoch: 10 and batch_num: 637\n",
      "Loss of train set: 0.2891067862510681 at epoch: 10 and batch_num: 638\n",
      "Loss of train set: 0.41538330912590027 at epoch: 10 and batch_num: 639\n",
      "Loss of train set: 0.25234389305114746 at epoch: 10 and batch_num: 640\n",
      "Loss of train set: 0.44034016132354736 at epoch: 10 and batch_num: 641\n",
      "Loss of train set: 0.2521328926086426 at epoch: 10 and batch_num: 642\n",
      "Loss of train set: 0.28058573603630066 at epoch: 10 and batch_num: 643\n",
      "Loss of train set: 0.2869569659233093 at epoch: 10 and batch_num: 644\n",
      "Loss of train set: 0.30105358362197876 at epoch: 10 and batch_num: 645\n",
      "Loss of train set: 0.18455949425697327 at epoch: 10 and batch_num: 646\n",
      "Loss of train set: 0.7185566425323486 at epoch: 10 and batch_num: 647\n",
      "Loss of train set: 0.28792184591293335 at epoch: 10 and batch_num: 648\n",
      "Loss of train set: 0.20697905123233795 at epoch: 10 and batch_num: 649\n",
      "Loss of train set: 0.20402732491493225 at epoch: 10 and batch_num: 650\n",
      "Loss of train set: 0.4038441777229309 at epoch: 10 and batch_num: 651\n",
      "Loss of train set: 0.2884889245033264 at epoch: 10 and batch_num: 652\n",
      "Loss of train set: 0.21504239737987518 at epoch: 10 and batch_num: 653\n",
      "Loss of train set: 0.24499186873435974 at epoch: 10 and batch_num: 654\n",
      "Loss of train set: 0.3494775891304016 at epoch: 10 and batch_num: 655\n",
      "Loss of train set: 0.4812605381011963 at epoch: 10 and batch_num: 656\n",
      "Loss of train set: 0.28284549713134766 at epoch: 10 and batch_num: 657\n",
      "Loss of train set: 0.4596085548400879 at epoch: 10 and batch_num: 658\n",
      "Loss of train set: 0.3965006470680237 at epoch: 10 and batch_num: 659\n",
      "Loss of train set: 0.29971587657928467 at epoch: 10 and batch_num: 660\n",
      "Loss of train set: 0.4263027608394623 at epoch: 10 and batch_num: 661\n",
      "Loss of train set: 0.23004953563213348 at epoch: 10 and batch_num: 662\n",
      "Loss of train set: 0.294335275888443 at epoch: 10 and batch_num: 663\n",
      "Loss of train set: 0.34588101506233215 at epoch: 10 and batch_num: 664\n",
      "Loss of train set: 0.2947039306163788 at epoch: 10 and batch_num: 665\n",
      "Loss of train set: 0.6449692249298096 at epoch: 10 and batch_num: 666\n",
      "Loss of train set: 0.4590269923210144 at epoch: 10 and batch_num: 667\n",
      "Loss of train set: 0.4754374623298645 at epoch: 10 and batch_num: 668\n",
      "Loss of train set: 0.3206331729888916 at epoch: 10 and batch_num: 669\n",
      "Loss of train set: 0.24594944715499878 at epoch: 10 and batch_num: 670\n",
      "Loss of train set: 0.26268306374549866 at epoch: 10 and batch_num: 671\n",
      "Loss of train set: 0.3076632618904114 at epoch: 10 and batch_num: 672\n",
      "Loss of train set: 0.26755422353744507 at epoch: 10 and batch_num: 673\n",
      "Loss of train set: 0.2561016380786896 at epoch: 10 and batch_num: 674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.37363201379776 at epoch: 10 and batch_num: 675\n",
      "Loss of train set: 0.2787538170814514 at epoch: 10 and batch_num: 676\n",
      "Loss of train set: 0.28695017099380493 at epoch: 10 and batch_num: 677\n",
      "Loss of train set: 0.18395337462425232 at epoch: 10 and batch_num: 678\n",
      "Loss of train set: 0.2706001102924347 at epoch: 10 and batch_num: 679\n",
      "Loss of train set: 0.19124585390090942 at epoch: 10 and batch_num: 680\n",
      "Loss of train set: 0.2258034646511078 at epoch: 10 and batch_num: 681\n",
      "Loss of train set: 0.2603294253349304 at epoch: 10 and batch_num: 682\n",
      "Loss of train set: 0.3382120132446289 at epoch: 10 and batch_num: 683\n",
      "Loss of train set: 0.12550131976604462 at epoch: 10 and batch_num: 684\n",
      "Loss of train set: 0.20978480577468872 at epoch: 10 and batch_num: 685\n",
      "Loss of train set: 0.2853356599807739 at epoch: 10 and batch_num: 686\n",
      "Loss of train set: 0.29525384306907654 at epoch: 10 and batch_num: 687\n",
      "Loss of train set: 0.2003290355205536 at epoch: 10 and batch_num: 688\n",
      "Loss of train set: 0.2621900141239166 at epoch: 10 and batch_num: 689\n",
      "Loss of train set: 0.2828458845615387 at epoch: 10 and batch_num: 690\n",
      "Loss of train set: 0.252841055393219 at epoch: 10 and batch_num: 691\n",
      "Loss of train set: 0.28620511293411255 at epoch: 10 and batch_num: 692\n",
      "Loss of train set: 0.24124856293201447 at epoch: 10 and batch_num: 693\n",
      "Loss of train set: 0.39234617352485657 at epoch: 10 and batch_num: 694\n",
      "Loss of train set: 0.19607095420360565 at epoch: 10 and batch_num: 695\n",
      "Loss of train set: 0.42994454503059387 at epoch: 10 and batch_num: 696\n",
      "Loss of train set: 0.2669875919818878 at epoch: 10 and batch_num: 697\n",
      "Loss of train set: 0.3193368911743164 at epoch: 10 and batch_num: 698\n",
      "Loss of train set: 0.28038647770881653 at epoch: 10 and batch_num: 699\n",
      "Loss of train set: 0.26364246010780334 at epoch: 10 and batch_num: 700\n",
      "Loss of train set: 0.26457422971725464 at epoch: 10 and batch_num: 701\n",
      "Loss of train set: 0.2770991623401642 at epoch: 10 and batch_num: 702\n",
      "Loss of train set: 0.3218742311000824 at epoch: 10 and batch_num: 703\n",
      "Loss of train set: 0.3831474184989929 at epoch: 10 and batch_num: 704\n",
      "Loss of train set: 0.23426854610443115 at epoch: 10 and batch_num: 705\n",
      "Loss of train set: 0.28695225715637207 at epoch: 10 and batch_num: 706\n",
      "Loss of train set: 0.33395200967788696 at epoch: 10 and batch_num: 707\n",
      "Loss of train set: 0.37671756744384766 at epoch: 10 and batch_num: 708\n",
      "Loss of train set: 0.4153248369693756 at epoch: 10 and batch_num: 709\n",
      "Loss of train set: 0.3015938103199005 at epoch: 10 and batch_num: 710\n",
      "Loss of train set: 0.2921740710735321 at epoch: 10 and batch_num: 711\n",
      "Loss of train set: 0.1961512714624405 at epoch: 10 and batch_num: 712\n",
      "Loss of train set: 0.28646934032440186 at epoch: 10 and batch_num: 713\n",
      "Loss of train set: 0.36996203660964966 at epoch: 10 and batch_num: 714\n",
      "Loss of train set: 0.2534328103065491 at epoch: 10 and batch_num: 715\n",
      "Loss of train set: 0.23304904997348785 at epoch: 10 and batch_num: 716\n",
      "Loss of train set: 0.2456214874982834 at epoch: 10 and batch_num: 717\n",
      "Loss of train set: 0.4894701838493347 at epoch: 10 and batch_num: 718\n",
      "Loss of train set: 0.24760763347148895 at epoch: 10 and batch_num: 719\n",
      "Loss of train set: 0.21987690031528473 at epoch: 10 and batch_num: 720\n",
      "Loss of train set: 0.2481972873210907 at epoch: 10 and batch_num: 721\n",
      "Loss of train set: 0.3303680419921875 at epoch: 10 and batch_num: 722\n",
      "Loss of train set: 0.22188714146614075 at epoch: 10 and batch_num: 723\n",
      "Loss of train set: 0.3375150263309479 at epoch: 10 and batch_num: 724\n",
      "Loss of train set: 0.41226378083229065 at epoch: 10 and batch_num: 725\n",
      "Loss of train set: 0.3729894161224365 at epoch: 10 and batch_num: 726\n",
      "Loss of train set: 0.26448145508766174 at epoch: 10 and batch_num: 727\n",
      "Loss of train set: 0.32076767086982727 at epoch: 10 and batch_num: 728\n",
      "Loss of train set: 0.23610875010490417 at epoch: 10 and batch_num: 729\n",
      "Loss of train set: 0.37758129835128784 at epoch: 10 and batch_num: 730\n",
      "Loss of train set: 0.24033582210540771 at epoch: 10 and batch_num: 731\n",
      "Loss of train set: 0.19787171483039856 at epoch: 10 and batch_num: 732\n",
      "Loss of train set: 0.33867067098617554 at epoch: 10 and batch_num: 733\n",
      "Loss of train set: 0.3177047371864319 at epoch: 10 and batch_num: 734\n",
      "Loss of train set: 0.28914594650268555 at epoch: 10 and batch_num: 735\n",
      "Loss of train set: 0.24483151733875275 at epoch: 10 and batch_num: 736\n",
      "Loss of train set: 0.26870793104171753 at epoch: 10 and batch_num: 737\n",
      "Loss of train set: 0.31102442741394043 at epoch: 10 and batch_num: 738\n",
      "Loss of train set: 0.3285045027732849 at epoch: 10 and batch_num: 739\n",
      "Loss of train set: 0.265146404504776 at epoch: 10 and batch_num: 740\n",
      "Loss of train set: 0.3324647545814514 at epoch: 10 and batch_num: 741\n",
      "Loss of train set: 0.43849265575408936 at epoch: 10 and batch_num: 742\n",
      "Loss of train set: 0.2516736686229706 at epoch: 10 and batch_num: 743\n",
      "Loss of train set: 0.42000284790992737 at epoch: 10 and batch_num: 744\n",
      "Loss of train set: 0.38791000843048096 at epoch: 10 and batch_num: 745\n",
      "Loss of train set: 0.3944784700870514 at epoch: 10 and batch_num: 746\n",
      "Loss of train set: 0.37666764855384827 at epoch: 10 and batch_num: 747\n",
      "Loss of train set: 0.39385175704956055 at epoch: 10 and batch_num: 748\n",
      "Loss of train set: 0.20682013034820557 at epoch: 10 and batch_num: 749\n",
      "Loss of train set: 0.27761396765708923 at epoch: 10 and batch_num: 750\n",
      "Loss of train set: 0.4729563593864441 at epoch: 10 and batch_num: 751\n",
      "Loss of train set: 0.26279085874557495 at epoch: 10 and batch_num: 752\n",
      "Loss of train set: 0.42307913303375244 at epoch: 10 and batch_num: 753\n",
      "Loss of train set: 0.3304193615913391 at epoch: 10 and batch_num: 754\n",
      "Loss of train set: 0.3909578323364258 at epoch: 10 and batch_num: 755\n",
      "Loss of train set: 0.29946276545524597 at epoch: 10 and batch_num: 756\n",
      "Loss of train set: 0.31899964809417725 at epoch: 10 and batch_num: 757\n",
      "Loss of train set: 0.23214030265808105 at epoch: 10 and batch_num: 758\n",
      "Loss of train set: 0.39994221925735474 at epoch: 10 and batch_num: 759\n",
      "Loss of train set: 0.45403704047203064 at epoch: 10 and batch_num: 760\n",
      "Loss of train set: 0.4420197010040283 at epoch: 10 and batch_num: 761\n",
      "Loss of train set: 0.2758217453956604 at epoch: 10 and batch_num: 762\n",
      "Loss of train set: 0.6822755336761475 at epoch: 10 and batch_num: 763\n",
      "Loss of train set: 0.41924479603767395 at epoch: 10 and batch_num: 764\n",
      "Loss of train set: 0.18852195143699646 at epoch: 10 and batch_num: 765\n",
      "Loss of train set: 0.34692347049713135 at epoch: 10 and batch_num: 766\n",
      "Loss of train set: 0.2922598421573639 at epoch: 10 and batch_num: 767\n",
      "Loss of train set: 0.1874745488166809 at epoch: 10 and batch_num: 768\n",
      "Loss of train set: 0.3529624342918396 at epoch: 10 and batch_num: 769\n",
      "Loss of train set: 0.2907552421092987 at epoch: 10 and batch_num: 770\n",
      "Loss of train set: 0.2021699845790863 at epoch: 10 and batch_num: 771\n",
      "Loss of train set: 0.21152648329734802 at epoch: 10 and batch_num: 772\n",
      "Loss of train set: 0.3389780521392822 at epoch: 10 and batch_num: 773\n",
      "Loss of train set: 0.241197407245636 at epoch: 10 and batch_num: 774\n",
      "Loss of train set: 0.43027982115745544 at epoch: 10 and batch_num: 775\n",
      "Loss of train set: 0.5566315650939941 at epoch: 10 and batch_num: 776\n",
      "Loss of train set: 0.17737272381782532 at epoch: 10 and batch_num: 777\n",
      "Loss of train set: 0.31185007095336914 at epoch: 10 and batch_num: 778\n",
      "Loss of train set: 0.30036550760269165 at epoch: 10 and batch_num: 779\n",
      "Loss of train set: 0.22753170132637024 at epoch: 10 and batch_num: 780\n",
      "Loss of train set: 0.2703002095222473 at epoch: 10 and batch_num: 781\n",
      "Loss of train set: 0.09882339090108871 at epoch: 10 and batch_num: 782\n",
      "Loss of train set: 0.3114926218986511 at epoch: 10 and batch_num: 783\n",
      "Loss of train set: 0.3243665397167206 at epoch: 10 and batch_num: 784\n",
      "Loss of train set: 0.45996740460395813 at epoch: 10 and batch_num: 785\n",
      "Loss of train set: 0.3530201315879822 at epoch: 10 and batch_num: 786\n",
      "Loss of train set: 0.3075786828994751 at epoch: 10 and batch_num: 787\n",
      "Loss of train set: 0.3712868094444275 at epoch: 10 and batch_num: 788\n",
      "Loss of train set: 0.34516000747680664 at epoch: 10 and batch_num: 789\n",
      "Loss of train set: 0.35854610800743103 at epoch: 10 and batch_num: 790\n",
      "Loss of train set: 0.47844138741493225 at epoch: 10 and batch_num: 791\n",
      "Loss of train set: 0.3092080354690552 at epoch: 10 and batch_num: 792\n",
      "Loss of train set: 0.19106614589691162 at epoch: 10 and batch_num: 793\n",
      "Loss of train set: 0.5148771405220032 at epoch: 10 and batch_num: 794\n",
      "Loss of train set: 0.39456820487976074 at epoch: 10 and batch_num: 795\n",
      "Loss of train set: 0.31906139850616455 at epoch: 10 and batch_num: 796\n",
      "Loss of train set: 0.46547749638557434 at epoch: 10 and batch_num: 797\n",
      "Loss of train set: 0.30165576934814453 at epoch: 10 and batch_num: 798\n",
      "Loss of train set: 0.34804877638816833 at epoch: 10 and batch_num: 799\n",
      "Loss of train set: 0.3680267333984375 at epoch: 10 and batch_num: 800\n",
      "Loss of train set: 0.34517407417297363 at epoch: 10 and batch_num: 801\n",
      "Loss of train set: 0.2169797718524933 at epoch: 10 and batch_num: 802\n",
      "Loss of train set: 0.3849342465400696 at epoch: 10 and batch_num: 803\n",
      "Loss of train set: 0.5209047794342041 at epoch: 10 and batch_num: 804\n",
      "Loss of train set: 0.27127164602279663 at epoch: 10 and batch_num: 805\n",
      "Loss of train set: 0.2754085063934326 at epoch: 10 and batch_num: 806\n",
      "Loss of train set: 0.32680007815361023 at epoch: 10 and batch_num: 807\n",
      "Loss of train set: 0.1820700466632843 at epoch: 10 and batch_num: 808\n",
      "Loss of train set: 0.3701256215572357 at epoch: 10 and batch_num: 809\n",
      "Loss of train set: 0.2590632438659668 at epoch: 10 and batch_num: 810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.40063607692718506 at epoch: 10 and batch_num: 811\n",
      "Loss of train set: 0.32817888259887695 at epoch: 10 and batch_num: 812\n",
      "Loss of train set: 0.18747150897979736 at epoch: 10 and batch_num: 813\n",
      "Loss of train set: 0.35825449228286743 at epoch: 10 and batch_num: 814\n",
      "Loss of train set: 0.17097140848636627 at epoch: 10 and batch_num: 815\n",
      "Loss of train set: 0.4787539839744568 at epoch: 10 and batch_num: 816\n",
      "Loss of train set: 0.39625072479248047 at epoch: 10 and batch_num: 817\n",
      "Loss of train set: 0.26916569471359253 at epoch: 10 and batch_num: 818\n",
      "Loss of train set: 0.3933667540550232 at epoch: 10 and batch_num: 819\n",
      "Loss of train set: 0.3129952847957611 at epoch: 10 and batch_num: 820\n",
      "Loss of train set: 0.335022509098053 at epoch: 10 and batch_num: 821\n",
      "Loss of train set: 0.2317916303873062 at epoch: 10 and batch_num: 822\n",
      "Loss of train set: 0.2085222601890564 at epoch: 10 and batch_num: 823\n",
      "Loss of train set: 0.589184582233429 at epoch: 10 and batch_num: 824\n",
      "Loss of train set: 0.15136636793613434 at epoch: 10 and batch_num: 825\n",
      "Loss of train set: 0.4083098769187927 at epoch: 10 and batch_num: 826\n",
      "Loss of train set: 0.31254762411117554 at epoch: 10 and batch_num: 827\n",
      "Loss of train set: 0.3504463732242584 at epoch: 10 and batch_num: 828\n",
      "Loss of train set: 0.2814710736274719 at epoch: 10 and batch_num: 829\n",
      "Loss of train set: 0.23978164792060852 at epoch: 10 and batch_num: 830\n",
      "Loss of train set: 0.43743976950645447 at epoch: 10 and batch_num: 831\n",
      "Loss of train set: 0.28718650341033936 at epoch: 10 and batch_num: 832\n",
      "Loss of train set: 0.17421677708625793 at epoch: 10 and batch_num: 833\n",
      "Loss of train set: 0.4628661274909973 at epoch: 10 and batch_num: 834\n",
      "Loss of train set: 0.2825300097465515 at epoch: 10 and batch_num: 835\n",
      "Loss of train set: 0.358849436044693 at epoch: 10 and batch_num: 836\n",
      "Loss of train set: 0.2673400044441223 at epoch: 10 and batch_num: 837\n",
      "Loss of train set: 0.341128408908844 at epoch: 10 and batch_num: 838\n",
      "Loss of train set: 0.3274572491645813 at epoch: 10 and batch_num: 839\n",
      "Loss of train set: 0.1840364784002304 at epoch: 10 and batch_num: 840\n",
      "Loss of train set: 0.2004021406173706 at epoch: 10 and batch_num: 841\n",
      "Loss of train set: 0.3008609712123871 at epoch: 10 and batch_num: 842\n",
      "Loss of train set: 0.28917983174324036 at epoch: 10 and batch_num: 843\n",
      "Loss of train set: 0.3018765151500702 at epoch: 10 and batch_num: 844\n",
      "Loss of train set: 0.42478328943252563 at epoch: 10 and batch_num: 845\n",
      "Loss of train set: 0.5136643648147583 at epoch: 10 and batch_num: 846\n",
      "Loss of train set: 0.35849064588546753 at epoch: 10 and batch_num: 847\n",
      "Loss of train set: 0.38435569405555725 at epoch: 10 and batch_num: 848\n",
      "Loss of train set: 0.18690240383148193 at epoch: 10 and batch_num: 849\n",
      "Loss of train set: 0.32626062631607056 at epoch: 10 and batch_num: 850\n",
      "Loss of train set: 0.37418872117996216 at epoch: 10 and batch_num: 851\n",
      "Loss of train set: 0.30147913098335266 at epoch: 10 and batch_num: 852\n",
      "Loss of train set: 0.30599653720855713 at epoch: 10 and batch_num: 853\n",
      "Loss of train set: 0.24474310874938965 at epoch: 10 and batch_num: 854\n",
      "Loss of train set: 0.31293171644210815 at epoch: 10 and batch_num: 855\n",
      "Loss of train set: 0.424956738948822 at epoch: 10 and batch_num: 856\n",
      "Loss of train set: 0.2827907204627991 at epoch: 10 and batch_num: 857\n",
      "Loss of train set: 0.2326592206954956 at epoch: 10 and batch_num: 858\n",
      "Loss of train set: 0.338046133518219 at epoch: 10 and batch_num: 859\n",
      "Loss of train set: 0.18978086113929749 at epoch: 10 and batch_num: 860\n",
      "Loss of train set: 0.22461077570915222 at epoch: 10 and batch_num: 861\n",
      "Loss of train set: 0.2981104254722595 at epoch: 10 and batch_num: 862\n",
      "Loss of train set: 0.3331109881401062 at epoch: 10 and batch_num: 863\n",
      "Loss of train set: 0.4034532904624939 at epoch: 10 and batch_num: 864\n",
      "Loss of train set: 0.2683674693107605 at epoch: 10 and batch_num: 865\n",
      "Loss of train set: 0.25635409355163574 at epoch: 10 and batch_num: 866\n",
      "Loss of train set: 0.27749767899513245 at epoch: 10 and batch_num: 867\n",
      "Loss of train set: 0.2537801265716553 at epoch: 10 and batch_num: 868\n",
      "Loss of train set: 0.30754536390304565 at epoch: 10 and batch_num: 869\n",
      "Loss of train set: 0.3250444531440735 at epoch: 10 and batch_num: 870\n",
      "Loss of train set: 0.11745209991931915 at epoch: 10 and batch_num: 871\n",
      "Loss of train set: 0.189285010099411 at epoch: 10 and batch_num: 872\n",
      "Loss of train set: 0.35868769884109497 at epoch: 10 and batch_num: 873\n",
      "Loss of train set: 0.38865014910697937 at epoch: 10 and batch_num: 874\n",
      "Loss of train set: 0.2640995383262634 at epoch: 10 and batch_num: 875\n",
      "Loss of train set: 0.2737812399864197 at epoch: 10 and batch_num: 876\n",
      "Loss of train set: 0.4199102222919464 at epoch: 10 and batch_num: 877\n",
      "Loss of train set: 0.15816181898117065 at epoch: 10 and batch_num: 878\n",
      "Loss of train set: 0.23636703193187714 at epoch: 10 and batch_num: 879\n",
      "Loss of train set: 0.48353147506713867 at epoch: 10 and batch_num: 880\n",
      "Loss of train set: 0.25517648458480835 at epoch: 10 and batch_num: 881\n",
      "Loss of train set: 0.23467205464839935 at epoch: 10 and batch_num: 882\n",
      "Loss of train set: 0.45634496212005615 at epoch: 10 and batch_num: 883\n",
      "Loss of train set: 0.38583502173423767 at epoch: 10 and batch_num: 884\n",
      "Loss of train set: 0.27813199162483215 at epoch: 10 and batch_num: 885\n",
      "Loss of train set: 0.42152875661849976 at epoch: 10 and batch_num: 886\n",
      "Loss of train set: 0.23948998749256134 at epoch: 10 and batch_num: 887\n",
      "Loss of train set: 0.27065742015838623 at epoch: 10 and batch_num: 888\n",
      "Loss of train set: 0.24585144221782684 at epoch: 10 and batch_num: 889\n",
      "Loss of train set: 0.37491294741630554 at epoch: 10 and batch_num: 890\n",
      "Loss of train set: 0.27268731594085693 at epoch: 10 and batch_num: 891\n",
      "Loss of train set: 0.413224458694458 at epoch: 10 and batch_num: 892\n",
      "Loss of train set: 0.30329596996307373 at epoch: 10 and batch_num: 893\n",
      "Loss of train set: 0.35513588786125183 at epoch: 10 and batch_num: 894\n",
      "Loss of train set: 0.4595620036125183 at epoch: 10 and batch_num: 895\n",
      "Loss of train set: 0.16422531008720398 at epoch: 10 and batch_num: 896\n",
      "Loss of train set: 0.28638866543769836 at epoch: 10 and batch_num: 897\n",
      "Loss of train set: 0.3760848939418793 at epoch: 10 and batch_num: 898\n",
      "Loss of train set: 0.2850794792175293 at epoch: 10 and batch_num: 899\n",
      "Loss of train set: 0.3085145950317383 at epoch: 10 and batch_num: 900\n",
      "Loss of train set: 0.3015440106391907 at epoch: 10 and batch_num: 901\n",
      "Loss of train set: 0.2830825448036194 at epoch: 10 and batch_num: 902\n",
      "Loss of train set: 0.3210088014602661 at epoch: 10 and batch_num: 903\n",
      "Loss of train set: 0.44784748554229736 at epoch: 10 and batch_num: 904\n",
      "Loss of train set: 0.19805064797401428 at epoch: 10 and batch_num: 905\n",
      "Loss of train set: 0.29378408193588257 at epoch: 10 and batch_num: 906\n",
      "Loss of train set: 0.2388281524181366 at epoch: 10 and batch_num: 907\n",
      "Loss of train set: 0.3812118172645569 at epoch: 10 and batch_num: 908\n",
      "Loss of train set: 0.40685737133026123 at epoch: 10 and batch_num: 909\n",
      "Loss of train set: 0.2883811593055725 at epoch: 10 and batch_num: 910\n",
      "Loss of train set: 0.24000978469848633 at epoch: 10 and batch_num: 911\n",
      "Loss of train set: 0.63377845287323 at epoch: 10 and batch_num: 912\n",
      "Loss of train set: 0.2607486844062805 at epoch: 10 and batch_num: 913\n",
      "Loss of train set: 0.2913017272949219 at epoch: 10 and batch_num: 914\n",
      "Loss of train set: 0.49217236042022705 at epoch: 10 and batch_num: 915\n",
      "Loss of train set: 0.26753681898117065 at epoch: 10 and batch_num: 916\n",
      "Loss of train set: 0.3891867399215698 at epoch: 10 and batch_num: 917\n",
      "Loss of train set: 0.40863656997680664 at epoch: 10 and batch_num: 918\n",
      "Loss of train set: 0.33373767137527466 at epoch: 10 and batch_num: 919\n",
      "Loss of train set: 0.2813677489757538 at epoch: 10 and batch_num: 920\n",
      "Loss of train set: 0.32846778631210327 at epoch: 10 and batch_num: 921\n",
      "Loss of train set: 0.2877776622772217 at epoch: 10 and batch_num: 922\n",
      "Loss of train set: 0.36095350980758667 at epoch: 10 and batch_num: 923\n",
      "Loss of train set: 0.44707173109054565 at epoch: 10 and batch_num: 924\n",
      "Loss of train set: 0.3308863639831543 at epoch: 10 and batch_num: 925\n",
      "Loss of train set: 0.23277020454406738 at epoch: 10 and batch_num: 926\n",
      "Loss of train set: 0.34153586626052856 at epoch: 10 and batch_num: 927\n",
      "Loss of train set: 0.35410845279693604 at epoch: 10 and batch_num: 928\n",
      "Loss of train set: 0.2835388481616974 at epoch: 10 and batch_num: 929\n",
      "Loss of train set: 0.24422456324100494 at epoch: 10 and batch_num: 930\n",
      "Loss of train set: 0.48113906383514404 at epoch: 10 and batch_num: 931\n",
      "Loss of train set: 0.5338344573974609 at epoch: 10 and batch_num: 932\n",
      "Loss of train set: 0.2560621201992035 at epoch: 10 and batch_num: 933\n",
      "Loss of train set: 0.42885464429855347 at epoch: 10 and batch_num: 934\n",
      "Loss of train set: 0.4995213747024536 at epoch: 10 and batch_num: 935\n",
      "Loss of train set: 0.3260524868965149 at epoch: 10 and batch_num: 936\n",
      "Loss of train set: 0.5261907577514648 at epoch: 10 and batch_num: 937\n",
      "Accuracy of train set: 0.8837333333333334\n",
      "Loss of test set: 0.32692950963974 at epoch: 10 and batch_num: 0\n",
      "Loss of test set: 0.4476231336593628 at epoch: 10 and batch_num: 1\n",
      "Loss of test set: 0.2290855348110199 at epoch: 10 and batch_num: 2\n",
      "Loss of test set: 0.4885396659374237 at epoch: 10 and batch_num: 3\n",
      "Loss of test set: 0.38550734519958496 at epoch: 10 and batch_num: 4\n",
      "Loss of test set: 0.25712475180625916 at epoch: 10 and batch_num: 5\n",
      "Loss of test set: 0.4168628752231598 at epoch: 10 and batch_num: 6\n",
      "Loss of test set: 0.28925567865371704 at epoch: 10 and batch_num: 7\n",
      "Loss of test set: 0.32156825065612793 at epoch: 10 and batch_num: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.2911604642868042 at epoch: 10 and batch_num: 9\n",
      "Loss of test set: 0.5021690130233765 at epoch: 10 and batch_num: 10\n",
      "Loss of test set: 0.37225186824798584 at epoch: 10 and batch_num: 11\n",
      "Loss of test set: 0.5799520611763 at epoch: 10 and batch_num: 12\n",
      "Loss of test set: 0.24627196788787842 at epoch: 10 and batch_num: 13\n",
      "Loss of test set: 0.5550637245178223 at epoch: 10 and batch_num: 14\n",
      "Loss of test set: 0.36798685789108276 at epoch: 10 and batch_num: 15\n",
      "Loss of test set: 0.261030375957489 at epoch: 10 and batch_num: 16\n",
      "Loss of test set: 0.2253480702638626 at epoch: 10 and batch_num: 17\n",
      "Loss of test set: 0.5399555563926697 at epoch: 10 and batch_num: 18\n",
      "Loss of test set: 0.47663336992263794 at epoch: 10 and batch_num: 19\n",
      "Loss of test set: 0.31228873133659363 at epoch: 10 and batch_num: 20\n",
      "Loss of test set: 0.378730446100235 at epoch: 10 and batch_num: 21\n",
      "Loss of test set: 0.3390370011329651 at epoch: 10 and batch_num: 22\n",
      "Loss of test set: 0.4310268759727478 at epoch: 10 and batch_num: 23\n",
      "Loss of test set: 0.2089444100856781 at epoch: 10 and batch_num: 24\n",
      "Loss of test set: 0.44797050952911377 at epoch: 10 and batch_num: 25\n",
      "Loss of test set: 0.33229368925094604 at epoch: 10 and batch_num: 26\n",
      "Loss of test set: 0.24257853627204895 at epoch: 10 and batch_num: 27\n",
      "Loss of test set: 0.2741208076477051 at epoch: 10 and batch_num: 28\n",
      "Loss of test set: 0.3366847038269043 at epoch: 10 and batch_num: 29\n",
      "Loss of test set: 0.22223496437072754 at epoch: 10 and batch_num: 30\n",
      "Loss of test set: 0.3138622045516968 at epoch: 10 and batch_num: 31\n",
      "Loss of test set: 0.31085652112960815 at epoch: 10 and batch_num: 32\n",
      "Loss of test set: 0.44129303097724915 at epoch: 10 and batch_num: 33\n",
      "Loss of test set: 0.43707576394081116 at epoch: 10 and batch_num: 34\n",
      "Loss of test set: 0.4267253279685974 at epoch: 10 and batch_num: 35\n",
      "Loss of test set: 0.3602827787399292 at epoch: 10 and batch_num: 36\n",
      "Loss of test set: 0.4591517448425293 at epoch: 10 and batch_num: 37\n",
      "Loss of test set: 0.27646705508232117 at epoch: 10 and batch_num: 38\n",
      "Loss of test set: 0.4145675301551819 at epoch: 10 and batch_num: 39\n",
      "Loss of test set: 0.45135897397994995 at epoch: 10 and batch_num: 40\n",
      "Loss of test set: 0.184577077627182 at epoch: 10 and batch_num: 41\n",
      "Loss of test set: 0.3608297109603882 at epoch: 10 and batch_num: 42\n",
      "Loss of test set: 0.4256887435913086 at epoch: 10 and batch_num: 43\n",
      "Loss of test set: 0.29398834705352783 at epoch: 10 and batch_num: 44\n",
      "Loss of test set: 0.37056562304496765 at epoch: 10 and batch_num: 45\n",
      "Loss of test set: 0.22852545976638794 at epoch: 10 and batch_num: 46\n",
      "Loss of test set: 0.32301488518714905 at epoch: 10 and batch_num: 47\n",
      "Loss of test set: 0.32053935527801514 at epoch: 10 and batch_num: 48\n",
      "Loss of test set: 0.33525240421295166 at epoch: 10 and batch_num: 49\n",
      "Loss of test set: 0.33769476413726807 at epoch: 10 and batch_num: 50\n",
      "Loss of test set: 0.4021587371826172 at epoch: 10 and batch_num: 51\n",
      "Loss of test set: 0.597548246383667 at epoch: 10 and batch_num: 52\n",
      "Loss of test set: 0.2833206057548523 at epoch: 10 and batch_num: 53\n",
      "Loss of test set: 0.3196602463722229 at epoch: 10 and batch_num: 54\n",
      "Loss of test set: 0.3808835744857788 at epoch: 10 and batch_num: 55\n",
      "Loss of test set: 0.253577321767807 at epoch: 10 and batch_num: 56\n",
      "Loss of test set: 0.46646493673324585 at epoch: 10 and batch_num: 57\n",
      "Loss of test set: 0.6180852651596069 at epoch: 10 and batch_num: 58\n",
      "Loss of test set: 0.4943958520889282 at epoch: 10 and batch_num: 59\n",
      "Loss of test set: 0.36973661184310913 at epoch: 10 and batch_num: 60\n",
      "Loss of test set: 0.4318578243255615 at epoch: 10 and batch_num: 61\n",
      "Loss of test set: 0.4469389319419861 at epoch: 10 and batch_num: 62\n",
      "Loss of test set: 0.48190245032310486 at epoch: 10 and batch_num: 63\n",
      "Loss of test set: 0.26278775930404663 at epoch: 10 and batch_num: 64\n",
      "Loss of test set: 0.40577933192253113 at epoch: 10 and batch_num: 65\n",
      "Loss of test set: 0.23677971959114075 at epoch: 10 and batch_num: 66\n",
      "Loss of test set: 0.3361867070198059 at epoch: 10 and batch_num: 67\n",
      "Loss of test set: 0.3098430037498474 at epoch: 10 and batch_num: 68\n",
      "Loss of test set: 0.4385683238506317 at epoch: 10 and batch_num: 69\n",
      "Loss of test set: 0.6176353693008423 at epoch: 10 and batch_num: 70\n",
      "Loss of test set: 0.4859142005443573 at epoch: 10 and batch_num: 71\n",
      "Loss of test set: 0.30692753195762634 at epoch: 10 and batch_num: 72\n",
      "Loss of test set: 0.27603214979171753 at epoch: 10 and batch_num: 73\n",
      "Loss of test set: 0.36097830533981323 at epoch: 10 and batch_num: 74\n",
      "Loss of test set: 0.4225097894668579 at epoch: 10 and batch_num: 75\n",
      "Loss of test set: 0.7765809297561646 at epoch: 10 and batch_num: 76\n",
      "Loss of test set: 0.38624560832977295 at epoch: 10 and batch_num: 77\n",
      "Loss of test set: 0.3177255094051361 at epoch: 10 and batch_num: 78\n",
      "Loss of test set: 0.24866819381713867 at epoch: 10 and batch_num: 79\n",
      "Loss of test set: 0.38420170545578003 at epoch: 10 and batch_num: 80\n",
      "Loss of test set: 0.412736177444458 at epoch: 10 and batch_num: 81\n",
      "Loss of test set: 0.3952037990093231 at epoch: 10 and batch_num: 82\n",
      "Loss of test set: 0.31749969720840454 at epoch: 10 and batch_num: 83\n",
      "Loss of test set: 0.4650070369243622 at epoch: 10 and batch_num: 84\n",
      "Loss of test set: 0.5633875727653503 at epoch: 10 and batch_num: 85\n",
      "Loss of test set: 0.6812382936477661 at epoch: 10 and batch_num: 86\n",
      "Loss of test set: 0.47028228640556335 at epoch: 10 and batch_num: 87\n",
      "Loss of test set: 0.5247414708137512 at epoch: 10 and batch_num: 88\n",
      "Loss of test set: 0.4618479609489441 at epoch: 10 and batch_num: 89\n",
      "Loss of test set: 0.24252992868423462 at epoch: 10 and batch_num: 90\n",
      "Loss of test set: 0.3959285616874695 at epoch: 10 and batch_num: 91\n",
      "Loss of test set: 0.32942432165145874 at epoch: 10 and batch_num: 92\n",
      "Loss of test set: 0.5128116011619568 at epoch: 10 and batch_num: 93\n",
      "Loss of test set: 0.5262689590454102 at epoch: 10 and batch_num: 94\n",
      "Loss of test set: 0.456548810005188 at epoch: 10 and batch_num: 95\n",
      "Loss of test set: 0.44836193323135376 at epoch: 10 and batch_num: 96\n",
      "Loss of test set: 0.3619294762611389 at epoch: 10 and batch_num: 97\n",
      "Loss of test set: 0.3635895848274231 at epoch: 10 and batch_num: 98\n",
      "Loss of test set: 0.36681997776031494 at epoch: 10 and batch_num: 99\n",
      "Loss of test set: 0.4589211940765381 at epoch: 10 and batch_num: 100\n",
      "Loss of test set: 0.3566414415836334 at epoch: 10 and batch_num: 101\n",
      "Loss of test set: 0.5806723237037659 at epoch: 10 and batch_num: 102\n",
      "Loss of test set: 0.26005223393440247 at epoch: 10 and batch_num: 103\n",
      "Loss of test set: 0.5571146011352539 at epoch: 10 and batch_num: 104\n",
      "Loss of test set: 0.3964465856552124 at epoch: 10 and batch_num: 105\n",
      "Loss of test set: 0.3570460379123688 at epoch: 10 and batch_num: 106\n",
      "Loss of test set: 0.2786247134208679 at epoch: 10 and batch_num: 107\n",
      "Loss of test set: 0.4145606756210327 at epoch: 10 and batch_num: 108\n",
      "Loss of test set: 0.4978490471839905 at epoch: 10 and batch_num: 109\n",
      "Loss of test set: 0.5629082918167114 at epoch: 10 and batch_num: 110\n",
      "Loss of test set: 0.34498634934425354 at epoch: 10 and batch_num: 111\n",
      "Loss of test set: 0.3050477206707001 at epoch: 10 and batch_num: 112\n",
      "Loss of test set: 0.5527138710021973 at epoch: 10 and batch_num: 113\n",
      "Loss of test set: 0.2454606592655182 at epoch: 10 and batch_num: 114\n",
      "Loss of test set: 0.39539626240730286 at epoch: 10 and batch_num: 115\n",
      "Loss of test set: 0.40873342752456665 at epoch: 10 and batch_num: 116\n",
      "Loss of test set: 0.24290831387043 at epoch: 10 and batch_num: 117\n",
      "Loss of test set: 0.4443132281303406 at epoch: 10 and batch_num: 118\n",
      "Loss of test set: 0.5576561689376831 at epoch: 10 and batch_num: 119\n",
      "Loss of test set: 0.26101282238960266 at epoch: 10 and batch_num: 120\n",
      "Loss of test set: 0.3917040228843689 at epoch: 10 and batch_num: 121\n",
      "Loss of test set: 0.48591193556785583 at epoch: 10 and batch_num: 122\n",
      "Loss of test set: 0.3867693543434143 at epoch: 10 and batch_num: 123\n",
      "Loss of test set: 0.6542718410491943 at epoch: 10 and batch_num: 124\n",
      "Loss of test set: 0.4146137833595276 at epoch: 10 and batch_num: 125\n",
      "Loss of test set: 0.30151230096817017 at epoch: 10 and batch_num: 126\n",
      "Loss of test set: 0.29626980423927307 at epoch: 10 and batch_num: 127\n",
      "Loss of test set: 0.3403339087963104 at epoch: 10 and batch_num: 128\n",
      "Loss of test set: 0.32320070266723633 at epoch: 10 and batch_num: 129\n",
      "Loss of test set: 0.5058608055114746 at epoch: 10 and batch_num: 130\n",
      "Loss of test set: 0.2703595459461212 at epoch: 10 and batch_num: 131\n",
      "Loss of test set: 0.29911476373672485 at epoch: 10 and batch_num: 132\n",
      "Loss of test set: 0.42536473274230957 at epoch: 10 and batch_num: 133\n",
      "Loss of test set: 0.49181199073791504 at epoch: 10 and batch_num: 134\n",
      "Loss of test set: 0.21299712359905243 at epoch: 10 and batch_num: 135\n",
      "Loss of test set: 0.4354466199874878 at epoch: 10 and batch_num: 136\n",
      "Loss of test set: 0.4640820622444153 at epoch: 10 and batch_num: 137\n",
      "Loss of test set: 0.324709951877594 at epoch: 10 and batch_num: 138\n",
      "Loss of test set: 0.539237916469574 at epoch: 10 and batch_num: 139\n",
      "Loss of test set: 0.44735628366470337 at epoch: 10 and batch_num: 140\n",
      "Loss of test set: 0.247939333319664 at epoch: 10 and batch_num: 141\n",
      "Loss of test set: 0.15642714500427246 at epoch: 10 and batch_num: 142\n",
      "Loss of test set: 0.3336193859577179 at epoch: 10 and batch_num: 143\n",
      "Loss of test set: 0.29901477694511414 at epoch: 10 and batch_num: 144\n",
      "Loss of test set: 0.22164902091026306 at epoch: 10 and batch_num: 145\n",
      "Loss of test set: 0.18567144870758057 at epoch: 10 and batch_num: 146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.3271045386791229 at epoch: 10 and batch_num: 147\n",
      "Loss of test set: 0.46995073556900024 at epoch: 10 and batch_num: 148\n",
      "Loss of test set: 0.5415816903114319 at epoch: 10 and batch_num: 149\n",
      "Loss of test set: 0.4114280343055725 at epoch: 10 and batch_num: 150\n",
      "Loss of test set: 0.28313466906547546 at epoch: 10 and batch_num: 151\n",
      "Loss of test set: 0.5224811434745789 at epoch: 10 and batch_num: 152\n",
      "Loss of test set: 0.4455907344818115 at epoch: 10 and batch_num: 153\n",
      "Loss of test set: 0.3821571171283722 at epoch: 10 and batch_num: 154\n",
      "Loss of test set: 0.5427506566047668 at epoch: 10 and batch_num: 155\n",
      "Loss of test set: 0.36778178811073303 at epoch: 10 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8604\n",
      "Loss of train set: 0.3285718560218811 at epoch: 11 and batch_num: 0\n",
      "Loss of train set: 0.13114826381206512 at epoch: 11 and batch_num: 1\n",
      "Loss of train set: 0.2624261975288391 at epoch: 11 and batch_num: 2\n",
      "Loss of train set: 0.21971526741981506 at epoch: 11 and batch_num: 3\n",
      "Loss of train set: 0.4078064262866974 at epoch: 11 and batch_num: 4\n",
      "Loss of train set: 0.3224036693572998 at epoch: 11 and batch_num: 5\n",
      "Loss of train set: 0.35055309534072876 at epoch: 11 and batch_num: 6\n",
      "Loss of train set: 0.22429700195789337 at epoch: 11 and batch_num: 7\n",
      "Loss of train set: 0.36705636978149414 at epoch: 11 and batch_num: 8\n",
      "Loss of train set: 0.3881263732910156 at epoch: 11 and batch_num: 9\n",
      "Loss of train set: 0.4425182640552521 at epoch: 11 and batch_num: 10\n",
      "Loss of train set: 0.25454962253570557 at epoch: 11 and batch_num: 11\n",
      "Loss of train set: 0.4377443790435791 at epoch: 11 and batch_num: 12\n",
      "Loss of train set: 0.3642579913139343 at epoch: 11 and batch_num: 13\n",
      "Loss of train set: 0.5144816637039185 at epoch: 11 and batch_num: 14\n",
      "Loss of train set: 0.2826427221298218 at epoch: 11 and batch_num: 15\n",
      "Loss of train set: 0.3155674934387207 at epoch: 11 and batch_num: 16\n",
      "Loss of train set: 0.24718575179576874 at epoch: 11 and batch_num: 17\n",
      "Loss of train set: 0.21048349142074585 at epoch: 11 and batch_num: 18\n",
      "Loss of train set: 0.4640849232673645 at epoch: 11 and batch_num: 19\n",
      "Loss of train set: 0.28576046228408813 at epoch: 11 and batch_num: 20\n",
      "Loss of train set: 0.2645605504512787 at epoch: 11 and batch_num: 21\n",
      "Loss of train set: 0.2257949411869049 at epoch: 11 and batch_num: 22\n",
      "Loss of train set: 0.283051073551178 at epoch: 11 and batch_num: 23\n",
      "Loss of train set: 0.25730299949645996 at epoch: 11 and batch_num: 24\n",
      "Loss of train set: 0.32842129468917847 at epoch: 11 and batch_num: 25\n",
      "Loss of train set: 0.2652513384819031 at epoch: 11 and batch_num: 26\n",
      "Loss of train set: 0.2733631134033203 at epoch: 11 and batch_num: 27\n",
      "Loss of train set: 0.41739606857299805 at epoch: 11 and batch_num: 28\n",
      "Loss of train set: 0.2852020263671875 at epoch: 11 and batch_num: 29\n",
      "Loss of train set: 0.29848527908325195 at epoch: 11 and batch_num: 30\n",
      "Loss of train set: 0.3686144948005676 at epoch: 11 and batch_num: 31\n",
      "Loss of train set: 0.1341407299041748 at epoch: 11 and batch_num: 32\n",
      "Loss of train set: 0.18977989256381989 at epoch: 11 and batch_num: 33\n",
      "Loss of train set: 0.3009687066078186 at epoch: 11 and batch_num: 34\n",
      "Loss of train set: 0.4161747097969055 at epoch: 11 and batch_num: 35\n",
      "Loss of train set: 0.31825217604637146 at epoch: 11 and batch_num: 36\n",
      "Loss of train set: 0.3896327316761017 at epoch: 11 and batch_num: 37\n",
      "Loss of train set: 0.3484448194503784 at epoch: 11 and batch_num: 38\n",
      "Loss of train set: 0.302820086479187 at epoch: 11 and batch_num: 39\n",
      "Loss of train set: 0.3445678949356079 at epoch: 11 and batch_num: 40\n",
      "Loss of train set: 0.3877866864204407 at epoch: 11 and batch_num: 41\n",
      "Loss of train set: 0.44627252221107483 at epoch: 11 and batch_num: 42\n",
      "Loss of train set: 0.2580307126045227 at epoch: 11 and batch_num: 43\n",
      "Loss of train set: 0.42907899618148804 at epoch: 11 and batch_num: 44\n",
      "Loss of train set: 0.21408706903457642 at epoch: 11 and batch_num: 45\n",
      "Loss of train set: 0.42985957860946655 at epoch: 11 and batch_num: 46\n",
      "Loss of train set: 0.35236334800720215 at epoch: 11 and batch_num: 47\n",
      "Loss of train set: 0.2998308539390564 at epoch: 11 and batch_num: 48\n",
      "Loss of train set: 0.28336548805236816 at epoch: 11 and batch_num: 49\n",
      "Loss of train set: 0.25843340158462524 at epoch: 11 and batch_num: 50\n",
      "Loss of train set: 0.5793967247009277 at epoch: 11 and batch_num: 51\n",
      "Loss of train set: 0.2935248911380768 at epoch: 11 and batch_num: 52\n",
      "Loss of train set: 0.2881155014038086 at epoch: 11 and batch_num: 53\n",
      "Loss of train set: 0.40287429094314575 at epoch: 11 and batch_num: 54\n",
      "Loss of train set: 0.23911409080028534 at epoch: 11 and batch_num: 55\n",
      "Loss of train set: 0.45708125829696655 at epoch: 11 and batch_num: 56\n",
      "Loss of train set: 0.21438553929328918 at epoch: 11 and batch_num: 57\n",
      "Loss of train set: 0.2610757052898407 at epoch: 11 and batch_num: 58\n",
      "Loss of train set: 0.18457293510437012 at epoch: 11 and batch_num: 59\n",
      "Loss of train set: 0.3895995616912842 at epoch: 11 and batch_num: 60\n",
      "Loss of train set: 0.34911084175109863 at epoch: 11 and batch_num: 61\n",
      "Loss of train set: 0.32867640256881714 at epoch: 11 and batch_num: 62\n",
      "Loss of train set: 0.23500333726406097 at epoch: 11 and batch_num: 63\n",
      "Loss of train set: 0.4015454053878784 at epoch: 11 and batch_num: 64\n",
      "Loss of train set: 0.19172164797782898 at epoch: 11 and batch_num: 65\n",
      "Loss of train set: 0.3512890636920929 at epoch: 11 and batch_num: 66\n",
      "Loss of train set: 0.48282065987586975 at epoch: 11 and batch_num: 67\n",
      "Loss of train set: 0.30484887957572937 at epoch: 11 and batch_num: 68\n",
      "Loss of train set: 0.2523179054260254 at epoch: 11 and batch_num: 69\n",
      "Loss of train set: 0.2165999412536621 at epoch: 11 and batch_num: 70\n",
      "Loss of train set: 0.3151545524597168 at epoch: 11 and batch_num: 71\n",
      "Loss of train set: 0.3177489638328552 at epoch: 11 and batch_num: 72\n",
      "Loss of train set: 0.4028598964214325 at epoch: 11 and batch_num: 73\n",
      "Loss of train set: 0.2860988974571228 at epoch: 11 and batch_num: 74\n",
      "Loss of train set: 0.31400179862976074 at epoch: 11 and batch_num: 75\n",
      "Loss of train set: 0.5604619383811951 at epoch: 11 and batch_num: 76\n",
      "Loss of train set: 0.4309110939502716 at epoch: 11 and batch_num: 77\n",
      "Loss of train set: 0.5217417478561401 at epoch: 11 and batch_num: 78\n",
      "Loss of train set: 0.36902058124542236 at epoch: 11 and batch_num: 79\n",
      "Loss of train set: 0.16532722115516663 at epoch: 11 and batch_num: 80\n",
      "Loss of train set: 0.3982335031032562 at epoch: 11 and batch_num: 81\n",
      "Loss of train set: 0.29350000619888306 at epoch: 11 and batch_num: 82\n",
      "Loss of train set: 0.31883925199508667 at epoch: 11 and batch_num: 83\n",
      "Loss of train set: 0.300760954618454 at epoch: 11 and batch_num: 84\n",
      "Loss of train set: 0.15409085154533386 at epoch: 11 and batch_num: 85\n",
      "Loss of train set: 0.19569669663906097 at epoch: 11 and batch_num: 86\n",
      "Loss of train set: 0.32114526629447937 at epoch: 11 and batch_num: 87\n",
      "Loss of train set: 0.3869042992591858 at epoch: 11 and batch_num: 88\n",
      "Loss of train set: 0.251348614692688 at epoch: 11 and batch_num: 89\n",
      "Loss of train set: 0.39151066541671753 at epoch: 11 and batch_num: 90\n",
      "Loss of train set: 0.34761226177215576 at epoch: 11 and batch_num: 91\n",
      "Loss of train set: 0.25389373302459717 at epoch: 11 and batch_num: 92\n",
      "Loss of train set: 0.10019746422767639 at epoch: 11 and batch_num: 93\n",
      "Loss of train set: 0.4186258614063263 at epoch: 11 and batch_num: 94\n",
      "Loss of train set: 0.41204655170440674 at epoch: 11 and batch_num: 95\n",
      "Loss of train set: 0.1304890513420105 at epoch: 11 and batch_num: 96\n",
      "Loss of train set: 0.380881130695343 at epoch: 11 and batch_num: 97\n",
      "Loss of train set: 0.516321063041687 at epoch: 11 and batch_num: 98\n",
      "Loss of train set: 0.35168933868408203 at epoch: 11 and batch_num: 99\n",
      "Loss of train set: 0.31795355677604675 at epoch: 11 and batch_num: 100\n",
      "Loss of train set: 0.30578258633613586 at epoch: 11 and batch_num: 101\n",
      "Loss of train set: 0.36530768871307373 at epoch: 11 and batch_num: 102\n",
      "Loss of train set: 0.3749280273914337 at epoch: 11 and batch_num: 103\n",
      "Loss of train set: 0.17430010437965393 at epoch: 11 and batch_num: 104\n",
      "Loss of train set: 0.29254963994026184 at epoch: 11 and batch_num: 105\n",
      "Loss of train set: 0.30769652128219604 at epoch: 11 and batch_num: 106\n",
      "Loss of train set: 0.43193385004997253 at epoch: 11 and batch_num: 107\n",
      "Loss of train set: 0.3476075530052185 at epoch: 11 and batch_num: 108\n",
      "Loss of train set: 0.2653370797634125 at epoch: 11 and batch_num: 109\n",
      "Loss of train set: 0.3807947635650635 at epoch: 11 and batch_num: 110\n",
      "Loss of train set: 0.21888235211372375 at epoch: 11 and batch_num: 111\n",
      "Loss of train set: 0.2819521427154541 at epoch: 11 and batch_num: 112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2681836485862732 at epoch: 11 and batch_num: 113\n",
      "Loss of train set: 0.38753747940063477 at epoch: 11 and batch_num: 114\n",
      "Loss of train set: 0.4597855806350708 at epoch: 11 and batch_num: 115\n",
      "Loss of train set: 0.3259275257587433 at epoch: 11 and batch_num: 116\n",
      "Loss of train set: 0.3927846848964691 at epoch: 11 and batch_num: 117\n",
      "Loss of train set: 0.32522889971733093 at epoch: 11 and batch_num: 118\n",
      "Loss of train set: 0.36790889501571655 at epoch: 11 and batch_num: 119\n",
      "Loss of train set: 0.3013884425163269 at epoch: 11 and batch_num: 120\n",
      "Loss of train set: 0.28362366557121277 at epoch: 11 and batch_num: 121\n",
      "Loss of train set: 0.2550141513347626 at epoch: 11 and batch_num: 122\n",
      "Loss of train set: 0.24379388988018036 at epoch: 11 and batch_num: 123\n",
      "Loss of train set: 0.408088743686676 at epoch: 11 and batch_num: 124\n",
      "Loss of train set: 0.42890775203704834 at epoch: 11 and batch_num: 125\n",
      "Loss of train set: 0.3931480348110199 at epoch: 11 and batch_num: 126\n",
      "Loss of train set: 0.2788430452346802 at epoch: 11 and batch_num: 127\n",
      "Loss of train set: 0.3025689125061035 at epoch: 11 and batch_num: 128\n",
      "Loss of train set: 0.206047922372818 at epoch: 11 and batch_num: 129\n",
      "Loss of train set: 0.21786323189735413 at epoch: 11 and batch_num: 130\n",
      "Loss of train set: 0.41568446159362793 at epoch: 11 and batch_num: 131\n",
      "Loss of train set: 0.2741234004497528 at epoch: 11 and batch_num: 132\n",
      "Loss of train set: 0.36329185962677 at epoch: 11 and batch_num: 133\n",
      "Loss of train set: 0.376953125 at epoch: 11 and batch_num: 134\n",
      "Loss of train set: 0.17238487303256989 at epoch: 11 and batch_num: 135\n",
      "Loss of train set: 0.4308329224586487 at epoch: 11 and batch_num: 136\n",
      "Loss of train set: 0.24590326845645905 at epoch: 11 and batch_num: 137\n",
      "Loss of train set: 0.23538821935653687 at epoch: 11 and batch_num: 138\n",
      "Loss of train set: 0.20114365220069885 at epoch: 11 and batch_num: 139\n",
      "Loss of train set: 0.39660078287124634 at epoch: 11 and batch_num: 140\n",
      "Loss of train set: 0.45814722776412964 at epoch: 11 and batch_num: 141\n",
      "Loss of train set: 0.34317484498023987 at epoch: 11 and batch_num: 142\n",
      "Loss of train set: 0.38461172580718994 at epoch: 11 and batch_num: 143\n",
      "Loss of train set: 0.14435699582099915 at epoch: 11 and batch_num: 144\n",
      "Loss of train set: 0.36307060718536377 at epoch: 11 and batch_num: 145\n",
      "Loss of train set: 0.4785354435443878 at epoch: 11 and batch_num: 146\n",
      "Loss of train set: 0.29634857177734375 at epoch: 11 and batch_num: 147\n",
      "Loss of train set: 0.5748247504234314 at epoch: 11 and batch_num: 148\n",
      "Loss of train set: 0.4769262671470642 at epoch: 11 and batch_num: 149\n",
      "Loss of train set: 0.5444628000259399 at epoch: 11 and batch_num: 150\n",
      "Loss of train set: 0.2944663166999817 at epoch: 11 and batch_num: 151\n",
      "Loss of train set: 0.289774626493454 at epoch: 11 and batch_num: 152\n",
      "Loss of train set: 0.4017100930213928 at epoch: 11 and batch_num: 153\n",
      "Loss of train set: 0.3751984238624573 at epoch: 11 and batch_num: 154\n",
      "Loss of train set: 0.2573400139808655 at epoch: 11 and batch_num: 155\n",
      "Loss of train set: 0.4445574879646301 at epoch: 11 and batch_num: 156\n",
      "Loss of train set: 0.22626808285713196 at epoch: 11 and batch_num: 157\n",
      "Loss of train set: 0.30549269914627075 at epoch: 11 and batch_num: 158\n",
      "Loss of train set: 0.2525327205657959 at epoch: 11 and batch_num: 159\n",
      "Loss of train set: 0.3187563419342041 at epoch: 11 and batch_num: 160\n",
      "Loss of train set: 0.4503384828567505 at epoch: 11 and batch_num: 161\n",
      "Loss of train set: 0.2230267971754074 at epoch: 11 and batch_num: 162\n",
      "Loss of train set: 0.338833749294281 at epoch: 11 and batch_num: 163\n",
      "Loss of train set: 0.4166339933872223 at epoch: 11 and batch_num: 164\n",
      "Loss of train set: 0.2630497217178345 at epoch: 11 and batch_num: 165\n",
      "Loss of train set: 0.35425493121147156 at epoch: 11 and batch_num: 166\n",
      "Loss of train set: 0.17731966078281403 at epoch: 11 and batch_num: 167\n",
      "Loss of train set: 0.2410178780555725 at epoch: 11 and batch_num: 168\n",
      "Loss of train set: 0.2669636607170105 at epoch: 11 and batch_num: 169\n",
      "Loss of train set: 0.18890540301799774 at epoch: 11 and batch_num: 170\n",
      "Loss of train set: 0.3553535044193268 at epoch: 11 and batch_num: 171\n",
      "Loss of train set: 0.3324517607688904 at epoch: 11 and batch_num: 172\n",
      "Loss of train set: 0.260520875453949 at epoch: 11 and batch_num: 173\n",
      "Loss of train set: 0.4954802989959717 at epoch: 11 and batch_num: 174\n",
      "Loss of train set: 0.304989755153656 at epoch: 11 and batch_num: 175\n",
      "Loss of train set: 0.34719544649124146 at epoch: 11 and batch_num: 176\n",
      "Loss of train set: 0.2169661819934845 at epoch: 11 and batch_num: 177\n",
      "Loss of train set: 0.26116853952407837 at epoch: 11 and batch_num: 178\n",
      "Loss of train set: 0.16114068031311035 at epoch: 11 and batch_num: 179\n",
      "Loss of train set: 0.35628312826156616 at epoch: 11 and batch_num: 180\n",
      "Loss of train set: 0.32723695039749146 at epoch: 11 and batch_num: 181\n",
      "Loss of train set: 0.21400538086891174 at epoch: 11 and batch_num: 182\n",
      "Loss of train set: 0.23685336112976074 at epoch: 11 and batch_num: 183\n",
      "Loss of train set: 0.33337122201919556 at epoch: 11 and batch_num: 184\n",
      "Loss of train set: 0.3229061961174011 at epoch: 11 and batch_num: 185\n",
      "Loss of train set: 0.2185797095298767 at epoch: 11 and batch_num: 186\n",
      "Loss of train set: 0.4727102518081665 at epoch: 11 and batch_num: 187\n",
      "Loss of train set: 0.2360278069972992 at epoch: 11 and batch_num: 188\n",
      "Loss of train set: 0.3811509609222412 at epoch: 11 and batch_num: 189\n",
      "Loss of train set: 0.39904242753982544 at epoch: 11 and batch_num: 190\n",
      "Loss of train set: 0.4389232397079468 at epoch: 11 and batch_num: 191\n",
      "Loss of train set: 0.19128772616386414 at epoch: 11 and batch_num: 192\n",
      "Loss of train set: 0.3535861372947693 at epoch: 11 and batch_num: 193\n",
      "Loss of train set: 0.3042770028114319 at epoch: 11 and batch_num: 194\n",
      "Loss of train set: 0.31077301502227783 at epoch: 11 and batch_num: 195\n",
      "Loss of train set: 0.24226120114326477 at epoch: 11 and batch_num: 196\n",
      "Loss of train set: 0.20428140461444855 at epoch: 11 and batch_num: 197\n",
      "Loss of train set: 0.33451247215270996 at epoch: 11 and batch_num: 198\n",
      "Loss of train set: 0.2613210678100586 at epoch: 11 and batch_num: 199\n",
      "Loss of train set: 0.3458220958709717 at epoch: 11 and batch_num: 200\n",
      "Loss of train set: 0.31039589643478394 at epoch: 11 and batch_num: 201\n",
      "Loss of train set: 0.28374356031417847 at epoch: 11 and batch_num: 202\n",
      "Loss of train set: 0.22002561390399933 at epoch: 11 and batch_num: 203\n",
      "Loss of train set: 0.16718566417694092 at epoch: 11 and batch_num: 204\n",
      "Loss of train set: 0.2666657865047455 at epoch: 11 and batch_num: 205\n",
      "Loss of train set: 0.3651777505874634 at epoch: 11 and batch_num: 206\n",
      "Loss of train set: 0.24340012669563293 at epoch: 11 and batch_num: 207\n",
      "Loss of train set: 0.28304946422576904 at epoch: 11 and batch_num: 208\n",
      "Loss of train set: 0.5333896279335022 at epoch: 11 and batch_num: 209\n",
      "Loss of train set: 0.3030994236469269 at epoch: 11 and batch_num: 210\n",
      "Loss of train set: 0.33016419410705566 at epoch: 11 and batch_num: 211\n",
      "Loss of train set: 0.4687524735927582 at epoch: 11 and batch_num: 212\n",
      "Loss of train set: 0.4013725519180298 at epoch: 11 and batch_num: 213\n",
      "Loss of train set: 0.3470330834388733 at epoch: 11 and batch_num: 214\n",
      "Loss of train set: 0.3195740282535553 at epoch: 11 and batch_num: 215\n",
      "Loss of train set: 0.19866620004177094 at epoch: 11 and batch_num: 216\n",
      "Loss of train set: 0.40913569927215576 at epoch: 11 and batch_num: 217\n",
      "Loss of train set: 0.27388742566108704 at epoch: 11 and batch_num: 218\n",
      "Loss of train set: 0.3804151117801666 at epoch: 11 and batch_num: 219\n",
      "Loss of train set: 0.26886358857154846 at epoch: 11 and batch_num: 220\n",
      "Loss of train set: 0.19022290408611298 at epoch: 11 and batch_num: 221\n",
      "Loss of train set: 0.31081879138946533 at epoch: 11 and batch_num: 222\n",
      "Loss of train set: 0.2928227186203003 at epoch: 11 and batch_num: 223\n",
      "Loss of train set: 0.4936479330062866 at epoch: 11 and batch_num: 224\n",
      "Loss of train set: 0.17092660069465637 at epoch: 11 and batch_num: 225\n",
      "Loss of train set: 0.41981929540634155 at epoch: 11 and batch_num: 226\n",
      "Loss of train set: 0.45378339290618896 at epoch: 11 and batch_num: 227\n",
      "Loss of train set: 0.25928059220314026 at epoch: 11 and batch_num: 228\n",
      "Loss of train set: 0.406366229057312 at epoch: 11 and batch_num: 229\n",
      "Loss of train set: 0.465644896030426 at epoch: 11 and batch_num: 230\n",
      "Loss of train set: 0.28418272733688354 at epoch: 11 and batch_num: 231\n",
      "Loss of train set: 0.21957528591156006 at epoch: 11 and batch_num: 232\n",
      "Loss of train set: 0.2861637473106384 at epoch: 11 and batch_num: 233\n",
      "Loss of train set: 0.18839868903160095 at epoch: 11 and batch_num: 234\n",
      "Loss of train set: 0.22657641768455505 at epoch: 11 and batch_num: 235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.36072689294815063 at epoch: 11 and batch_num: 236\n",
      "Loss of train set: 0.21998390555381775 at epoch: 11 and batch_num: 237\n",
      "Loss of train set: 0.3999529480934143 at epoch: 11 and batch_num: 238\n",
      "Loss of train set: 0.3763886094093323 at epoch: 11 and batch_num: 239\n",
      "Loss of train set: 0.33178484439849854 at epoch: 11 and batch_num: 240\n",
      "Loss of train set: 0.3884902596473694 at epoch: 11 and batch_num: 241\n",
      "Loss of train set: 0.3640938401222229 at epoch: 11 and batch_num: 242\n",
      "Loss of train set: 0.33267921209335327 at epoch: 11 and batch_num: 243\n",
      "Loss of train set: 0.35201898217201233 at epoch: 11 and batch_num: 244\n",
      "Loss of train set: 0.2601528465747833 at epoch: 11 and batch_num: 245\n",
      "Loss of train set: 0.3703330159187317 at epoch: 11 and batch_num: 246\n",
      "Loss of train set: 0.27578675746917725 at epoch: 11 and batch_num: 247\n",
      "Loss of train set: 0.3119223713874817 at epoch: 11 and batch_num: 248\n",
      "Loss of train set: 0.34404733777046204 at epoch: 11 and batch_num: 249\n",
      "Loss of train set: 0.24950957298278809 at epoch: 11 and batch_num: 250\n",
      "Loss of train set: 0.42545032501220703 at epoch: 11 and batch_num: 251\n",
      "Loss of train set: 0.42661717534065247 at epoch: 11 and batch_num: 252\n",
      "Loss of train set: 0.33001601696014404 at epoch: 11 and batch_num: 253\n",
      "Loss of train set: 0.16748592257499695 at epoch: 11 and batch_num: 254\n",
      "Loss of train set: 0.2864692211151123 at epoch: 11 and batch_num: 255\n",
      "Loss of train set: 0.3972302973270416 at epoch: 11 and batch_num: 256\n",
      "Loss of train set: 0.26202166080474854 at epoch: 11 and batch_num: 257\n",
      "Loss of train set: 0.5174703598022461 at epoch: 11 and batch_num: 258\n",
      "Loss of train set: 0.23236384987831116 at epoch: 11 and batch_num: 259\n",
      "Loss of train set: 0.3529502749443054 at epoch: 11 and batch_num: 260\n",
      "Loss of train set: 0.24287497997283936 at epoch: 11 and batch_num: 261\n",
      "Loss of train set: 0.1988023817539215 at epoch: 11 and batch_num: 262\n",
      "Loss of train set: 0.339242547750473 at epoch: 11 and batch_num: 263\n",
      "Loss of train set: 0.3171033561229706 at epoch: 11 and batch_num: 264\n",
      "Loss of train set: 0.3692026436328888 at epoch: 11 and batch_num: 265\n",
      "Loss of train set: 0.24635393917560577 at epoch: 11 and batch_num: 266\n",
      "Loss of train set: 0.3856545090675354 at epoch: 11 and batch_num: 267\n",
      "Loss of train set: 0.3145938813686371 at epoch: 11 and batch_num: 268\n",
      "Loss of train set: 0.16620980203151703 at epoch: 11 and batch_num: 269\n",
      "Loss of train set: 0.3496848940849304 at epoch: 11 and batch_num: 270\n",
      "Loss of train set: 0.5276875495910645 at epoch: 11 and batch_num: 271\n",
      "Loss of train set: 0.32834815979003906 at epoch: 11 and batch_num: 272\n",
      "Loss of train set: 0.3882652819156647 at epoch: 11 and batch_num: 273\n",
      "Loss of train set: 0.3582595884799957 at epoch: 11 and batch_num: 274\n",
      "Loss of train set: 0.3774262070655823 at epoch: 11 and batch_num: 275\n",
      "Loss of train set: 0.2688559591770172 at epoch: 11 and batch_num: 276\n",
      "Loss of train set: 0.2574690282344818 at epoch: 11 and batch_num: 277\n",
      "Loss of train set: 0.22607050836086273 at epoch: 11 and batch_num: 278\n",
      "Loss of train set: 0.2710944414138794 at epoch: 11 and batch_num: 279\n",
      "Loss of train set: 0.07481983304023743 at epoch: 11 and batch_num: 280\n",
      "Loss of train set: 0.47205331921577454 at epoch: 11 and batch_num: 281\n",
      "Loss of train set: 0.21905863285064697 at epoch: 11 and batch_num: 282\n",
      "Loss of train set: 0.3157932758331299 at epoch: 11 and batch_num: 283\n",
      "Loss of train set: 0.36368560791015625 at epoch: 11 and batch_num: 284\n",
      "Loss of train set: 0.10538984835147858 at epoch: 11 and batch_num: 285\n",
      "Loss of train set: 0.38552403450012207 at epoch: 11 and batch_num: 286\n",
      "Loss of train set: 0.27803370356559753 at epoch: 11 and batch_num: 287\n",
      "Loss of train set: 0.4782295525074005 at epoch: 11 and batch_num: 288\n",
      "Loss of train set: 0.24303419888019562 at epoch: 11 and batch_num: 289\n",
      "Loss of train set: 0.37279096245765686 at epoch: 11 and batch_num: 290\n",
      "Loss of train set: 0.33713528513908386 at epoch: 11 and batch_num: 291\n",
      "Loss of train set: 0.2794720530509949 at epoch: 11 and batch_num: 292\n",
      "Loss of train set: 0.23576679825782776 at epoch: 11 and batch_num: 293\n",
      "Loss of train set: 0.3892750144004822 at epoch: 11 and batch_num: 294\n",
      "Loss of train set: 0.26865410804748535 at epoch: 11 and batch_num: 295\n",
      "Loss of train set: 0.3124733865261078 at epoch: 11 and batch_num: 296\n",
      "Loss of train set: 0.29256874322891235 at epoch: 11 and batch_num: 297\n",
      "Loss of train set: 0.2649106979370117 at epoch: 11 and batch_num: 298\n",
      "Loss of train set: 0.26932913064956665 at epoch: 11 and batch_num: 299\n",
      "Loss of train set: 0.2443457543849945 at epoch: 11 and batch_num: 300\n",
      "Loss of train set: 0.3347642421722412 at epoch: 11 and batch_num: 301\n",
      "Loss of train set: 0.4902344346046448 at epoch: 11 and batch_num: 302\n",
      "Loss of train set: 0.2319323569536209 at epoch: 11 and batch_num: 303\n",
      "Loss of train set: 0.3715302050113678 at epoch: 11 and batch_num: 304\n",
      "Loss of train set: 0.3385812044143677 at epoch: 11 and batch_num: 305\n",
      "Loss of train set: 0.33181697130203247 at epoch: 11 and batch_num: 306\n",
      "Loss of train set: 0.347115695476532 at epoch: 11 and batch_num: 307\n",
      "Loss of train set: 0.2660751938819885 at epoch: 11 and batch_num: 308\n",
      "Loss of train set: 0.3335492014884949 at epoch: 11 and batch_num: 309\n",
      "Loss of train set: 0.2689903974533081 at epoch: 11 and batch_num: 310\n",
      "Loss of train set: 0.3517257273197174 at epoch: 11 and batch_num: 311\n",
      "Loss of train set: 0.37328851222991943 at epoch: 11 and batch_num: 312\n",
      "Loss of train set: 0.21507054567337036 at epoch: 11 and batch_num: 313\n",
      "Loss of train set: 0.3438034653663635 at epoch: 11 and batch_num: 314\n",
      "Loss of train set: 0.3377969563007355 at epoch: 11 and batch_num: 315\n",
      "Loss of train set: 0.2546755075454712 at epoch: 11 and batch_num: 316\n",
      "Loss of train set: 0.5321125984191895 at epoch: 11 and batch_num: 317\n",
      "Loss of train set: 0.3262021541595459 at epoch: 11 and batch_num: 318\n",
      "Loss of train set: 0.2311529517173767 at epoch: 11 and batch_num: 319\n",
      "Loss of train set: 0.24694450199604034 at epoch: 11 and batch_num: 320\n",
      "Loss of train set: 0.2221408188343048 at epoch: 11 and batch_num: 321\n",
      "Loss of train set: 0.29362261295318604 at epoch: 11 and batch_num: 322\n",
      "Loss of train set: 0.37262219190597534 at epoch: 11 and batch_num: 323\n",
      "Loss of train set: 0.2827227711677551 at epoch: 11 and batch_num: 324\n",
      "Loss of train set: 0.32988038659095764 at epoch: 11 and batch_num: 325\n",
      "Loss of train set: 0.32746589183807373 at epoch: 11 and batch_num: 326\n",
      "Loss of train set: 0.2646558880805969 at epoch: 11 and batch_num: 327\n",
      "Loss of train set: 0.29050517082214355 at epoch: 11 and batch_num: 328\n",
      "Loss of train set: 0.32663971185684204 at epoch: 11 and batch_num: 329\n",
      "Loss of train set: 0.15536296367645264 at epoch: 11 and batch_num: 330\n",
      "Loss of train set: 0.22765904664993286 at epoch: 11 and batch_num: 331\n",
      "Loss of train set: 0.5444470643997192 at epoch: 11 and batch_num: 332\n",
      "Loss of train set: 0.26570767164230347 at epoch: 11 and batch_num: 333\n",
      "Loss of train set: 0.22742490470409393 at epoch: 11 and batch_num: 334\n",
      "Loss of train set: 0.15588554739952087 at epoch: 11 and batch_num: 335\n",
      "Loss of train set: 0.30200648307800293 at epoch: 11 and batch_num: 336\n",
      "Loss of train set: 0.41447457671165466 at epoch: 11 and batch_num: 337\n",
      "Loss of train set: 0.16891255974769592 at epoch: 11 and batch_num: 338\n",
      "Loss of train set: 0.3692912757396698 at epoch: 11 and batch_num: 339\n",
      "Loss of train set: 0.22810441255569458 at epoch: 11 and batch_num: 340\n",
      "Loss of train set: 0.17058055102825165 at epoch: 11 and batch_num: 341\n",
      "Loss of train set: 0.4577294588088989 at epoch: 11 and batch_num: 342\n",
      "Loss of train set: 0.24441686272621155 at epoch: 11 and batch_num: 343\n",
      "Loss of train set: 0.3286130428314209 at epoch: 11 and batch_num: 344\n",
      "Loss of train set: 0.3429863750934601 at epoch: 11 and batch_num: 345\n",
      "Loss of train set: 0.23518085479736328 at epoch: 11 and batch_num: 346\n",
      "Loss of train set: 0.26158273220062256 at epoch: 11 and batch_num: 347\n",
      "Loss of train set: 0.37638425827026367 at epoch: 11 and batch_num: 348\n",
      "Loss of train set: 0.2572914958000183 at epoch: 11 and batch_num: 349\n",
      "Loss of train set: 0.42387887835502625 at epoch: 11 and batch_num: 350\n",
      "Loss of train set: 0.3336842358112335 at epoch: 11 and batch_num: 351\n",
      "Loss of train set: 0.35811975598335266 at epoch: 11 and batch_num: 352\n",
      "Loss of train set: 0.29001620411872864 at epoch: 11 and batch_num: 353\n",
      "Loss of train set: 0.5380871891975403 at epoch: 11 and batch_num: 354\n",
      "Loss of train set: 0.2884911000728607 at epoch: 11 and batch_num: 355\n",
      "Loss of train set: 0.3710247278213501 at epoch: 11 and batch_num: 356\n",
      "Loss of train set: 0.5295910239219666 at epoch: 11 and batch_num: 357\n",
      "Loss of train set: 0.16465531289577484 at epoch: 11 and batch_num: 358\n",
      "Loss of train set: 0.2774345874786377 at epoch: 11 and batch_num: 359\n",
      "Loss of train set: 0.3539447784423828 at epoch: 11 and batch_num: 360\n",
      "Loss of train set: 0.3454212546348572 at epoch: 11 and batch_num: 361\n",
      "Loss of train set: 0.524785041809082 at epoch: 11 and batch_num: 362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.37604838609695435 at epoch: 11 and batch_num: 363\n",
      "Loss of train set: 0.20931510627269745 at epoch: 11 and batch_num: 364\n",
      "Loss of train set: 0.31868380308151245 at epoch: 11 and batch_num: 365\n",
      "Loss of train set: 0.29794320464134216 at epoch: 11 and batch_num: 366\n",
      "Loss of train set: 0.2894766926765442 at epoch: 11 and batch_num: 367\n",
      "Loss of train set: 0.30855271220207214 at epoch: 11 and batch_num: 368\n",
      "Loss of train set: 0.28512856364250183 at epoch: 11 and batch_num: 369\n",
      "Loss of train set: 0.28112900257110596 at epoch: 11 and batch_num: 370\n",
      "Loss of train set: 0.30357667803764343 at epoch: 11 and batch_num: 371\n",
      "Loss of train set: 0.24502989649772644 at epoch: 11 and batch_num: 372\n",
      "Loss of train set: 0.2763153314590454 at epoch: 11 and batch_num: 373\n",
      "Loss of train set: 0.2804795503616333 at epoch: 11 and batch_num: 374\n",
      "Loss of train set: 0.21036043763160706 at epoch: 11 and batch_num: 375\n",
      "Loss of train set: 0.5565465688705444 at epoch: 11 and batch_num: 376\n",
      "Loss of train set: 0.25862622261047363 at epoch: 11 and batch_num: 377\n",
      "Loss of train set: 0.4386742115020752 at epoch: 11 and batch_num: 378\n",
      "Loss of train set: 0.3329707682132721 at epoch: 11 and batch_num: 379\n",
      "Loss of train set: 0.41757577657699585 at epoch: 11 and batch_num: 380\n",
      "Loss of train set: 0.36583906412124634 at epoch: 11 and batch_num: 381\n",
      "Loss of train set: 0.29945290088653564 at epoch: 11 and batch_num: 382\n",
      "Loss of train set: 0.15204408764839172 at epoch: 11 and batch_num: 383\n",
      "Loss of train set: 0.3756679892539978 at epoch: 11 and batch_num: 384\n",
      "Loss of train set: 0.3548354506492615 at epoch: 11 and batch_num: 385\n",
      "Loss of train set: 0.25613322854042053 at epoch: 11 and batch_num: 386\n",
      "Loss of train set: 0.3355928957462311 at epoch: 11 and batch_num: 387\n",
      "Loss of train set: 0.20714092254638672 at epoch: 11 and batch_num: 388\n",
      "Loss of train set: 0.43479016423225403 at epoch: 11 and batch_num: 389\n",
      "Loss of train set: 0.4971483051776886 at epoch: 11 and batch_num: 390\n",
      "Loss of train set: 0.2617223262786865 at epoch: 11 and batch_num: 391\n",
      "Loss of train set: 0.36470919847488403 at epoch: 11 and batch_num: 392\n",
      "Loss of train set: 0.30265480279922485 at epoch: 11 and batch_num: 393\n",
      "Loss of train set: 0.34218305349349976 at epoch: 11 and batch_num: 394\n",
      "Loss of train set: 0.2977011799812317 at epoch: 11 and batch_num: 395\n",
      "Loss of train set: 0.43937838077545166 at epoch: 11 and batch_num: 396\n",
      "Loss of train set: 0.2721520662307739 at epoch: 11 and batch_num: 397\n",
      "Loss of train set: 0.3372787535190582 at epoch: 11 and batch_num: 398\n",
      "Loss of train set: 0.3131555914878845 at epoch: 11 and batch_num: 399\n",
      "Loss of train set: 0.2605007290840149 at epoch: 11 and batch_num: 400\n",
      "Loss of train set: 0.3627883493900299 at epoch: 11 and batch_num: 401\n",
      "Loss of train set: 0.4038228392601013 at epoch: 11 and batch_num: 402\n",
      "Loss of train set: 0.42765527963638306 at epoch: 11 and batch_num: 403\n",
      "Loss of train set: 0.3914368748664856 at epoch: 11 and batch_num: 404\n",
      "Loss of train set: 0.2230186015367508 at epoch: 11 and batch_num: 405\n",
      "Loss of train set: 0.5676195621490479 at epoch: 11 and batch_num: 406\n",
      "Loss of train set: 0.407340407371521 at epoch: 11 and batch_num: 407\n",
      "Loss of train set: 0.35426217317581177 at epoch: 11 and batch_num: 408\n",
      "Loss of train set: 0.3828948140144348 at epoch: 11 and batch_num: 409\n",
      "Loss of train set: 0.3083454370498657 at epoch: 11 and batch_num: 410\n",
      "Loss of train set: 0.20973193645477295 at epoch: 11 and batch_num: 411\n",
      "Loss of train set: 0.35858434438705444 at epoch: 11 and batch_num: 412\n",
      "Loss of train set: 0.1949058324098587 at epoch: 11 and batch_num: 413\n",
      "Loss of train set: 0.21589726209640503 at epoch: 11 and batch_num: 414\n",
      "Loss of train set: 0.21167445182800293 at epoch: 11 and batch_num: 415\n",
      "Loss of train set: 0.20450347661972046 at epoch: 11 and batch_num: 416\n",
      "Loss of train set: 0.2268897294998169 at epoch: 11 and batch_num: 417\n",
      "Loss of train set: 0.40323710441589355 at epoch: 11 and batch_num: 418\n",
      "Loss of train set: 0.4801008105278015 at epoch: 11 and batch_num: 419\n",
      "Loss of train set: 0.33965378999710083 at epoch: 11 and batch_num: 420\n",
      "Loss of train set: 0.2735675275325775 at epoch: 11 and batch_num: 421\n",
      "Loss of train set: 0.3917531371116638 at epoch: 11 and batch_num: 422\n",
      "Loss of train set: 0.15211121737957 at epoch: 11 and batch_num: 423\n",
      "Loss of train set: 0.23263348639011383 at epoch: 11 and batch_num: 424\n",
      "Loss of train set: 0.19932103157043457 at epoch: 11 and batch_num: 425\n",
      "Loss of train set: 0.12359292805194855 at epoch: 11 and batch_num: 426\n",
      "Loss of train set: 0.3189530372619629 at epoch: 11 and batch_num: 427\n",
      "Loss of train set: 0.3899056315422058 at epoch: 11 and batch_num: 428\n",
      "Loss of train set: 0.2976990044116974 at epoch: 11 and batch_num: 429\n",
      "Loss of train set: 0.3040211796760559 at epoch: 11 and batch_num: 430\n",
      "Loss of train set: 0.397765576839447 at epoch: 11 and batch_num: 431\n",
      "Loss of train set: 0.3931404650211334 at epoch: 11 and batch_num: 432\n",
      "Loss of train set: 0.264091432094574 at epoch: 11 and batch_num: 433\n",
      "Loss of train set: 0.3778251111507416 at epoch: 11 and batch_num: 434\n",
      "Loss of train set: 0.24521583318710327 at epoch: 11 and batch_num: 435\n",
      "Loss of train set: 0.27352309226989746 at epoch: 11 and batch_num: 436\n",
      "Loss of train set: 0.29621103405952454 at epoch: 11 and batch_num: 437\n",
      "Loss of train set: 0.2507537007331848 at epoch: 11 and batch_num: 438\n",
      "Loss of train set: 0.30390989780426025 at epoch: 11 and batch_num: 439\n",
      "Loss of train set: 0.20975501835346222 at epoch: 11 and batch_num: 440\n",
      "Loss of train set: 0.3517037034034729 at epoch: 11 and batch_num: 441\n",
      "Loss of train set: 0.4263477325439453 at epoch: 11 and batch_num: 442\n",
      "Loss of train set: 0.2535978853702545 at epoch: 11 and batch_num: 443\n",
      "Loss of train set: 0.4235449433326721 at epoch: 11 and batch_num: 444\n",
      "Loss of train set: 0.29585981369018555 at epoch: 11 and batch_num: 445\n",
      "Loss of train set: 0.19745570421218872 at epoch: 11 and batch_num: 446\n",
      "Loss of train set: 0.5015863180160522 at epoch: 11 and batch_num: 447\n",
      "Loss of train set: 0.2446727603673935 at epoch: 11 and batch_num: 448\n",
      "Loss of train set: 0.26208895444869995 at epoch: 11 and batch_num: 449\n",
      "Loss of train set: 0.4650566279888153 at epoch: 11 and batch_num: 450\n",
      "Loss of train set: 0.23764118552207947 at epoch: 11 and batch_num: 451\n",
      "Loss of train set: 0.29089993238449097 at epoch: 11 and batch_num: 452\n",
      "Loss of train set: 0.18338149785995483 at epoch: 11 and batch_num: 453\n",
      "Loss of train set: 0.27252063155174255 at epoch: 11 and batch_num: 454\n",
      "Loss of train set: 0.3762737214565277 at epoch: 11 and batch_num: 455\n",
      "Loss of train set: 0.38965529203414917 at epoch: 11 and batch_num: 456\n",
      "Loss of train set: 0.26253676414489746 at epoch: 11 and batch_num: 457\n",
      "Loss of train set: 0.24658626317977905 at epoch: 11 and batch_num: 458\n",
      "Loss of train set: 0.22998562455177307 at epoch: 11 and batch_num: 459\n",
      "Loss of train set: 0.33309465646743774 at epoch: 11 and batch_num: 460\n",
      "Loss of train set: 0.2753508687019348 at epoch: 11 and batch_num: 461\n",
      "Loss of train set: 0.30681994557380676 at epoch: 11 and batch_num: 462\n",
      "Loss of train set: 0.22613736987113953 at epoch: 11 and batch_num: 463\n",
      "Loss of train set: 0.33308762311935425 at epoch: 11 and batch_num: 464\n",
      "Loss of train set: 0.3481239676475525 at epoch: 11 and batch_num: 465\n",
      "Loss of train set: 0.25747889280319214 at epoch: 11 and batch_num: 466\n",
      "Loss of train set: 0.25409024953842163 at epoch: 11 and batch_num: 467\n",
      "Loss of train set: 0.16913214325904846 at epoch: 11 and batch_num: 468\n",
      "Loss of train set: 0.18174031376838684 at epoch: 11 and batch_num: 469\n",
      "Loss of train set: 0.5255941152572632 at epoch: 11 and batch_num: 470\n",
      "Loss of train set: 0.36865514516830444 at epoch: 11 and batch_num: 471\n",
      "Loss of train set: 0.2017936408519745 at epoch: 11 and batch_num: 472\n",
      "Loss of train set: 0.5059715509414673 at epoch: 11 and batch_num: 473\n",
      "Loss of train set: 0.261394202709198 at epoch: 11 and batch_num: 474\n",
      "Loss of train set: 0.33865469694137573 at epoch: 11 and batch_num: 475\n",
      "Loss of train set: 0.21442583203315735 at epoch: 11 and batch_num: 476\n",
      "Loss of train set: 0.23135432600975037 at epoch: 11 and batch_num: 477\n",
      "Loss of train set: 0.28403228521347046 at epoch: 11 and batch_num: 478\n",
      "Loss of train set: 0.28499215841293335 at epoch: 11 and batch_num: 479\n",
      "Loss of train set: 0.33760571479797363 at epoch: 11 and batch_num: 480\n",
      "Loss of train set: 0.2139650285243988 at epoch: 11 and batch_num: 481\n",
      "Loss of train set: 0.4076055884361267 at epoch: 11 and batch_num: 482\n",
      "Loss of train set: 0.25529301166534424 at epoch: 11 and batch_num: 483\n",
      "Loss of train set: 0.5443587303161621 at epoch: 11 and batch_num: 484\n",
      "Loss of train set: 0.22291453182697296 at epoch: 11 and batch_num: 485\n",
      "Loss of train set: 0.2508334517478943 at epoch: 11 and batch_num: 486\n",
      "Loss of train set: 0.2511759400367737 at epoch: 11 and batch_num: 487\n",
      "Loss of train set: 0.3694784641265869 at epoch: 11 and batch_num: 488\n",
      "Loss of train set: 0.5022114515304565 at epoch: 11 and batch_num: 489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.16387610137462616 at epoch: 11 and batch_num: 490\n",
      "Loss of train set: 0.35420042276382446 at epoch: 11 and batch_num: 491\n",
      "Loss of train set: 0.35877102613449097 at epoch: 11 and batch_num: 492\n",
      "Loss of train set: 0.48150748014450073 at epoch: 11 and batch_num: 493\n",
      "Loss of train set: 0.3532203435897827 at epoch: 11 and batch_num: 494\n",
      "Loss of train set: 0.46220120787620544 at epoch: 11 and batch_num: 495\n",
      "Loss of train set: 0.20268958806991577 at epoch: 11 and batch_num: 496\n",
      "Loss of train set: 0.23152285814285278 at epoch: 11 and batch_num: 497\n",
      "Loss of train set: 0.28293073177337646 at epoch: 11 and batch_num: 498\n",
      "Loss of train set: 0.26033180952072144 at epoch: 11 and batch_num: 499\n",
      "Loss of train set: 0.45518675446510315 at epoch: 11 and batch_num: 500\n",
      "Loss of train set: 0.2955835461616516 at epoch: 11 and batch_num: 501\n",
      "Loss of train set: 0.27954331040382385 at epoch: 11 and batch_num: 502\n",
      "Loss of train set: 0.5035535097122192 at epoch: 11 and batch_num: 503\n",
      "Loss of train set: 0.2291007936000824 at epoch: 11 and batch_num: 504\n",
      "Loss of train set: 0.30472373962402344 at epoch: 11 and batch_num: 505\n",
      "Loss of train set: 0.30739617347717285 at epoch: 11 and batch_num: 506\n",
      "Loss of train set: 0.29427143931388855 at epoch: 11 and batch_num: 507\n",
      "Loss of train set: 0.5000152587890625 at epoch: 11 and batch_num: 508\n",
      "Loss of train set: 0.3274209201335907 at epoch: 11 and batch_num: 509\n",
      "Loss of train set: 0.33445149660110474 at epoch: 11 and batch_num: 510\n",
      "Loss of train set: 0.229498952627182 at epoch: 11 and batch_num: 511\n",
      "Loss of train set: 0.15555109083652496 at epoch: 11 and batch_num: 512\n",
      "Loss of train set: 0.2791404724121094 at epoch: 11 and batch_num: 513\n",
      "Loss of train set: 0.4806021451950073 at epoch: 11 and batch_num: 514\n",
      "Loss of train set: 0.2991679310798645 at epoch: 11 and batch_num: 515\n",
      "Loss of train set: 0.28275513648986816 at epoch: 11 and batch_num: 516\n",
      "Loss of train set: 0.468198299407959 at epoch: 11 and batch_num: 517\n",
      "Loss of train set: 0.5609514713287354 at epoch: 11 and batch_num: 518\n",
      "Loss of train set: 0.3782656788825989 at epoch: 11 and batch_num: 519\n",
      "Loss of train set: 0.4768204987049103 at epoch: 11 and batch_num: 520\n",
      "Loss of train set: 0.22494056820869446 at epoch: 11 and batch_num: 521\n",
      "Loss of train set: 0.34380966424942017 at epoch: 11 and batch_num: 522\n",
      "Loss of train set: 0.30470746755599976 at epoch: 11 and batch_num: 523\n",
      "Loss of train set: 0.39457741379737854 at epoch: 11 and batch_num: 524\n",
      "Loss of train set: 0.4454013705253601 at epoch: 11 and batch_num: 525\n",
      "Loss of train set: 0.3300592303276062 at epoch: 11 and batch_num: 526\n",
      "Loss of train set: 0.17997321486473083 at epoch: 11 and batch_num: 527\n",
      "Loss of train set: 0.20456887781620026 at epoch: 11 and batch_num: 528\n",
      "Loss of train set: 0.32791128754615784 at epoch: 11 and batch_num: 529\n",
      "Loss of train set: 0.3621371388435364 at epoch: 11 and batch_num: 530\n",
      "Loss of train set: 0.20049670338630676 at epoch: 11 and batch_num: 531\n",
      "Loss of train set: 0.4907495379447937 at epoch: 11 and batch_num: 532\n",
      "Loss of train set: 0.37684810161590576 at epoch: 11 and batch_num: 533\n",
      "Loss of train set: 0.44533929228782654 at epoch: 11 and batch_num: 534\n",
      "Loss of train set: 0.41373851895332336 at epoch: 11 and batch_num: 535\n",
      "Loss of train set: 0.4572180509567261 at epoch: 11 and batch_num: 536\n",
      "Loss of train set: 0.3007034659385681 at epoch: 11 and batch_num: 537\n",
      "Loss of train set: 0.20126216113567352 at epoch: 11 and batch_num: 538\n",
      "Loss of train set: 0.4589206576347351 at epoch: 11 and batch_num: 539\n",
      "Loss of train set: 0.3309629261493683 at epoch: 11 and batch_num: 540\n",
      "Loss of train set: 0.4084789752960205 at epoch: 11 and batch_num: 541\n",
      "Loss of train set: 0.2541131377220154 at epoch: 11 and batch_num: 542\n",
      "Loss of train set: 0.3618512749671936 at epoch: 11 and batch_num: 543\n",
      "Loss of train set: 0.37374401092529297 at epoch: 11 and batch_num: 544\n",
      "Loss of train set: 0.32047852873802185 at epoch: 11 and batch_num: 545\n",
      "Loss of train set: 0.32969528436660767 at epoch: 11 and batch_num: 546\n",
      "Loss of train set: 0.33356720209121704 at epoch: 11 and batch_num: 547\n",
      "Loss of train set: 0.45654916763305664 at epoch: 11 and batch_num: 548\n",
      "Loss of train set: 0.46107620000839233 at epoch: 11 and batch_num: 549\n",
      "Loss of train set: 0.22658571600914001 at epoch: 11 and batch_num: 550\n",
      "Loss of train set: 0.4098004698753357 at epoch: 11 and batch_num: 551\n",
      "Loss of train set: 0.341598242521286 at epoch: 11 and batch_num: 552\n",
      "Loss of train set: 0.4372856318950653 at epoch: 11 and batch_num: 553\n",
      "Loss of train set: 0.28877121210098267 at epoch: 11 and batch_num: 554\n",
      "Loss of train set: 0.33867499232292175 at epoch: 11 and batch_num: 555\n",
      "Loss of train set: 0.3252565860748291 at epoch: 11 and batch_num: 556\n",
      "Loss of train set: 0.31326520442962646 at epoch: 11 and batch_num: 557\n",
      "Loss of train set: 0.24564293026924133 at epoch: 11 and batch_num: 558\n",
      "Loss of train set: 0.17963430285453796 at epoch: 11 and batch_num: 559\n",
      "Loss of train set: 0.1837756335735321 at epoch: 11 and batch_num: 560\n",
      "Loss of train set: 0.22551633417606354 at epoch: 11 and batch_num: 561\n",
      "Loss of train set: 0.23749850690364838 at epoch: 11 and batch_num: 562\n",
      "Loss of train set: 0.1935676783323288 at epoch: 11 and batch_num: 563\n",
      "Loss of train set: 0.48368409276008606 at epoch: 11 and batch_num: 564\n",
      "Loss of train set: 0.15190191566944122 at epoch: 11 and batch_num: 565\n",
      "Loss of train set: 0.3327486515045166 at epoch: 11 and batch_num: 566\n",
      "Loss of train set: 0.49545082449913025 at epoch: 11 and batch_num: 567\n",
      "Loss of train set: 0.2629574239253998 at epoch: 11 and batch_num: 568\n",
      "Loss of train set: 0.3048848509788513 at epoch: 11 and batch_num: 569\n",
      "Loss of train set: 0.21198919415473938 at epoch: 11 and batch_num: 570\n",
      "Loss of train set: 0.25604185461997986 at epoch: 11 and batch_num: 571\n",
      "Loss of train set: 0.2994322180747986 at epoch: 11 and batch_num: 572\n",
      "Loss of train set: 0.2887381315231323 at epoch: 11 and batch_num: 573\n",
      "Loss of train set: 0.34636497497558594 at epoch: 11 and batch_num: 574\n",
      "Loss of train set: 0.21456588804721832 at epoch: 11 and batch_num: 575\n",
      "Loss of train set: 0.40173274278640747 at epoch: 11 and batch_num: 576\n",
      "Loss of train set: 0.2561500072479248 at epoch: 11 and batch_num: 577\n",
      "Loss of train set: 0.39326876401901245 at epoch: 11 and batch_num: 578\n",
      "Loss of train set: 0.2828919589519501 at epoch: 11 and batch_num: 579\n",
      "Loss of train set: 0.21507853269577026 at epoch: 11 and batch_num: 580\n",
      "Loss of train set: 0.31875449419021606 at epoch: 11 and batch_num: 581\n",
      "Loss of train set: 0.2592637240886688 at epoch: 11 and batch_num: 582\n",
      "Loss of train set: 0.2603621482849121 at epoch: 11 and batch_num: 583\n",
      "Loss of train set: 0.28129592537879944 at epoch: 11 and batch_num: 584\n",
      "Loss of train set: 0.21895597875118256 at epoch: 11 and batch_num: 585\n",
      "Loss of train set: 0.33896708488464355 at epoch: 11 and batch_num: 586\n",
      "Loss of train set: 0.5816222429275513 at epoch: 11 and batch_num: 587\n",
      "Loss of train set: 0.25710391998291016 at epoch: 11 and batch_num: 588\n",
      "Loss of train set: 0.2579072415828705 at epoch: 11 and batch_num: 589\n",
      "Loss of train set: 0.3516031503677368 at epoch: 11 and batch_num: 590\n",
      "Loss of train set: 0.4349760413169861 at epoch: 11 and batch_num: 591\n",
      "Loss of train set: 0.2701108157634735 at epoch: 11 and batch_num: 592\n",
      "Loss of train set: 0.5759508609771729 at epoch: 11 and batch_num: 593\n",
      "Loss of train set: 0.26544755697250366 at epoch: 11 and batch_num: 594\n",
      "Loss of train set: 0.40794605016708374 at epoch: 11 and batch_num: 595\n",
      "Loss of train set: 0.3989572525024414 at epoch: 11 and batch_num: 596\n",
      "Loss of train set: 0.36389264464378357 at epoch: 11 and batch_num: 597\n",
      "Loss of train set: 0.5467218160629272 at epoch: 11 and batch_num: 598\n",
      "Loss of train set: 0.24089716374874115 at epoch: 11 and batch_num: 599\n",
      "Loss of train set: 0.13850919902324677 at epoch: 11 and batch_num: 600\n",
      "Loss of train set: 0.2720786929130554 at epoch: 11 and batch_num: 601\n",
      "Loss of train set: 0.411891907453537 at epoch: 11 and batch_num: 602\n",
      "Loss of train set: 0.20335382223129272 at epoch: 11 and batch_num: 603\n",
      "Loss of train set: 0.24348603188991547 at epoch: 11 and batch_num: 604\n",
      "Loss of train set: 0.20138055086135864 at epoch: 11 and batch_num: 605\n",
      "Loss of train set: 0.3189864754676819 at epoch: 11 and batch_num: 606\n",
      "Loss of train set: 0.22250020503997803 at epoch: 11 and batch_num: 607\n",
      "Loss of train set: 0.2778010964393616 at epoch: 11 and batch_num: 608\n",
      "Loss of train set: 0.3802606463432312 at epoch: 11 and batch_num: 609\n",
      "Loss of train set: 0.24711471796035767 at epoch: 11 and batch_num: 610\n",
      "Loss of train set: 0.37334832549095154 at epoch: 11 and batch_num: 611\n",
      "Loss of train set: 0.4532609283924103 at epoch: 11 and batch_num: 612\n",
      "Loss of train set: 0.31968751549720764 at epoch: 11 and batch_num: 613\n",
      "Loss of train set: 0.2946152687072754 at epoch: 11 and batch_num: 614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.31701335310935974 at epoch: 11 and batch_num: 615\n",
      "Loss of train set: 0.2979942858219147 at epoch: 11 and batch_num: 616\n",
      "Loss of train set: 0.21030202507972717 at epoch: 11 and batch_num: 617\n",
      "Loss of train set: 0.38847631216049194 at epoch: 11 and batch_num: 618\n",
      "Loss of train set: 0.32396602630615234 at epoch: 11 and batch_num: 619\n",
      "Loss of train set: 0.3084287941455841 at epoch: 11 and batch_num: 620\n",
      "Loss of train set: 0.3216763138771057 at epoch: 11 and batch_num: 621\n",
      "Loss of train set: 0.2632083296775818 at epoch: 11 and batch_num: 622\n",
      "Loss of train set: 0.30738571286201477 at epoch: 11 and batch_num: 623\n",
      "Loss of train set: 0.2806723117828369 at epoch: 11 and batch_num: 624\n",
      "Loss of train set: 0.15543772280216217 at epoch: 11 and batch_num: 625\n",
      "Loss of train set: 0.6554181575775146 at epoch: 11 and batch_num: 626\n",
      "Loss of train set: 0.3039397597312927 at epoch: 11 and batch_num: 627\n",
      "Loss of train set: 0.2381812334060669 at epoch: 11 and batch_num: 628\n",
      "Loss of train set: 0.36636799573898315 at epoch: 11 and batch_num: 629\n",
      "Loss of train set: 0.3789824843406677 at epoch: 11 and batch_num: 630\n",
      "Loss of train set: 0.3597573935985565 at epoch: 11 and batch_num: 631\n",
      "Loss of train set: 0.3259086012840271 at epoch: 11 and batch_num: 632\n",
      "Loss of train set: 0.30546361207962036 at epoch: 11 and batch_num: 633\n",
      "Loss of train set: 0.3295304477214813 at epoch: 11 and batch_num: 634\n",
      "Loss of train set: 0.30598628520965576 at epoch: 11 and batch_num: 635\n",
      "Loss of train set: 0.20745849609375 at epoch: 11 and batch_num: 636\n",
      "Loss of train set: 0.47298771142959595 at epoch: 11 and batch_num: 637\n",
      "Loss of train set: 0.30942076444625854 at epoch: 11 and batch_num: 638\n",
      "Loss of train set: 0.25123074650764465 at epoch: 11 and batch_num: 639\n",
      "Loss of train set: 0.4651961326599121 at epoch: 11 and batch_num: 640\n",
      "Loss of train set: 0.3326878845691681 at epoch: 11 and batch_num: 641\n",
      "Loss of train set: 0.208777517080307 at epoch: 11 and batch_num: 642\n",
      "Loss of train set: 0.20745748281478882 at epoch: 11 and batch_num: 643\n",
      "Loss of train set: 0.33952316641807556 at epoch: 11 and batch_num: 644\n",
      "Loss of train set: 0.2848711609840393 at epoch: 11 and batch_num: 645\n",
      "Loss of train set: 0.28964051604270935 at epoch: 11 and batch_num: 646\n",
      "Loss of train set: 0.5415419340133667 at epoch: 11 and batch_num: 647\n",
      "Loss of train set: 0.4611964523792267 at epoch: 11 and batch_num: 648\n",
      "Loss of train set: 0.31909579038619995 at epoch: 11 and batch_num: 649\n",
      "Loss of train set: 0.43219202756881714 at epoch: 11 and batch_num: 650\n",
      "Loss of train set: 0.359618604183197 at epoch: 11 and batch_num: 651\n",
      "Loss of train set: 0.32488009333610535 at epoch: 11 and batch_num: 652\n",
      "Loss of train set: 0.20346319675445557 at epoch: 11 and batch_num: 653\n",
      "Loss of train set: 0.2899986505508423 at epoch: 11 and batch_num: 654\n",
      "Loss of train set: 0.3434431850910187 at epoch: 11 and batch_num: 655\n",
      "Loss of train set: 0.4857291579246521 at epoch: 11 and batch_num: 656\n",
      "Loss of train set: 0.21676763892173767 at epoch: 11 and batch_num: 657\n",
      "Loss of train set: 0.1806272715330124 at epoch: 11 and batch_num: 658\n",
      "Loss of train set: 0.31534117460250854 at epoch: 11 and batch_num: 659\n",
      "Loss of train set: 0.5443029403686523 at epoch: 11 and batch_num: 660\n",
      "Loss of train set: 0.34070688486099243 at epoch: 11 and batch_num: 661\n",
      "Loss of train set: 0.17514431476593018 at epoch: 11 and batch_num: 662\n",
      "Loss of train set: 0.32535335421562195 at epoch: 11 and batch_num: 663\n",
      "Loss of train set: 0.2716268002986908 at epoch: 11 and batch_num: 664\n",
      "Loss of train set: 0.3321529030799866 at epoch: 11 and batch_num: 665\n",
      "Loss of train set: 0.19271543622016907 at epoch: 11 and batch_num: 666\n",
      "Loss of train set: 0.3713003993034363 at epoch: 11 and batch_num: 667\n",
      "Loss of train set: 0.3336735963821411 at epoch: 11 and batch_num: 668\n",
      "Loss of train set: 0.3252873122692108 at epoch: 11 and batch_num: 669\n",
      "Loss of train set: 0.27923136949539185 at epoch: 11 and batch_num: 670\n",
      "Loss of train set: 0.1973627209663391 at epoch: 11 and batch_num: 671\n",
      "Loss of train set: 0.14763441681861877 at epoch: 11 and batch_num: 672\n",
      "Loss of train set: 0.44079136848449707 at epoch: 11 and batch_num: 673\n",
      "Loss of train set: 0.43606579303741455 at epoch: 11 and batch_num: 674\n",
      "Loss of train set: 0.23838573694229126 at epoch: 11 and batch_num: 675\n",
      "Loss of train set: 0.16865336894989014 at epoch: 11 and batch_num: 676\n",
      "Loss of train set: 0.2388567179441452 at epoch: 11 and batch_num: 677\n",
      "Loss of train set: 0.2960498332977295 at epoch: 11 and batch_num: 678\n",
      "Loss of train set: 0.2812371253967285 at epoch: 11 and batch_num: 679\n",
      "Loss of train set: 0.3106023073196411 at epoch: 11 and batch_num: 680\n",
      "Loss of train set: 0.3280442953109741 at epoch: 11 and batch_num: 681\n",
      "Loss of train set: 0.3443900942802429 at epoch: 11 and batch_num: 682\n",
      "Loss of train set: 0.3326603174209595 at epoch: 11 and batch_num: 683\n",
      "Loss of train set: 0.15524718165397644 at epoch: 11 and batch_num: 684\n",
      "Loss of train set: 0.3359997272491455 at epoch: 11 and batch_num: 685\n",
      "Loss of train set: 0.2526666522026062 at epoch: 11 and batch_num: 686\n",
      "Loss of train set: 0.3622630536556244 at epoch: 11 and batch_num: 687\n",
      "Loss of train set: 0.2577807307243347 at epoch: 11 and batch_num: 688\n",
      "Loss of train set: 0.3621710538864136 at epoch: 11 and batch_num: 689\n",
      "Loss of train set: 0.3102961778640747 at epoch: 11 and batch_num: 690\n",
      "Loss of train set: 0.29344311356544495 at epoch: 11 and batch_num: 691\n",
      "Loss of train set: 0.25621211528778076 at epoch: 11 and batch_num: 692\n",
      "Loss of train set: 0.14166831970214844 at epoch: 11 and batch_num: 693\n",
      "Loss of train set: 0.3455249071121216 at epoch: 11 and batch_num: 694\n",
      "Loss of train set: 0.27699121832847595 at epoch: 11 and batch_num: 695\n",
      "Loss of train set: 0.410358190536499 at epoch: 11 and batch_num: 696\n",
      "Loss of train set: 0.2669721841812134 at epoch: 11 and batch_num: 697\n",
      "Loss of train set: 0.24223566055297852 at epoch: 11 and batch_num: 698\n",
      "Loss of train set: 0.22730253636837006 at epoch: 11 and batch_num: 699\n",
      "Loss of train set: 0.30205315351486206 at epoch: 11 and batch_num: 700\n",
      "Loss of train set: 0.3793575167655945 at epoch: 11 and batch_num: 701\n",
      "Loss of train set: 0.2396688014268875 at epoch: 11 and batch_num: 702\n",
      "Loss of train set: 0.18904072046279907 at epoch: 11 and batch_num: 703\n",
      "Loss of train set: 0.256412148475647 at epoch: 11 and batch_num: 704\n",
      "Loss of train set: 0.31791335344314575 at epoch: 11 and batch_num: 705\n",
      "Loss of train set: 0.40030407905578613 at epoch: 11 and batch_num: 706\n",
      "Loss of train set: 0.5185253620147705 at epoch: 11 and batch_num: 707\n",
      "Loss of train set: 0.3319525718688965 at epoch: 11 and batch_num: 708\n",
      "Loss of train set: 0.25691312551498413 at epoch: 11 and batch_num: 709\n",
      "Loss of train set: 0.30298060178756714 at epoch: 11 and batch_num: 710\n",
      "Loss of train set: 0.2707904875278473 at epoch: 11 and batch_num: 711\n",
      "Loss of train set: 0.3617013692855835 at epoch: 11 and batch_num: 712\n",
      "Loss of train set: 0.23821337521076202 at epoch: 11 and batch_num: 713\n",
      "Loss of train set: 0.1826893389225006 at epoch: 11 and batch_num: 714\n",
      "Loss of train set: 0.3169257938861847 at epoch: 11 and batch_num: 715\n",
      "Loss of train set: 0.43649131059646606 at epoch: 11 and batch_num: 716\n",
      "Loss of train set: 0.29974421858787537 at epoch: 11 and batch_num: 717\n",
      "Loss of train set: 0.25333160161972046 at epoch: 11 and batch_num: 718\n",
      "Loss of train set: 0.31201356649398804 at epoch: 11 and batch_num: 719\n",
      "Loss of train set: 0.16226103901863098 at epoch: 11 and batch_num: 720\n",
      "Loss of train set: 0.4137871265411377 at epoch: 11 and batch_num: 721\n",
      "Loss of train set: 0.3257580101490021 at epoch: 11 and batch_num: 722\n",
      "Loss of train set: 0.3422897756099701 at epoch: 11 and batch_num: 723\n",
      "Loss of train set: 0.2600587010383606 at epoch: 11 and batch_num: 724\n",
      "Loss of train set: 0.2224709391593933 at epoch: 11 and batch_num: 725\n",
      "Loss of train set: 0.5137072205543518 at epoch: 11 and batch_num: 726\n",
      "Loss of train set: 0.2339172661304474 at epoch: 11 and batch_num: 727\n",
      "Loss of train set: 0.44998395442962646 at epoch: 11 and batch_num: 728\n",
      "Loss of train set: 0.27218735218048096 at epoch: 11 and batch_num: 729\n",
      "Loss of train set: 0.31600916385650635 at epoch: 11 and batch_num: 730\n",
      "Loss of train set: 0.2943023443222046 at epoch: 11 and batch_num: 731\n",
      "Loss of train set: 0.34409040212631226 at epoch: 11 and batch_num: 732\n",
      "Loss of train set: 0.35131973028182983 at epoch: 11 and batch_num: 733\n",
      "Loss of train set: 0.32689687609672546 at epoch: 11 and batch_num: 734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2329152524471283 at epoch: 11 and batch_num: 735\n",
      "Loss of train set: 0.308315247297287 at epoch: 11 and batch_num: 736\n",
      "Loss of train set: 0.4126480221748352 at epoch: 11 and batch_num: 737\n",
      "Loss of train set: 0.3790575861930847 at epoch: 11 and batch_num: 738\n",
      "Loss of train set: 0.25996482372283936 at epoch: 11 and batch_num: 739\n",
      "Loss of train set: 0.2990114688873291 at epoch: 11 and batch_num: 740\n",
      "Loss of train set: 0.3145141005516052 at epoch: 11 and batch_num: 741\n",
      "Loss of train set: 0.36173921823501587 at epoch: 11 and batch_num: 742\n",
      "Loss of train set: 0.2342694252729416 at epoch: 11 and batch_num: 743\n",
      "Loss of train set: 0.2703366279602051 at epoch: 11 and batch_num: 744\n",
      "Loss of train set: 0.35709166526794434 at epoch: 11 and batch_num: 745\n",
      "Loss of train set: 0.4458276927471161 at epoch: 11 and batch_num: 746\n",
      "Loss of train set: 0.3523387014865875 at epoch: 11 and batch_num: 747\n",
      "Loss of train set: 0.3971748650074005 at epoch: 11 and batch_num: 748\n",
      "Loss of train set: 0.2982666492462158 at epoch: 11 and batch_num: 749\n",
      "Loss of train set: 0.2639678418636322 at epoch: 11 and batch_num: 750\n",
      "Loss of train set: 0.2563101649284363 at epoch: 11 and batch_num: 751\n",
      "Loss of train set: 0.3409801423549652 at epoch: 11 and batch_num: 752\n",
      "Loss of train set: 0.5193450450897217 at epoch: 11 and batch_num: 753\n",
      "Loss of train set: 0.31692367792129517 at epoch: 11 and batch_num: 754\n",
      "Loss of train set: 0.3221358060836792 at epoch: 11 and batch_num: 755\n",
      "Loss of train set: 0.22187864780426025 at epoch: 11 and batch_num: 756\n",
      "Loss of train set: 0.2746328115463257 at epoch: 11 and batch_num: 757\n",
      "Loss of train set: 0.20220841467380524 at epoch: 11 and batch_num: 758\n",
      "Loss of train set: 0.21350203454494476 at epoch: 11 and batch_num: 759\n",
      "Loss of train set: 0.3195033669471741 at epoch: 11 and batch_num: 760\n",
      "Loss of train set: 0.29069793224334717 at epoch: 11 and batch_num: 761\n",
      "Loss of train set: 0.31536537408828735 at epoch: 11 and batch_num: 762\n",
      "Loss of train set: 0.19129328429698944 at epoch: 11 and batch_num: 763\n",
      "Loss of train set: 0.23350296914577484 at epoch: 11 and batch_num: 764\n",
      "Loss of train set: 0.27038949728012085 at epoch: 11 and batch_num: 765\n",
      "Loss of train set: 0.4322618842124939 at epoch: 11 and batch_num: 766\n",
      "Loss of train set: 0.23104777932167053 at epoch: 11 and batch_num: 767\n",
      "Loss of train set: 0.29862311482429504 at epoch: 11 and batch_num: 768\n",
      "Loss of train set: 0.2970089912414551 at epoch: 11 and batch_num: 769\n",
      "Loss of train set: 0.3158921003341675 at epoch: 11 and batch_num: 770\n",
      "Loss of train set: 0.3358733057975769 at epoch: 11 and batch_num: 771\n",
      "Loss of train set: 0.27697789669036865 at epoch: 11 and batch_num: 772\n",
      "Loss of train set: 0.2632494568824768 at epoch: 11 and batch_num: 773\n",
      "Loss of train set: 0.26708173751831055 at epoch: 11 and batch_num: 774\n",
      "Loss of train set: 0.28601962327957153 at epoch: 11 and batch_num: 775\n",
      "Loss of train set: 0.1901392638683319 at epoch: 11 and batch_num: 776\n",
      "Loss of train set: 0.19953587651252747 at epoch: 11 and batch_num: 777\n",
      "Loss of train set: 0.6014791131019592 at epoch: 11 and batch_num: 778\n",
      "Loss of train set: 0.20619474351406097 at epoch: 11 and batch_num: 779\n",
      "Loss of train set: 0.3152282238006592 at epoch: 11 and batch_num: 780\n",
      "Loss of train set: 0.3360636532306671 at epoch: 11 and batch_num: 781\n",
      "Loss of train set: 0.28847527503967285 at epoch: 11 and batch_num: 782\n",
      "Loss of train set: 0.2443092167377472 at epoch: 11 and batch_num: 783\n",
      "Loss of train set: 0.27187153697013855 at epoch: 11 and batch_num: 784\n",
      "Loss of train set: 0.3578367829322815 at epoch: 11 and batch_num: 785\n",
      "Loss of train set: 0.2576742172241211 at epoch: 11 and batch_num: 786\n",
      "Loss of train set: 0.2665671110153198 at epoch: 11 and batch_num: 787\n",
      "Loss of train set: 0.29537609219551086 at epoch: 11 and batch_num: 788\n",
      "Loss of train set: 0.4996369779109955 at epoch: 11 and batch_num: 789\n",
      "Loss of train set: 0.18825426697731018 at epoch: 11 and batch_num: 790\n",
      "Loss of train set: 0.3880124092102051 at epoch: 11 and batch_num: 791\n",
      "Loss of train set: 0.2924220860004425 at epoch: 11 and batch_num: 792\n",
      "Loss of train set: 0.383009672164917 at epoch: 11 and batch_num: 793\n",
      "Loss of train set: 0.6573432683944702 at epoch: 11 and batch_num: 794\n",
      "Loss of train set: 0.22773179411888123 at epoch: 11 and batch_num: 795\n",
      "Loss of train set: 0.20660150051116943 at epoch: 11 and batch_num: 796\n",
      "Loss of train set: 0.1802523136138916 at epoch: 11 and batch_num: 797\n",
      "Loss of train set: 0.5456233024597168 at epoch: 11 and batch_num: 798\n",
      "Loss of train set: 0.38330674171447754 at epoch: 11 and batch_num: 799\n",
      "Loss of train set: 0.3617675304412842 at epoch: 11 and batch_num: 800\n",
      "Loss of train set: 0.37367019057273865 at epoch: 11 and batch_num: 801\n",
      "Loss of train set: 0.2296738624572754 at epoch: 11 and batch_num: 802\n",
      "Loss of train set: 0.29755353927612305 at epoch: 11 and batch_num: 803\n",
      "Loss of train set: 0.41963621973991394 at epoch: 11 and batch_num: 804\n",
      "Loss of train set: 0.4663339853286743 at epoch: 11 and batch_num: 805\n",
      "Loss of train set: 0.38230693340301514 at epoch: 11 and batch_num: 806\n",
      "Loss of train set: 0.39325129985809326 at epoch: 11 and batch_num: 807\n",
      "Loss of train set: 0.2882171869277954 at epoch: 11 and batch_num: 808\n",
      "Loss of train set: 0.35766318440437317 at epoch: 11 and batch_num: 809\n",
      "Loss of train set: 0.35830068588256836 at epoch: 11 and batch_num: 810\n",
      "Loss of train set: 0.5260303616523743 at epoch: 11 and batch_num: 811\n",
      "Loss of train set: 0.274247944355011 at epoch: 11 and batch_num: 812\n",
      "Loss of train set: 0.4016207456588745 at epoch: 11 and batch_num: 813\n",
      "Loss of train set: 0.3364834785461426 at epoch: 11 and batch_num: 814\n",
      "Loss of train set: 0.3154306411743164 at epoch: 11 and batch_num: 815\n",
      "Loss of train set: 0.3047181963920593 at epoch: 11 and batch_num: 816\n",
      "Loss of train set: 0.31573963165283203 at epoch: 11 and batch_num: 817\n",
      "Loss of train set: 0.298958420753479 at epoch: 11 and batch_num: 818\n",
      "Loss of train set: 0.2234974205493927 at epoch: 11 and batch_num: 819\n",
      "Loss of train set: 0.3166923522949219 at epoch: 11 and batch_num: 820\n",
      "Loss of train set: 0.43573009967803955 at epoch: 11 and batch_num: 821\n",
      "Loss of train set: 0.3612763583660126 at epoch: 11 and batch_num: 822\n",
      "Loss of train set: 0.2235649824142456 at epoch: 11 and batch_num: 823\n",
      "Loss of train set: 0.24970529973506927 at epoch: 11 and batch_num: 824\n",
      "Loss of train set: 0.3230631351470947 at epoch: 11 and batch_num: 825\n",
      "Loss of train set: 0.271184504032135 at epoch: 11 and batch_num: 826\n",
      "Loss of train set: 0.28745269775390625 at epoch: 11 and batch_num: 827\n",
      "Loss of train set: 0.32428449392318726 at epoch: 11 and batch_num: 828\n",
      "Loss of train set: 0.3562440872192383 at epoch: 11 and batch_num: 829\n",
      "Loss of train set: 0.3705751895904541 at epoch: 11 and batch_num: 830\n",
      "Loss of train set: 0.21313944458961487 at epoch: 11 and batch_num: 831\n",
      "Loss of train set: 0.31024980545043945 at epoch: 11 and batch_num: 832\n",
      "Loss of train set: 0.234103262424469 at epoch: 11 and batch_num: 833\n",
      "Loss of train set: 0.25332334637641907 at epoch: 11 and batch_num: 834\n",
      "Loss of train set: 0.18177710473537445 at epoch: 11 and batch_num: 835\n",
      "Loss of train set: 0.3468184173107147 at epoch: 11 and batch_num: 836\n",
      "Loss of train set: 0.18425695598125458 at epoch: 11 and batch_num: 837\n",
      "Loss of train set: 0.24367493391036987 at epoch: 11 and batch_num: 838\n",
      "Loss of train set: 0.42451465129852295 at epoch: 11 and batch_num: 839\n",
      "Loss of train set: 0.30544519424438477 at epoch: 11 and batch_num: 840\n",
      "Loss of train set: 0.32365095615386963 at epoch: 11 and batch_num: 841\n",
      "Loss of train set: 0.33557945489883423 at epoch: 11 and batch_num: 842\n",
      "Loss of train set: 0.27091845870018005 at epoch: 11 and batch_num: 843\n",
      "Loss of train set: 0.1950843632221222 at epoch: 11 and batch_num: 844\n",
      "Loss of train set: 0.2961433529853821 at epoch: 11 and batch_num: 845\n",
      "Loss of train set: 0.28393495082855225 at epoch: 11 and batch_num: 846\n",
      "Loss of train set: 0.4336729049682617 at epoch: 11 and batch_num: 847\n",
      "Loss of train set: 0.29760169982910156 at epoch: 11 and batch_num: 848\n",
      "Loss of train set: 0.2955833673477173 at epoch: 11 and batch_num: 849\n",
      "Loss of train set: 0.22974693775177002 at epoch: 11 and batch_num: 850\n",
      "Loss of train set: 0.1946784406900406 at epoch: 11 and batch_num: 851\n",
      "Loss of train set: 0.2683509290218353 at epoch: 11 and batch_num: 852\n",
      "Loss of train set: 0.32315921783447266 at epoch: 11 and batch_num: 853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.42830395698547363 at epoch: 11 and batch_num: 854\n",
      "Loss of train set: 0.29398617148399353 at epoch: 11 and batch_num: 855\n",
      "Loss of train set: 0.32332751154899597 at epoch: 11 and batch_num: 856\n",
      "Loss of train set: 0.23743154108524323 at epoch: 11 and batch_num: 857\n",
      "Loss of train set: 0.3524421453475952 at epoch: 11 and batch_num: 858\n",
      "Loss of train set: 0.359719455242157 at epoch: 11 and batch_num: 859\n",
      "Loss of train set: 0.5934972763061523 at epoch: 11 and batch_num: 860\n",
      "Loss of train set: 0.3023022413253784 at epoch: 11 and batch_num: 861\n",
      "Loss of train set: 0.38407081365585327 at epoch: 11 and batch_num: 862\n",
      "Loss of train set: 0.3513082265853882 at epoch: 11 and batch_num: 863\n",
      "Loss of train set: 0.27225759625434875 at epoch: 11 and batch_num: 864\n",
      "Loss of train set: 0.25208261609077454 at epoch: 11 and batch_num: 865\n",
      "Loss of train set: 0.4156651496887207 at epoch: 11 and batch_num: 866\n",
      "Loss of train set: 0.39983487129211426 at epoch: 11 and batch_num: 867\n",
      "Loss of train set: 0.3098958134651184 at epoch: 11 and batch_num: 868\n",
      "Loss of train set: 0.5071167349815369 at epoch: 11 and batch_num: 869\n",
      "Loss of train set: 0.40699219703674316 at epoch: 11 and batch_num: 870\n",
      "Loss of train set: 0.30798065662384033 at epoch: 11 and batch_num: 871\n",
      "Loss of train set: 0.2188216596841812 at epoch: 11 and batch_num: 872\n",
      "Loss of train set: 0.29832908511161804 at epoch: 11 and batch_num: 873\n",
      "Loss of train set: 0.256372332572937 at epoch: 11 and batch_num: 874\n",
      "Loss of train set: 0.2032860517501831 at epoch: 11 and batch_num: 875\n",
      "Loss of train set: 0.2009754627943039 at epoch: 11 and batch_num: 876\n",
      "Loss of train set: 0.4341317415237427 at epoch: 11 and batch_num: 877\n",
      "Loss of train set: 0.2557930648326874 at epoch: 11 and batch_num: 878\n",
      "Loss of train set: 0.3309330344200134 at epoch: 11 and batch_num: 879\n",
      "Loss of train set: 0.35534417629241943 at epoch: 11 and batch_num: 880\n",
      "Loss of train set: 0.41367220878601074 at epoch: 11 and batch_num: 881\n",
      "Loss of train set: 0.20211447775363922 at epoch: 11 and batch_num: 882\n",
      "Loss of train set: 0.21731549501419067 at epoch: 11 and batch_num: 883\n",
      "Loss of train set: 0.4134257733821869 at epoch: 11 and batch_num: 884\n",
      "Loss of train set: 0.27334892749786377 at epoch: 11 and batch_num: 885\n",
      "Loss of train set: 0.3306691646575928 at epoch: 11 and batch_num: 886\n",
      "Loss of train set: 0.3611786663532257 at epoch: 11 and batch_num: 887\n",
      "Loss of train set: 0.3113793134689331 at epoch: 11 and batch_num: 888\n",
      "Loss of train set: 0.2605641484260559 at epoch: 11 and batch_num: 889\n",
      "Loss of train set: 0.33773183822631836 at epoch: 11 and batch_num: 890\n",
      "Loss of train set: 0.2342175841331482 at epoch: 11 and batch_num: 891\n",
      "Loss of train set: 0.25218522548675537 at epoch: 11 and batch_num: 892\n",
      "Loss of train set: 0.2699381113052368 at epoch: 11 and batch_num: 893\n",
      "Loss of train set: 0.27537035942077637 at epoch: 11 and batch_num: 894\n",
      "Loss of train set: 0.19602036476135254 at epoch: 11 and batch_num: 895\n",
      "Loss of train set: 0.3360104262828827 at epoch: 11 and batch_num: 896\n",
      "Loss of train set: 0.5194776058197021 at epoch: 11 and batch_num: 897\n",
      "Loss of train set: 0.3974984288215637 at epoch: 11 and batch_num: 898\n",
      "Loss of train set: 0.35092538595199585 at epoch: 11 and batch_num: 899\n",
      "Loss of train set: 0.40330442786216736 at epoch: 11 and batch_num: 900\n",
      "Loss of train set: 0.5990557670593262 at epoch: 11 and batch_num: 901\n",
      "Loss of train set: 0.296748548746109 at epoch: 11 and batch_num: 902\n",
      "Loss of train set: 0.1951388120651245 at epoch: 11 and batch_num: 903\n",
      "Loss of train set: 0.309359610080719 at epoch: 11 and batch_num: 904\n",
      "Loss of train set: 0.36111605167388916 at epoch: 11 and batch_num: 905\n",
      "Loss of train set: 0.3012988865375519 at epoch: 11 and batch_num: 906\n",
      "Loss of train set: 0.2883477210998535 at epoch: 11 and batch_num: 907\n",
      "Loss of train set: 0.24167948961257935 at epoch: 11 and batch_num: 908\n",
      "Loss of train set: 0.20385205745697021 at epoch: 11 and batch_num: 909\n",
      "Loss of train set: 0.5336565971374512 at epoch: 11 and batch_num: 910\n",
      "Loss of train set: 0.3013508915901184 at epoch: 11 and batch_num: 911\n",
      "Loss of train set: 0.417503297328949 at epoch: 11 and batch_num: 912\n",
      "Loss of train set: 0.3588348925113678 at epoch: 11 and batch_num: 913\n",
      "Loss of train set: 0.37643420696258545 at epoch: 11 and batch_num: 914\n",
      "Loss of train set: 0.30503514409065247 at epoch: 11 and batch_num: 915\n",
      "Loss of train set: 0.408258318901062 at epoch: 11 and batch_num: 916\n",
      "Loss of train set: 0.39271679520606995 at epoch: 11 and batch_num: 917\n",
      "Loss of train set: 0.45831045508384705 at epoch: 11 and batch_num: 918\n",
      "Loss of train set: 0.18249015510082245 at epoch: 11 and batch_num: 919\n",
      "Loss of train set: 0.29498091340065 at epoch: 11 and batch_num: 920\n",
      "Loss of train set: 0.2785359025001526 at epoch: 11 and batch_num: 921\n",
      "Loss of train set: 0.22369135916233063 at epoch: 11 and batch_num: 922\n",
      "Loss of train set: 0.325896680355072 at epoch: 11 and batch_num: 923\n",
      "Loss of train set: 0.2940651476383209 at epoch: 11 and batch_num: 924\n",
      "Loss of train set: 0.27667418122291565 at epoch: 11 and batch_num: 925\n",
      "Loss of train set: 0.30865368247032166 at epoch: 11 and batch_num: 926\n",
      "Loss of train set: 0.2986980378627777 at epoch: 11 and batch_num: 927\n",
      "Loss of train set: 0.2548472583293915 at epoch: 11 and batch_num: 928\n",
      "Loss of train set: 0.23224571347236633 at epoch: 11 and batch_num: 929\n",
      "Loss of train set: 0.2631361782550812 at epoch: 11 and batch_num: 930\n",
      "Loss of train set: 0.4725019335746765 at epoch: 11 and batch_num: 931\n",
      "Loss of train set: 0.4448312520980835 at epoch: 11 and batch_num: 932\n",
      "Loss of train set: 0.37307894229888916 at epoch: 11 and batch_num: 933\n",
      "Loss of train set: 0.32089412212371826 at epoch: 11 and batch_num: 934\n",
      "Loss of train set: 0.24522048234939575 at epoch: 11 and batch_num: 935\n",
      "Loss of train set: 0.23908662796020508 at epoch: 11 and batch_num: 936\n",
      "Loss of train set: 0.3421899378299713 at epoch: 11 and batch_num: 937\n",
      "Accuracy of train set: 0.8859666666666667\n",
      "Loss of test set: 0.47669973969459534 at epoch: 11 and batch_num: 0\n",
      "Loss of test set: 0.3099393844604492 at epoch: 11 and batch_num: 1\n",
      "Loss of test set: 0.43244290351867676 at epoch: 11 and batch_num: 2\n",
      "Loss of test set: 0.5449350476264954 at epoch: 11 and batch_num: 3\n",
      "Loss of test set: 0.5194514393806458 at epoch: 11 and batch_num: 4\n",
      "Loss of test set: 0.3312946557998657 at epoch: 11 and batch_num: 5\n",
      "Loss of test set: 0.6602668762207031 at epoch: 11 and batch_num: 6\n",
      "Loss of test set: 0.2203928530216217 at epoch: 11 and batch_num: 7\n",
      "Loss of test set: 0.3432394564151764 at epoch: 11 and batch_num: 8\n",
      "Loss of test set: 0.38940536975860596 at epoch: 11 and batch_num: 9\n",
      "Loss of test set: 0.4152008891105652 at epoch: 11 and batch_num: 10\n",
      "Loss of test set: 0.5358189344406128 at epoch: 11 and batch_num: 11\n",
      "Loss of test set: 0.3654062747955322 at epoch: 11 and batch_num: 12\n",
      "Loss of test set: 0.3207497000694275 at epoch: 11 and batch_num: 13\n",
      "Loss of test set: 0.3611777722835541 at epoch: 11 and batch_num: 14\n",
      "Loss of test set: 0.2762722373008728 at epoch: 11 and batch_num: 15\n",
      "Loss of test set: 0.35096949338912964 at epoch: 11 and batch_num: 16\n",
      "Loss of test set: 0.6524782180786133 at epoch: 11 and batch_num: 17\n",
      "Loss of test set: 0.5605484247207642 at epoch: 11 and batch_num: 18\n",
      "Loss of test set: 0.5102756023406982 at epoch: 11 and batch_num: 19\n",
      "Loss of test set: 0.5669771432876587 at epoch: 11 and batch_num: 20\n",
      "Loss of test set: 0.3652384281158447 at epoch: 11 and batch_num: 21\n",
      "Loss of test set: 0.4129610061645508 at epoch: 11 and batch_num: 22\n",
      "Loss of test set: 0.3567996919155121 at epoch: 11 and batch_num: 23\n",
      "Loss of test set: 0.22167590260505676 at epoch: 11 and batch_num: 24\n",
      "Loss of test set: 0.42151087522506714 at epoch: 11 and batch_num: 25\n",
      "Loss of test set: 0.3685382008552551 at epoch: 11 and batch_num: 26\n",
      "Loss of test set: 0.6360846161842346 at epoch: 11 and batch_num: 27\n",
      "Loss of test set: 0.2819236218929291 at epoch: 11 and batch_num: 28\n",
      "Loss of test set: 0.4639509320259094 at epoch: 11 and batch_num: 29\n",
      "Loss of test set: 0.4301421046257019 at epoch: 11 and batch_num: 30\n",
      "Loss of test set: 0.6062716841697693 at epoch: 11 and batch_num: 31\n",
      "Loss of test set: 0.2609245479106903 at epoch: 11 and batch_num: 32\n",
      "Loss of test set: 0.4545728266239166 at epoch: 11 and batch_num: 33\n",
      "Loss of test set: 0.325184166431427 at epoch: 11 and batch_num: 34\n",
      "Loss of test set: 0.3822404742240906 at epoch: 11 and batch_num: 35\n",
      "Loss of test set: 0.18981842696666718 at epoch: 11 and batch_num: 36\n",
      "Loss of test set: 0.2957046926021576 at epoch: 11 and batch_num: 37\n",
      "Loss of test set: 0.4701158106327057 at epoch: 11 and batch_num: 38\n",
      "Loss of test set: 0.3016727566719055 at epoch: 11 and batch_num: 39\n",
      "Loss of test set: 0.33979809284210205 at epoch: 11 and batch_num: 40\n",
      "Loss of test set: 0.2846291959285736 at epoch: 11 and batch_num: 41\n",
      "Loss of test set: 0.255639910697937 at epoch: 11 and batch_num: 42\n",
      "Loss of test set: 0.44466903805732727 at epoch: 11 and batch_num: 43\n",
      "Loss of test set: 0.4225285053253174 at epoch: 11 and batch_num: 44\n",
      "Loss of test set: 0.3662009835243225 at epoch: 11 and batch_num: 45\n",
      "Loss of test set: 0.4697186350822449 at epoch: 11 and batch_num: 46\n",
      "Loss of test set: 0.40940016508102417 at epoch: 11 and batch_num: 47\n",
      "Loss of test set: 0.3523709177970886 at epoch: 11 and batch_num: 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.42369964718818665 at epoch: 11 and batch_num: 49\n",
      "Loss of test set: 0.547305703163147 at epoch: 11 and batch_num: 50\n",
      "Loss of test set: 0.31543949246406555 at epoch: 11 and batch_num: 51\n",
      "Loss of test set: 0.4525020718574524 at epoch: 11 and batch_num: 52\n",
      "Loss of test set: 0.4373345971107483 at epoch: 11 and batch_num: 53\n",
      "Loss of test set: 0.33924686908721924 at epoch: 11 and batch_num: 54\n",
      "Loss of test set: 0.35728612542152405 at epoch: 11 and batch_num: 55\n",
      "Loss of test set: 0.506161093711853 at epoch: 11 and batch_num: 56\n",
      "Loss of test set: 0.42855724692344666 at epoch: 11 and batch_num: 57\n",
      "Loss of test set: 0.4208761751651764 at epoch: 11 and batch_num: 58\n",
      "Loss of test set: 0.37490716576576233 at epoch: 11 and batch_num: 59\n",
      "Loss of test set: 0.5415201187133789 at epoch: 11 and batch_num: 60\n",
      "Loss of test set: 0.26601943373680115 at epoch: 11 and batch_num: 61\n",
      "Loss of test set: 0.3700522184371948 at epoch: 11 and batch_num: 62\n",
      "Loss of test set: 0.549495279788971 at epoch: 11 and batch_num: 63\n",
      "Loss of test set: 0.4472806453704834 at epoch: 11 and batch_num: 64\n",
      "Loss of test set: 0.4936375319957733 at epoch: 11 and batch_num: 65\n",
      "Loss of test set: 0.4659821391105652 at epoch: 11 and batch_num: 66\n",
      "Loss of test set: 0.3915669918060303 at epoch: 11 and batch_num: 67\n",
      "Loss of test set: 0.4465577304363251 at epoch: 11 and batch_num: 68\n",
      "Loss of test set: 0.3690773844718933 at epoch: 11 and batch_num: 69\n",
      "Loss of test set: 0.31626075506210327 at epoch: 11 and batch_num: 70\n",
      "Loss of test set: 0.4608570337295532 at epoch: 11 and batch_num: 71\n",
      "Loss of test set: 0.29814252257347107 at epoch: 11 and batch_num: 72\n",
      "Loss of test set: 0.2753441333770752 at epoch: 11 and batch_num: 73\n",
      "Loss of test set: 0.5022491216659546 at epoch: 11 and batch_num: 74\n",
      "Loss of test set: 0.5100530385971069 at epoch: 11 and batch_num: 75\n",
      "Loss of test set: 0.44187861680984497 at epoch: 11 and batch_num: 76\n",
      "Loss of test set: 0.40108585357666016 at epoch: 11 and batch_num: 77\n",
      "Loss of test set: 0.49421921372413635 at epoch: 11 and batch_num: 78\n",
      "Loss of test set: 0.519859790802002 at epoch: 11 and batch_num: 79\n",
      "Loss of test set: 0.26906251907348633 at epoch: 11 and batch_num: 80\n",
      "Loss of test set: 0.442050039768219 at epoch: 11 and batch_num: 81\n",
      "Loss of test set: 0.21477051079273224 at epoch: 11 and batch_num: 82\n",
      "Loss of test set: 0.2870742082595825 at epoch: 11 and batch_num: 83\n",
      "Loss of test set: 0.35430413484573364 at epoch: 11 and batch_num: 84\n",
      "Loss of test set: 0.41830742359161377 at epoch: 11 and batch_num: 85\n",
      "Loss of test set: 0.4406365156173706 at epoch: 11 and batch_num: 86\n",
      "Loss of test set: 0.4029639959335327 at epoch: 11 and batch_num: 87\n",
      "Loss of test set: 0.5029385089874268 at epoch: 11 and batch_num: 88\n",
      "Loss of test set: 0.23161613941192627 at epoch: 11 and batch_num: 89\n",
      "Loss of test set: 0.49949511885643005 at epoch: 11 and batch_num: 90\n",
      "Loss of test set: 0.4700550436973572 at epoch: 11 and batch_num: 91\n",
      "Loss of test set: 0.3721568286418915 at epoch: 11 and batch_num: 92\n",
      "Loss of test set: 0.2590252161026001 at epoch: 11 and batch_num: 93\n",
      "Loss of test set: 0.432583212852478 at epoch: 11 and batch_num: 94\n",
      "Loss of test set: 0.33891811966896057 at epoch: 11 and batch_num: 95\n",
      "Loss of test set: 0.3419899344444275 at epoch: 11 and batch_num: 96\n",
      "Loss of test set: 0.4546794295310974 at epoch: 11 and batch_num: 97\n",
      "Loss of test set: 0.19113527238368988 at epoch: 11 and batch_num: 98\n",
      "Loss of test set: 0.45300349593162537 at epoch: 11 and batch_num: 99\n",
      "Loss of test set: 0.4525396525859833 at epoch: 11 and batch_num: 100\n",
      "Loss of test set: 0.45071256160736084 at epoch: 11 and batch_num: 101\n",
      "Loss of test set: 0.22666943073272705 at epoch: 11 and batch_num: 102\n",
      "Loss of test set: 0.26484906673431396 at epoch: 11 and batch_num: 103\n",
      "Loss of test set: 0.32737603783607483 at epoch: 11 and batch_num: 104\n",
      "Loss of test set: 0.37558141350746155 at epoch: 11 and batch_num: 105\n",
      "Loss of test set: 0.3067696690559387 at epoch: 11 and batch_num: 106\n",
      "Loss of test set: 0.39446574449539185 at epoch: 11 and batch_num: 107\n",
      "Loss of test set: 0.5229976177215576 at epoch: 11 and batch_num: 108\n",
      "Loss of test set: 0.37230074405670166 at epoch: 11 and batch_num: 109\n",
      "Loss of test set: 0.4366917312145233 at epoch: 11 and batch_num: 110\n",
      "Loss of test set: 0.38502931594848633 at epoch: 11 and batch_num: 111\n",
      "Loss of test set: 0.4215641915798187 at epoch: 11 and batch_num: 112\n",
      "Loss of test set: 0.33180052042007446 at epoch: 11 and batch_num: 113\n",
      "Loss of test set: 0.5097956657409668 at epoch: 11 and batch_num: 114\n",
      "Loss of test set: 0.3669232726097107 at epoch: 11 and batch_num: 115\n",
      "Loss of test set: 0.2686215341091156 at epoch: 11 and batch_num: 116\n",
      "Loss of test set: 0.419602632522583 at epoch: 11 and batch_num: 117\n",
      "Loss of test set: 0.4598577618598938 at epoch: 11 and batch_num: 118\n",
      "Loss of test set: 0.4080861210823059 at epoch: 11 and batch_num: 119\n",
      "Loss of test set: 0.4537963271141052 at epoch: 11 and batch_num: 120\n",
      "Loss of test set: 0.4022420644760132 at epoch: 11 and batch_num: 121\n",
      "Loss of test set: 0.3145066499710083 at epoch: 11 and batch_num: 122\n",
      "Loss of test set: 0.5961364507675171 at epoch: 11 and batch_num: 123\n",
      "Loss of test set: 0.4648028016090393 at epoch: 11 and batch_num: 124\n",
      "Loss of test set: 0.4462810754776001 at epoch: 11 and batch_num: 125\n",
      "Loss of test set: 0.4076809287071228 at epoch: 11 and batch_num: 126\n",
      "Loss of test set: 0.4094856381416321 at epoch: 11 and batch_num: 127\n",
      "Loss of test set: 0.644149661064148 at epoch: 11 and batch_num: 128\n",
      "Loss of test set: 0.1843063235282898 at epoch: 11 and batch_num: 129\n",
      "Loss of test set: 0.38186055421829224 at epoch: 11 and batch_num: 130\n",
      "Loss of test set: 0.37901556491851807 at epoch: 11 and batch_num: 131\n",
      "Loss of test set: 0.30789539217948914 at epoch: 11 and batch_num: 132\n",
      "Loss of test set: 0.3003448247909546 at epoch: 11 and batch_num: 133\n",
      "Loss of test set: 0.36889833211898804 at epoch: 11 and batch_num: 134\n",
      "Loss of test set: 0.42448878288269043 at epoch: 11 and batch_num: 135\n",
      "Loss of test set: 0.40850216150283813 at epoch: 11 and batch_num: 136\n",
      "Loss of test set: 0.2244599461555481 at epoch: 11 and batch_num: 137\n",
      "Loss of test set: 0.5216790437698364 at epoch: 11 and batch_num: 138\n",
      "Loss of test set: 0.35841667652130127 at epoch: 11 and batch_num: 139\n",
      "Loss of test set: 0.46792829036712646 at epoch: 11 and batch_num: 140\n",
      "Loss of test set: 0.25133025646209717 at epoch: 11 and batch_num: 141\n",
      "Loss of test set: 0.3566771149635315 at epoch: 11 and batch_num: 142\n",
      "Loss of test set: 0.3045039772987366 at epoch: 11 and batch_num: 143\n",
      "Loss of test set: 0.40335237979888916 at epoch: 11 and batch_num: 144\n",
      "Loss of test set: 0.45266908407211304 at epoch: 11 and batch_num: 145\n",
      "Loss of test set: 0.3145989775657654 at epoch: 11 and batch_num: 146\n",
      "Loss of test set: 0.49079999327659607 at epoch: 11 and batch_num: 147\n",
      "Loss of test set: 0.522878885269165 at epoch: 11 and batch_num: 148\n",
      "Loss of test set: 0.5526204705238342 at epoch: 11 and batch_num: 149\n",
      "Loss of test set: 0.3015460968017578 at epoch: 11 and batch_num: 150\n",
      "Loss of test set: 0.3890191912651062 at epoch: 11 and batch_num: 151\n",
      "Loss of test set: 0.6369695663452148 at epoch: 11 and batch_num: 152\n",
      "Loss of test set: 0.4148865342140198 at epoch: 11 and batch_num: 153\n",
      "Loss of test set: 0.561708390712738 at epoch: 11 and batch_num: 154\n",
      "Loss of test set: 0.3046143352985382 at epoch: 11 and batch_num: 155\n",
      "Loss of test set: 0.313747376203537 at epoch: 11 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8543\n",
      "Loss of train set: 0.2572435736656189 at epoch: 12 and batch_num: 0\n",
      "Loss of train set: 0.2761588990688324 at epoch: 12 and batch_num: 1\n",
      "Loss of train set: 0.3407244086265564 at epoch: 12 and batch_num: 2\n",
      "Loss of train set: 0.3074873387813568 at epoch: 12 and batch_num: 3\n",
      "Loss of train set: 0.27798140048980713 at epoch: 12 and batch_num: 4\n",
      "Loss of train set: 0.17151236534118652 at epoch: 12 and batch_num: 5\n",
      "Loss of train set: 0.3140588700771332 at epoch: 12 and batch_num: 6\n",
      "Loss of train set: 0.4302581548690796 at epoch: 12 and batch_num: 7\n",
      "Loss of train set: 0.2993203401565552 at epoch: 12 and batch_num: 8\n",
      "Loss of train set: 0.27771103382110596 at epoch: 12 and batch_num: 9\n",
      "Loss of train set: 0.3545904755592346 at epoch: 12 and batch_num: 10\n",
      "Loss of train set: 0.3148708641529083 at epoch: 12 and batch_num: 11\n",
      "Loss of train set: 0.35558021068573 at epoch: 12 and batch_num: 12\n",
      "Loss of train set: 0.3603084683418274 at epoch: 12 and batch_num: 13\n",
      "Loss of train set: 0.40278759598731995 at epoch: 12 and batch_num: 14\n",
      "Loss of train set: 0.20790058374404907 at epoch: 12 and batch_num: 15\n",
      "Loss of train set: 0.1421436071395874 at epoch: 12 and batch_num: 16\n",
      "Loss of train set: 0.42598026990890503 at epoch: 12 and batch_num: 17\n",
      "Loss of train set: 0.27732139825820923 at epoch: 12 and batch_num: 18\n",
      "Loss of train set: 0.37948328256607056 at epoch: 12 and batch_num: 19\n",
      "Loss of train set: 0.227589949965477 at epoch: 12 and batch_num: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.6231163740158081 at epoch: 12 and batch_num: 21\n",
      "Loss of train set: 0.21324566006660461 at epoch: 12 and batch_num: 22\n",
      "Loss of train set: 0.2966609001159668 at epoch: 12 and batch_num: 23\n",
      "Loss of train set: 0.421392560005188 at epoch: 12 and batch_num: 24\n",
      "Loss of train set: 0.26225554943084717 at epoch: 12 and batch_num: 25\n",
      "Loss of train set: 0.24299359321594238 at epoch: 12 and batch_num: 26\n",
      "Loss of train set: 0.33247941732406616 at epoch: 12 and batch_num: 27\n",
      "Loss of train set: 0.31658935546875 at epoch: 12 and batch_num: 28\n",
      "Loss of train set: 0.25658971071243286 at epoch: 12 and batch_num: 29\n",
      "Loss of train set: 0.3034311830997467 at epoch: 12 and batch_num: 30\n",
      "Loss of train set: 0.2973426282405853 at epoch: 12 and batch_num: 31\n",
      "Loss of train set: 0.24501843750476837 at epoch: 12 and batch_num: 32\n",
      "Loss of train set: 0.21621109545230865 at epoch: 12 and batch_num: 33\n",
      "Loss of train set: 0.41904202103614807 at epoch: 12 and batch_num: 34\n",
      "Loss of train set: 0.36097174882888794 at epoch: 12 and batch_num: 35\n",
      "Loss of train set: 0.5124247074127197 at epoch: 12 and batch_num: 36\n",
      "Loss of train set: 0.272933691740036 at epoch: 12 and batch_num: 37\n",
      "Loss of train set: 0.22441671788692474 at epoch: 12 and batch_num: 38\n",
      "Loss of train set: 0.31619134545326233 at epoch: 12 and batch_num: 39\n",
      "Loss of train set: 0.43034833669662476 at epoch: 12 and batch_num: 40\n",
      "Loss of train set: 0.2598801851272583 at epoch: 12 and batch_num: 41\n",
      "Loss of train set: 0.26610130071640015 at epoch: 12 and batch_num: 42\n",
      "Loss of train set: 0.40353307127952576 at epoch: 12 and batch_num: 43\n",
      "Loss of train set: 0.2681749761104584 at epoch: 12 and batch_num: 44\n",
      "Loss of train set: 0.2137075960636139 at epoch: 12 and batch_num: 45\n",
      "Loss of train set: 0.34078270196914673 at epoch: 12 and batch_num: 46\n",
      "Loss of train set: 0.2372213900089264 at epoch: 12 and batch_num: 47\n",
      "Loss of train set: 0.30853742361068726 at epoch: 12 and batch_num: 48\n",
      "Loss of train set: 0.31724703311920166 at epoch: 12 and batch_num: 49\n",
      "Loss of train set: 0.20821493864059448 at epoch: 12 and batch_num: 50\n",
      "Loss of train set: 0.49539071321487427 at epoch: 12 and batch_num: 51\n",
      "Loss of train set: 0.44104474782943726 at epoch: 12 and batch_num: 52\n",
      "Loss of train set: 0.5819451808929443 at epoch: 12 and batch_num: 53\n",
      "Loss of train set: 0.2576178014278412 at epoch: 12 and batch_num: 54\n",
      "Loss of train set: 0.37663137912750244 at epoch: 12 and batch_num: 55\n",
      "Loss of train set: 0.3646009564399719 at epoch: 12 and batch_num: 56\n",
      "Loss of train set: 0.23038069903850555 at epoch: 12 and batch_num: 57\n",
      "Loss of train set: 0.2989944517612457 at epoch: 12 and batch_num: 58\n",
      "Loss of train set: 0.3145950436592102 at epoch: 12 and batch_num: 59\n",
      "Loss of train set: 0.2805558145046234 at epoch: 12 and batch_num: 60\n",
      "Loss of train set: 0.26640987396240234 at epoch: 12 and batch_num: 61\n",
      "Loss of train set: 0.2657763957977295 at epoch: 12 and batch_num: 62\n",
      "Loss of train set: 0.16770969331264496 at epoch: 12 and batch_num: 63\n",
      "Loss of train set: 0.2583550810813904 at epoch: 12 and batch_num: 64\n",
      "Loss of train set: 0.3105431795120239 at epoch: 12 and batch_num: 65\n",
      "Loss of train set: 0.35976630449295044 at epoch: 12 and batch_num: 66\n",
      "Loss of train set: 0.4698423147201538 at epoch: 12 and batch_num: 67\n",
      "Loss of train set: 0.321130633354187 at epoch: 12 and batch_num: 68\n",
      "Loss of train set: 0.36074844002723694 at epoch: 12 and batch_num: 69\n",
      "Loss of train set: 0.3540870249271393 at epoch: 12 and batch_num: 70\n",
      "Loss of train set: 0.35924217104911804 at epoch: 12 and batch_num: 71\n",
      "Loss of train set: 0.31309783458709717 at epoch: 12 and batch_num: 72\n",
      "Loss of train set: 0.3011828660964966 at epoch: 12 and batch_num: 73\n",
      "Loss of train set: 0.30776435136795044 at epoch: 12 and batch_num: 74\n",
      "Loss of train set: 0.37762829661369324 at epoch: 12 and batch_num: 75\n",
      "Loss of train set: 0.3075876832008362 at epoch: 12 and batch_num: 76\n",
      "Loss of train set: 0.1544293761253357 at epoch: 12 and batch_num: 77\n",
      "Loss of train set: 0.34631410241127014 at epoch: 12 and batch_num: 78\n",
      "Loss of train set: 0.38095518946647644 at epoch: 12 and batch_num: 79\n",
      "Loss of train set: 0.17658355832099915 at epoch: 12 and batch_num: 80\n",
      "Loss of train set: 0.14360904693603516 at epoch: 12 and batch_num: 81\n",
      "Loss of train set: 0.2866228222846985 at epoch: 12 and batch_num: 82\n",
      "Loss of train set: 0.296647310256958 at epoch: 12 and batch_num: 83\n",
      "Loss of train set: 0.39200395345687866 at epoch: 12 and batch_num: 84\n",
      "Loss of train set: 0.4629485011100769 at epoch: 12 and batch_num: 85\n",
      "Loss of train set: 0.3494381308555603 at epoch: 12 and batch_num: 86\n",
      "Loss of train set: 0.21548448503017426 at epoch: 12 and batch_num: 87\n",
      "Loss of train set: 0.26935482025146484 at epoch: 12 and batch_num: 88\n",
      "Loss of train set: 0.2567601203918457 at epoch: 12 and batch_num: 89\n",
      "Loss of train set: 0.31675076484680176 at epoch: 12 and batch_num: 90\n",
      "Loss of train set: 0.16338658332824707 at epoch: 12 and batch_num: 91\n",
      "Loss of train set: 0.3943924307823181 at epoch: 12 and batch_num: 92\n",
      "Loss of train set: 0.3589368760585785 at epoch: 12 and batch_num: 93\n",
      "Loss of train set: 0.30111685395240784 at epoch: 12 and batch_num: 94\n",
      "Loss of train set: 0.2845633029937744 at epoch: 12 and batch_num: 95\n",
      "Loss of train set: 0.35824209451675415 at epoch: 12 and batch_num: 96\n",
      "Loss of train set: 0.3241115212440491 at epoch: 12 and batch_num: 97\n",
      "Loss of train set: 0.4456605315208435 at epoch: 12 and batch_num: 98\n",
      "Loss of train set: 0.17595884203910828 at epoch: 12 and batch_num: 99\n",
      "Loss of train set: 0.22862237691879272 at epoch: 12 and batch_num: 100\n",
      "Loss of train set: 0.32728803157806396 at epoch: 12 and batch_num: 101\n",
      "Loss of train set: 0.30041933059692383 at epoch: 12 and batch_num: 102\n",
      "Loss of train set: 0.2939784824848175 at epoch: 12 and batch_num: 103\n",
      "Loss of train set: 0.29803985357284546 at epoch: 12 and batch_num: 104\n",
      "Loss of train set: 0.6948472261428833 at epoch: 12 and batch_num: 105\n",
      "Loss of train set: 0.26762494444847107 at epoch: 12 and batch_num: 106\n",
      "Loss of train set: 0.3513909876346588 at epoch: 12 and batch_num: 107\n",
      "Loss of train set: 0.3319745659828186 at epoch: 12 and batch_num: 108\n",
      "Loss of train set: 0.22981733083724976 at epoch: 12 and batch_num: 109\n",
      "Loss of train set: 0.21781255304813385 at epoch: 12 and batch_num: 110\n",
      "Loss of train set: 0.24305927753448486 at epoch: 12 and batch_num: 111\n",
      "Loss of train set: 0.23284175992012024 at epoch: 12 and batch_num: 112\n",
      "Loss of train set: 0.3877316415309906 at epoch: 12 and batch_num: 113\n",
      "Loss of train set: 0.3116605281829834 at epoch: 12 and batch_num: 114\n",
      "Loss of train set: 0.4073396921157837 at epoch: 12 and batch_num: 115\n",
      "Loss of train set: 0.12819162011146545 at epoch: 12 and batch_num: 116\n",
      "Loss of train set: 0.30677539110183716 at epoch: 12 and batch_num: 117\n",
      "Loss of train set: 0.28173863887786865 at epoch: 12 and batch_num: 118\n",
      "Loss of train set: 0.30788207054138184 at epoch: 12 and batch_num: 119\n",
      "Loss of train set: 0.3365614712238312 at epoch: 12 and batch_num: 120\n",
      "Loss of train set: 0.3286511301994324 at epoch: 12 and batch_num: 121\n",
      "Loss of train set: 0.4548068642616272 at epoch: 12 and batch_num: 122\n",
      "Loss of train set: 0.224458709359169 at epoch: 12 and batch_num: 123\n",
      "Loss of train set: 0.21694515645503998 at epoch: 12 and batch_num: 124\n",
      "Loss of train set: 0.4137042462825775 at epoch: 12 and batch_num: 125\n",
      "Loss of train set: 0.30381497740745544 at epoch: 12 and batch_num: 126\n",
      "Loss of train set: 0.34011346101760864 at epoch: 12 and batch_num: 127\n",
      "Loss of train set: 0.24098439514636993 at epoch: 12 and batch_num: 128\n",
      "Loss of train set: 0.3645485043525696 at epoch: 12 and batch_num: 129\n",
      "Loss of train set: 0.40333274006843567 at epoch: 12 and batch_num: 130\n",
      "Loss of train set: 0.22368523478507996 at epoch: 12 and batch_num: 131\n",
      "Loss of train set: 0.4669402241706848 at epoch: 12 and batch_num: 132\n",
      "Loss of train set: 0.33007127046585083 at epoch: 12 and batch_num: 133\n",
      "Loss of train set: 0.20461849868297577 at epoch: 12 and batch_num: 134\n",
      "Loss of train set: 0.33309614658355713 at epoch: 12 and batch_num: 135\n",
      "Loss of train set: 0.39160269498825073 at epoch: 12 and batch_num: 136\n",
      "Loss of train set: 0.3944665193557739 at epoch: 12 and batch_num: 137\n",
      "Loss of train set: 0.44559818506240845 at epoch: 12 and batch_num: 138\n",
      "Loss of train set: 0.3523455560207367 at epoch: 12 and batch_num: 139\n",
      "Loss of train set: 0.2948727607727051 at epoch: 12 and batch_num: 140\n",
      "Loss of train set: 0.3618934154510498 at epoch: 12 and batch_num: 141\n",
      "Loss of train set: 0.1931343674659729 at epoch: 12 and batch_num: 142\n",
      "Loss of train set: 0.2758098840713501 at epoch: 12 and batch_num: 143\n",
      "Loss of train set: 0.3152320384979248 at epoch: 12 and batch_num: 144\n",
      "Loss of train set: 0.24703380465507507 at epoch: 12 and batch_num: 145\n",
      "Loss of train set: 0.3224935531616211 at epoch: 12 and batch_num: 146\n",
      "Loss of train set: 0.3391139805316925 at epoch: 12 and batch_num: 147\n",
      "Loss of train set: 0.2811852991580963 at epoch: 12 and batch_num: 148\n",
      "Loss of train set: 0.5047895908355713 at epoch: 12 and batch_num: 149\n",
      "Loss of train set: 0.4090918004512787 at epoch: 12 and batch_num: 150\n",
      "Loss of train set: 0.35646873712539673 at epoch: 12 and batch_num: 151\n",
      "Loss of train set: 0.3365497589111328 at epoch: 12 and batch_num: 152\n",
      "Loss of train set: 0.26188403367996216 at epoch: 12 and batch_num: 153\n",
      "Loss of train set: 0.3137381970882416 at epoch: 12 and batch_num: 154\n",
      "Loss of train set: 0.3239624798297882 at epoch: 12 and batch_num: 155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.32042402029037476 at epoch: 12 and batch_num: 156\n",
      "Loss of train set: 0.266643762588501 at epoch: 12 and batch_num: 157\n",
      "Loss of train set: 0.418746680021286 at epoch: 12 and batch_num: 158\n",
      "Loss of train set: 0.2955154478549957 at epoch: 12 and batch_num: 159\n",
      "Loss of train set: 0.44894999265670776 at epoch: 12 and batch_num: 160\n",
      "Loss of train set: 0.2944530248641968 at epoch: 12 and batch_num: 161\n",
      "Loss of train set: 0.2782559394836426 at epoch: 12 and batch_num: 162\n",
      "Loss of train set: 0.2572718858718872 at epoch: 12 and batch_num: 163\n",
      "Loss of train set: 0.5910252928733826 at epoch: 12 and batch_num: 164\n",
      "Loss of train set: 0.45048287510871887 at epoch: 12 and batch_num: 165\n",
      "Loss of train set: 0.47960996627807617 at epoch: 12 and batch_num: 166\n",
      "Loss of train set: 0.4437504708766937 at epoch: 12 and batch_num: 167\n",
      "Loss of train set: 0.2544839084148407 at epoch: 12 and batch_num: 168\n",
      "Loss of train set: 0.3999161124229431 at epoch: 12 and batch_num: 169\n",
      "Loss of train set: 0.37779197096824646 at epoch: 12 and batch_num: 170\n",
      "Loss of train set: 0.327031672000885 at epoch: 12 and batch_num: 171\n",
      "Loss of train set: 0.3517763614654541 at epoch: 12 and batch_num: 172\n",
      "Loss of train set: 0.20360879600048065 at epoch: 12 and batch_num: 173\n",
      "Loss of train set: 0.3321424722671509 at epoch: 12 and batch_num: 174\n",
      "Loss of train set: 0.25486981868743896 at epoch: 12 and batch_num: 175\n",
      "Loss of train set: 0.23231832683086395 at epoch: 12 and batch_num: 176\n",
      "Loss of train set: 0.30305445194244385 at epoch: 12 and batch_num: 177\n",
      "Loss of train set: 0.34126362204551697 at epoch: 12 and batch_num: 178\n",
      "Loss of train set: 0.29089608788490295 at epoch: 12 and batch_num: 179\n",
      "Loss of train set: 0.12972761690616608 at epoch: 12 and batch_num: 180\n",
      "Loss of train set: 0.1832485795021057 at epoch: 12 and batch_num: 181\n",
      "Loss of train set: 0.3389698266983032 at epoch: 12 and batch_num: 182\n",
      "Loss of train set: 0.35187578201293945 at epoch: 12 and batch_num: 183\n",
      "Loss of train set: 0.4919002652168274 at epoch: 12 and batch_num: 184\n",
      "Loss of train set: 0.22810813784599304 at epoch: 12 and batch_num: 185\n",
      "Loss of train set: 0.2017035186290741 at epoch: 12 and batch_num: 186\n",
      "Loss of train set: 0.2727828621864319 at epoch: 12 and batch_num: 187\n",
      "Loss of train set: 0.37196171283721924 at epoch: 12 and batch_num: 188\n",
      "Loss of train set: 0.3568095564842224 at epoch: 12 and batch_num: 189\n",
      "Loss of train set: 0.36586451530456543 at epoch: 12 and batch_num: 190\n",
      "Loss of train set: 0.23031848669052124 at epoch: 12 and batch_num: 191\n",
      "Loss of train set: 0.452711284160614 at epoch: 12 and batch_num: 192\n",
      "Loss of train set: 0.2861396074295044 at epoch: 12 and batch_num: 193\n",
      "Loss of train set: 0.41744765639305115 at epoch: 12 and batch_num: 194\n",
      "Loss of train set: 0.32780885696411133 at epoch: 12 and batch_num: 195\n",
      "Loss of train set: 0.33675530552864075 at epoch: 12 and batch_num: 196\n",
      "Loss of train set: 0.24679599702358246 at epoch: 12 and batch_num: 197\n",
      "Loss of train set: 0.35582301020622253 at epoch: 12 and batch_num: 198\n",
      "Loss of train set: 0.2759019732475281 at epoch: 12 and batch_num: 199\n",
      "Loss of train set: 0.1993386149406433 at epoch: 12 and batch_num: 200\n",
      "Loss of train set: 0.4488258957862854 at epoch: 12 and batch_num: 201\n",
      "Loss of train set: 0.3285885453224182 at epoch: 12 and batch_num: 202\n",
      "Loss of train set: 0.23341476917266846 at epoch: 12 and batch_num: 203\n",
      "Loss of train set: 0.27551788091659546 at epoch: 12 and batch_num: 204\n",
      "Loss of train set: 0.36751970648765564 at epoch: 12 and batch_num: 205\n",
      "Loss of train set: 0.35645973682403564 at epoch: 12 and batch_num: 206\n",
      "Loss of train set: 0.16913799941539764 at epoch: 12 and batch_num: 207\n",
      "Loss of train set: 0.39839252829551697 at epoch: 12 and batch_num: 208\n",
      "Loss of train set: 0.44218510389328003 at epoch: 12 and batch_num: 209\n",
      "Loss of train set: 0.2890615463256836 at epoch: 12 and batch_num: 210\n",
      "Loss of train set: 0.19721010327339172 at epoch: 12 and batch_num: 211\n",
      "Loss of train set: 0.2083066701889038 at epoch: 12 and batch_num: 212\n",
      "Loss of train set: 0.4614757299423218 at epoch: 12 and batch_num: 213\n",
      "Loss of train set: 0.36471307277679443 at epoch: 12 and batch_num: 214\n",
      "Loss of train set: 0.30309632420539856 at epoch: 12 and batch_num: 215\n",
      "Loss of train set: 0.20461764931678772 at epoch: 12 and batch_num: 216\n",
      "Loss of train set: 0.23238006234169006 at epoch: 12 and batch_num: 217\n",
      "Loss of train set: 0.3828844428062439 at epoch: 12 and batch_num: 218\n",
      "Loss of train set: 0.40594351291656494 at epoch: 12 and batch_num: 219\n",
      "Loss of train set: 0.22738103568553925 at epoch: 12 and batch_num: 220\n",
      "Loss of train set: 0.28791558742523193 at epoch: 12 and batch_num: 221\n",
      "Loss of train set: 0.1976647824048996 at epoch: 12 and batch_num: 222\n",
      "Loss of train set: 0.22768381237983704 at epoch: 12 and batch_num: 223\n",
      "Loss of train set: 0.24738982319831848 at epoch: 12 and batch_num: 224\n",
      "Loss of train set: 0.26296666264533997 at epoch: 12 and batch_num: 225\n",
      "Loss of train set: 0.26337897777557373 at epoch: 12 and batch_num: 226\n",
      "Loss of train set: 0.22922013700008392 at epoch: 12 and batch_num: 227\n",
      "Loss of train set: 0.4603671431541443 at epoch: 12 and batch_num: 228\n",
      "Loss of train set: 0.29315584897994995 at epoch: 12 and batch_num: 229\n",
      "Loss of train set: 0.31174737215042114 at epoch: 12 and batch_num: 230\n",
      "Loss of train set: 0.34321141242980957 at epoch: 12 and batch_num: 231\n",
      "Loss of train set: 0.22603407502174377 at epoch: 12 and batch_num: 232\n",
      "Loss of train set: 0.23840157687664032 at epoch: 12 and batch_num: 233\n",
      "Loss of train set: 0.40380460023880005 at epoch: 12 and batch_num: 234\n",
      "Loss of train set: 0.15910914540290833 at epoch: 12 and batch_num: 235\n",
      "Loss of train set: 0.2494397610425949 at epoch: 12 and batch_num: 236\n",
      "Loss of train set: 0.1558808982372284 at epoch: 12 and batch_num: 237\n",
      "Loss of train set: 0.41764402389526367 at epoch: 12 and batch_num: 238\n",
      "Loss of train set: 0.3546891510486603 at epoch: 12 and batch_num: 239\n",
      "Loss of train set: 0.25364625453948975 at epoch: 12 and batch_num: 240\n",
      "Loss of train set: 0.48197638988494873 at epoch: 12 and batch_num: 241\n",
      "Loss of train set: 0.3885657787322998 at epoch: 12 and batch_num: 242\n",
      "Loss of train set: 0.22494757175445557 at epoch: 12 and batch_num: 243\n",
      "Loss of train set: 0.3086869716644287 at epoch: 12 and batch_num: 244\n",
      "Loss of train set: 0.27389419078826904 at epoch: 12 and batch_num: 245\n",
      "Loss of train set: 0.2720632553100586 at epoch: 12 and batch_num: 246\n",
      "Loss of train set: 0.22693803906440735 at epoch: 12 and batch_num: 247\n",
      "Loss of train set: 0.5528186559677124 at epoch: 12 and batch_num: 248\n",
      "Loss of train set: 0.27390211820602417 at epoch: 12 and batch_num: 249\n",
      "Loss of train set: 0.4109199047088623 at epoch: 12 and batch_num: 250\n",
      "Loss of train set: 0.243377685546875 at epoch: 12 and batch_num: 251\n",
      "Loss of train set: 0.4925621747970581 at epoch: 12 and batch_num: 252\n",
      "Loss of train set: 0.5340057611465454 at epoch: 12 and batch_num: 253\n",
      "Loss of train set: 0.3554297089576721 at epoch: 12 and batch_num: 254\n",
      "Loss of train set: 0.18537156283855438 at epoch: 12 and batch_num: 255\n",
      "Loss of train set: 0.2524104118347168 at epoch: 12 and batch_num: 256\n",
      "Loss of train set: 0.3545995056629181 at epoch: 12 and batch_num: 257\n",
      "Loss of train set: 0.24180898070335388 at epoch: 12 and batch_num: 258\n",
      "Loss of train set: 0.3730732500553131 at epoch: 12 and batch_num: 259\n",
      "Loss of train set: 0.32969292998313904 at epoch: 12 and batch_num: 260\n",
      "Loss of train set: 0.24853678047657013 at epoch: 12 and batch_num: 261\n",
      "Loss of train set: 0.27710527181625366 at epoch: 12 and batch_num: 262\n",
      "Loss of train set: 0.42140811681747437 at epoch: 12 and batch_num: 263\n",
      "Loss of train set: 0.2800108790397644 at epoch: 12 and batch_num: 264\n",
      "Loss of train set: 0.27401626110076904 at epoch: 12 and batch_num: 265\n",
      "Loss of train set: 0.266933411359787 at epoch: 12 and batch_num: 266\n",
      "Loss of train set: 0.3848021328449249 at epoch: 12 and batch_num: 267\n",
      "Loss of train set: 0.28954368829727173 at epoch: 12 and batch_num: 268\n",
      "Loss of train set: 0.14508506655693054 at epoch: 12 and batch_num: 269\n",
      "Loss of train set: 0.28785014152526855 at epoch: 12 and batch_num: 270\n",
      "Loss of train set: 0.4134247303009033 at epoch: 12 and batch_num: 271\n",
      "Loss of train set: 0.3044254183769226 at epoch: 12 and batch_num: 272\n",
      "Loss of train set: 0.23233628273010254 at epoch: 12 and batch_num: 273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2756606340408325 at epoch: 12 and batch_num: 274\n",
      "Loss of train set: 0.1814115345478058 at epoch: 12 and batch_num: 275\n",
      "Loss of train set: 0.1864745020866394 at epoch: 12 and batch_num: 276\n",
      "Loss of train set: 0.6296209096908569 at epoch: 12 and batch_num: 277\n",
      "Loss of train set: 0.3201999068260193 at epoch: 12 and batch_num: 278\n",
      "Loss of train set: 0.5160768032073975 at epoch: 12 and batch_num: 279\n",
      "Loss of train set: 0.340187668800354 at epoch: 12 and batch_num: 280\n",
      "Loss of train set: 0.4319138526916504 at epoch: 12 and batch_num: 281\n",
      "Loss of train set: 0.2100287675857544 at epoch: 12 and batch_num: 282\n",
      "Loss of train set: 0.26535552740097046 at epoch: 12 and batch_num: 283\n",
      "Loss of train set: 0.12039925903081894 at epoch: 12 and batch_num: 284\n",
      "Loss of train set: 0.26812028884887695 at epoch: 12 and batch_num: 285\n",
      "Loss of train set: 0.25954318046569824 at epoch: 12 and batch_num: 286\n",
      "Loss of train set: 0.3351532220840454 at epoch: 12 and batch_num: 287\n",
      "Loss of train set: 0.3694138526916504 at epoch: 12 and batch_num: 288\n",
      "Loss of train set: 0.31958627700805664 at epoch: 12 and batch_num: 289\n",
      "Loss of train set: 0.2816983163356781 at epoch: 12 and batch_num: 290\n",
      "Loss of train set: 0.2571724057197571 at epoch: 12 and batch_num: 291\n",
      "Loss of train set: 0.37173572182655334 at epoch: 12 and batch_num: 292\n",
      "Loss of train set: 0.29082730412483215 at epoch: 12 and batch_num: 293\n",
      "Loss of train set: 0.2963620126247406 at epoch: 12 and batch_num: 294\n",
      "Loss of train set: 0.32126784324645996 at epoch: 12 and batch_num: 295\n",
      "Loss of train set: 0.2816610038280487 at epoch: 12 and batch_num: 296\n",
      "Loss of train set: 0.1868336796760559 at epoch: 12 and batch_num: 297\n",
      "Loss of train set: 0.2943580150604248 at epoch: 12 and batch_num: 298\n",
      "Loss of train set: 0.124452143907547 at epoch: 12 and batch_num: 299\n",
      "Loss of train set: 0.2856239676475525 at epoch: 12 and batch_num: 300\n",
      "Loss of train set: 0.2958031892776489 at epoch: 12 and batch_num: 301\n",
      "Loss of train set: 0.24600523710250854 at epoch: 12 and batch_num: 302\n",
      "Loss of train set: 0.2500517964363098 at epoch: 12 and batch_num: 303\n",
      "Loss of train set: 0.38205820322036743 at epoch: 12 and batch_num: 304\n",
      "Loss of train set: 0.4637152850627899 at epoch: 12 and batch_num: 305\n",
      "Loss of train set: 0.451009064912796 at epoch: 12 and batch_num: 306\n",
      "Loss of train set: 0.349793016910553 at epoch: 12 and batch_num: 307\n",
      "Loss of train set: 0.24584293365478516 at epoch: 12 and batch_num: 308\n",
      "Loss of train set: 0.3880109190940857 at epoch: 12 and batch_num: 309\n",
      "Loss of train set: 0.4235369861125946 at epoch: 12 and batch_num: 310\n",
      "Loss of train set: 0.3698982000350952 at epoch: 12 and batch_num: 311\n",
      "Loss of train set: 0.15588590502738953 at epoch: 12 and batch_num: 312\n",
      "Loss of train set: 0.2383018583059311 at epoch: 12 and batch_num: 313\n",
      "Loss of train set: 0.31107190251350403 at epoch: 12 and batch_num: 314\n",
      "Loss of train set: 0.3368752598762512 at epoch: 12 and batch_num: 315\n",
      "Loss of train set: 0.2123260796070099 at epoch: 12 and batch_num: 316\n",
      "Loss of train set: 0.25528067350387573 at epoch: 12 and batch_num: 317\n",
      "Loss of train set: 0.2756081819534302 at epoch: 12 and batch_num: 318\n",
      "Loss of train set: 0.5208585262298584 at epoch: 12 and batch_num: 319\n",
      "Loss of train set: 0.4717279374599457 at epoch: 12 and batch_num: 320\n",
      "Loss of train set: 0.4861431419849396 at epoch: 12 and batch_num: 321\n",
      "Loss of train set: 0.32630208134651184 at epoch: 12 and batch_num: 322\n",
      "Loss of train set: 0.2468002289533615 at epoch: 12 and batch_num: 323\n",
      "Loss of train set: 0.4549707770347595 at epoch: 12 and batch_num: 324\n",
      "Loss of train set: 0.2449224293231964 at epoch: 12 and batch_num: 325\n",
      "Loss of train set: 0.2891852557659149 at epoch: 12 and batch_num: 326\n",
      "Loss of train set: 0.2866780161857605 at epoch: 12 and batch_num: 327\n",
      "Loss of train set: 0.28247618675231934 at epoch: 12 and batch_num: 328\n",
      "Loss of train set: 0.23077452182769775 at epoch: 12 and batch_num: 329\n",
      "Loss of train set: 0.3719229996204376 at epoch: 12 and batch_num: 330\n",
      "Loss of train set: 0.34367477893829346 at epoch: 12 and batch_num: 331\n",
      "Loss of train set: 0.2476871907711029 at epoch: 12 and batch_num: 332\n",
      "Loss of train set: 0.22406800091266632 at epoch: 12 and batch_num: 333\n",
      "Loss of train set: 0.2637016177177429 at epoch: 12 and batch_num: 334\n",
      "Loss of train set: 0.327155202627182 at epoch: 12 and batch_num: 335\n",
      "Loss of train set: 0.3935730457305908 at epoch: 12 and batch_num: 336\n",
      "Loss of train set: 0.42565062642097473 at epoch: 12 and batch_num: 337\n",
      "Loss of train set: 0.37463417649269104 at epoch: 12 and batch_num: 338\n",
      "Loss of train set: 0.33209651708602905 at epoch: 12 and batch_num: 339\n",
      "Loss of train set: 0.34817928075790405 at epoch: 12 and batch_num: 340\n",
      "Loss of train set: 0.30817362666130066 at epoch: 12 and batch_num: 341\n",
      "Loss of train set: 0.2392752766609192 at epoch: 12 and batch_num: 342\n",
      "Loss of train set: 0.2506537437438965 at epoch: 12 and batch_num: 343\n",
      "Loss of train set: 0.22358837723731995 at epoch: 12 and batch_num: 344\n",
      "Loss of train set: 0.30042383074760437 at epoch: 12 and batch_num: 345\n",
      "Loss of train set: 0.3077717125415802 at epoch: 12 and batch_num: 346\n",
      "Loss of train set: 0.3446771502494812 at epoch: 12 and batch_num: 347\n",
      "Loss of train set: 0.1963949203491211 at epoch: 12 and batch_num: 348\n",
      "Loss of train set: 0.3248787820339203 at epoch: 12 and batch_num: 349\n",
      "Loss of train set: 0.2703831195831299 at epoch: 12 and batch_num: 350\n",
      "Loss of train set: 0.223054438829422 at epoch: 12 and batch_num: 351\n",
      "Loss of train set: 0.2362041026353836 at epoch: 12 and batch_num: 352\n",
      "Loss of train set: 0.21067211031913757 at epoch: 12 and batch_num: 353\n",
      "Loss of train set: 0.29115378856658936 at epoch: 12 and batch_num: 354\n",
      "Loss of train set: 0.516675591468811 at epoch: 12 and batch_num: 355\n",
      "Loss of train set: 0.44059741497039795 at epoch: 12 and batch_num: 356\n",
      "Loss of train set: 0.3446716070175171 at epoch: 12 and batch_num: 357\n",
      "Loss of train set: 0.23753207921981812 at epoch: 12 and batch_num: 358\n",
      "Loss of train set: 0.4075995683670044 at epoch: 12 and batch_num: 359\n",
      "Loss of train set: 0.45774784684181213 at epoch: 12 and batch_num: 360\n",
      "Loss of train set: 0.1966664344072342 at epoch: 12 and batch_num: 361\n",
      "Loss of train set: 0.34484684467315674 at epoch: 12 and batch_num: 362\n",
      "Loss of train set: 0.30877774953842163 at epoch: 12 and batch_num: 363\n",
      "Loss of train set: 0.20876017212867737 at epoch: 12 and batch_num: 364\n",
      "Loss of train set: 0.33556967973709106 at epoch: 12 and batch_num: 365\n",
      "Loss of train set: 0.36355483531951904 at epoch: 12 and batch_num: 366\n",
      "Loss of train set: 0.21196162700653076 at epoch: 12 and batch_num: 367\n",
      "Loss of train set: 0.3131449818611145 at epoch: 12 and batch_num: 368\n",
      "Loss of train set: 0.374556303024292 at epoch: 12 and batch_num: 369\n",
      "Loss of train set: 0.2576994299888611 at epoch: 12 and batch_num: 370\n",
      "Loss of train set: 0.2932118773460388 at epoch: 12 and batch_num: 371\n",
      "Loss of train set: 0.3990125060081482 at epoch: 12 and batch_num: 372\n",
      "Loss of train set: 0.3169885575771332 at epoch: 12 and batch_num: 373\n",
      "Loss of train set: 0.44084542989730835 at epoch: 12 and batch_num: 374\n",
      "Loss of train set: 0.34198707342147827 at epoch: 12 and batch_num: 375\n",
      "Loss of train set: 0.23478130996227264 at epoch: 12 and batch_num: 376\n",
      "Loss of train set: 0.26227208971977234 at epoch: 12 and batch_num: 377\n",
      "Loss of train set: 0.5164182186126709 at epoch: 12 and batch_num: 378\n",
      "Loss of train set: 0.38557320833206177 at epoch: 12 and batch_num: 379\n",
      "Loss of train set: 0.2693325877189636 at epoch: 12 and batch_num: 380\n",
      "Loss of train set: 0.5023371577262878 at epoch: 12 and batch_num: 381\n",
      "Loss of train set: 0.46097904443740845 at epoch: 12 and batch_num: 382\n",
      "Loss of train set: 0.3728950619697571 at epoch: 12 and batch_num: 383\n",
      "Loss of train set: 0.3307172358036041 at epoch: 12 and batch_num: 384\n",
      "Loss of train set: 0.3052660822868347 at epoch: 12 and batch_num: 385\n",
      "Loss of train set: 0.20744386315345764 at epoch: 12 and batch_num: 386\n",
      "Loss of train set: 0.3185528516769409 at epoch: 12 and batch_num: 387\n",
      "Loss of train set: 0.3565927743911743 at epoch: 12 and batch_num: 388\n",
      "Loss of train set: 0.2823600769042969 at epoch: 12 and batch_num: 389\n",
      "Loss of train set: 0.2656874358654022 at epoch: 12 and batch_num: 390\n",
      "Loss of train set: 0.2453976720571518 at epoch: 12 and batch_num: 391\n",
      "Loss of train set: 0.19668906927108765 at epoch: 12 and batch_num: 392\n",
      "Loss of train set: 0.46414411067962646 at epoch: 12 and batch_num: 393\n",
      "Loss of train set: 0.3582938313484192 at epoch: 12 and batch_num: 394\n",
      "Loss of train set: 0.34991690516471863 at epoch: 12 and batch_num: 395\n",
      "Loss of train set: 0.20754539966583252 at epoch: 12 and batch_num: 396\n",
      "Loss of train set: 0.3283652365207672 at epoch: 12 and batch_num: 397\n",
      "Loss of train set: 0.3371678590774536 at epoch: 12 and batch_num: 398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3256128430366516 at epoch: 12 and batch_num: 399\n",
      "Loss of train set: 0.2904670238494873 at epoch: 12 and batch_num: 400\n",
      "Loss of train set: 0.28834274411201477 at epoch: 12 and batch_num: 401\n",
      "Loss of train set: 0.24872562289237976 at epoch: 12 and batch_num: 402\n",
      "Loss of train set: 0.249747633934021 at epoch: 12 and batch_num: 403\n",
      "Loss of train set: 0.3410845994949341 at epoch: 12 and batch_num: 404\n",
      "Loss of train set: 0.41620463132858276 at epoch: 12 and batch_num: 405\n",
      "Loss of train set: 0.23149260878562927 at epoch: 12 and batch_num: 406\n",
      "Loss of train set: 0.42308029532432556 at epoch: 12 and batch_num: 407\n",
      "Loss of train set: 0.3620714545249939 at epoch: 12 and batch_num: 408\n",
      "Loss of train set: 0.5223478078842163 at epoch: 12 and batch_num: 409\n",
      "Loss of train set: 0.2777969241142273 at epoch: 12 and batch_num: 410\n",
      "Loss of train set: 0.3144794702529907 at epoch: 12 and batch_num: 411\n",
      "Loss of train set: 0.38542091846466064 at epoch: 12 and batch_num: 412\n",
      "Loss of train set: 0.2810859680175781 at epoch: 12 and batch_num: 413\n",
      "Loss of train set: 0.20592418313026428 at epoch: 12 and batch_num: 414\n",
      "Loss of train set: 0.2673008143901825 at epoch: 12 and batch_num: 415\n",
      "Loss of train set: 0.37039613723754883 at epoch: 12 and batch_num: 416\n",
      "Loss of train set: 0.33279794454574585 at epoch: 12 and batch_num: 417\n",
      "Loss of train set: 0.31382501125335693 at epoch: 12 and batch_num: 418\n",
      "Loss of train set: 0.27402740716934204 at epoch: 12 and batch_num: 419\n",
      "Loss of train set: 0.1962544322013855 at epoch: 12 and batch_num: 420\n",
      "Loss of train set: 0.4072573482990265 at epoch: 12 and batch_num: 421\n",
      "Loss of train set: 0.3361436724662781 at epoch: 12 and batch_num: 422\n",
      "Loss of train set: 0.24428725242614746 at epoch: 12 and batch_num: 423\n",
      "Loss of train set: 0.28257909417152405 at epoch: 12 and batch_num: 424\n",
      "Loss of train set: 0.24631650745868683 at epoch: 12 and batch_num: 425\n",
      "Loss of train set: 0.3721877932548523 at epoch: 12 and batch_num: 426\n",
      "Loss of train set: 0.23317456245422363 at epoch: 12 and batch_num: 427\n",
      "Loss of train set: 0.17273402214050293 at epoch: 12 and batch_num: 428\n",
      "Loss of train set: 0.2911102771759033 at epoch: 12 and batch_num: 429\n",
      "Loss of train set: 0.3907700181007385 at epoch: 12 and batch_num: 430\n",
      "Loss of train set: 0.2398107349872589 at epoch: 12 and batch_num: 431\n",
      "Loss of train set: 0.38874268531799316 at epoch: 12 and batch_num: 432\n",
      "Loss of train set: 0.2781687378883362 at epoch: 12 and batch_num: 433\n",
      "Loss of train set: 0.44595253467559814 at epoch: 12 and batch_num: 434\n",
      "Loss of train set: 0.38850635290145874 at epoch: 12 and batch_num: 435\n",
      "Loss of train set: 0.486908495426178 at epoch: 12 and batch_num: 436\n",
      "Loss of train set: 0.32710492610931396 at epoch: 12 and batch_num: 437\n",
      "Loss of train set: 0.32745498418807983 at epoch: 12 and batch_num: 438\n",
      "Loss of train set: 0.20899005234241486 at epoch: 12 and batch_num: 439\n",
      "Loss of train set: 0.5952617526054382 at epoch: 12 and batch_num: 440\n",
      "Loss of train set: 0.3788157105445862 at epoch: 12 and batch_num: 441\n",
      "Loss of train set: 0.4898856282234192 at epoch: 12 and batch_num: 442\n",
      "Loss of train set: 0.3337758183479309 at epoch: 12 and batch_num: 443\n",
      "Loss of train set: 0.2952749729156494 at epoch: 12 and batch_num: 444\n",
      "Loss of train set: 0.38873568177223206 at epoch: 12 and batch_num: 445\n",
      "Loss of train set: 0.22279605269432068 at epoch: 12 and batch_num: 446\n",
      "Loss of train set: 0.22911527752876282 at epoch: 12 and batch_num: 447\n",
      "Loss of train set: 0.30699482560157776 at epoch: 12 and batch_num: 448\n",
      "Loss of train set: 0.33405962586402893 at epoch: 12 and batch_num: 449\n",
      "Loss of train set: 0.41081446409225464 at epoch: 12 and batch_num: 450\n",
      "Loss of train set: 0.2667914628982544 at epoch: 12 and batch_num: 451\n",
      "Loss of train set: 0.32217466831207275 at epoch: 12 and batch_num: 452\n",
      "Loss of train set: 0.3054389953613281 at epoch: 12 and batch_num: 453\n",
      "Loss of train set: 0.38921114802360535 at epoch: 12 and batch_num: 454\n",
      "Loss of train set: 0.4504052996635437 at epoch: 12 and batch_num: 455\n",
      "Loss of train set: 0.29380959272384644 at epoch: 12 and batch_num: 456\n",
      "Loss of train set: 0.3326283097267151 at epoch: 12 and batch_num: 457\n",
      "Loss of train set: 0.2319994866847992 at epoch: 12 and batch_num: 458\n",
      "Loss of train set: 0.3423946499824524 at epoch: 12 and batch_num: 459\n",
      "Loss of train set: 0.37812626361846924 at epoch: 12 and batch_num: 460\n",
      "Loss of train set: 0.30184176564216614 at epoch: 12 and batch_num: 461\n",
      "Loss of train set: 0.26130831241607666 at epoch: 12 and batch_num: 462\n",
      "Loss of train set: 0.23086130619049072 at epoch: 12 and batch_num: 463\n",
      "Loss of train set: 0.2502065598964691 at epoch: 12 and batch_num: 464\n",
      "Loss of train set: 0.3049885332584381 at epoch: 12 and batch_num: 465\n",
      "Loss of train set: 0.2265782654285431 at epoch: 12 and batch_num: 466\n",
      "Loss of train set: 0.2776517868041992 at epoch: 12 and batch_num: 467\n",
      "Loss of train set: 0.32527971267700195 at epoch: 12 and batch_num: 468\n",
      "Loss of train set: 0.20882396399974823 at epoch: 12 and batch_num: 469\n",
      "Loss of train set: 0.30185163021087646 at epoch: 12 and batch_num: 470\n",
      "Loss of train set: 0.22727833688259125 at epoch: 12 and batch_num: 471\n",
      "Loss of train set: 0.1843903362751007 at epoch: 12 and batch_num: 472\n",
      "Loss of train set: 0.2389616072177887 at epoch: 12 and batch_num: 473\n",
      "Loss of train set: 0.20416398346424103 at epoch: 12 and batch_num: 474\n",
      "Loss of train set: 0.2609710097312927 at epoch: 12 and batch_num: 475\n",
      "Loss of train set: 0.2964540719985962 at epoch: 12 and batch_num: 476\n",
      "Loss of train set: 0.387229323387146 at epoch: 12 and batch_num: 477\n",
      "Loss of train set: 0.3787367343902588 at epoch: 12 and batch_num: 478\n",
      "Loss of train set: 0.35073497891426086 at epoch: 12 and batch_num: 479\n",
      "Loss of train set: 0.23662322759628296 at epoch: 12 and batch_num: 480\n",
      "Loss of train set: 0.35762259364128113 at epoch: 12 and batch_num: 481\n",
      "Loss of train set: 0.4493071138858795 at epoch: 12 and batch_num: 482\n",
      "Loss of train set: 0.2804611325263977 at epoch: 12 and batch_num: 483\n",
      "Loss of train set: 0.40437382459640503 at epoch: 12 and batch_num: 484\n",
      "Loss of train set: 0.3875599503517151 at epoch: 12 and batch_num: 485\n",
      "Loss of train set: 0.37043559551239014 at epoch: 12 and batch_num: 486\n",
      "Loss of train set: 0.2831572890281677 at epoch: 12 and batch_num: 487\n",
      "Loss of train set: 0.2728607654571533 at epoch: 12 and batch_num: 488\n",
      "Loss of train set: 0.38130682706832886 at epoch: 12 and batch_num: 489\n",
      "Loss of train set: 0.29903867840766907 at epoch: 12 and batch_num: 490\n",
      "Loss of train set: 0.4094791114330292 at epoch: 12 and batch_num: 491\n",
      "Loss of train set: 0.37308967113494873 at epoch: 12 and batch_num: 492\n",
      "Loss of train set: 0.2208750993013382 at epoch: 12 and batch_num: 493\n",
      "Loss of train set: 0.4536881744861603 at epoch: 12 and batch_num: 494\n",
      "Loss of train set: 0.2654837965965271 at epoch: 12 and batch_num: 495\n",
      "Loss of train set: 0.2736504077911377 at epoch: 12 and batch_num: 496\n",
      "Loss of train set: 0.25445425510406494 at epoch: 12 and batch_num: 497\n",
      "Loss of train set: 0.3388550281524658 at epoch: 12 and batch_num: 498\n",
      "Loss of train set: 0.2598024606704712 at epoch: 12 and batch_num: 499\n",
      "Loss of train set: 0.18731242418289185 at epoch: 12 and batch_num: 500\n",
      "Loss of train set: 0.2792981266975403 at epoch: 12 and batch_num: 501\n",
      "Loss of train set: 0.3629850745201111 at epoch: 12 and batch_num: 502\n",
      "Loss of train set: 0.29191163182258606 at epoch: 12 and batch_num: 503\n",
      "Loss of train set: 0.32047712802886963 at epoch: 12 and batch_num: 504\n",
      "Loss of train set: 0.3074939548969269 at epoch: 12 and batch_num: 505\n",
      "Loss of train set: 0.25030624866485596 at epoch: 12 and batch_num: 506\n",
      "Loss of train set: 0.4247294068336487 at epoch: 12 and batch_num: 507\n",
      "Loss of train set: 0.27723628282546997 at epoch: 12 and batch_num: 508\n",
      "Loss of train set: 0.3201586604118347 at epoch: 12 and batch_num: 509\n",
      "Loss of train set: 0.24846437573432922 at epoch: 12 and batch_num: 510\n",
      "Loss of train set: 0.29771125316619873 at epoch: 12 and batch_num: 511\n",
      "Loss of train set: 0.3172479271888733 at epoch: 12 and batch_num: 512\n",
      "Loss of train set: 0.19678041338920593 at epoch: 12 and batch_num: 513\n",
      "Loss of train set: 0.28437504172325134 at epoch: 12 and batch_num: 514\n",
      "Loss of train set: 0.5726068615913391 at epoch: 12 and batch_num: 515\n",
      "Loss of train set: 0.44965243339538574 at epoch: 12 and batch_num: 516\n",
      "Loss of train set: 0.35663041472435 at epoch: 12 and batch_num: 517\n",
      "Loss of train set: 0.3753281831741333 at epoch: 12 and batch_num: 518\n",
      "Loss of train set: 0.3157784640789032 at epoch: 12 and batch_num: 519\n",
      "Loss of train set: 0.34340834617614746 at epoch: 12 and batch_num: 520\n",
      "Loss of train set: 0.2297075092792511 at epoch: 12 and batch_num: 521\n",
      "Loss of train set: 0.40715861320495605 at epoch: 12 and batch_num: 522\n",
      "Loss of train set: 0.2393728643655777 at epoch: 12 and batch_num: 523\n",
      "Loss of train set: 0.17173480987548828 at epoch: 12 and batch_num: 524\n",
      "Loss of train set: 0.3628808259963989 at epoch: 12 and batch_num: 525\n",
      "Loss of train set: 0.550335168838501 at epoch: 12 and batch_num: 526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.25005412101745605 at epoch: 12 and batch_num: 527\n",
      "Loss of train set: 0.4155738651752472 at epoch: 12 and batch_num: 528\n",
      "Loss of train set: 0.20827364921569824 at epoch: 12 and batch_num: 529\n",
      "Loss of train set: 0.21538391709327698 at epoch: 12 and batch_num: 530\n",
      "Loss of train set: 0.30420249700546265 at epoch: 12 and batch_num: 531\n",
      "Loss of train set: 0.20814856886863708 at epoch: 12 and batch_num: 532\n",
      "Loss of train set: 0.29511380195617676 at epoch: 12 and batch_num: 533\n",
      "Loss of train set: 0.18493947386741638 at epoch: 12 and batch_num: 534\n",
      "Loss of train set: 0.3312325179576874 at epoch: 12 and batch_num: 535\n",
      "Loss of train set: 0.3475174605846405 at epoch: 12 and batch_num: 536\n",
      "Loss of train set: 0.3246946930885315 at epoch: 12 and batch_num: 537\n",
      "Loss of train set: 0.5692456960678101 at epoch: 12 and batch_num: 538\n",
      "Loss of train set: 0.44529423117637634 at epoch: 12 and batch_num: 539\n",
      "Loss of train set: 0.2896454632282257 at epoch: 12 and batch_num: 540\n",
      "Loss of train set: 0.259569376707077 at epoch: 12 and batch_num: 541\n",
      "Loss of train set: 0.28342777490615845 at epoch: 12 and batch_num: 542\n",
      "Loss of train set: 0.30466705560684204 at epoch: 12 and batch_num: 543\n",
      "Loss of train set: 0.29461368918418884 at epoch: 12 and batch_num: 544\n",
      "Loss of train set: 0.17546291649341583 at epoch: 12 and batch_num: 545\n",
      "Loss of train set: 0.2728220522403717 at epoch: 12 and batch_num: 546\n",
      "Loss of train set: 0.3392754793167114 at epoch: 12 and batch_num: 547\n",
      "Loss of train set: 0.21724970638751984 at epoch: 12 and batch_num: 548\n",
      "Loss of train set: 0.195342555642128 at epoch: 12 and batch_num: 549\n",
      "Loss of train set: 0.23810651898384094 at epoch: 12 and batch_num: 550\n",
      "Loss of train set: 0.2068902850151062 at epoch: 12 and batch_num: 551\n",
      "Loss of train set: 0.3319677710533142 at epoch: 12 and batch_num: 552\n",
      "Loss of train set: 0.2923806309700012 at epoch: 12 and batch_num: 553\n",
      "Loss of train set: 0.3417547643184662 at epoch: 12 and batch_num: 554\n",
      "Loss of train set: 0.4156697988510132 at epoch: 12 and batch_num: 555\n",
      "Loss of train set: 0.4446892738342285 at epoch: 12 and batch_num: 556\n",
      "Loss of train set: 0.3870534896850586 at epoch: 12 and batch_num: 557\n",
      "Loss of train set: 0.24528497457504272 at epoch: 12 and batch_num: 558\n",
      "Loss of train set: 0.3893510103225708 at epoch: 12 and batch_num: 559\n",
      "Loss of train set: 0.2768079340457916 at epoch: 12 and batch_num: 560\n",
      "Loss of train set: 0.28511402010917664 at epoch: 12 and batch_num: 561\n",
      "Loss of train set: 0.2972750961780548 at epoch: 12 and batch_num: 562\n",
      "Loss of train set: 0.20715764164924622 at epoch: 12 and batch_num: 563\n",
      "Loss of train set: 0.33083677291870117 at epoch: 12 and batch_num: 564\n",
      "Loss of train set: 0.4266752004623413 at epoch: 12 and batch_num: 565\n",
      "Loss of train set: 0.25298523902893066 at epoch: 12 and batch_num: 566\n",
      "Loss of train set: 0.29494619369506836 at epoch: 12 and batch_num: 567\n",
      "Loss of train set: 0.3507813513278961 at epoch: 12 and batch_num: 568\n",
      "Loss of train set: 0.243403822183609 at epoch: 12 and batch_num: 569\n",
      "Loss of train set: 0.15712979435920715 at epoch: 12 and batch_num: 570\n",
      "Loss of train set: 0.2654605507850647 at epoch: 12 and batch_num: 571\n",
      "Loss of train set: 0.3011578321456909 at epoch: 12 and batch_num: 572\n",
      "Loss of train set: 0.18735730648040771 at epoch: 12 and batch_num: 573\n",
      "Loss of train set: 0.25368523597717285 at epoch: 12 and batch_num: 574\n",
      "Loss of train set: 0.5468740463256836 at epoch: 12 and batch_num: 575\n",
      "Loss of train set: 0.23321188986301422 at epoch: 12 and batch_num: 576\n",
      "Loss of train set: 0.36751532554626465 at epoch: 12 and batch_num: 577\n",
      "Loss of train set: 0.39593908190727234 at epoch: 12 and batch_num: 578\n",
      "Loss of train set: 0.11242331564426422 at epoch: 12 and batch_num: 579\n",
      "Loss of train set: 0.20795035362243652 at epoch: 12 and batch_num: 580\n",
      "Loss of train set: 0.16789627075195312 at epoch: 12 and batch_num: 581\n",
      "Loss of train set: 0.2674994170665741 at epoch: 12 and batch_num: 582\n",
      "Loss of train set: 0.25559312105178833 at epoch: 12 and batch_num: 583\n",
      "Loss of train set: 0.28374314308166504 at epoch: 12 and batch_num: 584\n",
      "Loss of train set: 0.24322369694709778 at epoch: 12 and batch_num: 585\n",
      "Loss of train set: 0.35741472244262695 at epoch: 12 and batch_num: 586\n",
      "Loss of train set: 0.21326416730880737 at epoch: 12 and batch_num: 587\n",
      "Loss of train set: 0.3527912199497223 at epoch: 12 and batch_num: 588\n",
      "Loss of train set: 0.39951464533805847 at epoch: 12 and batch_num: 589\n",
      "Loss of train set: 0.2123103141784668 at epoch: 12 and batch_num: 590\n",
      "Loss of train set: 0.38846004009246826 at epoch: 12 and batch_num: 591\n",
      "Loss of train set: 0.3452546000480652 at epoch: 12 and batch_num: 592\n",
      "Loss of train set: 0.2349754273891449 at epoch: 12 and batch_num: 593\n",
      "Loss of train set: 0.15644483268260956 at epoch: 12 and batch_num: 594\n",
      "Loss of train set: 0.12306350469589233 at epoch: 12 and batch_num: 595\n",
      "Loss of train set: 0.21231874823570251 at epoch: 12 and batch_num: 596\n",
      "Loss of train set: 0.4318411946296692 at epoch: 12 and batch_num: 597\n",
      "Loss of train set: 0.32638296484947205 at epoch: 12 and batch_num: 598\n",
      "Loss of train set: 0.19554263353347778 at epoch: 12 and batch_num: 599\n",
      "Loss of train set: 0.207978755235672 at epoch: 12 and batch_num: 600\n",
      "Loss of train set: 0.19733811914920807 at epoch: 12 and batch_num: 601\n",
      "Loss of train set: 0.20322127640247345 at epoch: 12 and batch_num: 602\n",
      "Loss of train set: 0.5445840358734131 at epoch: 12 and batch_num: 603\n",
      "Loss of train set: 0.24563784897327423 at epoch: 12 and batch_num: 604\n",
      "Loss of train set: 0.1822003275156021 at epoch: 12 and batch_num: 605\n",
      "Loss of train set: 0.18315967917442322 at epoch: 12 and batch_num: 606\n",
      "Loss of train set: 0.34783583879470825 at epoch: 12 and batch_num: 607\n",
      "Loss of train set: 0.24645406007766724 at epoch: 12 and batch_num: 608\n",
      "Loss of train set: 0.3050438165664673 at epoch: 12 and batch_num: 609\n",
      "Loss of train set: 0.28679919242858887 at epoch: 12 and batch_num: 610\n",
      "Loss of train set: 0.3835301995277405 at epoch: 12 and batch_num: 611\n",
      "Loss of train set: 0.23547665774822235 at epoch: 12 and batch_num: 612\n",
      "Loss of train set: 0.2608756721019745 at epoch: 12 and batch_num: 613\n",
      "Loss of train set: 0.43798017501831055 at epoch: 12 and batch_num: 614\n",
      "Loss of train set: 0.19569817185401917 at epoch: 12 and batch_num: 615\n",
      "Loss of train set: 0.3476128578186035 at epoch: 12 and batch_num: 616\n",
      "Loss of train set: 0.4762607514858246 at epoch: 12 and batch_num: 617\n",
      "Loss of train set: 0.32987290620803833 at epoch: 12 and batch_num: 618\n",
      "Loss of train set: 0.2580990195274353 at epoch: 12 and batch_num: 619\n",
      "Loss of train set: 0.23025138676166534 at epoch: 12 and batch_num: 620\n",
      "Loss of train set: 0.3981698155403137 at epoch: 12 and batch_num: 621\n",
      "Loss of train set: 0.40430888533592224 at epoch: 12 and batch_num: 622\n",
      "Loss of train set: 0.266934871673584 at epoch: 12 and batch_num: 623\n",
      "Loss of train set: 0.2824624180793762 at epoch: 12 and batch_num: 624\n",
      "Loss of train set: 0.20959411561489105 at epoch: 12 and batch_num: 625\n",
      "Loss of train set: 0.23282307386398315 at epoch: 12 and batch_num: 626\n",
      "Loss of train set: 0.3422357738018036 at epoch: 12 and batch_num: 627\n",
      "Loss of train set: 0.21271979808807373 at epoch: 12 and batch_num: 628\n",
      "Loss of train set: 0.4042072892189026 at epoch: 12 and batch_num: 629\n",
      "Loss of train set: 0.36837151646614075 at epoch: 12 and batch_num: 630\n",
      "Loss of train set: 0.46236762404441833 at epoch: 12 and batch_num: 631\n",
      "Loss of train set: 0.24096888303756714 at epoch: 12 and batch_num: 632\n",
      "Loss of train set: 0.42125311493873596 at epoch: 12 and batch_num: 633\n",
      "Loss of train set: 0.4484778642654419 at epoch: 12 and batch_num: 634\n",
      "Loss of train set: 0.29101482033729553 at epoch: 12 and batch_num: 635\n",
      "Loss of train set: 0.27435171604156494 at epoch: 12 and batch_num: 636\n",
      "Loss of train set: 0.407306045293808 at epoch: 12 and batch_num: 637\n",
      "Loss of train set: 0.3096948266029358 at epoch: 12 and batch_num: 638\n",
      "Loss of train set: 0.24929159879684448 at epoch: 12 and batch_num: 639\n",
      "Loss of train set: 0.5152267813682556 at epoch: 12 and batch_num: 640\n",
      "Loss of train set: 0.22331219911575317 at epoch: 12 and batch_num: 641\n",
      "Loss of train set: 0.2726321816444397 at epoch: 12 and batch_num: 642\n",
      "Loss of train set: 0.34245187044143677 at epoch: 12 and batch_num: 643\n",
      "Loss of train set: 0.3514537811279297 at epoch: 12 and batch_num: 644\n",
      "Loss of train set: 0.374578058719635 at epoch: 12 and batch_num: 645\n",
      "Loss of train set: 0.15644128620624542 at epoch: 12 and batch_num: 646\n",
      "Loss of train set: 0.23455381393432617 at epoch: 12 and batch_num: 647\n",
      "Loss of train set: 0.23961485922336578 at epoch: 12 and batch_num: 648\n",
      "Loss of train set: 0.26185423135757446 at epoch: 12 and batch_num: 649\n",
      "Loss of train set: 0.3011589050292969 at epoch: 12 and batch_num: 650\n",
      "Loss of train set: 0.17376002669334412 at epoch: 12 and batch_num: 651\n",
      "Loss of train set: 0.26570597290992737 at epoch: 12 and batch_num: 652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2381487637758255 at epoch: 12 and batch_num: 653\n",
      "Loss of train set: 0.3873264193534851 at epoch: 12 and batch_num: 654\n",
      "Loss of train set: 0.3844379186630249 at epoch: 12 and batch_num: 655\n",
      "Loss of train set: 0.3813883662223816 at epoch: 12 and batch_num: 656\n",
      "Loss of train set: 0.4623450040817261 at epoch: 12 and batch_num: 657\n",
      "Loss of train set: 0.21047502756118774 at epoch: 12 and batch_num: 658\n",
      "Loss of train set: 0.41807255148887634 at epoch: 12 and batch_num: 659\n",
      "Loss of train set: 0.24735990166664124 at epoch: 12 and batch_num: 660\n",
      "Loss of train set: 0.6207600831985474 at epoch: 12 and batch_num: 661\n",
      "Loss of train set: 0.2768189311027527 at epoch: 12 and batch_num: 662\n",
      "Loss of train set: 0.253727525472641 at epoch: 12 and batch_num: 663\n",
      "Loss of train set: 0.23355638980865479 at epoch: 12 and batch_num: 664\n",
      "Loss of train set: 0.2542206645011902 at epoch: 12 and batch_num: 665\n",
      "Loss of train set: 0.5193249583244324 at epoch: 12 and batch_num: 666\n",
      "Loss of train set: 0.35584256052970886 at epoch: 12 and batch_num: 667\n",
      "Loss of train set: 0.2178342491388321 at epoch: 12 and batch_num: 668\n",
      "Loss of train set: 0.18498045206069946 at epoch: 12 and batch_num: 669\n",
      "Loss of train set: 0.28569740056991577 at epoch: 12 and batch_num: 670\n",
      "Loss of train set: 0.34041285514831543 at epoch: 12 and batch_num: 671\n",
      "Loss of train set: 0.18919417262077332 at epoch: 12 and batch_num: 672\n",
      "Loss of train set: 0.38730210065841675 at epoch: 12 and batch_num: 673\n",
      "Loss of train set: 0.26832395792007446 at epoch: 12 and batch_num: 674\n",
      "Loss of train set: 0.2830737233161926 at epoch: 12 and batch_num: 675\n",
      "Loss of train set: 0.29153358936309814 at epoch: 12 and batch_num: 676\n",
      "Loss of train set: 0.32134827971458435 at epoch: 12 and batch_num: 677\n",
      "Loss of train set: 0.2720033526420593 at epoch: 12 and batch_num: 678\n",
      "Loss of train set: 0.29090121388435364 at epoch: 12 and batch_num: 679\n",
      "Loss of train set: 0.3291933536529541 at epoch: 12 and batch_num: 680\n",
      "Loss of train set: 0.3836425840854645 at epoch: 12 and batch_num: 681\n",
      "Loss of train set: 0.3699449300765991 at epoch: 12 and batch_num: 682\n",
      "Loss of train set: 0.37392866611480713 at epoch: 12 and batch_num: 683\n",
      "Loss of train set: 0.3937145173549652 at epoch: 12 and batch_num: 684\n",
      "Loss of train set: 0.21180886030197144 at epoch: 12 and batch_num: 685\n",
      "Loss of train set: 0.35592809319496155 at epoch: 12 and batch_num: 686\n",
      "Loss of train set: 0.47163689136505127 at epoch: 12 and batch_num: 687\n",
      "Loss of train set: 0.13422761857509613 at epoch: 12 and batch_num: 688\n",
      "Loss of train set: 0.3162965178489685 at epoch: 12 and batch_num: 689\n",
      "Loss of train set: 0.194289892911911 at epoch: 12 and batch_num: 690\n",
      "Loss of train set: 0.3898763656616211 at epoch: 12 and batch_num: 691\n",
      "Loss of train set: 0.259968638420105 at epoch: 12 and batch_num: 692\n",
      "Loss of train set: 0.3672453761100769 at epoch: 12 and batch_num: 693\n",
      "Loss of train set: 0.24732545018196106 at epoch: 12 and batch_num: 694\n",
      "Loss of train set: 0.30822867155075073 at epoch: 12 and batch_num: 695\n",
      "Loss of train set: 0.30856603384017944 at epoch: 12 and batch_num: 696\n",
      "Loss of train set: 0.38532179594039917 at epoch: 12 and batch_num: 697\n",
      "Loss of train set: 0.21077832579612732 at epoch: 12 and batch_num: 698\n",
      "Loss of train set: 0.2994501292705536 at epoch: 12 and batch_num: 699\n",
      "Loss of train set: 0.32507529854774475 at epoch: 12 and batch_num: 700\n",
      "Loss of train set: 0.3069566786289215 at epoch: 12 and batch_num: 701\n",
      "Loss of train set: 0.34582003951072693 at epoch: 12 and batch_num: 702\n",
      "Loss of train set: 0.3548470139503479 at epoch: 12 and batch_num: 703\n",
      "Loss of train set: 0.3779160678386688 at epoch: 12 and batch_num: 704\n",
      "Loss of train set: 0.4612765908241272 at epoch: 12 and batch_num: 705\n",
      "Loss of train set: 0.479550302028656 at epoch: 12 and batch_num: 706\n",
      "Loss of train set: 0.3046512007713318 at epoch: 12 and batch_num: 707\n",
      "Loss of train set: 0.42221587896347046 at epoch: 12 and batch_num: 708\n",
      "Loss of train set: 0.290723979473114 at epoch: 12 and batch_num: 709\n",
      "Loss of train set: 0.27832716703414917 at epoch: 12 and batch_num: 710\n",
      "Loss of train set: 0.41364967823028564 at epoch: 12 and batch_num: 711\n",
      "Loss of train set: 0.506612241268158 at epoch: 12 and batch_num: 712\n",
      "Loss of train set: 0.2955668270587921 at epoch: 12 and batch_num: 713\n",
      "Loss of train set: 0.28036734461784363 at epoch: 12 and batch_num: 714\n",
      "Loss of train set: 0.3374759256839752 at epoch: 12 and batch_num: 715\n",
      "Loss of train set: 0.21119827032089233 at epoch: 12 and batch_num: 716\n",
      "Loss of train set: 0.2106764316558838 at epoch: 12 and batch_num: 717\n",
      "Loss of train set: 0.5115090608596802 at epoch: 12 and batch_num: 718\n",
      "Loss of train set: 0.2504723072052002 at epoch: 12 and batch_num: 719\n",
      "Loss of train set: 0.29613256454467773 at epoch: 12 and batch_num: 720\n",
      "Loss of train set: 0.2148224115371704 at epoch: 12 and batch_num: 721\n",
      "Loss of train set: 0.40750840306282043 at epoch: 12 and batch_num: 722\n",
      "Loss of train set: 0.384164422750473 at epoch: 12 and batch_num: 723\n",
      "Loss of train set: 0.3014233112335205 at epoch: 12 and batch_num: 724\n",
      "Loss of train set: 0.3470386564731598 at epoch: 12 and batch_num: 725\n",
      "Loss of train set: 0.38002148270606995 at epoch: 12 and batch_num: 726\n",
      "Loss of train set: 0.26194822788238525 at epoch: 12 and batch_num: 727\n",
      "Loss of train set: 0.5063279271125793 at epoch: 12 and batch_num: 728\n",
      "Loss of train set: 0.21478040516376495 at epoch: 12 and batch_num: 729\n",
      "Loss of train set: 0.3107569217681885 at epoch: 12 and batch_num: 730\n",
      "Loss of train set: 0.4652988314628601 at epoch: 12 and batch_num: 731\n",
      "Loss of train set: 0.2541026473045349 at epoch: 12 and batch_num: 732\n",
      "Loss of train set: 0.2636033892631531 at epoch: 12 and batch_num: 733\n",
      "Loss of train set: 0.30928468704223633 at epoch: 12 and batch_num: 734\n",
      "Loss of train set: 0.3264896869659424 at epoch: 12 and batch_num: 735\n",
      "Loss of train set: 0.3539440929889679 at epoch: 12 and batch_num: 736\n",
      "Loss of train set: 0.29791709780693054 at epoch: 12 and batch_num: 737\n",
      "Loss of train set: 0.47242188453674316 at epoch: 12 and batch_num: 738\n",
      "Loss of train set: 0.34923332929611206 at epoch: 12 and batch_num: 739\n",
      "Loss of train set: 0.5041730403900146 at epoch: 12 and batch_num: 740\n",
      "Loss of train set: 0.3202076554298401 at epoch: 12 and batch_num: 741\n",
      "Loss of train set: 0.4189697504043579 at epoch: 12 and batch_num: 742\n",
      "Loss of train set: 0.19713541865348816 at epoch: 12 and batch_num: 743\n",
      "Loss of train set: 0.23077097535133362 at epoch: 12 and batch_num: 744\n",
      "Loss of train set: 0.3169577717781067 at epoch: 12 and batch_num: 745\n",
      "Loss of train set: 0.29557138681411743 at epoch: 12 and batch_num: 746\n",
      "Loss of train set: 0.334031879901886 at epoch: 12 and batch_num: 747\n",
      "Loss of train set: 0.5598313808441162 at epoch: 12 and batch_num: 748\n",
      "Loss of train set: 0.30761808156967163 at epoch: 12 and batch_num: 749\n",
      "Loss of train set: 0.2937942445278168 at epoch: 12 and batch_num: 750\n",
      "Loss of train set: 0.4394707679748535 at epoch: 12 and batch_num: 751\n",
      "Loss of train set: 0.21944282948970795 at epoch: 12 and batch_num: 752\n",
      "Loss of train set: 0.3880971670150757 at epoch: 12 and batch_num: 753\n",
      "Loss of train set: 0.28380274772644043 at epoch: 12 and batch_num: 754\n",
      "Loss of train set: 0.369168221950531 at epoch: 12 and batch_num: 755\n",
      "Loss of train set: 0.4382220208644867 at epoch: 12 and batch_num: 756\n",
      "Loss of train set: 0.33128786087036133 at epoch: 12 and batch_num: 757\n",
      "Loss of train set: 0.4291774332523346 at epoch: 12 and batch_num: 758\n",
      "Loss of train set: 0.21221129596233368 at epoch: 12 and batch_num: 759\n",
      "Loss of train set: 0.125898540019989 at epoch: 12 and batch_num: 760\n",
      "Loss of train set: 0.2219451367855072 at epoch: 12 and batch_num: 761\n",
      "Loss of train set: 0.22565911710262299 at epoch: 12 and batch_num: 762\n",
      "Loss of train set: 0.293981671333313 at epoch: 12 and batch_num: 763\n",
      "Loss of train set: 0.36531466245651245 at epoch: 12 and batch_num: 764\n",
      "Loss of train set: 0.31954425573349 at epoch: 12 and batch_num: 765\n",
      "Loss of train set: 0.28184765577316284 at epoch: 12 and batch_num: 766\n",
      "Loss of train set: 0.15067698061466217 at epoch: 12 and batch_num: 767\n",
      "Loss of train set: 0.3295572102069855 at epoch: 12 and batch_num: 768\n",
      "Loss of train set: 0.3448125123977661 at epoch: 12 and batch_num: 769\n",
      "Loss of train set: 0.40492895245552063 at epoch: 12 and batch_num: 770\n",
      "Loss of train set: 0.3484669327735901 at epoch: 12 and batch_num: 771\n",
      "Loss of train set: 0.34503287076950073 at epoch: 12 and batch_num: 772\n",
      "Loss of train set: 0.21577289700508118 at epoch: 12 and batch_num: 773\n",
      "Loss of train set: 0.3605487048625946 at epoch: 12 and batch_num: 774\n",
      "Loss of train set: 0.49873244762420654 at epoch: 12 and batch_num: 775\n",
      "Loss of train set: 0.2263292521238327 at epoch: 12 and batch_num: 776\n",
      "Loss of train set: 0.25150883197784424 at epoch: 12 and batch_num: 777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.22324998676776886 at epoch: 12 and batch_num: 778\n",
      "Loss of train set: 0.2996710240840912 at epoch: 12 and batch_num: 779\n",
      "Loss of train set: 0.3352237045764923 at epoch: 12 and batch_num: 780\n",
      "Loss of train set: 0.34311050176620483 at epoch: 12 and batch_num: 781\n",
      "Loss of train set: 0.23413261771202087 at epoch: 12 and batch_num: 782\n",
      "Loss of train set: 0.1835443377494812 at epoch: 12 and batch_num: 783\n",
      "Loss of train set: 0.3401067852973938 at epoch: 12 and batch_num: 784\n",
      "Loss of train set: 0.20330646634101868 at epoch: 12 and batch_num: 785\n",
      "Loss of train set: 0.2845015525817871 at epoch: 12 and batch_num: 786\n",
      "Loss of train set: 0.2565639615058899 at epoch: 12 and batch_num: 787\n",
      "Loss of train set: 0.4216768741607666 at epoch: 12 and batch_num: 788\n",
      "Loss of train set: 0.3054797351360321 at epoch: 12 and batch_num: 789\n",
      "Loss of train set: 0.4603188633918762 at epoch: 12 and batch_num: 790\n",
      "Loss of train set: 0.3220086693763733 at epoch: 12 and batch_num: 791\n",
      "Loss of train set: 0.2207474261522293 at epoch: 12 and batch_num: 792\n",
      "Loss of train set: 0.20062977075576782 at epoch: 12 and batch_num: 793\n",
      "Loss of train set: 0.2774427831172943 at epoch: 12 and batch_num: 794\n",
      "Loss of train set: 0.2851625382900238 at epoch: 12 and batch_num: 795\n",
      "Loss of train set: 0.29233166575431824 at epoch: 12 and batch_num: 796\n",
      "Loss of train set: 0.31574589014053345 at epoch: 12 and batch_num: 797\n",
      "Loss of train set: 0.34918662905693054 at epoch: 12 and batch_num: 798\n",
      "Loss of train set: 0.38875341415405273 at epoch: 12 and batch_num: 799\n",
      "Loss of train set: 0.39581426978111267 at epoch: 12 and batch_num: 800\n",
      "Loss of train set: 0.3526827394962311 at epoch: 12 and batch_num: 801\n",
      "Loss of train set: 0.21368540823459625 at epoch: 12 and batch_num: 802\n",
      "Loss of train set: 0.21257631480693817 at epoch: 12 and batch_num: 803\n",
      "Loss of train set: 0.24688038229942322 at epoch: 12 and batch_num: 804\n",
      "Loss of train set: 0.25710564851760864 at epoch: 12 and batch_num: 805\n",
      "Loss of train set: 0.38770514726638794 at epoch: 12 and batch_num: 806\n",
      "Loss of train set: 0.4065478444099426 at epoch: 12 and batch_num: 807\n",
      "Loss of train set: 0.43091022968292236 at epoch: 12 and batch_num: 808\n",
      "Loss of train set: 0.6153351068496704 at epoch: 12 and batch_num: 809\n",
      "Loss of train set: 0.26985907554626465 at epoch: 12 and batch_num: 810\n",
      "Loss of train set: 0.31990379095077515 at epoch: 12 and batch_num: 811\n",
      "Loss of train set: 0.28480690717697144 at epoch: 12 and batch_num: 812\n",
      "Loss of train set: 0.4061206579208374 at epoch: 12 and batch_num: 813\n",
      "Loss of train set: 0.33479470014572144 at epoch: 12 and batch_num: 814\n",
      "Loss of train set: 0.35525578260421753 at epoch: 12 and batch_num: 815\n",
      "Loss of train set: 0.39283931255340576 at epoch: 12 and batch_num: 816\n",
      "Loss of train set: 0.33792340755462646 at epoch: 12 and batch_num: 817\n",
      "Loss of train set: 0.35851389169692993 at epoch: 12 and batch_num: 818\n",
      "Loss of train set: 0.3565584123134613 at epoch: 12 and batch_num: 819\n",
      "Loss of train set: 0.3722763657569885 at epoch: 12 and batch_num: 820\n",
      "Loss of train set: 0.24130256474018097 at epoch: 12 and batch_num: 821\n",
      "Loss of train set: 0.2572382688522339 at epoch: 12 and batch_num: 822\n",
      "Loss of train set: 0.34341704845428467 at epoch: 12 and batch_num: 823\n",
      "Loss of train set: 0.1736484169960022 at epoch: 12 and batch_num: 824\n",
      "Loss of train set: 0.3922266364097595 at epoch: 12 and batch_num: 825\n",
      "Loss of train set: 0.3037376403808594 at epoch: 12 and batch_num: 826\n",
      "Loss of train set: 0.3528459668159485 at epoch: 12 and batch_num: 827\n",
      "Loss of train set: 0.2597953677177429 at epoch: 12 and batch_num: 828\n",
      "Loss of train set: 0.29955559968948364 at epoch: 12 and batch_num: 829\n",
      "Loss of train set: 0.41458940505981445 at epoch: 12 and batch_num: 830\n",
      "Loss of train set: 0.2259809374809265 at epoch: 12 and batch_num: 831\n",
      "Loss of train set: 0.263313889503479 at epoch: 12 and batch_num: 832\n",
      "Loss of train set: 0.24685759842395782 at epoch: 12 and batch_num: 833\n",
      "Loss of train set: 0.361819863319397 at epoch: 12 and batch_num: 834\n",
      "Loss of train set: 0.322668194770813 at epoch: 12 and batch_num: 835\n",
      "Loss of train set: 0.3207823932170868 at epoch: 12 and batch_num: 836\n",
      "Loss of train set: 0.29242074489593506 at epoch: 12 and batch_num: 837\n",
      "Loss of train set: 0.23263710737228394 at epoch: 12 and batch_num: 838\n",
      "Loss of train set: 0.36638158559799194 at epoch: 12 and batch_num: 839\n",
      "Loss of train set: 0.18652626872062683 at epoch: 12 and batch_num: 840\n",
      "Loss of train set: 0.2775586247444153 at epoch: 12 and batch_num: 841\n",
      "Loss of train set: 0.33472535014152527 at epoch: 12 and batch_num: 842\n",
      "Loss of train set: 0.29324913024902344 at epoch: 12 and batch_num: 843\n",
      "Loss of train set: 0.25038617849349976 at epoch: 12 and batch_num: 844\n",
      "Loss of train set: 0.1825065314769745 at epoch: 12 and batch_num: 845\n",
      "Loss of train set: 0.2962992787361145 at epoch: 12 and batch_num: 846\n",
      "Loss of train set: 0.29611292481422424 at epoch: 12 and batch_num: 847\n",
      "Loss of train set: 0.23169255256652832 at epoch: 12 and batch_num: 848\n",
      "Loss of train set: 0.316266268491745 at epoch: 12 and batch_num: 849\n",
      "Loss of train set: 0.2240067720413208 at epoch: 12 and batch_num: 850\n",
      "Loss of train set: 0.5580853819847107 at epoch: 12 and batch_num: 851\n",
      "Loss of train set: 0.17993684113025665 at epoch: 12 and batch_num: 852\n",
      "Loss of train set: 0.3176003694534302 at epoch: 12 and batch_num: 853\n",
      "Loss of train set: 0.38520383834838867 at epoch: 12 and batch_num: 854\n",
      "Loss of train set: 0.36332809925079346 at epoch: 12 and batch_num: 855\n",
      "Loss of train set: 0.3402825593948364 at epoch: 12 and batch_num: 856\n",
      "Loss of train set: 0.3379761874675751 at epoch: 12 and batch_num: 857\n",
      "Loss of train set: 0.2776741087436676 at epoch: 12 and batch_num: 858\n",
      "Loss of train set: 0.25046294927597046 at epoch: 12 and batch_num: 859\n",
      "Loss of train set: 0.3469417095184326 at epoch: 12 and batch_num: 860\n",
      "Loss of train set: 0.4016086161136627 at epoch: 12 and batch_num: 861\n",
      "Loss of train set: 0.2820690870285034 at epoch: 12 and batch_num: 862\n",
      "Loss of train set: 0.2370205521583557 at epoch: 12 and batch_num: 863\n",
      "Loss of train set: 0.2554885149002075 at epoch: 12 and batch_num: 864\n",
      "Loss of train set: 0.31090009212493896 at epoch: 12 and batch_num: 865\n",
      "Loss of train set: 0.17875489592552185 at epoch: 12 and batch_num: 866\n",
      "Loss of train set: 0.3005949556827545 at epoch: 12 and batch_num: 867\n",
      "Loss of train set: 0.16760322451591492 at epoch: 12 and batch_num: 868\n",
      "Loss of train set: 0.28776729106903076 at epoch: 12 and batch_num: 869\n",
      "Loss of train set: 0.42121291160583496 at epoch: 12 and batch_num: 870\n",
      "Loss of train set: 0.5528450012207031 at epoch: 12 and batch_num: 871\n",
      "Loss of train set: 0.2555691599845886 at epoch: 12 and batch_num: 872\n",
      "Loss of train set: 0.37270742654800415 at epoch: 12 and batch_num: 873\n",
      "Loss of train set: 0.2924730181694031 at epoch: 12 and batch_num: 874\n",
      "Loss of train set: 0.3547692596912384 at epoch: 12 and batch_num: 875\n",
      "Loss of train set: 0.26263296604156494 at epoch: 12 and batch_num: 876\n",
      "Loss of train set: 0.16314314305782318 at epoch: 12 and batch_num: 877\n",
      "Loss of train set: 0.27942681312561035 at epoch: 12 and batch_num: 878\n",
      "Loss of train set: 0.38284367322921753 at epoch: 12 and batch_num: 879\n",
      "Loss of train set: 0.5121588706970215 at epoch: 12 and batch_num: 880\n",
      "Loss of train set: 0.2116893231868744 at epoch: 12 and batch_num: 881\n",
      "Loss of train set: 0.5453656911849976 at epoch: 12 and batch_num: 882\n",
      "Loss of train set: 0.20005130767822266 at epoch: 12 and batch_num: 883\n",
      "Loss of train set: 0.2724638879299164 at epoch: 12 and batch_num: 884\n",
      "Loss of train set: 0.35886865854263306 at epoch: 12 and batch_num: 885\n",
      "Loss of train set: 0.4420502781867981 at epoch: 12 and batch_num: 886\n",
      "Loss of train set: 0.5248007774353027 at epoch: 12 and batch_num: 887\n",
      "Loss of train set: 0.34055715799331665 at epoch: 12 and batch_num: 888\n",
      "Loss of train set: 0.49379193782806396 at epoch: 12 and batch_num: 889\n",
      "Loss of train set: 0.2690107822418213 at epoch: 12 and batch_num: 890\n",
      "Loss of train set: 0.18151211738586426 at epoch: 12 and batch_num: 891\n",
      "Loss of train set: 0.4458942413330078 at epoch: 12 and batch_num: 892\n",
      "Loss of train set: 0.3694031238555908 at epoch: 12 and batch_num: 893\n",
      "Loss of train set: 0.24362367391586304 at epoch: 12 and batch_num: 894\n",
      "Loss of train set: 0.2627313733100891 at epoch: 12 and batch_num: 895\n",
      "Loss of train set: 0.5222997069358826 at epoch: 12 and batch_num: 896\n",
      "Loss of train set: 0.2786288261413574 at epoch: 12 and batch_num: 897\n",
      "Loss of train set: 0.31401491165161133 at epoch: 12 and batch_num: 898\n",
      "Loss of train set: 0.26777011156082153 at epoch: 12 and batch_num: 899\n",
      "Loss of train set: 0.3070850670337677 at epoch: 12 and batch_num: 900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.22003087401390076 at epoch: 12 and batch_num: 901\n",
      "Loss of train set: 0.350771963596344 at epoch: 12 and batch_num: 902\n",
      "Loss of train set: 0.32397985458374023 at epoch: 12 and batch_num: 903\n",
      "Loss of train set: 0.3070050776004791 at epoch: 12 and batch_num: 904\n",
      "Loss of train set: 0.160049170255661 at epoch: 12 and batch_num: 905\n",
      "Loss of train set: 0.2578663229942322 at epoch: 12 and batch_num: 906\n",
      "Loss of train set: 0.35650718212127686 at epoch: 12 and batch_num: 907\n",
      "Loss of train set: 0.3294198513031006 at epoch: 12 and batch_num: 908\n",
      "Loss of train set: 0.25970548391342163 at epoch: 12 and batch_num: 909\n",
      "Loss of train set: 0.3768268823623657 at epoch: 12 and batch_num: 910\n",
      "Loss of train set: 0.1218462586402893 at epoch: 12 and batch_num: 911\n",
      "Loss of train set: 0.2504812180995941 at epoch: 12 and batch_num: 912\n",
      "Loss of train set: 0.18323025107383728 at epoch: 12 and batch_num: 913\n",
      "Loss of train set: 0.3135582208633423 at epoch: 12 and batch_num: 914\n",
      "Loss of train set: 0.2830333113670349 at epoch: 12 and batch_num: 915\n",
      "Loss of train set: 0.2250038981437683 at epoch: 12 and batch_num: 916\n",
      "Loss of train set: 0.34887972474098206 at epoch: 12 and batch_num: 917\n",
      "Loss of train set: 0.30456435680389404 at epoch: 12 and batch_num: 918\n",
      "Loss of train set: 0.34966447949409485 at epoch: 12 and batch_num: 919\n",
      "Loss of train set: 0.16121618449687958 at epoch: 12 and batch_num: 920\n",
      "Loss of train set: 0.20144908130168915 at epoch: 12 and batch_num: 921\n",
      "Loss of train set: 0.19018979370594025 at epoch: 12 and batch_num: 922\n",
      "Loss of train set: 0.18038123846054077 at epoch: 12 and batch_num: 923\n",
      "Loss of train set: 0.2545829713344574 at epoch: 12 and batch_num: 924\n",
      "Loss of train set: 0.2988593876361847 at epoch: 12 and batch_num: 925\n",
      "Loss of train set: 0.5512640476226807 at epoch: 12 and batch_num: 926\n",
      "Loss of train set: 0.2747751474380493 at epoch: 12 and batch_num: 927\n",
      "Loss of train set: 0.24160446226596832 at epoch: 12 and batch_num: 928\n",
      "Loss of train set: 0.33562564849853516 at epoch: 12 and batch_num: 929\n",
      "Loss of train set: 0.2083812654018402 at epoch: 12 and batch_num: 930\n",
      "Loss of train set: 0.37085914611816406 at epoch: 12 and batch_num: 931\n",
      "Loss of train set: 0.3266727328300476 at epoch: 12 and batch_num: 932\n",
      "Loss of train set: 0.2890937924385071 at epoch: 12 and batch_num: 933\n",
      "Loss of train set: 0.24957692623138428 at epoch: 12 and batch_num: 934\n",
      "Loss of train set: 0.2713925242424011 at epoch: 12 and batch_num: 935\n",
      "Loss of train set: 0.17062552273273468 at epoch: 12 and batch_num: 936\n",
      "Loss of train set: 0.2792840003967285 at epoch: 12 and batch_num: 937\n",
      "Accuracy of train set: 0.8875333333333333\n",
      "Loss of test set: 0.4419909417629242 at epoch: 12 and batch_num: 0\n",
      "Loss of test set: 0.36856669187545776 at epoch: 12 and batch_num: 1\n",
      "Loss of test set: 0.47091007232666016 at epoch: 12 and batch_num: 2\n",
      "Loss of test set: 0.3641936480998993 at epoch: 12 and batch_num: 3\n",
      "Loss of test set: 0.3657337427139282 at epoch: 12 and batch_num: 4\n",
      "Loss of test set: 0.4687468409538269 at epoch: 12 and batch_num: 5\n",
      "Loss of test set: 0.2707362174987793 at epoch: 12 and batch_num: 6\n",
      "Loss of test set: 0.33375048637390137 at epoch: 12 and batch_num: 7\n",
      "Loss of test set: 0.35892871022224426 at epoch: 12 and batch_num: 8\n",
      "Loss of test set: 0.389504075050354 at epoch: 12 and batch_num: 9\n",
      "Loss of test set: 0.5076093673706055 at epoch: 12 and batch_num: 10\n",
      "Loss of test set: 0.34320443868637085 at epoch: 12 and batch_num: 11\n",
      "Loss of test set: 0.36442774534225464 at epoch: 12 and batch_num: 12\n",
      "Loss of test set: 0.43574321269989014 at epoch: 12 and batch_num: 13\n",
      "Loss of test set: 0.20833885669708252 at epoch: 12 and batch_num: 14\n",
      "Loss of test set: 0.4391856789588928 at epoch: 12 and batch_num: 15\n",
      "Loss of test set: 0.33958160877227783 at epoch: 12 and batch_num: 16\n",
      "Loss of test set: 0.4167424440383911 at epoch: 12 and batch_num: 17\n",
      "Loss of test set: 0.5377258658409119 at epoch: 12 and batch_num: 18\n",
      "Loss of test set: 0.5919928550720215 at epoch: 12 and batch_num: 19\n",
      "Loss of test set: 0.5172072649002075 at epoch: 12 and batch_num: 20\n",
      "Loss of test set: 0.23545107245445251 at epoch: 12 and batch_num: 21\n",
      "Loss of test set: 0.48107388615608215 at epoch: 12 and batch_num: 22\n",
      "Loss of test set: 0.4503403306007385 at epoch: 12 and batch_num: 23\n",
      "Loss of test set: 0.21542687714099884 at epoch: 12 and batch_num: 24\n",
      "Loss of test set: 0.440926194190979 at epoch: 12 and batch_num: 25\n",
      "Loss of test set: 0.22764761745929718 at epoch: 12 and batch_num: 26\n",
      "Loss of test set: 0.3785794973373413 at epoch: 12 and batch_num: 27\n",
      "Loss of test set: 0.21279272437095642 at epoch: 12 and batch_num: 28\n",
      "Loss of test set: 0.32510462403297424 at epoch: 12 and batch_num: 29\n",
      "Loss of test set: 0.3237771689891815 at epoch: 12 and batch_num: 30\n",
      "Loss of test set: 0.36429959535598755 at epoch: 12 and batch_num: 31\n",
      "Loss of test set: 0.3357924818992615 at epoch: 12 and batch_num: 32\n",
      "Loss of test set: 0.2667883336544037 at epoch: 12 and batch_num: 33\n",
      "Loss of test set: 0.3146107792854309 at epoch: 12 and batch_num: 34\n",
      "Loss of test set: 0.3960241675376892 at epoch: 12 and batch_num: 35\n",
      "Loss of test set: 0.3498828411102295 at epoch: 12 and batch_num: 36\n",
      "Loss of test set: 0.3621474802494049 at epoch: 12 and batch_num: 37\n",
      "Loss of test set: 0.19829079508781433 at epoch: 12 and batch_num: 38\n",
      "Loss of test set: 0.3626072406768799 at epoch: 12 and batch_num: 39\n",
      "Loss of test set: 0.2655869722366333 at epoch: 12 and batch_num: 40\n",
      "Loss of test set: 0.28103768825531006 at epoch: 12 and batch_num: 41\n",
      "Loss of test set: 0.20867052674293518 at epoch: 12 and batch_num: 42\n",
      "Loss of test set: 0.33818161487579346 at epoch: 12 and batch_num: 43\n",
      "Loss of test set: 0.29201340675354004 at epoch: 12 and batch_num: 44\n",
      "Loss of test set: 0.3643007278442383 at epoch: 12 and batch_num: 45\n",
      "Loss of test set: 0.2663007378578186 at epoch: 12 and batch_num: 46\n",
      "Loss of test set: 0.2906019687652588 at epoch: 12 and batch_num: 47\n",
      "Loss of test set: 0.18675604462623596 at epoch: 12 and batch_num: 48\n",
      "Loss of test set: 0.4925062656402588 at epoch: 12 and batch_num: 49\n",
      "Loss of test set: 0.2554042637348175 at epoch: 12 and batch_num: 50\n",
      "Loss of test set: 0.4289015531539917 at epoch: 12 and batch_num: 51\n",
      "Loss of test set: 0.3702940344810486 at epoch: 12 and batch_num: 52\n",
      "Loss of test set: 0.29589715600013733 at epoch: 12 and batch_num: 53\n",
      "Loss of test set: 0.32111966609954834 at epoch: 12 and batch_num: 54\n",
      "Loss of test set: 0.4560946226119995 at epoch: 12 and batch_num: 55\n",
      "Loss of test set: 0.520205020904541 at epoch: 12 and batch_num: 56\n",
      "Loss of test set: 0.35086897015571594 at epoch: 12 and batch_num: 57\n",
      "Loss of test set: 0.5007134079933167 at epoch: 12 and batch_num: 58\n",
      "Loss of test set: 0.34674322605133057 at epoch: 12 and batch_num: 59\n",
      "Loss of test set: 0.2745794653892517 at epoch: 12 and batch_num: 60\n",
      "Loss of test set: 0.3678704500198364 at epoch: 12 and batch_num: 61\n",
      "Loss of test set: 0.4276527166366577 at epoch: 12 and batch_num: 62\n",
      "Loss of test set: 0.3266788721084595 at epoch: 12 and batch_num: 63\n",
      "Loss of test set: 0.3172162175178528 at epoch: 12 and batch_num: 64\n",
      "Loss of test set: 0.3493817448616028 at epoch: 12 and batch_num: 65\n",
      "Loss of test set: 0.2900925278663635 at epoch: 12 and batch_num: 66\n",
      "Loss of test set: 0.39945387840270996 at epoch: 12 and batch_num: 67\n",
      "Loss of test set: 0.29016679525375366 at epoch: 12 and batch_num: 68\n",
      "Loss of test set: 0.3864409327507019 at epoch: 12 and batch_num: 69\n",
      "Loss of test set: 0.3685658276081085 at epoch: 12 and batch_num: 70\n",
      "Loss of test set: 0.598598301410675 at epoch: 12 and batch_num: 71\n",
      "Loss of test set: 0.40809184312820435 at epoch: 12 and batch_num: 72\n",
      "Loss of test set: 0.39669984579086304 at epoch: 12 and batch_num: 73\n",
      "Loss of test set: 0.4638845920562744 at epoch: 12 and batch_num: 74\n",
      "Loss of test set: 0.4398574233055115 at epoch: 12 and batch_num: 75\n",
      "Loss of test set: 0.38525307178497314 at epoch: 12 and batch_num: 76\n",
      "Loss of test set: 0.3018338084220886 at epoch: 12 and batch_num: 77\n",
      "Loss of test set: 0.5455636978149414 at epoch: 12 and batch_num: 78\n",
      "Loss of test set: 0.3437027037143707 at epoch: 12 and batch_num: 79\n",
      "Loss of test set: 0.32627683877944946 at epoch: 12 and batch_num: 80\n",
      "Loss of test set: 0.3665206730365753 at epoch: 12 and batch_num: 81\n",
      "Loss of test set: 0.37410616874694824 at epoch: 12 and batch_num: 82\n",
      "Loss of test set: 0.5204372406005859 at epoch: 12 and batch_num: 83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.34902065992355347 at epoch: 12 and batch_num: 84\n",
      "Loss of test set: 0.5092304944992065 at epoch: 12 and batch_num: 85\n",
      "Loss of test set: 0.4179996848106384 at epoch: 12 and batch_num: 86\n",
      "Loss of test set: 0.4490968883037567 at epoch: 12 and batch_num: 87\n",
      "Loss of test set: 0.34537339210510254 at epoch: 12 and batch_num: 88\n",
      "Loss of test set: 0.27523452043533325 at epoch: 12 and batch_num: 89\n",
      "Loss of test set: 0.2993725538253784 at epoch: 12 and batch_num: 90\n",
      "Loss of test set: 0.35679200291633606 at epoch: 12 and batch_num: 91\n",
      "Loss of test set: 0.30153888463974 at epoch: 12 and batch_num: 92\n",
      "Loss of test set: 0.40505722165107727 at epoch: 12 and batch_num: 93\n",
      "Loss of test set: 0.4123207926750183 at epoch: 12 and batch_num: 94\n",
      "Loss of test set: 0.4489790201187134 at epoch: 12 and batch_num: 95\n",
      "Loss of test set: 0.4636877775192261 at epoch: 12 and batch_num: 96\n",
      "Loss of test set: 0.3422220051288605 at epoch: 12 and batch_num: 97\n",
      "Loss of test set: 0.4315010905265808 at epoch: 12 and batch_num: 98\n",
      "Loss of test set: 0.4992104172706604 at epoch: 12 and batch_num: 99\n",
      "Loss of test set: 0.35945528745651245 at epoch: 12 and batch_num: 100\n",
      "Loss of test set: 0.4679643511772156 at epoch: 12 and batch_num: 101\n",
      "Loss of test set: 0.5123374462127686 at epoch: 12 and batch_num: 102\n",
      "Loss of test set: 0.31497931480407715 at epoch: 12 and batch_num: 103\n",
      "Loss of test set: 0.6485868096351624 at epoch: 12 and batch_num: 104\n",
      "Loss of test set: 0.2764827609062195 at epoch: 12 and batch_num: 105\n",
      "Loss of test set: 0.3293340504169464 at epoch: 12 and batch_num: 106\n",
      "Loss of test set: 0.48324155807495117 at epoch: 12 and batch_num: 107\n",
      "Loss of test set: 0.4188896119594574 at epoch: 12 and batch_num: 108\n",
      "Loss of test set: 0.391292929649353 at epoch: 12 and batch_num: 109\n",
      "Loss of test set: 0.2838281989097595 at epoch: 12 and batch_num: 110\n",
      "Loss of test set: 0.4624004364013672 at epoch: 12 and batch_num: 111\n",
      "Loss of test set: 0.5567464232444763 at epoch: 12 and batch_num: 112\n",
      "Loss of test set: 0.44418275356292725 at epoch: 12 and batch_num: 113\n",
      "Loss of test set: 0.2540706396102905 at epoch: 12 and batch_num: 114\n",
      "Loss of test set: 0.5128529071807861 at epoch: 12 and batch_num: 115\n",
      "Loss of test set: 0.4386391341686249 at epoch: 12 and batch_num: 116\n",
      "Loss of test set: 0.3520694971084595 at epoch: 12 and batch_num: 117\n",
      "Loss of test set: 0.24358844757080078 at epoch: 12 and batch_num: 118\n",
      "Loss of test set: 0.4530870318412781 at epoch: 12 and batch_num: 119\n",
      "Loss of test set: 0.3614315688610077 at epoch: 12 and batch_num: 120\n",
      "Loss of test set: 0.21986089646816254 at epoch: 12 and batch_num: 121\n",
      "Loss of test set: 0.4519531726837158 at epoch: 12 and batch_num: 122\n",
      "Loss of test set: 0.4265214204788208 at epoch: 12 and batch_num: 123\n",
      "Loss of test set: 0.21108707785606384 at epoch: 12 and batch_num: 124\n",
      "Loss of test set: 0.28043675422668457 at epoch: 12 and batch_num: 125\n",
      "Loss of test set: 0.3563559055328369 at epoch: 12 and batch_num: 126\n",
      "Loss of test set: 0.43152159452438354 at epoch: 12 and batch_num: 127\n",
      "Loss of test set: 0.3655165135860443 at epoch: 12 and batch_num: 128\n",
      "Loss of test set: 0.5860130786895752 at epoch: 12 and batch_num: 129\n",
      "Loss of test set: 0.42418205738067627 at epoch: 12 and batch_num: 130\n",
      "Loss of test set: 0.642054557800293 at epoch: 12 and batch_num: 131\n",
      "Loss of test set: 0.34053727984428406 at epoch: 12 and batch_num: 132\n",
      "Loss of test set: 0.449594646692276 at epoch: 12 and batch_num: 133\n",
      "Loss of test set: 0.4076640009880066 at epoch: 12 and batch_num: 134\n",
      "Loss of test set: 0.4533279538154602 at epoch: 12 and batch_num: 135\n",
      "Loss of test set: 0.30598628520965576 at epoch: 12 and batch_num: 136\n",
      "Loss of test set: 0.338083416223526 at epoch: 12 and batch_num: 137\n",
      "Loss of test set: 0.20090019702911377 at epoch: 12 and batch_num: 138\n",
      "Loss of test set: 0.2472267597913742 at epoch: 12 and batch_num: 139\n",
      "Loss of test set: 0.2671031355857849 at epoch: 12 and batch_num: 140\n",
      "Loss of test set: 0.40200984477996826 at epoch: 12 and batch_num: 141\n",
      "Loss of test set: 0.41618889570236206 at epoch: 12 and batch_num: 142\n",
      "Loss of test set: 0.45254620909690857 at epoch: 12 and batch_num: 143\n",
      "Loss of test set: 0.33725064992904663 at epoch: 12 and batch_num: 144\n",
      "Loss of test set: 0.3283989429473877 at epoch: 12 and batch_num: 145\n",
      "Loss of test set: 0.3847009241580963 at epoch: 12 and batch_num: 146\n",
      "Loss of test set: 0.5185545682907104 at epoch: 12 and batch_num: 147\n",
      "Loss of test set: 0.4542168378829956 at epoch: 12 and batch_num: 148\n",
      "Loss of test set: 0.2340077906847 at epoch: 12 and batch_num: 149\n",
      "Loss of test set: 0.3122503161430359 at epoch: 12 and batch_num: 150\n",
      "Loss of test set: 0.4263196885585785 at epoch: 12 and batch_num: 151\n",
      "Loss of test set: 0.3168354034423828 at epoch: 12 and batch_num: 152\n",
      "Loss of test set: 0.38157355785369873 at epoch: 12 and batch_num: 153\n",
      "Loss of test set: 0.37553536891937256 at epoch: 12 and batch_num: 154\n",
      "Loss of test set: 0.6279717683792114 at epoch: 12 and batch_num: 155\n",
      "Loss of test set: 0.9687856435775757 at epoch: 12 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8639\n",
      "Loss of train set: 0.2401476502418518 at epoch: 13 and batch_num: 0\n",
      "Loss of train set: 0.22826102375984192 at epoch: 13 and batch_num: 1\n",
      "Loss of train set: 0.502495288848877 at epoch: 13 and batch_num: 2\n",
      "Loss of train set: 0.22771495580673218 at epoch: 13 and batch_num: 3\n",
      "Loss of train set: 0.18461011350154877 at epoch: 13 and batch_num: 4\n",
      "Loss of train set: 0.4784165918827057 at epoch: 13 and batch_num: 5\n",
      "Loss of train set: 0.27950331568717957 at epoch: 13 and batch_num: 6\n",
      "Loss of train set: 0.21908488869667053 at epoch: 13 and batch_num: 7\n",
      "Loss of train set: 0.3156343400478363 at epoch: 13 and batch_num: 8\n",
      "Loss of train set: 0.37619340419769287 at epoch: 13 and batch_num: 9\n",
      "Loss of train set: 0.29608064889907837 at epoch: 13 and batch_num: 10\n",
      "Loss of train set: 0.16317933797836304 at epoch: 13 and batch_num: 11\n",
      "Loss of train set: 0.2984595000743866 at epoch: 13 and batch_num: 12\n",
      "Loss of train set: 0.3538590669631958 at epoch: 13 and batch_num: 13\n",
      "Loss of train set: 0.3552091121673584 at epoch: 13 and batch_num: 14\n",
      "Loss of train set: 0.398296058177948 at epoch: 13 and batch_num: 15\n",
      "Loss of train set: 0.4487687647342682 at epoch: 13 and batch_num: 16\n",
      "Loss of train set: 0.3045940697193146 at epoch: 13 and batch_num: 17\n",
      "Loss of train set: 0.5325717926025391 at epoch: 13 and batch_num: 18\n",
      "Loss of train set: 0.21340733766555786 at epoch: 13 and batch_num: 19\n",
      "Loss of train set: 0.2853504419326782 at epoch: 13 and batch_num: 20\n",
      "Loss of train set: 0.2488989233970642 at epoch: 13 and batch_num: 21\n",
      "Loss of train set: 0.2773509919643402 at epoch: 13 and batch_num: 22\n",
      "Loss of train set: 0.27374887466430664 at epoch: 13 and batch_num: 23\n",
      "Loss of train set: 0.18235450983047485 at epoch: 13 and batch_num: 24\n",
      "Loss of train set: 0.2769879698753357 at epoch: 13 and batch_num: 25\n",
      "Loss of train set: 0.295684278011322 at epoch: 13 and batch_num: 26\n",
      "Loss of train set: 0.41027015447616577 at epoch: 13 and batch_num: 27\n",
      "Loss of train set: 0.19869402050971985 at epoch: 13 and batch_num: 28\n",
      "Loss of train set: 0.27481627464294434 at epoch: 13 and batch_num: 29\n",
      "Loss of train set: 0.2025415003299713 at epoch: 13 and batch_num: 30\n",
      "Loss of train set: 0.18444670736789703 at epoch: 13 and batch_num: 31\n",
      "Loss of train set: 0.32357174158096313 at epoch: 13 and batch_num: 32\n",
      "Loss of train set: 0.37097927927970886 at epoch: 13 and batch_num: 33\n",
      "Loss of train set: 0.258758008480072 at epoch: 13 and batch_num: 34\n",
      "Loss of train set: 0.16638483107089996 at epoch: 13 and batch_num: 35\n",
      "Loss of train set: 0.14855539798736572 at epoch: 13 and batch_num: 36\n",
      "Loss of train set: 0.3102032244205475 at epoch: 13 and batch_num: 37\n",
      "Loss of train set: 0.37355443835258484 at epoch: 13 and batch_num: 38\n",
      "Loss of train set: 0.34594741463661194 at epoch: 13 and batch_num: 39\n",
      "Loss of train set: 0.34610188007354736 at epoch: 13 and batch_num: 40\n",
      "Loss of train set: 0.29924294352531433 at epoch: 13 and batch_num: 41\n",
      "Loss of train set: 0.29703348875045776 at epoch: 13 and batch_num: 42\n",
      "Loss of train set: 0.17973527312278748 at epoch: 13 and batch_num: 43\n",
      "Loss of train set: 0.42628198862075806 at epoch: 13 and batch_num: 44\n",
      "Loss of train set: 0.29586470127105713 at epoch: 13 and batch_num: 45\n",
      "Loss of train set: 0.5352464914321899 at epoch: 13 and batch_num: 46\n",
      "Loss of train set: 0.3790203928947449 at epoch: 13 and batch_num: 47\n",
      "Loss of train set: 0.330841064453125 at epoch: 13 and batch_num: 48\n",
      "Loss of train set: 0.2825315296649933 at epoch: 13 and batch_num: 49\n",
      "Loss of train set: 0.16306202113628387 at epoch: 13 and batch_num: 50\n",
      "Loss of train set: 0.1951025128364563 at epoch: 13 and batch_num: 51\n",
      "Loss of train set: 0.22329102456569672 at epoch: 13 and batch_num: 52\n",
      "Loss of train set: 0.23362308740615845 at epoch: 13 and batch_num: 53\n",
      "Loss of train set: 0.26636362075805664 at epoch: 13 and batch_num: 54\n",
      "Loss of train set: 0.2647654712200165 at epoch: 13 and batch_num: 55\n",
      "Loss of train set: 0.39377087354660034 at epoch: 13 and batch_num: 56\n",
      "Loss of train set: 0.24859501421451569 at epoch: 13 and batch_num: 57\n",
      "Loss of train set: 0.30855074524879456 at epoch: 13 and batch_num: 58\n",
      "Loss of train set: 0.36815041303634644 at epoch: 13 and batch_num: 59\n",
      "Loss of train set: 0.35440802574157715 at epoch: 13 and batch_num: 60\n",
      "Loss of train set: 0.385621041059494 at epoch: 13 and batch_num: 61\n",
      "Loss of train set: 0.3638267517089844 at epoch: 13 and batch_num: 62\n",
      "Loss of train set: 0.3638438582420349 at epoch: 13 and batch_num: 63\n",
      "Loss of train set: 0.25079846382141113 at epoch: 13 and batch_num: 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.12178996205329895 at epoch: 13 and batch_num: 65\n",
      "Loss of train set: 0.21182259917259216 at epoch: 13 and batch_num: 66\n",
      "Loss of train set: 0.23557789623737335 at epoch: 13 and batch_num: 67\n",
      "Loss of train set: 0.305927574634552 at epoch: 13 and batch_num: 68\n",
      "Loss of train set: 0.20800381898880005 at epoch: 13 and batch_num: 69\n",
      "Loss of train set: 0.1928650289773941 at epoch: 13 and batch_num: 70\n",
      "Loss of train set: 0.16979628801345825 at epoch: 13 and batch_num: 71\n",
      "Loss of train set: 0.3091279864311218 at epoch: 13 and batch_num: 72\n",
      "Loss of train set: 0.31648844480514526 at epoch: 13 and batch_num: 73\n",
      "Loss of train set: 0.3105849027633667 at epoch: 13 and batch_num: 74\n",
      "Loss of train set: 0.48369529843330383 at epoch: 13 and batch_num: 75\n",
      "Loss of train set: 0.11204735189676285 at epoch: 13 and batch_num: 76\n",
      "Loss of train set: 0.19924533367156982 at epoch: 13 and batch_num: 77\n",
      "Loss of train set: 0.2607225477695465 at epoch: 13 and batch_num: 78\n",
      "Loss of train set: 0.30153927206993103 at epoch: 13 and batch_num: 79\n",
      "Loss of train set: 0.29129883646965027 at epoch: 13 and batch_num: 80\n",
      "Loss of train set: 0.28194063901901245 at epoch: 13 and batch_num: 81\n",
      "Loss of train set: 0.3479914665222168 at epoch: 13 and batch_num: 82\n",
      "Loss of train set: 0.3316494822502136 at epoch: 13 and batch_num: 83\n",
      "Loss of train set: 0.3363798260688782 at epoch: 13 and batch_num: 84\n",
      "Loss of train set: 0.3101333975791931 at epoch: 13 and batch_num: 85\n",
      "Loss of train set: 0.3292597830295563 at epoch: 13 and batch_num: 86\n",
      "Loss of train set: 0.22005382180213928 at epoch: 13 and batch_num: 87\n",
      "Loss of train set: 0.39112478494644165 at epoch: 13 and batch_num: 88\n",
      "Loss of train set: 0.2661813199520111 at epoch: 13 and batch_num: 89\n",
      "Loss of train set: 0.3823167681694031 at epoch: 13 and batch_num: 90\n",
      "Loss of train set: 0.33907264471054077 at epoch: 13 and batch_num: 91\n",
      "Loss of train set: 0.24459455907344818 at epoch: 13 and batch_num: 92\n",
      "Loss of train set: 0.28143060207366943 at epoch: 13 and batch_num: 93\n",
      "Loss of train set: 0.2855922281742096 at epoch: 13 and batch_num: 94\n",
      "Loss of train set: 0.31646960973739624 at epoch: 13 and batch_num: 95\n",
      "Loss of train set: 0.38213688135147095 at epoch: 13 and batch_num: 96\n",
      "Loss of train set: 0.40427878499031067 at epoch: 13 and batch_num: 97\n",
      "Loss of train set: 0.4578860402107239 at epoch: 13 and batch_num: 98\n",
      "Loss of train set: 0.44731730222702026 at epoch: 13 and batch_num: 99\n",
      "Loss of train set: 0.25454258918762207 at epoch: 13 and batch_num: 100\n",
      "Loss of train set: 0.46010878682136536 at epoch: 13 and batch_num: 101\n",
      "Loss of train set: 0.19598917663097382 at epoch: 13 and batch_num: 102\n",
      "Loss of train set: 0.1965939700603485 at epoch: 13 and batch_num: 103\n",
      "Loss of train set: 0.21344299614429474 at epoch: 13 and batch_num: 104\n",
      "Loss of train set: 0.20514976978302002 at epoch: 13 and batch_num: 105\n",
      "Loss of train set: 0.3621336817741394 at epoch: 13 and batch_num: 106\n",
      "Loss of train set: 0.21177935600280762 at epoch: 13 and batch_num: 107\n",
      "Loss of train set: 0.20291227102279663 at epoch: 13 and batch_num: 108\n",
      "Loss of train set: 0.28957486152648926 at epoch: 13 and batch_num: 109\n",
      "Loss of train set: 0.22366294264793396 at epoch: 13 and batch_num: 110\n",
      "Loss of train set: 0.18410027027130127 at epoch: 13 and batch_num: 111\n",
      "Loss of train set: 0.4318144917488098 at epoch: 13 and batch_num: 112\n",
      "Loss of train set: 0.29785168170928955 at epoch: 13 and batch_num: 113\n",
      "Loss of train set: 0.26743561029434204 at epoch: 13 and batch_num: 114\n",
      "Loss of train set: 0.22774112224578857 at epoch: 13 and batch_num: 115\n",
      "Loss of train set: 0.274850070476532 at epoch: 13 and batch_num: 116\n",
      "Loss of train set: 0.19272369146347046 at epoch: 13 and batch_num: 117\n",
      "Loss of train set: 0.3663480877876282 at epoch: 13 and batch_num: 118\n",
      "Loss of train set: 0.21949639916419983 at epoch: 13 and batch_num: 119\n",
      "Loss of train set: 0.37460148334503174 at epoch: 13 and batch_num: 120\n",
      "Loss of train set: 0.19912958145141602 at epoch: 13 and batch_num: 121\n",
      "Loss of train set: 0.3478311598300934 at epoch: 13 and batch_num: 122\n",
      "Loss of train set: 0.28163206577301025 at epoch: 13 and batch_num: 123\n",
      "Loss of train set: 0.2481268048286438 at epoch: 13 and batch_num: 124\n",
      "Loss of train set: 0.24087253212928772 at epoch: 13 and batch_num: 125\n",
      "Loss of train set: 0.19820092618465424 at epoch: 13 and batch_num: 126\n",
      "Loss of train set: 0.4506104588508606 at epoch: 13 and batch_num: 127\n",
      "Loss of train set: 0.23588493466377258 at epoch: 13 and batch_num: 128\n",
      "Loss of train set: 0.36202144622802734 at epoch: 13 and batch_num: 129\n",
      "Loss of train set: 0.30579495429992676 at epoch: 13 and batch_num: 130\n",
      "Loss of train set: 0.2967844605445862 at epoch: 13 and batch_num: 131\n",
      "Loss of train set: 0.20665419101715088 at epoch: 13 and batch_num: 132\n",
      "Loss of train set: 0.3289931118488312 at epoch: 13 and batch_num: 133\n",
      "Loss of train set: 0.3312752842903137 at epoch: 13 and batch_num: 134\n",
      "Loss of train set: 0.30792236328125 at epoch: 13 and batch_num: 135\n",
      "Loss of train set: 0.27129119634628296 at epoch: 13 and batch_num: 136\n",
      "Loss of train set: 0.21363094449043274 at epoch: 13 and batch_num: 137\n",
      "Loss of train set: 0.13446062803268433 at epoch: 13 and batch_num: 138\n",
      "Loss of train set: 0.36692288517951965 at epoch: 13 and batch_num: 139\n",
      "Loss of train set: 0.2649666666984558 at epoch: 13 and batch_num: 140\n",
      "Loss of train set: 0.3682495951652527 at epoch: 13 and batch_num: 141\n",
      "Loss of train set: 0.4664340615272522 at epoch: 13 and batch_num: 142\n",
      "Loss of train set: 0.29305535554885864 at epoch: 13 and batch_num: 143\n",
      "Loss of train set: 0.2608593702316284 at epoch: 13 and batch_num: 144\n",
      "Loss of train set: 0.3082267642021179 at epoch: 13 and batch_num: 145\n",
      "Loss of train set: 0.3357990086078644 at epoch: 13 and batch_num: 146\n",
      "Loss of train set: 0.4067843556404114 at epoch: 13 and batch_num: 147\n",
      "Loss of train set: 0.38855576515197754 at epoch: 13 and batch_num: 148\n",
      "Loss of train set: 0.25283077359199524 at epoch: 13 and batch_num: 149\n",
      "Loss of train set: 0.19154231250286102 at epoch: 13 and batch_num: 150\n",
      "Loss of train set: 0.2528035342693329 at epoch: 13 and batch_num: 151\n",
      "Loss of train set: 0.24048921465873718 at epoch: 13 and batch_num: 152\n",
      "Loss of train set: 0.3941493034362793 at epoch: 13 and batch_num: 153\n",
      "Loss of train set: 0.33908170461654663 at epoch: 13 and batch_num: 154\n",
      "Loss of train set: 0.2832378149032593 at epoch: 13 and batch_num: 155\n",
      "Loss of train set: 0.20053935050964355 at epoch: 13 and batch_num: 156\n",
      "Loss of train set: 0.1875290870666504 at epoch: 13 and batch_num: 157\n",
      "Loss of train set: 0.22199350595474243 at epoch: 13 and batch_num: 158\n",
      "Loss of train set: 0.2232205718755722 at epoch: 13 and batch_num: 159\n",
      "Loss of train set: 0.23843806982040405 at epoch: 13 and batch_num: 160\n",
      "Loss of train set: 0.1923932135105133 at epoch: 13 and batch_num: 161\n",
      "Loss of train set: 0.38110634684562683 at epoch: 13 and batch_num: 162\n",
      "Loss of train set: 0.14726196229457855 at epoch: 13 and batch_num: 163\n",
      "Loss of train set: 0.43162399530410767 at epoch: 13 and batch_num: 164\n",
      "Loss of train set: 0.382327675819397 at epoch: 13 and batch_num: 165\n",
      "Loss of train set: 0.26748722791671753 at epoch: 13 and batch_num: 166\n",
      "Loss of train set: 0.21890908479690552 at epoch: 13 and batch_num: 167\n",
      "Loss of train set: 0.37136125564575195 at epoch: 13 and batch_num: 168\n",
      "Loss of train set: 0.4358749985694885 at epoch: 13 and batch_num: 169\n",
      "Loss of train set: 0.3925783634185791 at epoch: 13 and batch_num: 170\n",
      "Loss of train set: 0.38409343361854553 at epoch: 13 and batch_num: 171\n",
      "Loss of train set: 0.24013292789459229 at epoch: 13 and batch_num: 172\n",
      "Loss of train set: 0.3210757374763489 at epoch: 13 and batch_num: 173\n",
      "Loss of train set: 0.3086124658584595 at epoch: 13 and batch_num: 174\n",
      "Loss of train set: 0.25837457180023193 at epoch: 13 and batch_num: 175\n",
      "Loss of train set: 0.203848734498024 at epoch: 13 and batch_num: 176\n",
      "Loss of train set: 0.25927963852882385 at epoch: 13 and batch_num: 177\n",
      "Loss of train set: 0.4005258083343506 at epoch: 13 and batch_num: 178\n",
      "Loss of train set: 0.35350242257118225 at epoch: 13 and batch_num: 179\n",
      "Loss of train set: 0.3224226236343384 at epoch: 13 and batch_num: 180\n",
      "Loss of train set: 0.2242380827665329 at epoch: 13 and batch_num: 181\n",
      "Loss of train set: 0.26686540246009827 at epoch: 13 and batch_num: 182\n",
      "Loss of train set: 0.12422456592321396 at epoch: 13 and batch_num: 183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.23825067281723022 at epoch: 13 and batch_num: 184\n",
      "Loss of train set: 0.42460888624191284 at epoch: 13 and batch_num: 185\n",
      "Loss of train set: 0.3089892566204071 at epoch: 13 and batch_num: 186\n",
      "Loss of train set: 0.41618627309799194 at epoch: 13 and batch_num: 187\n",
      "Loss of train set: 0.34459924697875977 at epoch: 13 and batch_num: 188\n",
      "Loss of train set: 0.34820735454559326 at epoch: 13 and batch_num: 189\n",
      "Loss of train set: 0.2832917869091034 at epoch: 13 and batch_num: 190\n",
      "Loss of train set: 0.22122623026371002 at epoch: 13 and batch_num: 191\n",
      "Loss of train set: 0.39287233352661133 at epoch: 13 and batch_num: 192\n",
      "Loss of train set: 0.28529268503189087 at epoch: 13 and batch_num: 193\n",
      "Loss of train set: 0.31981438398361206 at epoch: 13 and batch_num: 194\n",
      "Loss of train set: 0.33588868379592896 at epoch: 13 and batch_num: 195\n",
      "Loss of train set: 0.2050490826368332 at epoch: 13 and batch_num: 196\n",
      "Loss of train set: 0.16630278527736664 at epoch: 13 and batch_num: 197\n",
      "Loss of train set: 0.18556448817253113 at epoch: 13 and batch_num: 198\n",
      "Loss of train set: 0.5573568344116211 at epoch: 13 and batch_num: 199\n",
      "Loss of train set: 0.5987290143966675 at epoch: 13 and batch_num: 200\n",
      "Loss of train set: 0.2910359501838684 at epoch: 13 and batch_num: 201\n",
      "Loss of train set: 0.26741325855255127 at epoch: 13 and batch_num: 202\n",
      "Loss of train set: 0.19390997290611267 at epoch: 13 and batch_num: 203\n",
      "Loss of train set: 0.3585606813430786 at epoch: 13 and batch_num: 204\n",
      "Loss of train set: 0.30269694328308105 at epoch: 13 and batch_num: 205\n",
      "Loss of train set: 0.32446783781051636 at epoch: 13 and batch_num: 206\n",
      "Loss of train set: 0.274232417345047 at epoch: 13 and batch_num: 207\n",
      "Loss of train set: 0.24854084849357605 at epoch: 13 and batch_num: 208\n",
      "Loss of train set: 0.2609877288341522 at epoch: 13 and batch_num: 209\n",
      "Loss of train set: 0.3693707585334778 at epoch: 13 and batch_num: 210\n",
      "Loss of train set: 0.3361513018608093 at epoch: 13 and batch_num: 211\n",
      "Loss of train set: 0.38210058212280273 at epoch: 13 and batch_num: 212\n",
      "Loss of train set: 0.34127479791641235 at epoch: 13 and batch_num: 213\n",
      "Loss of train set: 0.32862311601638794 at epoch: 13 and batch_num: 214\n",
      "Loss of train set: 0.2707051634788513 at epoch: 13 and batch_num: 215\n",
      "Loss of train set: 0.389026015996933 at epoch: 13 and batch_num: 216\n",
      "Loss of train set: 0.27370625734329224 at epoch: 13 and batch_num: 217\n",
      "Loss of train set: 0.27980491518974304 at epoch: 13 and batch_num: 218\n",
      "Loss of train set: 0.3251096308231354 at epoch: 13 and batch_num: 219\n",
      "Loss of train set: 0.15197691321372986 at epoch: 13 and batch_num: 220\n",
      "Loss of train set: 0.29114651679992676 at epoch: 13 and batch_num: 221\n",
      "Loss of train set: 0.30275729298591614 at epoch: 13 and batch_num: 222\n",
      "Loss of train set: 0.2305523008108139 at epoch: 13 and batch_num: 223\n",
      "Loss of train set: 0.2248380482196808 at epoch: 13 and batch_num: 224\n",
      "Loss of train set: 0.2491474151611328 at epoch: 13 and batch_num: 225\n",
      "Loss of train set: 0.390899658203125 at epoch: 13 and batch_num: 226\n",
      "Loss of train set: 0.2557644546031952 at epoch: 13 and batch_num: 227\n",
      "Loss of train set: 0.25800591707229614 at epoch: 13 and batch_num: 228\n",
      "Loss of train set: 0.40303850173950195 at epoch: 13 and batch_num: 229\n",
      "Loss of train set: 0.320570170879364 at epoch: 13 and batch_num: 230\n",
      "Loss of train set: 0.24502764642238617 at epoch: 13 and batch_num: 231\n",
      "Loss of train set: 0.43054112792015076 at epoch: 13 and batch_num: 232\n",
      "Loss of train set: 0.3088957667350769 at epoch: 13 and batch_num: 233\n",
      "Loss of train set: 0.35045289993286133 at epoch: 13 and batch_num: 234\n",
      "Loss of train set: 0.17612507939338684 at epoch: 13 and batch_num: 235\n",
      "Loss of train set: 0.2931014895439148 at epoch: 13 and batch_num: 236\n",
      "Loss of train set: 0.3966803252696991 at epoch: 13 and batch_num: 237\n",
      "Loss of train set: 0.3870420455932617 at epoch: 13 and batch_num: 238\n",
      "Loss of train set: 0.15539073944091797 at epoch: 13 and batch_num: 239\n",
      "Loss of train set: 0.2414090931415558 at epoch: 13 and batch_num: 240\n",
      "Loss of train set: 0.23057830333709717 at epoch: 13 and batch_num: 241\n",
      "Loss of train set: 0.31873345375061035 at epoch: 13 and batch_num: 242\n",
      "Loss of train set: 0.23309575021266937 at epoch: 13 and batch_num: 243\n",
      "Loss of train set: 0.34433722496032715 at epoch: 13 and batch_num: 244\n",
      "Loss of train set: 0.3671928644180298 at epoch: 13 and batch_num: 245\n",
      "Loss of train set: 0.3329516649246216 at epoch: 13 and batch_num: 246\n",
      "Loss of train set: 0.4140032231807709 at epoch: 13 and batch_num: 247\n",
      "Loss of train set: 0.5544082522392273 at epoch: 13 and batch_num: 248\n",
      "Loss of train set: 0.3062615692615509 at epoch: 13 and batch_num: 249\n",
      "Loss of train set: 0.4642711281776428 at epoch: 13 and batch_num: 250\n",
      "Loss of train set: 0.14668455719947815 at epoch: 13 and batch_num: 251\n",
      "Loss of train set: 0.4851112961769104 at epoch: 13 and batch_num: 252\n",
      "Loss of train set: 0.1912003755569458 at epoch: 13 and batch_num: 253\n",
      "Loss of train set: 0.33742615580558777 at epoch: 13 and batch_num: 254\n",
      "Loss of train set: 0.38373202085494995 at epoch: 13 and batch_num: 255\n",
      "Loss of train set: 0.3383813202381134 at epoch: 13 and batch_num: 256\n",
      "Loss of train set: 0.3008359968662262 at epoch: 13 and batch_num: 257\n",
      "Loss of train set: 0.29939553141593933 at epoch: 13 and batch_num: 258\n",
      "Loss of train set: 0.22370824217796326 at epoch: 13 and batch_num: 259\n",
      "Loss of train set: 0.2618553936481476 at epoch: 13 and batch_num: 260\n",
      "Loss of train set: 0.3320125639438629 at epoch: 13 and batch_num: 261\n",
      "Loss of train set: 0.2991465628147125 at epoch: 13 and batch_num: 262\n",
      "Loss of train set: 0.32467934489250183 at epoch: 13 and batch_num: 263\n",
      "Loss of train set: 0.2111457884311676 at epoch: 13 and batch_num: 264\n",
      "Loss of train set: 0.25932586193084717 at epoch: 13 and batch_num: 265\n",
      "Loss of train set: 0.3230326473712921 at epoch: 13 and batch_num: 266\n",
      "Loss of train set: 0.22855785489082336 at epoch: 13 and batch_num: 267\n",
      "Loss of train set: 0.42004477977752686 at epoch: 13 and batch_num: 268\n",
      "Loss of train set: 0.29440075159072876 at epoch: 13 and batch_num: 269\n",
      "Loss of train set: 0.22455322742462158 at epoch: 13 and batch_num: 270\n",
      "Loss of train set: 0.20225940644741058 at epoch: 13 and batch_num: 271\n",
      "Loss of train set: 0.3557755947113037 at epoch: 13 and batch_num: 272\n",
      "Loss of train set: 0.17975759506225586 at epoch: 13 and batch_num: 273\n",
      "Loss of train set: 0.42851492762565613 at epoch: 13 and batch_num: 274\n",
      "Loss of train set: 0.34781065583229065 at epoch: 13 and batch_num: 275\n",
      "Loss of train set: 0.3191404342651367 at epoch: 13 and batch_num: 276\n",
      "Loss of train set: 0.31036508083343506 at epoch: 13 and batch_num: 277\n",
      "Loss of train set: 0.44742903113365173 at epoch: 13 and batch_num: 278\n",
      "Loss of train set: 0.3771257996559143 at epoch: 13 and batch_num: 279\n",
      "Loss of train set: 0.24902789294719696 at epoch: 13 and batch_num: 280\n",
      "Loss of train set: 0.24084551632404327 at epoch: 13 and batch_num: 281\n",
      "Loss of train set: 0.1233973503112793 at epoch: 13 and batch_num: 282\n",
      "Loss of train set: 0.2416105568408966 at epoch: 13 and batch_num: 283\n",
      "Loss of train set: 0.16425621509552002 at epoch: 13 and batch_num: 284\n",
      "Loss of train set: 0.2970488369464874 at epoch: 13 and batch_num: 285\n",
      "Loss of train set: 0.3952866792678833 at epoch: 13 and batch_num: 286\n",
      "Loss of train set: 0.16507834196090698 at epoch: 13 and batch_num: 287\n",
      "Loss of train set: 0.3568405508995056 at epoch: 13 and batch_num: 288\n",
      "Loss of train set: 0.26768961548805237 at epoch: 13 and batch_num: 289\n",
      "Loss of train set: 0.2761785089969635 at epoch: 13 and batch_num: 290\n",
      "Loss of train set: 0.23646044731140137 at epoch: 13 and batch_num: 291\n",
      "Loss of train set: 0.3228222727775574 at epoch: 13 and batch_num: 292\n",
      "Loss of train set: 0.33028000593185425 at epoch: 13 and batch_num: 293\n",
      "Loss of train set: 0.29574647545814514 at epoch: 13 and batch_num: 294\n",
      "Loss of train set: 0.3275347054004669 at epoch: 13 and batch_num: 295\n",
      "Loss of train set: 0.21385200321674347 at epoch: 13 and batch_num: 296\n",
      "Loss of train set: 0.29438918828964233 at epoch: 13 and batch_num: 297\n",
      "Loss of train set: 0.22850945591926575 at epoch: 13 and batch_num: 298\n",
      "Loss of train set: 0.4789746403694153 at epoch: 13 and batch_num: 299\n",
      "Loss of train set: 0.2736429274082184 at epoch: 13 and batch_num: 300\n",
      "Loss of train set: 0.4118824005126953 at epoch: 13 and batch_num: 301\n",
      "Loss of train set: 0.507702648639679 at epoch: 13 and batch_num: 302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2640574872493744 at epoch: 13 and batch_num: 303\n",
      "Loss of train set: 0.41571223735809326 at epoch: 13 and batch_num: 304\n",
      "Loss of train set: 0.23962879180908203 at epoch: 13 and batch_num: 305\n",
      "Loss of train set: 0.44310158491134644 at epoch: 13 and batch_num: 306\n",
      "Loss of train set: 0.34045642614364624 at epoch: 13 and batch_num: 307\n",
      "Loss of train set: 0.540783166885376 at epoch: 13 and batch_num: 308\n",
      "Loss of train set: 0.26230329275131226 at epoch: 13 and batch_num: 309\n",
      "Loss of train set: 0.4376046061515808 at epoch: 13 and batch_num: 310\n",
      "Loss of train set: 0.21926487982273102 at epoch: 13 and batch_num: 311\n",
      "Loss of train set: 0.2730840742588043 at epoch: 13 and batch_num: 312\n",
      "Loss of train set: 0.2572845220565796 at epoch: 13 and batch_num: 313\n",
      "Loss of train set: 0.24563682079315186 at epoch: 13 and batch_num: 314\n",
      "Loss of train set: 0.17050109803676605 at epoch: 13 and batch_num: 315\n",
      "Loss of train set: 0.41263529658317566 at epoch: 13 and batch_num: 316\n",
      "Loss of train set: 0.16206398606300354 at epoch: 13 and batch_num: 317\n",
      "Loss of train set: 0.22550749778747559 at epoch: 13 and batch_num: 318\n",
      "Loss of train set: 0.22330424189567566 at epoch: 13 and batch_num: 319\n",
      "Loss of train set: 0.32385265827178955 at epoch: 13 and batch_num: 320\n",
      "Loss of train set: 0.20166169106960297 at epoch: 13 and batch_num: 321\n",
      "Loss of train set: 0.23500388860702515 at epoch: 13 and batch_num: 322\n",
      "Loss of train set: 0.21146441996097565 at epoch: 13 and batch_num: 323\n",
      "Loss of train set: 0.48132628202438354 at epoch: 13 and batch_num: 324\n",
      "Loss of train set: 0.2108841836452484 at epoch: 13 and batch_num: 325\n",
      "Loss of train set: 0.3269612193107605 at epoch: 13 and batch_num: 326\n",
      "Loss of train set: 0.17568901181221008 at epoch: 13 and batch_num: 327\n",
      "Loss of train set: 0.3573217988014221 at epoch: 13 and batch_num: 328\n",
      "Loss of train set: 0.41491347551345825 at epoch: 13 and batch_num: 329\n",
      "Loss of train set: 0.2977602481842041 at epoch: 13 and batch_num: 330\n",
      "Loss of train set: 0.23796144127845764 at epoch: 13 and batch_num: 331\n",
      "Loss of train set: 0.3273478150367737 at epoch: 13 and batch_num: 332\n",
      "Loss of train set: 0.37008750438690186 at epoch: 13 and batch_num: 333\n",
      "Loss of train set: 0.36736565828323364 at epoch: 13 and batch_num: 334\n",
      "Loss of train set: 0.17865914106369019 at epoch: 13 and batch_num: 335\n",
      "Loss of train set: 0.1786801815032959 at epoch: 13 and batch_num: 336\n",
      "Loss of train set: 0.18923711776733398 at epoch: 13 and batch_num: 337\n",
      "Loss of train set: 0.18676304817199707 at epoch: 13 and batch_num: 338\n",
      "Loss of train set: 0.19198760390281677 at epoch: 13 and batch_num: 339\n",
      "Loss of train set: 0.1821642518043518 at epoch: 13 and batch_num: 340\n",
      "Loss of train set: 0.4411601424217224 at epoch: 13 and batch_num: 341\n",
      "Loss of train set: 0.2793521285057068 at epoch: 13 and batch_num: 342\n",
      "Loss of train set: 0.3304702043533325 at epoch: 13 and batch_num: 343\n",
      "Loss of train set: 0.2466222494840622 at epoch: 13 and batch_num: 344\n",
      "Loss of train set: 0.22181746363639832 at epoch: 13 and batch_num: 345\n",
      "Loss of train set: 0.2795734405517578 at epoch: 13 and batch_num: 346\n",
      "Loss of train set: 0.5047195553779602 at epoch: 13 and batch_num: 347\n",
      "Loss of train set: 0.32146134972572327 at epoch: 13 and batch_num: 348\n",
      "Loss of train set: 0.3285685181617737 at epoch: 13 and batch_num: 349\n",
      "Loss of train set: 0.2762831151485443 at epoch: 13 and batch_num: 350\n",
      "Loss of train set: 0.19328249990940094 at epoch: 13 and batch_num: 351\n",
      "Loss of train set: 0.18886257708072662 at epoch: 13 and batch_num: 352\n",
      "Loss of train set: 0.6452337503433228 at epoch: 13 and batch_num: 353\n",
      "Loss of train set: 0.43073877692222595 at epoch: 13 and batch_num: 354\n",
      "Loss of train set: 0.2720401883125305 at epoch: 13 and batch_num: 355\n",
      "Loss of train set: 0.34162530303001404 at epoch: 13 and batch_num: 356\n",
      "Loss of train set: 0.24951758980751038 at epoch: 13 and batch_num: 357\n",
      "Loss of train set: 0.30775126814842224 at epoch: 13 and batch_num: 358\n",
      "Loss of train set: 0.36112791299819946 at epoch: 13 and batch_num: 359\n",
      "Loss of train set: 0.26964592933654785 at epoch: 13 and batch_num: 360\n",
      "Loss of train set: 0.34756964445114136 at epoch: 13 and batch_num: 361\n",
      "Loss of train set: 0.412020206451416 at epoch: 13 and batch_num: 362\n",
      "Loss of train set: 0.24031902849674225 at epoch: 13 and batch_num: 363\n",
      "Loss of train set: 0.32199299335479736 at epoch: 13 and batch_num: 364\n",
      "Loss of train set: 0.3619742691516876 at epoch: 13 and batch_num: 365\n",
      "Loss of train set: 0.2504962384700775 at epoch: 13 and batch_num: 366\n",
      "Loss of train set: 0.16591131687164307 at epoch: 13 and batch_num: 367\n",
      "Loss of train set: 0.29078373312950134 at epoch: 13 and batch_num: 368\n",
      "Loss of train set: 0.37879717350006104 at epoch: 13 and batch_num: 369\n",
      "Loss of train set: 0.356107234954834 at epoch: 13 and batch_num: 370\n",
      "Loss of train set: 0.24926820397377014 at epoch: 13 and batch_num: 371\n",
      "Loss of train set: 0.2766605615615845 at epoch: 13 and batch_num: 372\n",
      "Loss of train set: 0.3161211907863617 at epoch: 13 and batch_num: 373\n",
      "Loss of train set: 0.3808338940143585 at epoch: 13 and batch_num: 374\n",
      "Loss of train set: 0.3217400312423706 at epoch: 13 and batch_num: 375\n",
      "Loss of train set: 0.38414597511291504 at epoch: 13 and batch_num: 376\n",
      "Loss of train set: 0.23492872714996338 at epoch: 13 and batch_num: 377\n",
      "Loss of train set: 0.2859628200531006 at epoch: 13 and batch_num: 378\n",
      "Loss of train set: 0.24714341759681702 at epoch: 13 and batch_num: 379\n",
      "Loss of train set: 0.3022640645503998 at epoch: 13 and batch_num: 380\n",
      "Loss of train set: 0.2069140523672104 at epoch: 13 and batch_num: 381\n",
      "Loss of train set: 0.3648221790790558 at epoch: 13 and batch_num: 382\n",
      "Loss of train set: 0.3432047367095947 at epoch: 13 and batch_num: 383\n",
      "Loss of train set: 0.13407811522483826 at epoch: 13 and batch_num: 384\n",
      "Loss of train set: 0.2710686922073364 at epoch: 13 and batch_num: 385\n",
      "Loss of train set: 0.24190634489059448 at epoch: 13 and batch_num: 386\n",
      "Loss of train set: 0.2715356945991516 at epoch: 13 and batch_num: 387\n",
      "Loss of train set: 0.29054564237594604 at epoch: 13 and batch_num: 388\n",
      "Loss of train set: 0.42482855916023254 at epoch: 13 and batch_num: 389\n",
      "Loss of train set: 0.180904820561409 at epoch: 13 and batch_num: 390\n",
      "Loss of train set: 0.22387126088142395 at epoch: 13 and batch_num: 391\n",
      "Loss of train set: 0.3765619397163391 at epoch: 13 and batch_num: 392\n",
      "Loss of train set: 0.43059268593788147 at epoch: 13 and batch_num: 393\n",
      "Loss of train set: 0.3498445153236389 at epoch: 13 and batch_num: 394\n",
      "Loss of train set: 0.4002024233341217 at epoch: 13 and batch_num: 395\n",
      "Loss of train set: 0.2207585871219635 at epoch: 13 and batch_num: 396\n",
      "Loss of train set: 0.2954533100128174 at epoch: 13 and batch_num: 397\n",
      "Loss of train set: 0.18513688445091248 at epoch: 13 and batch_num: 398\n",
      "Loss of train set: 0.3110916316509247 at epoch: 13 and batch_num: 399\n",
      "Loss of train set: 0.28282299637794495 at epoch: 13 and batch_num: 400\n",
      "Loss of train set: 0.3167479336261749 at epoch: 13 and batch_num: 401\n",
      "Loss of train set: 0.3515492379665375 at epoch: 13 and batch_num: 402\n",
      "Loss of train set: 0.27070629596710205 at epoch: 13 and batch_num: 403\n",
      "Loss of train set: 0.25041258335113525 at epoch: 13 and batch_num: 404\n",
      "Loss of train set: 0.20531263947486877 at epoch: 13 and batch_num: 405\n",
      "Loss of train set: 0.4213189482688904 at epoch: 13 and batch_num: 406\n",
      "Loss of train set: 0.4168645441532135 at epoch: 13 and batch_num: 407\n",
      "Loss of train set: 0.25824683904647827 at epoch: 13 and batch_num: 408\n",
      "Loss of train set: 0.19909384846687317 at epoch: 13 and batch_num: 409\n",
      "Loss of train set: 0.4183062016963959 at epoch: 13 and batch_num: 410\n",
      "Loss of train set: 0.37111160159111023 at epoch: 13 and batch_num: 411\n",
      "Loss of train set: 0.5026161670684814 at epoch: 13 and batch_num: 412\n",
      "Loss of train set: 0.18354792892932892 at epoch: 13 and batch_num: 413\n",
      "Loss of train set: 0.2729030251502991 at epoch: 13 and batch_num: 414\n",
      "Loss of train set: 0.291083425283432 at epoch: 13 and batch_num: 415\n",
      "Loss of train set: 0.4622344672679901 at epoch: 13 and batch_num: 416\n",
      "Loss of train set: 0.5024535655975342 at epoch: 13 and batch_num: 417\n",
      "Loss of train set: 0.3051355481147766 at epoch: 13 and batch_num: 418\n",
      "Loss of train set: 0.3777657151222229 at epoch: 13 and batch_num: 419\n",
      "Loss of train set: 0.4725605845451355 at epoch: 13 and batch_num: 420\n",
      "Loss of train set: 0.346210777759552 at epoch: 13 and batch_num: 421\n",
      "Loss of train set: 0.2574988603591919 at epoch: 13 and batch_num: 422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3078579008579254 at epoch: 13 and batch_num: 423\n",
      "Loss of train set: 0.2554369866847992 at epoch: 13 and batch_num: 424\n",
      "Loss of train set: 0.39342671632766724 at epoch: 13 and batch_num: 425\n",
      "Loss of train set: 0.2431405782699585 at epoch: 13 and batch_num: 426\n",
      "Loss of train set: 0.23796984553337097 at epoch: 13 and batch_num: 427\n",
      "Loss of train set: 0.19809561967849731 at epoch: 13 and batch_num: 428\n",
      "Loss of train set: 0.23719704151153564 at epoch: 13 and batch_num: 429\n",
      "Loss of train set: 0.3236563205718994 at epoch: 13 and batch_num: 430\n",
      "Loss of train set: 0.17975163459777832 at epoch: 13 and batch_num: 431\n",
      "Loss of train set: 0.283820778131485 at epoch: 13 and batch_num: 432\n",
      "Loss of train set: 0.3593013882637024 at epoch: 13 and batch_num: 433\n",
      "Loss of train set: 0.43134111166000366 at epoch: 13 and batch_num: 434\n",
      "Loss of train set: 0.32856637239456177 at epoch: 13 and batch_num: 435\n",
      "Loss of train set: 0.20829716324806213 at epoch: 13 and batch_num: 436\n",
      "Loss of train set: 0.09974298626184464 at epoch: 13 and batch_num: 437\n",
      "Loss of train set: 0.3041495084762573 at epoch: 13 and batch_num: 438\n",
      "Loss of train set: 0.30054062604904175 at epoch: 13 and batch_num: 439\n",
      "Loss of train set: 0.28342217206954956 at epoch: 13 and batch_num: 440\n",
      "Loss of train set: 0.4733635187149048 at epoch: 13 and batch_num: 441\n",
      "Loss of train set: 0.44381433725357056 at epoch: 13 and batch_num: 442\n",
      "Loss of train set: 0.15393981337547302 at epoch: 13 and batch_num: 443\n",
      "Loss of train set: 0.46877384185791016 at epoch: 13 and batch_num: 444\n",
      "Loss of train set: 0.2790193557739258 at epoch: 13 and batch_num: 445\n",
      "Loss of train set: 0.261102557182312 at epoch: 13 and batch_num: 446\n",
      "Loss of train set: 0.4298647940158844 at epoch: 13 and batch_num: 447\n",
      "Loss of train set: 0.45730286836624146 at epoch: 13 and batch_num: 448\n",
      "Loss of train set: 0.355947881937027 at epoch: 13 and batch_num: 449\n",
      "Loss of train set: 0.3506626784801483 at epoch: 13 and batch_num: 450\n",
      "Loss of train set: 0.12738582491874695 at epoch: 13 and batch_num: 451\n",
      "Loss of train set: 0.2131020724773407 at epoch: 13 and batch_num: 452\n",
      "Loss of train set: 0.321341335773468 at epoch: 13 and batch_num: 453\n",
      "Loss of train set: 0.33228808641433716 at epoch: 13 and batch_num: 454\n",
      "Loss of train set: 0.2873396575450897 at epoch: 13 and batch_num: 455\n",
      "Loss of train set: 0.34757179021835327 at epoch: 13 and batch_num: 456\n",
      "Loss of train set: 0.272953599691391 at epoch: 13 and batch_num: 457\n",
      "Loss of train set: 0.187064528465271 at epoch: 13 and batch_num: 458\n",
      "Loss of train set: 0.19251009821891785 at epoch: 13 and batch_num: 459\n",
      "Loss of train set: 0.21219107508659363 at epoch: 13 and batch_num: 460\n",
      "Loss of train set: 0.17829038202762604 at epoch: 13 and batch_num: 461\n",
      "Loss of train set: 0.3349142372608185 at epoch: 13 and batch_num: 462\n",
      "Loss of train set: 0.32235968112945557 at epoch: 13 and batch_num: 463\n",
      "Loss of train set: 0.24424336850643158 at epoch: 13 and batch_num: 464\n",
      "Loss of train set: 0.4388781189918518 at epoch: 13 and batch_num: 465\n",
      "Loss of train set: 0.11281999945640564 at epoch: 13 and batch_num: 466\n",
      "Loss of train set: 0.33955448865890503 at epoch: 13 and batch_num: 467\n",
      "Loss of train set: 0.2201601266860962 at epoch: 13 and batch_num: 468\n",
      "Loss of train set: 0.29129093885421753 at epoch: 13 and batch_num: 469\n",
      "Loss of train set: 0.17584319412708282 at epoch: 13 and batch_num: 470\n",
      "Loss of train set: 0.4160113036632538 at epoch: 13 and batch_num: 471\n",
      "Loss of train set: 0.30834221839904785 at epoch: 13 and batch_num: 472\n",
      "Loss of train set: 0.23013077676296234 at epoch: 13 and batch_num: 473\n",
      "Loss of train set: 0.32873767614364624 at epoch: 13 and batch_num: 474\n",
      "Loss of train set: 0.25778263807296753 at epoch: 13 and batch_num: 475\n",
      "Loss of train set: 0.5317944288253784 at epoch: 13 and batch_num: 476\n",
      "Loss of train set: 0.3071087598800659 at epoch: 13 and batch_num: 477\n",
      "Loss of train set: 0.3930804431438446 at epoch: 13 and batch_num: 478\n",
      "Loss of train set: 0.32954728603363037 at epoch: 13 and batch_num: 479\n",
      "Loss of train set: 0.21311676502227783 at epoch: 13 and batch_num: 480\n",
      "Loss of train set: 0.18378961086273193 at epoch: 13 and batch_num: 481\n",
      "Loss of train set: 0.24853970110416412 at epoch: 13 and batch_num: 482\n",
      "Loss of train set: 0.5277862548828125 at epoch: 13 and batch_num: 483\n",
      "Loss of train set: 0.2613823413848877 at epoch: 13 and batch_num: 484\n",
      "Loss of train set: 0.28266042470932007 at epoch: 13 and batch_num: 485\n",
      "Loss of train set: 0.17544569075107574 at epoch: 13 and batch_num: 486\n",
      "Loss of train set: 0.36050114035606384 at epoch: 13 and batch_num: 487\n",
      "Loss of train set: 0.23622846603393555 at epoch: 13 and batch_num: 488\n",
      "Loss of train set: 0.22789818048477173 at epoch: 13 and batch_num: 489\n",
      "Loss of train set: 0.19585254788398743 at epoch: 13 and batch_num: 490\n",
      "Loss of train set: 0.311587929725647 at epoch: 13 and batch_num: 491\n",
      "Loss of train set: 0.32355797290802 at epoch: 13 and batch_num: 492\n",
      "Loss of train set: 0.2720220685005188 at epoch: 13 and batch_num: 493\n",
      "Loss of train set: 0.17364314198493958 at epoch: 13 and batch_num: 494\n",
      "Loss of train set: 0.3289610743522644 at epoch: 13 and batch_num: 495\n",
      "Loss of train set: 0.4109877347946167 at epoch: 13 and batch_num: 496\n",
      "Loss of train set: 0.33219027519226074 at epoch: 13 and batch_num: 497\n",
      "Loss of train set: 0.5099979043006897 at epoch: 13 and batch_num: 498\n",
      "Loss of train set: 0.29947930574417114 at epoch: 13 and batch_num: 499\n",
      "Loss of train set: 0.29756829142570496 at epoch: 13 and batch_num: 500\n",
      "Loss of train set: 0.48224174976348877 at epoch: 13 and batch_num: 501\n",
      "Loss of train set: 0.31101301312446594 at epoch: 13 and batch_num: 502\n",
      "Loss of train set: 0.30371174216270447 at epoch: 13 and batch_num: 503\n",
      "Loss of train set: 0.23736217617988586 at epoch: 13 and batch_num: 504\n",
      "Loss of train set: 0.31814706325531006 at epoch: 13 and batch_num: 505\n",
      "Loss of train set: 0.3763815760612488 at epoch: 13 and batch_num: 506\n",
      "Loss of train set: 0.30952978134155273 at epoch: 13 and batch_num: 507\n",
      "Loss of train set: 0.3494897484779358 at epoch: 13 and batch_num: 508\n",
      "Loss of train set: 0.445848673582077 at epoch: 13 and batch_num: 509\n",
      "Loss of train set: 0.2193978726863861 at epoch: 13 and batch_num: 510\n",
      "Loss of train set: 0.4113895297050476 at epoch: 13 and batch_num: 511\n",
      "Loss of train set: 0.2455153912305832 at epoch: 13 and batch_num: 512\n",
      "Loss of train set: 0.5150916576385498 at epoch: 13 and batch_num: 513\n",
      "Loss of train set: 0.38111406564712524 at epoch: 13 and batch_num: 514\n",
      "Loss of train set: 0.49891993403434753 at epoch: 13 and batch_num: 515\n",
      "Loss of train set: 0.36672595143318176 at epoch: 13 and batch_num: 516\n",
      "Loss of train set: 0.1781427413225174 at epoch: 13 and batch_num: 517\n",
      "Loss of train set: 0.41797253489494324 at epoch: 13 and batch_num: 518\n",
      "Loss of train set: 0.35224878787994385 at epoch: 13 and batch_num: 519\n",
      "Loss of train set: 0.45870235562324524 at epoch: 13 and batch_num: 520\n",
      "Loss of train set: 0.3913787603378296 at epoch: 13 and batch_num: 521\n",
      "Loss of train set: 0.36969542503356934 at epoch: 13 and batch_num: 522\n",
      "Loss of train set: 0.26592981815338135 at epoch: 13 and batch_num: 523\n",
      "Loss of train set: 0.3689705729484558 at epoch: 13 and batch_num: 524\n",
      "Loss of train set: 0.2526395916938782 at epoch: 13 and batch_num: 525\n",
      "Loss of train set: 0.20862680673599243 at epoch: 13 and batch_num: 526\n",
      "Loss of train set: 0.31517744064331055 at epoch: 13 and batch_num: 527\n",
      "Loss of train set: 0.4489189386367798 at epoch: 13 and batch_num: 528\n",
      "Loss of train set: 0.39524853229522705 at epoch: 13 and batch_num: 529\n",
      "Loss of train set: 0.1857508271932602 at epoch: 13 and batch_num: 530\n",
      "Loss of train set: 0.20331081748008728 at epoch: 13 and batch_num: 531\n",
      "Loss of train set: 0.3799815773963928 at epoch: 13 and batch_num: 532\n",
      "Loss of train set: 0.2938254177570343 at epoch: 13 and batch_num: 533\n",
      "Loss of train set: 0.18404974043369293 at epoch: 13 and batch_num: 534\n",
      "Loss of train set: 0.34466129541397095 at epoch: 13 and batch_num: 535\n",
      "Loss of train set: 0.2720671594142914 at epoch: 13 and batch_num: 536\n",
      "Loss of train set: 0.31174778938293457 at epoch: 13 and batch_num: 537\n",
      "Loss of train set: 0.33022361993789673 at epoch: 13 and batch_num: 538\n",
      "Loss of train set: 0.33128970861434937 at epoch: 13 and batch_num: 539\n",
      "Loss of train set: 0.2648729681968689 at epoch: 13 and batch_num: 540\n",
      "Loss of train set: 0.48945167660713196 at epoch: 13 and batch_num: 541\n",
      "Loss of train set: 0.3073822557926178 at epoch: 13 and batch_num: 542\n",
      "Loss of train set: 0.2505631148815155 at epoch: 13 and batch_num: 543\n",
      "Loss of train set: 0.3162027597427368 at epoch: 13 and batch_num: 544\n",
      "Loss of train set: 0.3726079761981964 at epoch: 13 and batch_num: 545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.31388574838638306 at epoch: 13 and batch_num: 546\n",
      "Loss of train set: 0.35596880316734314 at epoch: 13 and batch_num: 547\n",
      "Loss of train set: 0.41141459345817566 at epoch: 13 and batch_num: 548\n",
      "Loss of train set: 0.35978567600250244 at epoch: 13 and batch_num: 549\n",
      "Loss of train set: 0.28973811864852905 at epoch: 13 and batch_num: 550\n",
      "Loss of train set: 0.3482443690299988 at epoch: 13 and batch_num: 551\n",
      "Loss of train set: 0.23398467898368835 at epoch: 13 and batch_num: 552\n",
      "Loss of train set: 0.2822912931442261 at epoch: 13 and batch_num: 553\n",
      "Loss of train set: 0.4484940767288208 at epoch: 13 and batch_num: 554\n",
      "Loss of train set: 0.44544076919555664 at epoch: 13 and batch_num: 555\n",
      "Loss of train set: 0.36894381046295166 at epoch: 13 and batch_num: 556\n",
      "Loss of train set: 0.24500402808189392 at epoch: 13 and batch_num: 557\n",
      "Loss of train set: 0.1803821623325348 at epoch: 13 and batch_num: 558\n",
      "Loss of train set: 0.24332883954048157 at epoch: 13 and batch_num: 559\n",
      "Loss of train set: 0.26246368885040283 at epoch: 13 and batch_num: 560\n",
      "Loss of train set: 0.41933709383010864 at epoch: 13 and batch_num: 561\n",
      "Loss of train set: 0.3622548282146454 at epoch: 13 and batch_num: 562\n",
      "Loss of train set: 0.26101335883140564 at epoch: 13 and batch_num: 563\n",
      "Loss of train set: 0.6412538886070251 at epoch: 13 and batch_num: 564\n",
      "Loss of train set: 0.2993331551551819 at epoch: 13 and batch_num: 565\n",
      "Loss of train set: 0.325229287147522 at epoch: 13 and batch_num: 566\n",
      "Loss of train set: 0.5000549554824829 at epoch: 13 and batch_num: 567\n",
      "Loss of train set: 0.5507739782333374 at epoch: 13 and batch_num: 568\n",
      "Loss of train set: 0.32235822081565857 at epoch: 13 and batch_num: 569\n",
      "Loss of train set: 0.3465874195098877 at epoch: 13 and batch_num: 570\n",
      "Loss of train set: 0.2940768301486969 at epoch: 13 and batch_num: 571\n",
      "Loss of train set: 0.3624998927116394 at epoch: 13 and batch_num: 572\n",
      "Loss of train set: 0.3647209405899048 at epoch: 13 and batch_num: 573\n",
      "Loss of train set: 0.2656731605529785 at epoch: 13 and batch_num: 574\n",
      "Loss of train set: 0.2525896430015564 at epoch: 13 and batch_num: 575\n",
      "Loss of train set: 0.38535910844802856 at epoch: 13 and batch_num: 576\n",
      "Loss of train set: 0.4730378985404968 at epoch: 13 and batch_num: 577\n",
      "Loss of train set: 0.43792277574539185 at epoch: 13 and batch_num: 578\n",
      "Loss of train set: 0.2244977056980133 at epoch: 13 and batch_num: 579\n",
      "Loss of train set: 0.3865922689437866 at epoch: 13 and batch_num: 580\n",
      "Loss of train set: 0.32332170009613037 at epoch: 13 and batch_num: 581\n",
      "Loss of train set: 0.2088954597711563 at epoch: 13 and batch_num: 582\n",
      "Loss of train set: 0.30831414461135864 at epoch: 13 and batch_num: 583\n",
      "Loss of train set: 0.37740740180015564 at epoch: 13 and batch_num: 584\n",
      "Loss of train set: 0.3783762454986572 at epoch: 13 and batch_num: 585\n",
      "Loss of train set: 0.4666740298271179 at epoch: 13 and batch_num: 586\n",
      "Loss of train set: 0.3411034345626831 at epoch: 13 and batch_num: 587\n",
      "Loss of train set: 0.2612729072570801 at epoch: 13 and batch_num: 588\n",
      "Loss of train set: 0.44350600242614746 at epoch: 13 and batch_num: 589\n",
      "Loss of train set: 0.4545213580131531 at epoch: 13 and batch_num: 590\n",
      "Loss of train set: 0.37380102276802063 at epoch: 13 and batch_num: 591\n",
      "Loss of train set: 0.31758561730384827 at epoch: 13 and batch_num: 592\n",
      "Loss of train set: 0.279326856136322 at epoch: 13 and batch_num: 593\n",
      "Loss of train set: 0.23754236102104187 at epoch: 13 and batch_num: 594\n",
      "Loss of train set: 0.30116546154022217 at epoch: 13 and batch_num: 595\n",
      "Loss of train set: 0.44781145453453064 at epoch: 13 and batch_num: 596\n",
      "Loss of train set: 0.25563809275627136 at epoch: 13 and batch_num: 597\n",
      "Loss of train set: 0.3376372158527374 at epoch: 13 and batch_num: 598\n",
      "Loss of train set: 0.27281856536865234 at epoch: 13 and batch_num: 599\n",
      "Loss of train set: 0.3556809425354004 at epoch: 13 and batch_num: 600\n",
      "Loss of train set: 0.21291106939315796 at epoch: 13 and batch_num: 601\n",
      "Loss of train set: 0.21988429129123688 at epoch: 13 and batch_num: 602\n",
      "Loss of train set: 0.27551019191741943 at epoch: 13 and batch_num: 603\n",
      "Loss of train set: 0.22189408540725708 at epoch: 13 and batch_num: 604\n",
      "Loss of train set: 0.3261048197746277 at epoch: 13 and batch_num: 605\n",
      "Loss of train set: 0.4289577007293701 at epoch: 13 and batch_num: 606\n",
      "Loss of train set: 0.2008880078792572 at epoch: 13 and batch_num: 607\n",
      "Loss of train set: 0.24258355796337128 at epoch: 13 and batch_num: 608\n",
      "Loss of train set: 0.13813458383083344 at epoch: 13 and batch_num: 609\n",
      "Loss of train set: 0.31152841448783875 at epoch: 13 and batch_num: 610\n",
      "Loss of train set: 0.27077731490135193 at epoch: 13 and batch_num: 611\n",
      "Loss of train set: 0.3541887402534485 at epoch: 13 and batch_num: 612\n",
      "Loss of train set: 0.36098504066467285 at epoch: 13 and batch_num: 613\n",
      "Loss of train set: 0.2882876396179199 at epoch: 13 and batch_num: 614\n",
      "Loss of train set: 0.2865983843803406 at epoch: 13 and batch_num: 615\n",
      "Loss of train set: 0.1277477890253067 at epoch: 13 and batch_num: 616\n",
      "Loss of train set: 0.24020712077617645 at epoch: 13 and batch_num: 617\n",
      "Loss of train set: 0.19723741710186005 at epoch: 13 and batch_num: 618\n",
      "Loss of train set: 0.23413310945034027 at epoch: 13 and batch_num: 619\n",
      "Loss of train set: 0.22626817226409912 at epoch: 13 and batch_num: 620\n",
      "Loss of train set: 0.27643659710884094 at epoch: 13 and batch_num: 621\n",
      "Loss of train set: 0.24067987501621246 at epoch: 13 and batch_num: 622\n",
      "Loss of train set: 0.35956907272338867 at epoch: 13 and batch_num: 623\n",
      "Loss of train set: 0.467257559299469 at epoch: 13 and batch_num: 624\n",
      "Loss of train set: 0.4427098333835602 at epoch: 13 and batch_num: 625\n",
      "Loss of train set: 0.34138983488082886 at epoch: 13 and batch_num: 626\n",
      "Loss of train set: 0.21754872798919678 at epoch: 13 and batch_num: 627\n",
      "Loss of train set: 0.2248879075050354 at epoch: 13 and batch_num: 628\n",
      "Loss of train set: 0.30458810925483704 at epoch: 13 and batch_num: 629\n",
      "Loss of train set: 0.42778143286705017 at epoch: 13 and batch_num: 630\n",
      "Loss of train set: 0.24157369136810303 at epoch: 13 and batch_num: 631\n",
      "Loss of train set: 0.3601830005645752 at epoch: 13 and batch_num: 632\n",
      "Loss of train set: 0.3143574893474579 at epoch: 13 and batch_num: 633\n",
      "Loss of train set: 0.42774373292922974 at epoch: 13 and batch_num: 634\n",
      "Loss of train set: 0.3677056133747101 at epoch: 13 and batch_num: 635\n",
      "Loss of train set: 0.22304680943489075 at epoch: 13 and batch_num: 636\n",
      "Loss of train set: 0.2309514880180359 at epoch: 13 and batch_num: 637\n",
      "Loss of train set: 0.3536524176597595 at epoch: 13 and batch_num: 638\n",
      "Loss of train set: 0.4479565918445587 at epoch: 13 and batch_num: 639\n",
      "Loss of train set: 0.2044246345758438 at epoch: 13 and batch_num: 640\n",
      "Loss of train set: 0.3419395685195923 at epoch: 13 and batch_num: 641\n",
      "Loss of train set: 0.3171917796134949 at epoch: 13 and batch_num: 642\n",
      "Loss of train set: 0.3821660876274109 at epoch: 13 and batch_num: 643\n",
      "Loss of train set: 0.4998508095741272 at epoch: 13 and batch_num: 644\n",
      "Loss of train set: 0.29318028688430786 at epoch: 13 and batch_num: 645\n",
      "Loss of train set: 0.20874303579330444 at epoch: 13 and batch_num: 646\n",
      "Loss of train set: 0.32722508907318115 at epoch: 13 and batch_num: 647\n",
      "Loss of train set: 0.27207425236701965 at epoch: 13 and batch_num: 648\n",
      "Loss of train set: 0.2591911852359772 at epoch: 13 and batch_num: 649\n",
      "Loss of train set: 0.16694575548171997 at epoch: 13 and batch_num: 650\n",
      "Loss of train set: 0.5344594120979309 at epoch: 13 and batch_num: 651\n",
      "Loss of train set: 0.2158612608909607 at epoch: 13 and batch_num: 652\n",
      "Loss of train set: 0.22574859857559204 at epoch: 13 and batch_num: 653\n",
      "Loss of train set: 0.2281874567270279 at epoch: 13 and batch_num: 654\n",
      "Loss of train set: 0.41680413484573364 at epoch: 13 and batch_num: 655\n",
      "Loss of train set: 0.25669389963150024 at epoch: 13 and batch_num: 656\n",
      "Loss of train set: 0.29666176438331604 at epoch: 13 and batch_num: 657\n",
      "Loss of train set: 0.34896913170814514 at epoch: 13 and batch_num: 658\n",
      "Loss of train set: 0.21240557730197906 at epoch: 13 and batch_num: 659\n",
      "Loss of train set: 0.29011160135269165 at epoch: 13 and batch_num: 660\n",
      "Loss of train set: 0.25921252369880676 at epoch: 13 and batch_num: 661\n",
      "Loss of train set: 0.2175593227148056 at epoch: 13 and batch_num: 662\n",
      "Loss of train set: 0.3684793710708618 at epoch: 13 and batch_num: 663\n",
      "Loss of train set: 0.296697199344635 at epoch: 13 and batch_num: 664\n",
      "Loss of train set: 0.37446579337120056 at epoch: 13 and batch_num: 665\n",
      "Loss of train set: 0.25546154379844666 at epoch: 13 and batch_num: 666\n",
      "Loss of train set: 0.3398893475532532 at epoch: 13 and batch_num: 667\n",
      "Loss of train set: 0.2643735706806183 at epoch: 13 and batch_num: 668\n",
      "Loss of train set: 0.43344777822494507 at epoch: 13 and batch_num: 669\n",
      "Loss of train set: 0.3765125274658203 at epoch: 13 and batch_num: 670\n",
      "Loss of train set: 0.2910718321800232 at epoch: 13 and batch_num: 671\n",
      "Loss of train set: 0.1898166388273239 at epoch: 13 and batch_num: 672\n",
      "Loss of train set: 0.16075453162193298 at epoch: 13 and batch_num: 673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.4306128919124603 at epoch: 13 and batch_num: 674\n",
      "Loss of train set: 0.4199477434158325 at epoch: 13 and batch_num: 675\n",
      "Loss of train set: 0.3836108446121216 at epoch: 13 and batch_num: 676\n",
      "Loss of train set: 0.35518985986709595 at epoch: 13 and batch_num: 677\n",
      "Loss of train set: 0.5057793855667114 at epoch: 13 and batch_num: 678\n",
      "Loss of train set: 0.20828452706336975 at epoch: 13 and batch_num: 679\n",
      "Loss of train set: 0.3708612024784088 at epoch: 13 and batch_num: 680\n",
      "Loss of train set: 0.36074620485305786 at epoch: 13 and batch_num: 681\n",
      "Loss of train set: 0.3268008232116699 at epoch: 13 and batch_num: 682\n",
      "Loss of train set: 0.2835635542869568 at epoch: 13 and batch_num: 683\n",
      "Loss of train set: 0.4032753109931946 at epoch: 13 and batch_num: 684\n",
      "Loss of train set: 0.18097977340221405 at epoch: 13 and batch_num: 685\n",
      "Loss of train set: 0.2869917154312134 at epoch: 13 and batch_num: 686\n",
      "Loss of train set: 0.3245868980884552 at epoch: 13 and batch_num: 687\n",
      "Loss of train set: 0.2932358384132385 at epoch: 13 and batch_num: 688\n",
      "Loss of train set: 0.4589659571647644 at epoch: 13 and batch_num: 689\n",
      "Loss of train set: 0.2735283374786377 at epoch: 13 and batch_num: 690\n",
      "Loss of train set: 0.16471605002880096 at epoch: 13 and batch_num: 691\n",
      "Loss of train set: 0.20814980566501617 at epoch: 13 and batch_num: 692\n",
      "Loss of train set: 0.3153720498085022 at epoch: 13 and batch_num: 693\n",
      "Loss of train set: 0.2306508719921112 at epoch: 13 and batch_num: 694\n",
      "Loss of train set: 0.3507618308067322 at epoch: 13 and batch_num: 695\n",
      "Loss of train set: 0.3471480906009674 at epoch: 13 and batch_num: 696\n",
      "Loss of train set: 0.4151938557624817 at epoch: 13 and batch_num: 697\n",
      "Loss of train set: 0.25942909717559814 at epoch: 13 and batch_num: 698\n",
      "Loss of train set: 0.37268227338790894 at epoch: 13 and batch_num: 699\n",
      "Loss of train set: 0.1972542703151703 at epoch: 13 and batch_num: 700\n",
      "Loss of train set: 0.19988057017326355 at epoch: 13 and batch_num: 701\n",
      "Loss of train set: 0.32528021931648254 at epoch: 13 and batch_num: 702\n",
      "Loss of train set: 0.34606337547302246 at epoch: 13 and batch_num: 703\n",
      "Loss of train set: 0.17653487622737885 at epoch: 13 and batch_num: 704\n",
      "Loss of train set: 0.31066444516181946 at epoch: 13 and batch_num: 705\n",
      "Loss of train set: 0.3720170855522156 at epoch: 13 and batch_num: 706\n",
      "Loss of train set: 0.2288404405117035 at epoch: 13 and batch_num: 707\n",
      "Loss of train set: 0.295834481716156 at epoch: 13 and batch_num: 708\n",
      "Loss of train set: 0.24049562215805054 at epoch: 13 and batch_num: 709\n",
      "Loss of train set: 0.153070867061615 at epoch: 13 and batch_num: 710\n",
      "Loss of train set: 0.46415677666664124 at epoch: 13 and batch_num: 711\n",
      "Loss of train set: 0.2793358862400055 at epoch: 13 and batch_num: 712\n",
      "Loss of train set: 0.29815924167633057 at epoch: 13 and batch_num: 713\n",
      "Loss of train set: 0.2848763167858124 at epoch: 13 and batch_num: 714\n",
      "Loss of train set: 0.5105339884757996 at epoch: 13 and batch_num: 715\n",
      "Loss of train set: 0.2651672661304474 at epoch: 13 and batch_num: 716\n",
      "Loss of train set: 0.2551209628582001 at epoch: 13 and batch_num: 717\n",
      "Loss of train set: 0.3630697727203369 at epoch: 13 and batch_num: 718\n",
      "Loss of train set: 0.2352311760187149 at epoch: 13 and batch_num: 719\n",
      "Loss of train set: 0.3731965720653534 at epoch: 13 and batch_num: 720\n",
      "Loss of train set: 0.4431104063987732 at epoch: 13 and batch_num: 721\n",
      "Loss of train set: 0.4318210184574127 at epoch: 13 and batch_num: 722\n",
      "Loss of train set: 0.3111177086830139 at epoch: 13 and batch_num: 723\n",
      "Loss of train set: 0.2688441276550293 at epoch: 13 and batch_num: 724\n",
      "Loss of train set: 0.4695022702217102 at epoch: 13 and batch_num: 725\n",
      "Loss of train set: 0.4333533048629761 at epoch: 13 and batch_num: 726\n",
      "Loss of train set: 0.2511197328567505 at epoch: 13 and batch_num: 727\n",
      "Loss of train set: 0.33129701018333435 at epoch: 13 and batch_num: 728\n",
      "Loss of train set: 0.29840999841690063 at epoch: 13 and batch_num: 729\n",
      "Loss of train set: 0.26179027557373047 at epoch: 13 and batch_num: 730\n",
      "Loss of train set: 0.20454035699367523 at epoch: 13 and batch_num: 731\n",
      "Loss of train set: 0.30977484583854675 at epoch: 13 and batch_num: 732\n",
      "Loss of train set: 0.2405475676059723 at epoch: 13 and batch_num: 733\n",
      "Loss of train set: 0.3299475312232971 at epoch: 13 and batch_num: 734\n",
      "Loss of train set: 0.2826639413833618 at epoch: 13 and batch_num: 735\n",
      "Loss of train set: 0.20360055565834045 at epoch: 13 and batch_num: 736\n",
      "Loss of train set: 0.27651289105415344 at epoch: 13 and batch_num: 737\n",
      "Loss of train set: 0.5005863904953003 at epoch: 13 and batch_num: 738\n",
      "Loss of train set: 0.36227256059646606 at epoch: 13 and batch_num: 739\n",
      "Loss of train set: 0.34990862011909485 at epoch: 13 and batch_num: 740\n",
      "Loss of train set: 0.34146469831466675 at epoch: 13 and batch_num: 741\n",
      "Loss of train set: 0.29306328296661377 at epoch: 13 and batch_num: 742\n",
      "Loss of train set: 0.4538891315460205 at epoch: 13 and batch_num: 743\n",
      "Loss of train set: 0.23278486728668213 at epoch: 13 and batch_num: 744\n",
      "Loss of train set: 0.3419949412345886 at epoch: 13 and batch_num: 745\n",
      "Loss of train set: 0.540992259979248 at epoch: 13 and batch_num: 746\n",
      "Loss of train set: 0.37750861048698425 at epoch: 13 and batch_num: 747\n",
      "Loss of train set: 0.4264656901359558 at epoch: 13 and batch_num: 748\n",
      "Loss of train set: 0.23273706436157227 at epoch: 13 and batch_num: 749\n",
      "Loss of train set: 0.4924401640892029 at epoch: 13 and batch_num: 750\n",
      "Loss of train set: 0.661084771156311 at epoch: 13 and batch_num: 751\n",
      "Loss of train set: 0.3991053104400635 at epoch: 13 and batch_num: 752\n",
      "Loss of train set: 0.287356972694397 at epoch: 13 and batch_num: 753\n",
      "Loss of train set: 0.31572091579437256 at epoch: 13 and batch_num: 754\n",
      "Loss of train set: 0.20112444460391998 at epoch: 13 and batch_num: 755\n",
      "Loss of train set: 0.4367366433143616 at epoch: 13 and batch_num: 756\n",
      "Loss of train set: 0.31304603815078735 at epoch: 13 and batch_num: 757\n",
      "Loss of train set: 0.25834500789642334 at epoch: 13 and batch_num: 758\n",
      "Loss of train set: 0.28271156549453735 at epoch: 13 and batch_num: 759\n",
      "Loss of train set: 0.5601122975349426 at epoch: 13 and batch_num: 760\n",
      "Loss of train set: 0.3772929310798645 at epoch: 13 and batch_num: 761\n",
      "Loss of train set: 0.40447109937667847 at epoch: 13 and batch_num: 762\n",
      "Loss of train set: 0.29701972007751465 at epoch: 13 and batch_num: 763\n",
      "Loss of train set: 0.3204497992992401 at epoch: 13 and batch_num: 764\n",
      "Loss of train set: 0.3297470808029175 at epoch: 13 and batch_num: 765\n",
      "Loss of train set: 0.24994423985481262 at epoch: 13 and batch_num: 766\n",
      "Loss of train set: 0.28206026554107666 at epoch: 13 and batch_num: 767\n",
      "Loss of train set: 0.386493980884552 at epoch: 13 and batch_num: 768\n",
      "Loss of train set: 0.25586003065109253 at epoch: 13 and batch_num: 769\n",
      "Loss of train set: 0.29402685165405273 at epoch: 13 and batch_num: 770\n",
      "Loss of train set: 0.26074832677841187 at epoch: 13 and batch_num: 771\n",
      "Loss of train set: 0.17729300260543823 at epoch: 13 and batch_num: 772\n",
      "Loss of train set: 0.3189356327056885 at epoch: 13 and batch_num: 773\n",
      "Loss of train set: 0.26031333208084106 at epoch: 13 and batch_num: 774\n",
      "Loss of train set: 0.3101789355278015 at epoch: 13 and batch_num: 775\n",
      "Loss of train set: 0.2994256913661957 at epoch: 13 and batch_num: 776\n",
      "Loss of train set: 0.2617659866809845 at epoch: 13 and batch_num: 777\n",
      "Loss of train set: 0.24546685814857483 at epoch: 13 and batch_num: 778\n",
      "Loss of train set: 0.2872816324234009 at epoch: 13 and batch_num: 779\n",
      "Loss of train set: 0.45820096135139465 at epoch: 13 and batch_num: 780\n",
      "Loss of train set: 0.2753484547138214 at epoch: 13 and batch_num: 781\n",
      "Loss of train set: 0.23446670174598694 at epoch: 13 and batch_num: 782\n",
      "Loss of train set: 0.4499584436416626 at epoch: 13 and batch_num: 783\n",
      "Loss of train set: 0.2078753411769867 at epoch: 13 and batch_num: 784\n",
      "Loss of train set: 0.37312591075897217 at epoch: 13 and batch_num: 785\n",
      "Loss of train set: 0.30534857511520386 at epoch: 13 and batch_num: 786\n",
      "Loss of train set: 0.24444128572940826 at epoch: 13 and batch_num: 787\n",
      "Loss of train set: 0.24757882952690125 at epoch: 13 and batch_num: 788\n",
      "Loss of train set: 0.41405317187309265 at epoch: 13 and batch_num: 789\n",
      "Loss of train set: 0.36077696084976196 at epoch: 13 and batch_num: 790\n",
      "Loss of train set: 0.35755109786987305 at epoch: 13 and batch_num: 791\n",
      "Loss of train set: 0.23297721147537231 at epoch: 13 and batch_num: 792\n",
      "Loss of train set: 0.1915585845708847 at epoch: 13 and batch_num: 793\n",
      "Loss of train set: 0.18404272198677063 at epoch: 13 and batch_num: 794\n",
      "Loss of train set: 0.2461288571357727 at epoch: 13 and batch_num: 795\n",
      "Loss of train set: 0.29468870162963867 at epoch: 13 and batch_num: 796\n",
      "Loss of train set: 0.34786975383758545 at epoch: 13 and batch_num: 797\n",
      "Loss of train set: 0.2829752564430237 at epoch: 13 and batch_num: 798\n",
      "Loss of train set: 0.28530222177505493 at epoch: 13 and batch_num: 799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.38793158531188965 at epoch: 13 and batch_num: 800\n",
      "Loss of train set: 0.30926498770713806 at epoch: 13 and batch_num: 801\n",
      "Loss of train set: 0.2992229461669922 at epoch: 13 and batch_num: 802\n",
      "Loss of train set: 0.3596500754356384 at epoch: 13 and batch_num: 803\n",
      "Loss of train set: 0.4154510796070099 at epoch: 13 and batch_num: 804\n",
      "Loss of train set: 0.17530417442321777 at epoch: 13 and batch_num: 805\n",
      "Loss of train set: 0.1913605034351349 at epoch: 13 and batch_num: 806\n",
      "Loss of train set: 0.2531859278678894 at epoch: 13 and batch_num: 807\n",
      "Loss of train set: 0.36196592450141907 at epoch: 13 and batch_num: 808\n",
      "Loss of train set: 0.280545711517334 at epoch: 13 and batch_num: 809\n",
      "Loss of train set: 0.3783426880836487 at epoch: 13 and batch_num: 810\n",
      "Loss of train set: 0.314822793006897 at epoch: 13 and batch_num: 811\n",
      "Loss of train set: 0.2664271295070648 at epoch: 13 and batch_num: 812\n",
      "Loss of train set: 0.334664911031723 at epoch: 13 and batch_num: 813\n",
      "Loss of train set: 0.25280579924583435 at epoch: 13 and batch_num: 814\n",
      "Loss of train set: 0.4941036105155945 at epoch: 13 and batch_num: 815\n",
      "Loss of train set: 0.35458511114120483 at epoch: 13 and batch_num: 816\n",
      "Loss of train set: 0.25506219267845154 at epoch: 13 and batch_num: 817\n",
      "Loss of train set: 0.3723738193511963 at epoch: 13 and batch_num: 818\n",
      "Loss of train set: 0.38387686014175415 at epoch: 13 and batch_num: 819\n",
      "Loss of train set: 0.29648804664611816 at epoch: 13 and batch_num: 820\n",
      "Loss of train set: 0.2671739161014557 at epoch: 13 and batch_num: 821\n",
      "Loss of train set: 0.37910112738609314 at epoch: 13 and batch_num: 822\n",
      "Loss of train set: 0.3805783689022064 at epoch: 13 and batch_num: 823\n",
      "Loss of train set: 0.21961617469787598 at epoch: 13 and batch_num: 824\n",
      "Loss of train set: 0.2827831208705902 at epoch: 13 and batch_num: 825\n",
      "Loss of train set: 0.30743885040283203 at epoch: 13 and batch_num: 826\n",
      "Loss of train set: 0.35912150144577026 at epoch: 13 and batch_num: 827\n",
      "Loss of train set: 0.18772032856941223 at epoch: 13 and batch_num: 828\n",
      "Loss of train set: 0.2512403726577759 at epoch: 13 and batch_num: 829\n",
      "Loss of train set: 0.2937084436416626 at epoch: 13 and batch_num: 830\n",
      "Loss of train set: 0.09673118591308594 at epoch: 13 and batch_num: 831\n",
      "Loss of train set: 0.3185349106788635 at epoch: 13 and batch_num: 832\n",
      "Loss of train set: 0.6461160778999329 at epoch: 13 and batch_num: 833\n",
      "Loss of train set: 0.3261299729347229 at epoch: 13 and batch_num: 834\n",
      "Loss of train set: 0.2873309254646301 at epoch: 13 and batch_num: 835\n",
      "Loss of train set: 0.3293503522872925 at epoch: 13 and batch_num: 836\n",
      "Loss of train set: 0.401345431804657 at epoch: 13 and batch_num: 837\n",
      "Loss of train set: 0.372815877199173 at epoch: 13 and batch_num: 838\n",
      "Loss of train set: 0.30561983585357666 at epoch: 13 and batch_num: 839\n",
      "Loss of train set: 0.31544363498687744 at epoch: 13 and batch_num: 840\n",
      "Loss of train set: 0.37678593397140503 at epoch: 13 and batch_num: 841\n",
      "Loss of train set: 0.19881075620651245 at epoch: 13 and batch_num: 842\n",
      "Loss of train set: 0.3445543050765991 at epoch: 13 and batch_num: 843\n",
      "Loss of train set: 0.2411622405052185 at epoch: 13 and batch_num: 844\n",
      "Loss of train set: 0.31487518548965454 at epoch: 13 and batch_num: 845\n",
      "Loss of train set: 0.36659806966781616 at epoch: 13 and batch_num: 846\n",
      "Loss of train set: 0.36346668004989624 at epoch: 13 and batch_num: 847\n",
      "Loss of train set: 0.3380565643310547 at epoch: 13 and batch_num: 848\n",
      "Loss of train set: 0.334880530834198 at epoch: 13 and batch_num: 849\n",
      "Loss of train set: 0.18991348147392273 at epoch: 13 and batch_num: 850\n",
      "Loss of train set: 0.30168259143829346 at epoch: 13 and batch_num: 851\n",
      "Loss of train set: 0.259152352809906 at epoch: 13 and batch_num: 852\n",
      "Loss of train set: 0.3679913878440857 at epoch: 13 and batch_num: 853\n",
      "Loss of train set: 0.2990514636039734 at epoch: 13 and batch_num: 854\n",
      "Loss of train set: 0.30294108390808105 at epoch: 13 and batch_num: 855\n",
      "Loss of train set: 0.18871307373046875 at epoch: 13 and batch_num: 856\n",
      "Loss of train set: 0.36598777770996094 at epoch: 13 and batch_num: 857\n",
      "Loss of train set: 0.404954731464386 at epoch: 13 and batch_num: 858\n",
      "Loss of train set: 0.3592299222946167 at epoch: 13 and batch_num: 859\n",
      "Loss of train set: 0.4566901922225952 at epoch: 13 and batch_num: 860\n",
      "Loss of train set: 0.2282843291759491 at epoch: 13 and batch_num: 861\n",
      "Loss of train set: 0.2642446756362915 at epoch: 13 and batch_num: 862\n",
      "Loss of train set: 0.3417600989341736 at epoch: 13 and batch_num: 863\n",
      "Loss of train set: 0.5353487730026245 at epoch: 13 and batch_num: 864\n",
      "Loss of train set: 0.21557489037513733 at epoch: 13 and batch_num: 865\n",
      "Loss of train set: 0.38769879937171936 at epoch: 13 and batch_num: 866\n",
      "Loss of train set: 0.31154781579971313 at epoch: 13 and batch_num: 867\n",
      "Loss of train set: 0.33378544449806213 at epoch: 13 and batch_num: 868\n",
      "Loss of train set: 0.23283551633358002 at epoch: 13 and batch_num: 869\n",
      "Loss of train set: 0.21743667125701904 at epoch: 13 and batch_num: 870\n",
      "Loss of train set: 0.3391296863555908 at epoch: 13 and batch_num: 871\n",
      "Loss of train set: 0.394411563873291 at epoch: 13 and batch_num: 872\n",
      "Loss of train set: 0.40781325101852417 at epoch: 13 and batch_num: 873\n",
      "Loss of train set: 0.2694416642189026 at epoch: 13 and batch_num: 874\n",
      "Loss of train set: 0.36111980676651 at epoch: 13 and batch_num: 875\n",
      "Loss of train set: 0.1804359257221222 at epoch: 13 and batch_num: 876\n",
      "Loss of train set: 0.20611192286014557 at epoch: 13 and batch_num: 877\n",
      "Loss of train set: 0.31894412636756897 at epoch: 13 and batch_num: 878\n",
      "Loss of train set: 0.2063395380973816 at epoch: 13 and batch_num: 879\n",
      "Loss of train set: 0.4014824330806732 at epoch: 13 and batch_num: 880\n",
      "Loss of train set: 0.3227940797805786 at epoch: 13 and batch_num: 881\n",
      "Loss of train set: 0.39246803522109985 at epoch: 13 and batch_num: 882\n",
      "Loss of train set: 0.3220769166946411 at epoch: 13 and batch_num: 883\n",
      "Loss of train set: 0.37097811698913574 at epoch: 13 and batch_num: 884\n",
      "Loss of train set: 0.35943150520324707 at epoch: 13 and batch_num: 885\n",
      "Loss of train set: 0.3242390751838684 at epoch: 13 and batch_num: 886\n",
      "Loss of train set: 0.3544590473175049 at epoch: 13 and batch_num: 887\n",
      "Loss of train set: 0.2443731129169464 at epoch: 13 and batch_num: 888\n",
      "Loss of train set: 0.34952348470687866 at epoch: 13 and batch_num: 889\n",
      "Loss of train set: 0.2609435021877289 at epoch: 13 and batch_num: 890\n",
      "Loss of train set: 0.17542403936386108 at epoch: 13 and batch_num: 891\n",
      "Loss of train set: 0.26153185963630676 at epoch: 13 and batch_num: 892\n",
      "Loss of train set: 0.42438676953315735 at epoch: 13 and batch_num: 893\n",
      "Loss of train set: 0.4193307161331177 at epoch: 13 and batch_num: 894\n",
      "Loss of train set: 0.24057519435882568 at epoch: 13 and batch_num: 895\n",
      "Loss of train set: 0.256422758102417 at epoch: 13 and batch_num: 896\n",
      "Loss of train set: 0.32151469588279724 at epoch: 13 and batch_num: 897\n",
      "Loss of train set: 0.24344086647033691 at epoch: 13 and batch_num: 898\n",
      "Loss of train set: 0.2706487774848938 at epoch: 13 and batch_num: 899\n",
      "Loss of train set: 0.46211498975753784 at epoch: 13 and batch_num: 900\n",
      "Loss of train set: 0.2822420001029968 at epoch: 13 and batch_num: 901\n",
      "Loss of train set: 0.3086039125919342 at epoch: 13 and batch_num: 902\n",
      "Loss of train set: 0.39293402433395386 at epoch: 13 and batch_num: 903\n",
      "Loss of train set: 0.3705343008041382 at epoch: 13 and batch_num: 904\n",
      "Loss of train set: 0.3680019676685333 at epoch: 13 and batch_num: 905\n",
      "Loss of train set: 0.24224825203418732 at epoch: 13 and batch_num: 906\n",
      "Loss of train set: 0.35756808519363403 at epoch: 13 and batch_num: 907\n",
      "Loss of train set: 0.25736647844314575 at epoch: 13 and batch_num: 908\n",
      "Loss of train set: 0.3854105770587921 at epoch: 13 and batch_num: 909\n",
      "Loss of train set: 0.23723019659519196 at epoch: 13 and batch_num: 910\n",
      "Loss of train set: 0.3377631604671478 at epoch: 13 and batch_num: 911\n",
      "Loss of train set: 0.17850910127162933 at epoch: 13 and batch_num: 912\n",
      "Loss of train set: 0.30864113569259644 at epoch: 13 and batch_num: 913\n",
      "Loss of train set: 0.2601151466369629 at epoch: 13 and batch_num: 914\n",
      "Loss of train set: 0.38127976655960083 at epoch: 13 and batch_num: 915\n",
      "Loss of train set: 0.19353975355625153 at epoch: 13 and batch_num: 916\n",
      "Loss of train set: 0.22189246118068695 at epoch: 13 and batch_num: 917\n",
      "Loss of train set: 0.3030267357826233 at epoch: 13 and batch_num: 918\n",
      "Loss of train set: 0.24241222441196442 at epoch: 13 and batch_num: 919\n",
      "Loss of train set: 0.44345128536224365 at epoch: 13 and batch_num: 920\n",
      "Loss of train set: 0.2674892246723175 at epoch: 13 and batch_num: 921\n",
      "Loss of train set: 0.2511855363845825 at epoch: 13 and batch_num: 922\n",
      "Loss of train set: 0.2970241904258728 at epoch: 13 and batch_num: 923\n",
      "Loss of train set: 0.31933143734931946 at epoch: 13 and batch_num: 924\n",
      "Loss of train set: 0.3791756331920624 at epoch: 13 and batch_num: 925\n",
      "Loss of train set: 0.22350357472896576 at epoch: 13 and batch_num: 926\n",
      "Loss of train set: 0.619810163974762 at epoch: 13 and batch_num: 927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.44774436950683594 at epoch: 13 and batch_num: 928\n",
      "Loss of train set: 0.2672659754753113 at epoch: 13 and batch_num: 929\n",
      "Loss of train set: 0.37523066997528076 at epoch: 13 and batch_num: 930\n",
      "Loss of train set: 0.22208347916603088 at epoch: 13 and batch_num: 931\n",
      "Loss of train set: 0.29541832208633423 at epoch: 13 and batch_num: 932\n",
      "Loss of train set: 0.38031959533691406 at epoch: 13 and batch_num: 933\n",
      "Loss of train set: 0.22509989142417908 at epoch: 13 and batch_num: 934\n",
      "Loss of train set: 0.375161349773407 at epoch: 13 and batch_num: 935\n",
      "Loss of train set: 0.2924533188343048 at epoch: 13 and batch_num: 936\n",
      "Loss of train set: 0.15949177742004395 at epoch: 13 and batch_num: 937\n",
      "Accuracy of train set: 0.8891666666666667\n",
      "Loss of test set: 0.5125433802604675 at epoch: 13 and batch_num: 0\n",
      "Loss of test set: 0.3673088550567627 at epoch: 13 and batch_num: 1\n",
      "Loss of test set: 0.46706438064575195 at epoch: 13 and batch_num: 2\n",
      "Loss of test set: 0.35107502341270447 at epoch: 13 and batch_num: 3\n",
      "Loss of test set: 0.47032949328422546 at epoch: 13 and batch_num: 4\n",
      "Loss of test set: 0.427863746881485 at epoch: 13 and batch_num: 5\n",
      "Loss of test set: 0.22167983651161194 at epoch: 13 and batch_num: 6\n",
      "Loss of test set: 0.33791810274124146 at epoch: 13 and batch_num: 7\n",
      "Loss of test set: 0.3539789319038391 at epoch: 13 and batch_num: 8\n",
      "Loss of test set: 0.4034242630004883 at epoch: 13 and batch_num: 9\n",
      "Loss of test set: 0.2219361513853073 at epoch: 13 and batch_num: 10\n",
      "Loss of test set: 0.25181493163108826 at epoch: 13 and batch_num: 11\n",
      "Loss of test set: 0.3531155586242676 at epoch: 13 and batch_num: 12\n",
      "Loss of test set: 0.28463059663772583 at epoch: 13 and batch_num: 13\n",
      "Loss of test set: 0.33275967836380005 at epoch: 13 and batch_num: 14\n",
      "Loss of test set: 0.49352413415908813 at epoch: 13 and batch_num: 15\n",
      "Loss of test set: 0.33265790343284607 at epoch: 13 and batch_num: 16\n",
      "Loss of test set: 0.30052056908607483 at epoch: 13 and batch_num: 17\n",
      "Loss of test set: 0.39393350481987 at epoch: 13 and batch_num: 18\n",
      "Loss of test set: 0.38185566663742065 at epoch: 13 and batch_num: 19\n",
      "Loss of test set: 0.2786581218242645 at epoch: 13 and batch_num: 20\n",
      "Loss of test set: 0.2720254063606262 at epoch: 13 and batch_num: 21\n",
      "Loss of test set: 0.46805936098098755 at epoch: 13 and batch_num: 22\n",
      "Loss of test set: 0.4211735129356384 at epoch: 13 and batch_num: 23\n",
      "Loss of test set: 0.4078928232192993 at epoch: 13 and batch_num: 24\n",
      "Loss of test set: 0.46070724725723267 at epoch: 13 and batch_num: 25\n",
      "Loss of test set: 0.46813535690307617 at epoch: 13 and batch_num: 26\n",
      "Loss of test set: 0.2522119879722595 at epoch: 13 and batch_num: 27\n",
      "Loss of test set: 0.38966983556747437 at epoch: 13 and batch_num: 28\n",
      "Loss of test set: 0.27776914834976196 at epoch: 13 and batch_num: 29\n",
      "Loss of test set: 0.3838828504085541 at epoch: 13 and batch_num: 30\n",
      "Loss of test set: 0.5068079233169556 at epoch: 13 and batch_num: 31\n",
      "Loss of test set: 0.31960272789001465 at epoch: 13 and batch_num: 32\n",
      "Loss of test set: 0.3311152458190918 at epoch: 13 and batch_num: 33\n",
      "Loss of test set: 0.24405452609062195 at epoch: 13 and batch_num: 34\n",
      "Loss of test set: 0.7136280536651611 at epoch: 13 and batch_num: 35\n",
      "Loss of test set: 0.39191558957099915 at epoch: 13 and batch_num: 36\n",
      "Loss of test set: 0.2822161316871643 at epoch: 13 and batch_num: 37\n",
      "Loss of test set: 0.31165337562561035 at epoch: 13 and batch_num: 38\n",
      "Loss of test set: 0.4283550977706909 at epoch: 13 and batch_num: 39\n",
      "Loss of test set: 0.548470675945282 at epoch: 13 and batch_num: 40\n",
      "Loss of test set: 0.36283087730407715 at epoch: 13 and batch_num: 41\n",
      "Loss of test set: 0.35571494698524475 at epoch: 13 and batch_num: 42\n",
      "Loss of test set: 0.3729628026485443 at epoch: 13 and batch_num: 43\n",
      "Loss of test set: 0.3872094452381134 at epoch: 13 and batch_num: 44\n",
      "Loss of test set: 0.2696092128753662 at epoch: 13 and batch_num: 45\n",
      "Loss of test set: 0.29176849126815796 at epoch: 13 and batch_num: 46\n",
      "Loss of test set: 0.20642443001270294 at epoch: 13 and batch_num: 47\n",
      "Loss of test set: 0.2452513575553894 at epoch: 13 and batch_num: 48\n",
      "Loss of test set: 0.3521174490451813 at epoch: 13 and batch_num: 49\n",
      "Loss of test set: 0.4096967577934265 at epoch: 13 and batch_num: 50\n",
      "Loss of test set: 0.24383597075939178 at epoch: 13 and batch_num: 51\n",
      "Loss of test set: 0.25893858075141907 at epoch: 13 and batch_num: 52\n",
      "Loss of test set: 0.44441741704940796 at epoch: 13 and batch_num: 53\n",
      "Loss of test set: 0.5285218954086304 at epoch: 13 and batch_num: 54\n",
      "Loss of test set: 0.4056175649166107 at epoch: 13 and batch_num: 55\n",
      "Loss of test set: 0.44338369369506836 at epoch: 13 and batch_num: 56\n",
      "Loss of test set: 0.37799379229545593 at epoch: 13 and batch_num: 57\n",
      "Loss of test set: 0.28019654750823975 at epoch: 13 and batch_num: 58\n",
      "Loss of test set: 0.25033003091812134 at epoch: 13 and batch_num: 59\n",
      "Loss of test set: 0.3718284070491791 at epoch: 13 and batch_num: 60\n",
      "Loss of test set: 0.22983485460281372 at epoch: 13 and batch_num: 61\n",
      "Loss of test set: 0.2764565050601959 at epoch: 13 and batch_num: 62\n",
      "Loss of test set: 0.2514922022819519 at epoch: 13 and batch_num: 63\n",
      "Loss of test set: 0.3201819658279419 at epoch: 13 and batch_num: 64\n",
      "Loss of test set: 0.2958216071128845 at epoch: 13 and batch_num: 65\n",
      "Loss of test set: 0.5181822776794434 at epoch: 13 and batch_num: 66\n",
      "Loss of test set: 0.3355672061443329 at epoch: 13 and batch_num: 67\n",
      "Loss of test set: 0.2689058780670166 at epoch: 13 and batch_num: 68\n",
      "Loss of test set: 0.42799943685531616 at epoch: 13 and batch_num: 69\n",
      "Loss of test set: 0.314020574092865 at epoch: 13 and batch_num: 70\n",
      "Loss of test set: 0.35358962416648865 at epoch: 13 and batch_num: 71\n",
      "Loss of test set: 0.40805062651634216 at epoch: 13 and batch_num: 72\n",
      "Loss of test set: 0.5619340538978577 at epoch: 13 and batch_num: 73\n",
      "Loss of test set: 0.2543562054634094 at epoch: 13 and batch_num: 74\n",
      "Loss of test set: 0.4515811502933502 at epoch: 13 and batch_num: 75\n",
      "Loss of test set: 0.4031985104084015 at epoch: 13 and batch_num: 76\n",
      "Loss of test set: 0.21957719326019287 at epoch: 13 and batch_num: 77\n",
      "Loss of test set: 0.31958937644958496 at epoch: 13 and batch_num: 78\n",
      "Loss of test set: 0.37797480821609497 at epoch: 13 and batch_num: 79\n",
      "Loss of test set: 0.360058456659317 at epoch: 13 and batch_num: 80\n",
      "Loss of test set: 0.36362960934638977 at epoch: 13 and batch_num: 81\n",
      "Loss of test set: 0.37375563383102417 at epoch: 13 and batch_num: 82\n",
      "Loss of test set: 0.28645509481430054 at epoch: 13 and batch_num: 83\n",
      "Loss of test set: 0.5214356780052185 at epoch: 13 and batch_num: 84\n",
      "Loss of test set: 0.2843175530433655 at epoch: 13 and batch_num: 85\n",
      "Loss of test set: 0.408677875995636 at epoch: 13 and batch_num: 86\n",
      "Loss of test set: 0.37937217950820923 at epoch: 13 and batch_num: 87\n",
      "Loss of test set: 0.4609757363796234 at epoch: 13 and batch_num: 88\n",
      "Loss of test set: 0.4211297631263733 at epoch: 13 and batch_num: 89\n",
      "Loss of test set: 0.39156588912010193 at epoch: 13 and batch_num: 90\n",
      "Loss of test set: 0.5123804807662964 at epoch: 13 and batch_num: 91\n",
      "Loss of test set: 0.2868855893611908 at epoch: 13 and batch_num: 92\n",
      "Loss of test set: 0.4685254693031311 at epoch: 13 and batch_num: 93\n",
      "Loss of test set: 0.3254634141921997 at epoch: 13 and batch_num: 94\n",
      "Loss of test set: 0.432216078042984 at epoch: 13 and batch_num: 95\n",
      "Loss of test set: 0.31725266575813293 at epoch: 13 and batch_num: 96\n",
      "Loss of test set: 0.40650907158851624 at epoch: 13 and batch_num: 97\n",
      "Loss of test set: 0.4321076273918152 at epoch: 13 and batch_num: 98\n",
      "Loss of test set: 0.4093593955039978 at epoch: 13 and batch_num: 99\n",
      "Loss of test set: 0.3812723159790039 at epoch: 13 and batch_num: 100\n",
      "Loss of test set: 0.49591171741485596 at epoch: 13 and batch_num: 101\n",
      "Loss of test set: 0.4303966462612152 at epoch: 13 and batch_num: 102\n",
      "Loss of test set: 0.4450646638870239 at epoch: 13 and batch_num: 103\n",
      "Loss of test set: 0.4536707103252411 at epoch: 13 and batch_num: 104\n",
      "Loss of test set: 0.35144615173339844 at epoch: 13 and batch_num: 105\n",
      "Loss of test set: 0.48227614164352417 at epoch: 13 and batch_num: 106\n",
      "Loss of test set: 0.22282645106315613 at epoch: 13 and batch_num: 107\n",
      "Loss of test set: 0.27094173431396484 at epoch: 13 and batch_num: 108\n",
      "Loss of test set: 0.2630019187927246 at epoch: 13 and batch_num: 109\n",
      "Loss of test set: 0.4167081117630005 at epoch: 13 and batch_num: 110\n",
      "Loss of test set: 0.31457993388175964 at epoch: 13 and batch_num: 111\n",
      "Loss of test set: 0.44390004873275757 at epoch: 13 and batch_num: 112\n",
      "Loss of test set: 0.36278408765792847 at epoch: 13 and batch_num: 113\n",
      "Loss of test set: 0.2285454422235489 at epoch: 13 and batch_num: 114\n",
      "Loss of test set: 0.2996194362640381 at epoch: 13 and batch_num: 115\n",
      "Loss of test set: 0.40906184911727905 at epoch: 13 and batch_num: 116\n",
      "Loss of test set: 0.2512114942073822 at epoch: 13 and batch_num: 117\n",
      "Loss of test set: 0.4662504196166992 at epoch: 13 and batch_num: 118\n",
      "Loss of test set: 0.42819279432296753 at epoch: 13 and batch_num: 119\n",
      "Loss of test set: 0.42544832825660706 at epoch: 13 and batch_num: 120\n",
      "Loss of test set: 0.2279857099056244 at epoch: 13 and batch_num: 121\n",
      "Loss of test set: 0.28999823331832886 at epoch: 13 and batch_num: 122\n",
      "Loss of test set: 0.31005436182022095 at epoch: 13 and batch_num: 123\n",
      "Loss of test set: 0.4478423297405243 at epoch: 13 and batch_num: 124\n",
      "Loss of test set: 0.21919552981853485 at epoch: 13 and batch_num: 125\n",
      "Loss of test set: 0.4226657748222351 at epoch: 13 and batch_num: 126\n",
      "Loss of test set: 0.289739727973938 at epoch: 13 and batch_num: 127\n",
      "Loss of test set: 0.47648510336875916 at epoch: 13 and batch_num: 128\n",
      "Loss of test set: 0.22459062933921814 at epoch: 13 and batch_num: 129\n",
      "Loss of test set: 0.43612611293792725 at epoch: 13 and batch_num: 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.4341611862182617 at epoch: 13 and batch_num: 131\n",
      "Loss of test set: 0.35548168420791626 at epoch: 13 and batch_num: 132\n",
      "Loss of test set: 0.35455894470214844 at epoch: 13 and batch_num: 133\n",
      "Loss of test set: 0.1833743155002594 at epoch: 13 and batch_num: 134\n",
      "Loss of test set: 0.48962920904159546 at epoch: 13 and batch_num: 135\n",
      "Loss of test set: 0.2371804267168045 at epoch: 13 and batch_num: 136\n",
      "Loss of test set: 0.3231077790260315 at epoch: 13 and batch_num: 137\n",
      "Loss of test set: 0.2309493124485016 at epoch: 13 and batch_num: 138\n",
      "Loss of test set: 0.21525953710079193 at epoch: 13 and batch_num: 139\n",
      "Loss of test set: 0.5153203010559082 at epoch: 13 and batch_num: 140\n",
      "Loss of test set: 0.37691688537597656 at epoch: 13 and batch_num: 141\n",
      "Loss of test set: 0.2621786594390869 at epoch: 13 and batch_num: 142\n",
      "Loss of test set: 0.524652361869812 at epoch: 13 and batch_num: 143\n",
      "Loss of test set: 0.3100205659866333 at epoch: 13 and batch_num: 144\n",
      "Loss of test set: 0.19624640047550201 at epoch: 13 and batch_num: 145\n",
      "Loss of test set: 0.2016918659210205 at epoch: 13 and batch_num: 146\n",
      "Loss of test set: 0.40913912653923035 at epoch: 13 and batch_num: 147\n",
      "Loss of test set: 0.3393998444080353 at epoch: 13 and batch_num: 148\n",
      "Loss of test set: 0.21773116290569305 at epoch: 13 and batch_num: 149\n",
      "Loss of test set: 0.32504111528396606 at epoch: 13 and batch_num: 150\n",
      "Loss of test set: 0.5669742226600647 at epoch: 13 and batch_num: 151\n",
      "Loss of test set: 0.23443195223808289 at epoch: 13 and batch_num: 152\n",
      "Loss of test set: 0.49853911995887756 at epoch: 13 and batch_num: 153\n",
      "Loss of test set: 0.21846885979175568 at epoch: 13 and batch_num: 154\n",
      "Loss of test set: 0.21827901899814606 at epoch: 13 and batch_num: 155\n",
      "Loss of test set: 0.14734798669815063 at epoch: 13 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8741\n",
      "Loss of train set: 0.20496812462806702 at epoch: 14 and batch_num: 0\n",
      "Loss of train set: 0.250507652759552 at epoch: 14 and batch_num: 1\n",
      "Loss of train set: 0.4944528043270111 at epoch: 14 and batch_num: 2\n",
      "Loss of train set: 0.2846437394618988 at epoch: 14 and batch_num: 3\n",
      "Loss of train set: 0.20688074827194214 at epoch: 14 and batch_num: 4\n",
      "Loss of train set: 0.26856815814971924 at epoch: 14 and batch_num: 5\n",
      "Loss of train set: 0.3835136294364929 at epoch: 14 and batch_num: 6\n",
      "Loss of train set: 0.2953781485557556 at epoch: 14 and batch_num: 7\n",
      "Loss of train set: 0.40922433137893677 at epoch: 14 and batch_num: 8\n",
      "Loss of train set: 0.3314843773841858 at epoch: 14 and batch_num: 9\n",
      "Loss of train set: 0.17223644256591797 at epoch: 14 and batch_num: 10\n",
      "Loss of train set: 0.3367948532104492 at epoch: 14 and batch_num: 11\n",
      "Loss of train set: 0.6889970302581787 at epoch: 14 and batch_num: 12\n",
      "Loss of train set: 0.1909765899181366 at epoch: 14 and batch_num: 13\n",
      "Loss of train set: 0.19973653554916382 at epoch: 14 and batch_num: 14\n",
      "Loss of train set: 0.34777748584747314 at epoch: 14 and batch_num: 15\n",
      "Loss of train set: 0.22408169507980347 at epoch: 14 and batch_num: 16\n",
      "Loss of train set: 0.3188575506210327 at epoch: 14 and batch_num: 17\n",
      "Loss of train set: 0.23699671030044556 at epoch: 14 and batch_num: 18\n",
      "Loss of train set: 0.24596765637397766 at epoch: 14 and batch_num: 19\n",
      "Loss of train set: 0.2271219938993454 at epoch: 14 and batch_num: 20\n",
      "Loss of train set: 0.3001522123813629 at epoch: 14 and batch_num: 21\n",
      "Loss of train set: 0.22834722697734833 at epoch: 14 and batch_num: 22\n",
      "Loss of train set: 0.3823428452014923 at epoch: 14 and batch_num: 23\n",
      "Loss of train set: 0.2050049751996994 at epoch: 14 and batch_num: 24\n",
      "Loss of train set: 0.1833534836769104 at epoch: 14 and batch_num: 25\n",
      "Loss of train set: 0.34552305936813354 at epoch: 14 and batch_num: 26\n",
      "Loss of train set: 0.41050493717193604 at epoch: 14 and batch_num: 27\n",
      "Loss of train set: 0.2092340588569641 at epoch: 14 and batch_num: 28\n",
      "Loss of train set: 0.2752806842327118 at epoch: 14 and batch_num: 29\n",
      "Loss of train set: 0.32288265228271484 at epoch: 14 and batch_num: 30\n",
      "Loss of train set: 0.4004429578781128 at epoch: 14 and batch_num: 31\n",
      "Loss of train set: 0.24718964099884033 at epoch: 14 and batch_num: 32\n",
      "Loss of train set: 0.23674291372299194 at epoch: 14 and batch_num: 33\n",
      "Loss of train set: 0.2713654041290283 at epoch: 14 and batch_num: 34\n",
      "Loss of train set: 0.3009694814682007 at epoch: 14 and batch_num: 35\n",
      "Loss of train set: 0.5363714694976807 at epoch: 14 and batch_num: 36\n",
      "Loss of train set: 0.3275033235549927 at epoch: 14 and batch_num: 37\n",
      "Loss of train set: 0.4463598132133484 at epoch: 14 and batch_num: 38\n",
      "Loss of train set: 0.22664010524749756 at epoch: 14 and batch_num: 39\n",
      "Loss of train set: 0.30900466442108154 at epoch: 14 and batch_num: 40\n",
      "Loss of train set: 0.30252254009246826 at epoch: 14 and batch_num: 41\n",
      "Loss of train set: 0.12927180528640747 at epoch: 14 and batch_num: 42\n",
      "Loss of train set: 0.2725512683391571 at epoch: 14 and batch_num: 43\n",
      "Loss of train set: 0.33526456356048584 at epoch: 14 and batch_num: 44\n",
      "Loss of train set: 0.2124706357717514 at epoch: 14 and batch_num: 45\n",
      "Loss of train set: 0.35932159423828125 at epoch: 14 and batch_num: 46\n",
      "Loss of train set: 0.2081412971019745 at epoch: 14 and batch_num: 47\n",
      "Loss of train set: 0.3229753375053406 at epoch: 14 and batch_num: 48\n",
      "Loss of train set: 0.3556078374385834 at epoch: 14 and batch_num: 49\n",
      "Loss of train set: 0.14954492449760437 at epoch: 14 and batch_num: 50\n",
      "Loss of train set: 0.3326563239097595 at epoch: 14 and batch_num: 51\n",
      "Loss of train set: 0.3123577833175659 at epoch: 14 and batch_num: 52\n",
      "Loss of train set: 0.2649766206741333 at epoch: 14 and batch_num: 53\n",
      "Loss of train set: 0.34638679027557373 at epoch: 14 and batch_num: 54\n",
      "Loss of train set: 0.24270398914813995 at epoch: 14 and batch_num: 55\n",
      "Loss of train set: 0.35452327132225037 at epoch: 14 and batch_num: 56\n",
      "Loss of train set: 0.18177181482315063 at epoch: 14 and batch_num: 57\n",
      "Loss of train set: 0.3071528375148773 at epoch: 14 and batch_num: 58\n",
      "Loss of train set: 0.3267469108104706 at epoch: 14 and batch_num: 59\n",
      "Loss of train set: 0.18218393623828888 at epoch: 14 and batch_num: 60\n",
      "Loss of train set: 0.5048171877861023 at epoch: 14 and batch_num: 61\n",
      "Loss of train set: 0.30339375138282776 at epoch: 14 and batch_num: 62\n",
      "Loss of train set: 0.27928125858306885 at epoch: 14 and batch_num: 63\n",
      "Loss of train set: 0.21246296167373657 at epoch: 14 and batch_num: 64\n",
      "Loss of train set: 0.2874337434768677 at epoch: 14 and batch_num: 65\n",
      "Loss of train set: 0.10135727375745773 at epoch: 14 and batch_num: 66\n",
      "Loss of train set: 0.21569688618183136 at epoch: 14 and batch_num: 67\n",
      "Loss of train set: 0.2883714437484741 at epoch: 14 and batch_num: 68\n",
      "Loss of train set: 0.3146846294403076 at epoch: 14 and batch_num: 69\n",
      "Loss of train set: 0.3907295763492584 at epoch: 14 and batch_num: 70\n",
      "Loss of train set: 0.2077212929725647 at epoch: 14 and batch_num: 71\n",
      "Loss of train set: 0.24557757377624512 at epoch: 14 and batch_num: 72\n",
      "Loss of train set: 0.5326913595199585 at epoch: 14 and batch_num: 73\n",
      "Loss of train set: 0.36470913887023926 at epoch: 14 and batch_num: 74\n",
      "Loss of train set: 0.18407952785491943 at epoch: 14 and batch_num: 75\n",
      "Loss of train set: 0.27893853187561035 at epoch: 14 and batch_num: 76\n",
      "Loss of train set: 0.2901776432991028 at epoch: 14 and batch_num: 77\n",
      "Loss of train set: 0.4148268699645996 at epoch: 14 and batch_num: 78\n",
      "Loss of train set: 0.3012318015098572 at epoch: 14 and batch_num: 79\n",
      "Loss of train set: 0.19326269626617432 at epoch: 14 and batch_num: 80\n",
      "Loss of train set: 0.35279032588005066 at epoch: 14 and batch_num: 81\n",
      "Loss of train set: 0.2650756239891052 at epoch: 14 and batch_num: 82\n",
      "Loss of train set: 0.3696715235710144 at epoch: 14 and batch_num: 83\n",
      "Loss of train set: 0.41312944889068604 at epoch: 14 and batch_num: 84\n",
      "Loss of train set: 0.2940772771835327 at epoch: 14 and batch_num: 85\n",
      "Loss of train set: 0.2338148057460785 at epoch: 14 and batch_num: 86\n",
      "Loss of train set: 0.19987520575523376 at epoch: 14 and batch_num: 87\n",
      "Loss of train set: 0.26000720262527466 at epoch: 14 and batch_num: 88\n",
      "Loss of train set: 0.3304310441017151 at epoch: 14 and batch_num: 89\n",
      "Loss of train set: 0.2412174791097641 at epoch: 14 and batch_num: 90\n",
      "Loss of train set: 0.19270575046539307 at epoch: 14 and batch_num: 91\n",
      "Loss of train set: 0.3575410842895508 at epoch: 14 and batch_num: 92\n",
      "Loss of train set: 0.411984384059906 at epoch: 14 and batch_num: 93\n",
      "Loss of train set: 0.22863289713859558 at epoch: 14 and batch_num: 94\n",
      "Loss of train set: 0.29106467962265015 at epoch: 14 and batch_num: 95\n",
      "Loss of train set: 0.4111042618751526 at epoch: 14 and batch_num: 96\n",
      "Loss of train set: 0.2532414197921753 at epoch: 14 and batch_num: 97\n",
      "Loss of train set: 0.21132692694664001 at epoch: 14 and batch_num: 98\n",
      "Loss of train set: 0.1858891248703003 at epoch: 14 and batch_num: 99\n",
      "Loss of train set: 0.183919295668602 at epoch: 14 and batch_num: 100\n",
      "Loss of train set: 0.28400468826293945 at epoch: 14 and batch_num: 101\n",
      "Loss of train set: 0.3797106444835663 at epoch: 14 and batch_num: 102\n",
      "Loss of train set: 0.48729372024536133 at epoch: 14 and batch_num: 103\n",
      "Loss of train set: 0.17446622252464294 at epoch: 14 and batch_num: 104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2756589949131012 at epoch: 14 and batch_num: 105\n",
      "Loss of train set: 0.26939821243286133 at epoch: 14 and batch_num: 106\n",
      "Loss of train set: 0.2286386787891388 at epoch: 14 and batch_num: 107\n",
      "Loss of train set: 0.40142637491226196 at epoch: 14 and batch_num: 108\n",
      "Loss of train set: 0.25121957063674927 at epoch: 14 and batch_num: 109\n",
      "Loss of train set: 0.2966534197330475 at epoch: 14 and batch_num: 110\n",
      "Loss of train set: 0.41062334179878235 at epoch: 14 and batch_num: 111\n",
      "Loss of train set: 0.3856941759586334 at epoch: 14 and batch_num: 112\n",
      "Loss of train set: 0.2840733230113983 at epoch: 14 and batch_num: 113\n",
      "Loss of train set: 0.17046642303466797 at epoch: 14 and batch_num: 114\n",
      "Loss of train set: 0.3483169376850128 at epoch: 14 and batch_num: 115\n",
      "Loss of train set: 0.20112445950508118 at epoch: 14 and batch_num: 116\n",
      "Loss of train set: 0.408336877822876 at epoch: 14 and batch_num: 117\n",
      "Loss of train set: 0.2664531469345093 at epoch: 14 and batch_num: 118\n",
      "Loss of train set: 0.3599717617034912 at epoch: 14 and batch_num: 119\n",
      "Loss of train set: 0.2842111587524414 at epoch: 14 and batch_num: 120\n",
      "Loss of train set: 0.24491018056869507 at epoch: 14 and batch_num: 121\n",
      "Loss of train set: 0.3348652720451355 at epoch: 14 and batch_num: 122\n",
      "Loss of train set: 0.3537159264087677 at epoch: 14 and batch_num: 123\n",
      "Loss of train set: 0.31704622507095337 at epoch: 14 and batch_num: 124\n",
      "Loss of train set: 0.26866501569747925 at epoch: 14 and batch_num: 125\n",
      "Loss of train set: 0.27045053243637085 at epoch: 14 and batch_num: 126\n",
      "Loss of train set: 0.22650954127311707 at epoch: 14 and batch_num: 127\n",
      "Loss of train set: 0.2314930409193039 at epoch: 14 and batch_num: 128\n",
      "Loss of train set: 0.1529141366481781 at epoch: 14 and batch_num: 129\n",
      "Loss of train set: 0.187462717294693 at epoch: 14 and batch_num: 130\n",
      "Loss of train set: 0.42310214042663574 at epoch: 14 and batch_num: 131\n",
      "Loss of train set: 0.34818312525749207 at epoch: 14 and batch_num: 132\n",
      "Loss of train set: 0.24666056036949158 at epoch: 14 and batch_num: 133\n",
      "Loss of train set: 0.20749500393867493 at epoch: 14 and batch_num: 134\n",
      "Loss of train set: 0.1415339708328247 at epoch: 14 and batch_num: 135\n",
      "Loss of train set: 0.33337122201919556 at epoch: 14 and batch_num: 136\n",
      "Loss of train set: 0.22614620625972748 at epoch: 14 and batch_num: 137\n",
      "Loss of train set: 0.3414750397205353 at epoch: 14 and batch_num: 138\n",
      "Loss of train set: 0.30438321828842163 at epoch: 14 and batch_num: 139\n",
      "Loss of train set: 0.4746376872062683 at epoch: 14 and batch_num: 140\n",
      "Loss of train set: 0.3700937032699585 at epoch: 14 and batch_num: 141\n",
      "Loss of train set: 0.23319104313850403 at epoch: 14 and batch_num: 142\n",
      "Loss of train set: 0.2526095509529114 at epoch: 14 and batch_num: 143\n",
      "Loss of train set: 0.22320443391799927 at epoch: 14 and batch_num: 144\n",
      "Loss of train set: 0.24119523167610168 at epoch: 14 and batch_num: 145\n",
      "Loss of train set: 0.3254305422306061 at epoch: 14 and batch_num: 146\n",
      "Loss of train set: 0.2935859262943268 at epoch: 14 and batch_num: 147\n",
      "Loss of train set: 0.33282309770584106 at epoch: 14 and batch_num: 148\n",
      "Loss of train set: 0.2432013750076294 at epoch: 14 and batch_num: 149\n",
      "Loss of train set: 0.3646141290664673 at epoch: 14 and batch_num: 150\n",
      "Loss of train set: 0.32442712783813477 at epoch: 14 and batch_num: 151\n",
      "Loss of train set: 0.5095014572143555 at epoch: 14 and batch_num: 152\n",
      "Loss of train set: 0.3135887384414673 at epoch: 14 and batch_num: 153\n",
      "Loss of train set: 0.36926788091659546 at epoch: 14 and batch_num: 154\n",
      "Loss of train set: 0.24657726287841797 at epoch: 14 and batch_num: 155\n",
      "Loss of train set: 0.38102853298187256 at epoch: 14 and batch_num: 156\n",
      "Loss of train set: 0.33072394132614136 at epoch: 14 and batch_num: 157\n",
      "Loss of train set: 0.3786596357822418 at epoch: 14 and batch_num: 158\n",
      "Loss of train set: 0.30091702938079834 at epoch: 14 and batch_num: 159\n",
      "Loss of train set: 0.33775073289871216 at epoch: 14 and batch_num: 160\n",
      "Loss of train set: 0.3951088786125183 at epoch: 14 and batch_num: 161\n",
      "Loss of train set: 0.4050382375717163 at epoch: 14 and batch_num: 162\n",
      "Loss of train set: 0.19314701855182648 at epoch: 14 and batch_num: 163\n",
      "Loss of train set: 0.33134210109710693 at epoch: 14 and batch_num: 164\n",
      "Loss of train set: 0.40196606516838074 at epoch: 14 and batch_num: 165\n",
      "Loss of train set: 0.3968694806098938 at epoch: 14 and batch_num: 166\n",
      "Loss of train set: 0.2873119115829468 at epoch: 14 and batch_num: 167\n",
      "Loss of train set: 0.46794602274894714 at epoch: 14 and batch_num: 168\n",
      "Loss of train set: 0.29761117696762085 at epoch: 14 and batch_num: 169\n",
      "Loss of train set: 0.27680981159210205 at epoch: 14 and batch_num: 170\n",
      "Loss of train set: 0.2556737959384918 at epoch: 14 and batch_num: 171\n",
      "Loss of train set: 0.21629279851913452 at epoch: 14 and batch_num: 172\n",
      "Loss of train set: 0.3410422205924988 at epoch: 14 and batch_num: 173\n",
      "Loss of train set: 0.3952997326850891 at epoch: 14 and batch_num: 174\n",
      "Loss of train set: 0.3334711194038391 at epoch: 14 and batch_num: 175\n",
      "Loss of train set: 0.3277635872364044 at epoch: 14 and batch_num: 176\n",
      "Loss of train set: 0.23126548528671265 at epoch: 14 and batch_num: 177\n",
      "Loss of train set: 0.22377389669418335 at epoch: 14 and batch_num: 178\n",
      "Loss of train set: 0.2904247045516968 at epoch: 14 and batch_num: 179\n",
      "Loss of train set: 0.3327749967575073 at epoch: 14 and batch_num: 180\n",
      "Loss of train set: 0.2432701587677002 at epoch: 14 and batch_num: 181\n",
      "Loss of train set: 0.30201685428619385 at epoch: 14 and batch_num: 182\n",
      "Loss of train set: 0.2865670621395111 at epoch: 14 and batch_num: 183\n",
      "Loss of train set: 0.3551631569862366 at epoch: 14 and batch_num: 184\n",
      "Loss of train set: 0.24325969815254211 at epoch: 14 and batch_num: 185\n",
      "Loss of train set: 0.2304001748561859 at epoch: 14 and batch_num: 186\n",
      "Loss of train set: 0.19316554069519043 at epoch: 14 and batch_num: 187\n",
      "Loss of train set: 0.4046253263950348 at epoch: 14 and batch_num: 188\n",
      "Loss of train set: 0.35037532448768616 at epoch: 14 and batch_num: 189\n",
      "Loss of train set: 0.2693432867527008 at epoch: 14 and batch_num: 190\n",
      "Loss of train set: 0.3399255871772766 at epoch: 14 and batch_num: 191\n",
      "Loss of train set: 0.2312173843383789 at epoch: 14 and batch_num: 192\n",
      "Loss of train set: 0.25923240184783936 at epoch: 14 and batch_num: 193\n",
      "Loss of train set: 0.324083149433136 at epoch: 14 and batch_num: 194\n",
      "Loss of train set: 0.2383960336446762 at epoch: 14 and batch_num: 195\n",
      "Loss of train set: 0.29285377264022827 at epoch: 14 and batch_num: 196\n",
      "Loss of train set: 0.3874892294406891 at epoch: 14 and batch_num: 197\n",
      "Loss of train set: 0.3033897280693054 at epoch: 14 and batch_num: 198\n",
      "Loss of train set: 0.3085622787475586 at epoch: 14 and batch_num: 199\n",
      "Loss of train set: 0.5311287045478821 at epoch: 14 and batch_num: 200\n",
      "Loss of train set: 0.3599552512168884 at epoch: 14 and batch_num: 201\n",
      "Loss of train set: 0.34289035201072693 at epoch: 14 and batch_num: 202\n",
      "Loss of train set: 0.3201655149459839 at epoch: 14 and batch_num: 203\n",
      "Loss of train set: 0.33358901739120483 at epoch: 14 and batch_num: 204\n",
      "Loss of train set: 0.4017094075679779 at epoch: 14 and batch_num: 205\n",
      "Loss of train set: 0.3545849621295929 at epoch: 14 and batch_num: 206\n",
      "Loss of train set: 0.2550084590911865 at epoch: 14 and batch_num: 207\n",
      "Loss of train set: 0.2288939356803894 at epoch: 14 and batch_num: 208\n",
      "Loss of train set: 0.16211926937103271 at epoch: 14 and batch_num: 209\n",
      "Loss of train set: 0.3013918399810791 at epoch: 14 and batch_num: 210\n",
      "Loss of train set: 0.3702473044395447 at epoch: 14 and batch_num: 211\n",
      "Loss of train set: 0.2555588185787201 at epoch: 14 and batch_num: 212\n",
      "Loss of train set: 0.2871863543987274 at epoch: 14 and batch_num: 213\n",
      "Loss of train set: 0.18485939502716064 at epoch: 14 and batch_num: 214\n",
      "Loss of train set: 0.2835109829902649 at epoch: 14 and batch_num: 215\n",
      "Loss of train set: 0.2636181712150574 at epoch: 14 and batch_num: 216\n",
      "Loss of train set: 0.2789957523345947 at epoch: 14 and batch_num: 217\n",
      "Loss of train set: 0.3468683063983917 at epoch: 14 and batch_num: 218\n",
      "Loss of train set: 0.23761770129203796 at epoch: 14 and batch_num: 219\n",
      "Loss of train set: 0.18413862586021423 at epoch: 14 and batch_num: 220\n",
      "Loss of train set: 0.3168030083179474 at epoch: 14 and batch_num: 221\n",
      "Loss of train set: 0.26437997817993164 at epoch: 14 and batch_num: 222\n",
      "Loss of train set: 0.3392844796180725 at epoch: 14 and batch_num: 223\n",
      "Loss of train set: 0.3393654525279999 at epoch: 14 and batch_num: 224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.37115514278411865 at epoch: 14 and batch_num: 225\n",
      "Loss of train set: 0.19365152716636658 at epoch: 14 and batch_num: 226\n",
      "Loss of train set: 0.4548147916793823 at epoch: 14 and batch_num: 227\n",
      "Loss of train set: 0.3447306752204895 at epoch: 14 and batch_num: 228\n",
      "Loss of train set: 0.1799459606409073 at epoch: 14 and batch_num: 229\n",
      "Loss of train set: 0.39962947368621826 at epoch: 14 and batch_num: 230\n",
      "Loss of train set: 0.4385829567909241 at epoch: 14 and batch_num: 231\n",
      "Loss of train set: 0.32860714197158813 at epoch: 14 and batch_num: 232\n",
      "Loss of train set: 0.3401518166065216 at epoch: 14 and batch_num: 233\n",
      "Loss of train set: 0.31814470887184143 at epoch: 14 and batch_num: 234\n",
      "Loss of train set: 0.4646384119987488 at epoch: 14 and batch_num: 235\n",
      "Loss of train set: 0.26393815875053406 at epoch: 14 and batch_num: 236\n",
      "Loss of train set: 0.32965850830078125 at epoch: 14 and batch_num: 237\n",
      "Loss of train set: 0.5551729202270508 at epoch: 14 and batch_num: 238\n",
      "Loss of train set: 0.425892174243927 at epoch: 14 and batch_num: 239\n",
      "Loss of train set: 0.45753389596939087 at epoch: 14 and batch_num: 240\n",
      "Loss of train set: 0.29581218957901 at epoch: 14 and batch_num: 241\n",
      "Loss of train set: 0.3548363447189331 at epoch: 14 and batch_num: 242\n",
      "Loss of train set: 0.27981263399124146 at epoch: 14 and batch_num: 243\n",
      "Loss of train set: 0.392507404088974 at epoch: 14 and batch_num: 244\n",
      "Loss of train set: 0.31177234649658203 at epoch: 14 and batch_num: 245\n",
      "Loss of train set: 0.23743440210819244 at epoch: 14 and batch_num: 246\n",
      "Loss of train set: 0.26295363903045654 at epoch: 14 and batch_num: 247\n",
      "Loss of train set: 0.28973573446273804 at epoch: 14 and batch_num: 248\n",
      "Loss of train set: 0.285083532333374 at epoch: 14 and batch_num: 249\n",
      "Loss of train set: 0.3168480694293976 at epoch: 14 and batch_num: 250\n",
      "Loss of train set: 0.3418601155281067 at epoch: 14 and batch_num: 251\n",
      "Loss of train set: 0.17752160131931305 at epoch: 14 and batch_num: 252\n",
      "Loss of train set: 0.2803873121738434 at epoch: 14 and batch_num: 253\n",
      "Loss of train set: 0.3005484640598297 at epoch: 14 and batch_num: 254\n",
      "Loss of train set: 0.24245132505893707 at epoch: 14 and batch_num: 255\n",
      "Loss of train set: 0.39178887009620667 at epoch: 14 and batch_num: 256\n",
      "Loss of train set: 0.28434836864471436 at epoch: 14 and batch_num: 257\n",
      "Loss of train set: 0.264112651348114 at epoch: 14 and batch_num: 258\n",
      "Loss of train set: 0.22982271015644073 at epoch: 14 and batch_num: 259\n",
      "Loss of train set: 0.26593348383903503 at epoch: 14 and batch_num: 260\n",
      "Loss of train set: 0.25981035828590393 at epoch: 14 and batch_num: 261\n",
      "Loss of train set: 0.2996644675731659 at epoch: 14 and batch_num: 262\n",
      "Loss of train set: 0.36318740248680115 at epoch: 14 and batch_num: 263\n",
      "Loss of train set: 0.14289237558841705 at epoch: 14 and batch_num: 264\n",
      "Loss of train set: 0.2093476951122284 at epoch: 14 and batch_num: 265\n",
      "Loss of train set: 0.369393914937973 at epoch: 14 and batch_num: 266\n",
      "Loss of train set: 0.28877973556518555 at epoch: 14 and batch_num: 267\n",
      "Loss of train set: 0.39555931091308594 at epoch: 14 and batch_num: 268\n",
      "Loss of train set: 0.3549484610557556 at epoch: 14 and batch_num: 269\n",
      "Loss of train set: 0.45564666390419006 at epoch: 14 and batch_num: 270\n",
      "Loss of train set: 0.26616978645324707 at epoch: 14 and batch_num: 271\n",
      "Loss of train set: 0.31334245204925537 at epoch: 14 and batch_num: 272\n",
      "Loss of train set: 0.23293745517730713 at epoch: 14 and batch_num: 273\n",
      "Loss of train set: 0.23037102818489075 at epoch: 14 and batch_num: 274\n",
      "Loss of train set: 0.18893003463745117 at epoch: 14 and batch_num: 275\n",
      "Loss of train set: 0.17392215132713318 at epoch: 14 and batch_num: 276\n",
      "Loss of train set: 0.16473877429962158 at epoch: 14 and batch_num: 277\n",
      "Loss of train set: 0.359822154045105 at epoch: 14 and batch_num: 278\n",
      "Loss of train set: 0.43160656094551086 at epoch: 14 and batch_num: 279\n",
      "Loss of train set: 0.3050074279308319 at epoch: 14 and batch_num: 280\n",
      "Loss of train set: 0.2684299349784851 at epoch: 14 and batch_num: 281\n",
      "Loss of train set: 0.36990851163864136 at epoch: 14 and batch_num: 282\n",
      "Loss of train set: 0.3477756083011627 at epoch: 14 and batch_num: 283\n",
      "Loss of train set: 0.4389216899871826 at epoch: 14 and batch_num: 284\n",
      "Loss of train set: 0.42752379179000854 at epoch: 14 and batch_num: 285\n",
      "Loss of train set: 0.4204682409763336 at epoch: 14 and batch_num: 286\n",
      "Loss of train set: 0.3087330460548401 at epoch: 14 and batch_num: 287\n",
      "Loss of train set: 0.35104900598526 at epoch: 14 and batch_num: 288\n",
      "Loss of train set: 0.26117825508117676 at epoch: 14 and batch_num: 289\n",
      "Loss of train set: 0.4225580096244812 at epoch: 14 and batch_num: 290\n",
      "Loss of train set: 0.32940948009490967 at epoch: 14 and batch_num: 291\n",
      "Loss of train set: 0.34271061420440674 at epoch: 14 and batch_num: 292\n",
      "Loss of train set: 0.24407193064689636 at epoch: 14 and batch_num: 293\n",
      "Loss of train set: 0.3459489345550537 at epoch: 14 and batch_num: 294\n",
      "Loss of train set: 0.2684271037578583 at epoch: 14 and batch_num: 295\n",
      "Loss of train set: 0.4446526765823364 at epoch: 14 and batch_num: 296\n",
      "Loss of train set: 0.4408290386199951 at epoch: 14 and batch_num: 297\n",
      "Loss of train set: 0.28460395336151123 at epoch: 14 and batch_num: 298\n",
      "Loss of train set: 0.18351534008979797 at epoch: 14 and batch_num: 299\n",
      "Loss of train set: 0.33357200026512146 at epoch: 14 and batch_num: 300\n",
      "Loss of train set: 0.2658917307853699 at epoch: 14 and batch_num: 301\n",
      "Loss of train set: 0.2967343032360077 at epoch: 14 and batch_num: 302\n",
      "Loss of train set: 0.3061385750770569 at epoch: 14 and batch_num: 303\n",
      "Loss of train set: 0.27664610743522644 at epoch: 14 and batch_num: 304\n",
      "Loss of train set: 0.17911961674690247 at epoch: 14 and batch_num: 305\n",
      "Loss of train set: 0.29874804615974426 at epoch: 14 and batch_num: 306\n",
      "Loss of train set: 0.3483046889305115 at epoch: 14 and batch_num: 307\n",
      "Loss of train set: 0.1743277907371521 at epoch: 14 and batch_num: 308\n",
      "Loss of train set: 0.4681299328804016 at epoch: 14 and batch_num: 309\n",
      "Loss of train set: 0.17390498518943787 at epoch: 14 and batch_num: 310\n",
      "Loss of train set: 0.2660202383995056 at epoch: 14 and batch_num: 311\n",
      "Loss of train set: 0.2816934883594513 at epoch: 14 and batch_num: 312\n",
      "Loss of train set: 0.3104895353317261 at epoch: 14 and batch_num: 313\n",
      "Loss of train set: 0.28601470589637756 at epoch: 14 and batch_num: 314\n",
      "Loss of train set: 0.16285055875778198 at epoch: 14 and batch_num: 315\n",
      "Loss of train set: 0.19051340222358704 at epoch: 14 and batch_num: 316\n",
      "Loss of train set: 0.25699520111083984 at epoch: 14 and batch_num: 317\n",
      "Loss of train set: 0.26919135451316833 at epoch: 14 and batch_num: 318\n",
      "Loss of train set: 0.5017477869987488 at epoch: 14 and batch_num: 319\n",
      "Loss of train set: 0.3336937725543976 at epoch: 14 and batch_num: 320\n",
      "Loss of train set: 0.4085681438446045 at epoch: 14 and batch_num: 321\n",
      "Loss of train set: 0.19828766584396362 at epoch: 14 and batch_num: 322\n",
      "Loss of train set: 0.5441364049911499 at epoch: 14 and batch_num: 323\n",
      "Loss of train set: 0.2610219120979309 at epoch: 14 and batch_num: 324\n",
      "Loss of train set: 0.24484127759933472 at epoch: 14 and batch_num: 325\n",
      "Loss of train set: 0.3198525011539459 at epoch: 14 and batch_num: 326\n",
      "Loss of train set: 0.3866480588912964 at epoch: 14 and batch_num: 327\n",
      "Loss of train set: 0.253757119178772 at epoch: 14 and batch_num: 328\n",
      "Loss of train set: 0.3541569411754608 at epoch: 14 and batch_num: 329\n",
      "Loss of train set: 0.3581627607345581 at epoch: 14 and batch_num: 330\n",
      "Loss of train set: 0.3228846490383148 at epoch: 14 and batch_num: 331\n",
      "Loss of train set: 0.26611608266830444 at epoch: 14 and batch_num: 332\n",
      "Loss of train set: 0.4135250449180603 at epoch: 14 and batch_num: 333\n",
      "Loss of train set: 0.36033129692077637 at epoch: 14 and batch_num: 334\n",
      "Loss of train set: 0.43463295698165894 at epoch: 14 and batch_num: 335\n",
      "Loss of train set: 0.466936320066452 at epoch: 14 and batch_num: 336\n",
      "Loss of train set: 0.35430780053138733 at epoch: 14 and batch_num: 337\n",
      "Loss of train set: 0.31582897901535034 at epoch: 14 and batch_num: 338\n",
      "Loss of train set: 0.42022135853767395 at epoch: 14 and batch_num: 339\n",
      "Loss of train set: 0.2589305341243744 at epoch: 14 and batch_num: 340\n",
      "Loss of train set: 0.14313946664333344 at epoch: 14 and batch_num: 341\n",
      "Loss of train set: 0.2757279872894287 at epoch: 14 and batch_num: 342\n",
      "Loss of train set: 0.3906710147857666 at epoch: 14 and batch_num: 343\n",
      "Loss of train set: 0.32950401306152344 at epoch: 14 and batch_num: 344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.29205119609832764 at epoch: 14 and batch_num: 345\n",
      "Loss of train set: 0.26357710361480713 at epoch: 14 and batch_num: 346\n",
      "Loss of train set: 0.3163251280784607 at epoch: 14 and batch_num: 347\n",
      "Loss of train set: 0.3579877018928528 at epoch: 14 and batch_num: 348\n",
      "Loss of train set: 0.2869880497455597 at epoch: 14 and batch_num: 349\n",
      "Loss of train set: 0.2249346375465393 at epoch: 14 and batch_num: 350\n",
      "Loss of train set: 0.31073033809661865 at epoch: 14 and batch_num: 351\n",
      "Loss of train set: 0.41343140602111816 at epoch: 14 and batch_num: 352\n",
      "Loss of train set: 0.3904711902141571 at epoch: 14 and batch_num: 353\n",
      "Loss of train set: 0.4996616244316101 at epoch: 14 and batch_num: 354\n",
      "Loss of train set: 0.3354945778846741 at epoch: 14 and batch_num: 355\n",
      "Loss of train set: 0.36400753259658813 at epoch: 14 and batch_num: 356\n",
      "Loss of train set: 0.19673898816108704 at epoch: 14 and batch_num: 357\n",
      "Loss of train set: 0.13742280006408691 at epoch: 14 and batch_num: 358\n",
      "Loss of train set: 0.13440930843353271 at epoch: 14 and batch_num: 359\n",
      "Loss of train set: 0.19932323694229126 at epoch: 14 and batch_num: 360\n",
      "Loss of train set: 0.26776933670043945 at epoch: 14 and batch_num: 361\n",
      "Loss of train set: 0.4276007115840912 at epoch: 14 and batch_num: 362\n",
      "Loss of train set: 0.3234608471393585 at epoch: 14 and batch_num: 363\n",
      "Loss of train set: 0.21958249807357788 at epoch: 14 and batch_num: 364\n",
      "Loss of train set: 0.2219838947057724 at epoch: 14 and batch_num: 365\n",
      "Loss of train set: 0.24412153661251068 at epoch: 14 and batch_num: 366\n",
      "Loss of train set: 0.2097235471010208 at epoch: 14 and batch_num: 367\n",
      "Loss of train set: 0.2377031296491623 at epoch: 14 and batch_num: 368\n",
      "Loss of train set: 0.3483859896659851 at epoch: 14 and batch_num: 369\n",
      "Loss of train set: 0.2778516411781311 at epoch: 14 and batch_num: 370\n",
      "Loss of train set: 0.17000696063041687 at epoch: 14 and batch_num: 371\n",
      "Loss of train set: 0.40896663069725037 at epoch: 14 and batch_num: 372\n",
      "Loss of train set: 0.27620360255241394 at epoch: 14 and batch_num: 373\n",
      "Loss of train set: 0.3380143940448761 at epoch: 14 and batch_num: 374\n",
      "Loss of train set: 0.24962982535362244 at epoch: 14 and batch_num: 375\n",
      "Loss of train set: 0.393294095993042 at epoch: 14 and batch_num: 376\n",
      "Loss of train set: 0.2555316984653473 at epoch: 14 and batch_num: 377\n",
      "Loss of train set: 0.3799065947532654 at epoch: 14 and batch_num: 378\n",
      "Loss of train set: 0.3425601124763489 at epoch: 14 and batch_num: 379\n",
      "Loss of train set: 0.2780195474624634 at epoch: 14 and batch_num: 380\n",
      "Loss of train set: 0.24114395678043365 at epoch: 14 and batch_num: 381\n",
      "Loss of train set: 0.36224305629730225 at epoch: 14 and batch_num: 382\n",
      "Loss of train set: 0.2840616703033447 at epoch: 14 and batch_num: 383\n",
      "Loss of train set: 0.16378629207611084 at epoch: 14 and batch_num: 384\n",
      "Loss of train set: 0.22064194083213806 at epoch: 14 and batch_num: 385\n",
      "Loss of train set: 0.35642194747924805 at epoch: 14 and batch_num: 386\n",
      "Loss of train set: 0.404614120721817 at epoch: 14 and batch_num: 387\n",
      "Loss of train set: 0.23157542943954468 at epoch: 14 and batch_num: 388\n",
      "Loss of train set: 0.4907691776752472 at epoch: 14 and batch_num: 389\n",
      "Loss of train set: 0.29748833179473877 at epoch: 14 and batch_num: 390\n",
      "Loss of train set: 0.22137773036956787 at epoch: 14 and batch_num: 391\n",
      "Loss of train set: 0.3988337218761444 at epoch: 14 and batch_num: 392\n",
      "Loss of train set: 0.30539512634277344 at epoch: 14 and batch_num: 393\n",
      "Loss of train set: 0.2114102989435196 at epoch: 14 and batch_num: 394\n",
      "Loss of train set: 0.5095227956771851 at epoch: 14 and batch_num: 395\n",
      "Loss of train set: 0.22014398872852325 at epoch: 14 and batch_num: 396\n",
      "Loss of train set: 0.3648225963115692 at epoch: 14 and batch_num: 397\n",
      "Loss of train set: 0.35534417629241943 at epoch: 14 and batch_num: 398\n",
      "Loss of train set: 0.28248193860054016 at epoch: 14 and batch_num: 399\n",
      "Loss of train set: 0.2143712043762207 at epoch: 14 and batch_num: 400\n",
      "Loss of train set: 0.4669671058654785 at epoch: 14 and batch_num: 401\n",
      "Loss of train set: 0.3205161988735199 at epoch: 14 and batch_num: 402\n",
      "Loss of train set: 0.3662313222885132 at epoch: 14 and batch_num: 403\n",
      "Loss of train set: 0.27949708700180054 at epoch: 14 and batch_num: 404\n",
      "Loss of train set: 0.22070324420928955 at epoch: 14 and batch_num: 405\n",
      "Loss of train set: 0.24740542471408844 at epoch: 14 and batch_num: 406\n",
      "Loss of train set: 0.16147136688232422 at epoch: 14 and batch_num: 407\n",
      "Loss of train set: 0.26202312111854553 at epoch: 14 and batch_num: 408\n",
      "Loss of train set: 0.37490642070770264 at epoch: 14 and batch_num: 409\n",
      "Loss of train set: 0.29448020458221436 at epoch: 14 and batch_num: 410\n",
      "Loss of train set: 0.255867063999176 at epoch: 14 and batch_num: 411\n",
      "Loss of train set: 0.2566491663455963 at epoch: 14 and batch_num: 412\n",
      "Loss of train set: 0.4091283082962036 at epoch: 14 and batch_num: 413\n",
      "Loss of train set: 0.3550727665424347 at epoch: 14 and batch_num: 414\n",
      "Loss of train set: 0.2817457318305969 at epoch: 14 and batch_num: 415\n",
      "Loss of train set: 0.27013036608695984 at epoch: 14 and batch_num: 416\n",
      "Loss of train set: 0.34861838817596436 at epoch: 14 and batch_num: 417\n",
      "Loss of train set: 0.4291532039642334 at epoch: 14 and batch_num: 418\n",
      "Loss of train set: 0.28621938824653625 at epoch: 14 and batch_num: 419\n",
      "Loss of train set: 0.3034738302230835 at epoch: 14 and batch_num: 420\n",
      "Loss of train set: 0.30174732208251953 at epoch: 14 and batch_num: 421\n",
      "Loss of train set: 0.21098560094833374 at epoch: 14 and batch_num: 422\n",
      "Loss of train set: 0.43011075258255005 at epoch: 14 and batch_num: 423\n",
      "Loss of train set: 0.1147972047328949 at epoch: 14 and batch_num: 424\n",
      "Loss of train set: 0.29965290427207947 at epoch: 14 and batch_num: 425\n",
      "Loss of train set: 0.4183635413646698 at epoch: 14 and batch_num: 426\n",
      "Loss of train set: 0.2821793258190155 at epoch: 14 and batch_num: 427\n",
      "Loss of train set: 0.2863766551017761 at epoch: 14 and batch_num: 428\n",
      "Loss of train set: 0.3496486246585846 at epoch: 14 and batch_num: 429\n",
      "Loss of train set: 0.41322875022888184 at epoch: 14 and batch_num: 430\n",
      "Loss of train set: 0.302083820104599 at epoch: 14 and batch_num: 431\n",
      "Loss of train set: 0.36169883608818054 at epoch: 14 and batch_num: 432\n",
      "Loss of train set: 0.40498581528663635 at epoch: 14 and batch_num: 433\n",
      "Loss of train set: 0.2117655724287033 at epoch: 14 and batch_num: 434\n",
      "Loss of train set: 0.3730957508087158 at epoch: 14 and batch_num: 435\n",
      "Loss of train set: 0.35900580883026123 at epoch: 14 and batch_num: 436\n",
      "Loss of train set: 0.43118512630462646 at epoch: 14 and batch_num: 437\n",
      "Loss of train set: 0.33101171255111694 at epoch: 14 and batch_num: 438\n",
      "Loss of train set: 0.3292991518974304 at epoch: 14 and batch_num: 439\n",
      "Loss of train set: 0.2671278715133667 at epoch: 14 and batch_num: 440\n",
      "Loss of train set: 0.2896035611629486 at epoch: 14 and batch_num: 441\n",
      "Loss of train set: 0.31935903429985046 at epoch: 14 and batch_num: 442\n",
      "Loss of train set: 0.2177816927433014 at epoch: 14 and batch_num: 443\n",
      "Loss of train set: 0.16475796699523926 at epoch: 14 and batch_num: 444\n",
      "Loss of train set: 0.39118117094039917 at epoch: 14 and batch_num: 445\n",
      "Loss of train set: 0.2751358151435852 at epoch: 14 and batch_num: 446\n",
      "Loss of train set: 0.24543051421642303 at epoch: 14 and batch_num: 447\n",
      "Loss of train set: 0.22585240006446838 at epoch: 14 and batch_num: 448\n",
      "Loss of train set: 0.4142704904079437 at epoch: 14 and batch_num: 449\n",
      "Loss of train set: 0.18203280866146088 at epoch: 14 and batch_num: 450\n",
      "Loss of train set: 0.25046485662460327 at epoch: 14 and batch_num: 451\n",
      "Loss of train set: 0.5874773263931274 at epoch: 14 and batch_num: 452\n",
      "Loss of train set: 0.38433223962783813 at epoch: 14 and batch_num: 453\n",
      "Loss of train set: 0.25847864151000977 at epoch: 14 and batch_num: 454\n",
      "Loss of train set: 0.35696232318878174 at epoch: 14 and batch_num: 455\n",
      "Loss of train set: 0.3605602979660034 at epoch: 14 and batch_num: 456\n",
      "Loss of train set: 0.4071611762046814 at epoch: 14 and batch_num: 457\n",
      "Loss of train set: 0.33358651399612427 at epoch: 14 and batch_num: 458\n",
      "Loss of train set: 0.38013535737991333 at epoch: 14 and batch_num: 459\n",
      "Loss of train set: 0.28190726041793823 at epoch: 14 and batch_num: 460\n",
      "Loss of train set: 0.4192139506340027 at epoch: 14 and batch_num: 461\n",
      "Loss of train set: 0.4236868619918823 at epoch: 14 and batch_num: 462\n",
      "Loss of train set: 0.3383076786994934 at epoch: 14 and batch_num: 463\n",
      "Loss of train set: 0.2476174235343933 at epoch: 14 and batch_num: 464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.23528119921684265 at epoch: 14 and batch_num: 465\n",
      "Loss of train set: 0.36550211906433105 at epoch: 14 and batch_num: 466\n",
      "Loss of train set: 0.30415356159210205 at epoch: 14 and batch_num: 467\n",
      "Loss of train set: 0.24496060609817505 at epoch: 14 and batch_num: 468\n",
      "Loss of train set: 0.2378043234348297 at epoch: 14 and batch_num: 469\n",
      "Loss of train set: 0.325517475605011 at epoch: 14 and batch_num: 470\n",
      "Loss of train set: 0.12615299224853516 at epoch: 14 and batch_num: 471\n",
      "Loss of train set: 0.4545939564704895 at epoch: 14 and batch_num: 472\n",
      "Loss of train set: 0.25526565313339233 at epoch: 14 and batch_num: 473\n",
      "Loss of train set: 0.2961837649345398 at epoch: 14 and batch_num: 474\n",
      "Loss of train set: 0.12027440220117569 at epoch: 14 and batch_num: 475\n",
      "Loss of train set: 0.23491011559963226 at epoch: 14 and batch_num: 476\n",
      "Loss of train set: 0.22492842376232147 at epoch: 14 and batch_num: 477\n",
      "Loss of train set: 0.25962769985198975 at epoch: 14 and batch_num: 478\n",
      "Loss of train set: 0.2398310899734497 at epoch: 14 and batch_num: 479\n",
      "Loss of train set: 0.25466102361679077 at epoch: 14 and batch_num: 480\n",
      "Loss of train set: 0.21570728719234467 at epoch: 14 and batch_num: 481\n",
      "Loss of train set: 0.24591487646102905 at epoch: 14 and batch_num: 482\n",
      "Loss of train set: 0.24137035012245178 at epoch: 14 and batch_num: 483\n",
      "Loss of train set: 0.3588567078113556 at epoch: 14 and batch_num: 484\n",
      "Loss of train set: 0.4352076053619385 at epoch: 14 and batch_num: 485\n",
      "Loss of train set: 0.25888943672180176 at epoch: 14 and batch_num: 486\n",
      "Loss of train set: 0.4860479235649109 at epoch: 14 and batch_num: 487\n",
      "Loss of train set: 0.3323013186454773 at epoch: 14 and batch_num: 488\n",
      "Loss of train set: 0.13889142870903015 at epoch: 14 and batch_num: 489\n",
      "Loss of train set: 0.39677694439888 at epoch: 14 and batch_num: 490\n",
      "Loss of train set: 0.5537903308868408 at epoch: 14 and batch_num: 491\n",
      "Loss of train set: 0.3241981267929077 at epoch: 14 and batch_num: 492\n",
      "Loss of train set: 0.27770325541496277 at epoch: 14 and batch_num: 493\n",
      "Loss of train set: 0.3825792968273163 at epoch: 14 and batch_num: 494\n",
      "Loss of train set: 0.262129008769989 at epoch: 14 and batch_num: 495\n",
      "Loss of train set: 0.4123527407646179 at epoch: 14 and batch_num: 496\n",
      "Loss of train set: 0.3091236650943756 at epoch: 14 and batch_num: 497\n",
      "Loss of train set: 0.4510642886161804 at epoch: 14 and batch_num: 498\n",
      "Loss of train set: 0.3572149872779846 at epoch: 14 and batch_num: 499\n",
      "Loss of train set: 0.29372602701187134 at epoch: 14 and batch_num: 500\n",
      "Loss of train set: 0.3249983787536621 at epoch: 14 and batch_num: 501\n",
      "Loss of train set: 0.28155234456062317 at epoch: 14 and batch_num: 502\n",
      "Loss of train set: 0.3772103190422058 at epoch: 14 and batch_num: 503\n",
      "Loss of train set: 0.2655033469200134 at epoch: 14 and batch_num: 504\n",
      "Loss of train set: 0.38069742918014526 at epoch: 14 and batch_num: 505\n",
      "Loss of train set: 0.38032710552215576 at epoch: 14 and batch_num: 506\n",
      "Loss of train set: 0.2623400092124939 at epoch: 14 and batch_num: 507\n",
      "Loss of train set: 0.21453794836997986 at epoch: 14 and batch_num: 508\n",
      "Loss of train set: 0.2683188319206238 at epoch: 14 and batch_num: 509\n",
      "Loss of train set: 0.38664618134498596 at epoch: 14 and batch_num: 510\n",
      "Loss of train set: 0.3221891522407532 at epoch: 14 and batch_num: 511\n",
      "Loss of train set: 0.30562812089920044 at epoch: 14 and batch_num: 512\n",
      "Loss of train set: 0.3441884219646454 at epoch: 14 and batch_num: 513\n",
      "Loss of train set: 0.29638761281967163 at epoch: 14 and batch_num: 514\n",
      "Loss of train set: 0.2395656853914261 at epoch: 14 and batch_num: 515\n",
      "Loss of train set: 0.2811523675918579 at epoch: 14 and batch_num: 516\n",
      "Loss of train set: 0.17668837308883667 at epoch: 14 and batch_num: 517\n",
      "Loss of train set: 0.2268967479467392 at epoch: 14 and batch_num: 518\n",
      "Loss of train set: 0.2735329270362854 at epoch: 14 and batch_num: 519\n",
      "Loss of train set: 0.4224482774734497 at epoch: 14 and batch_num: 520\n",
      "Loss of train set: 0.22019916772842407 at epoch: 14 and batch_num: 521\n",
      "Loss of train set: 0.2104533314704895 at epoch: 14 and batch_num: 522\n",
      "Loss of train set: 0.40211981534957886 at epoch: 14 and batch_num: 523\n",
      "Loss of train set: 0.30042025446891785 at epoch: 14 and batch_num: 524\n",
      "Loss of train set: 0.13210099935531616 at epoch: 14 and batch_num: 525\n",
      "Loss of train set: 0.3819175958633423 at epoch: 14 and batch_num: 526\n",
      "Loss of train set: 0.3418944478034973 at epoch: 14 and batch_num: 527\n",
      "Loss of train set: 0.17367446422576904 at epoch: 14 and batch_num: 528\n",
      "Loss of train set: 0.339226633310318 at epoch: 14 and batch_num: 529\n",
      "Loss of train set: 0.3286159038543701 at epoch: 14 and batch_num: 530\n",
      "Loss of train set: 0.30776363611221313 at epoch: 14 and batch_num: 531\n",
      "Loss of train set: 0.3376808762550354 at epoch: 14 and batch_num: 532\n",
      "Loss of train set: 0.37895387411117554 at epoch: 14 and batch_num: 533\n",
      "Loss of train set: 0.3747481107711792 at epoch: 14 and batch_num: 534\n",
      "Loss of train set: 0.2697141170501709 at epoch: 14 and batch_num: 535\n",
      "Loss of train set: 0.30511313676834106 at epoch: 14 and batch_num: 536\n",
      "Loss of train set: 0.33698832988739014 at epoch: 14 and batch_num: 537\n",
      "Loss of train set: 0.19502511620521545 at epoch: 14 and batch_num: 538\n",
      "Loss of train set: 0.527251124382019 at epoch: 14 and batch_num: 539\n",
      "Loss of train set: 0.3361075520515442 at epoch: 14 and batch_num: 540\n",
      "Loss of train set: 0.32779890298843384 at epoch: 14 and batch_num: 541\n",
      "Loss of train set: 0.333109587430954 at epoch: 14 and batch_num: 542\n",
      "Loss of train set: 0.41362065076828003 at epoch: 14 and batch_num: 543\n",
      "Loss of train set: 0.21377934515476227 at epoch: 14 and batch_num: 544\n",
      "Loss of train set: 0.21585139632225037 at epoch: 14 and batch_num: 545\n",
      "Loss of train set: 0.47992971539497375 at epoch: 14 and batch_num: 546\n",
      "Loss of train set: 0.15850575268268585 at epoch: 14 and batch_num: 547\n",
      "Loss of train set: 0.26468586921691895 at epoch: 14 and batch_num: 548\n",
      "Loss of train set: 0.29567214846611023 at epoch: 14 and batch_num: 549\n",
      "Loss of train set: 0.23077094554901123 at epoch: 14 and batch_num: 550\n",
      "Loss of train set: 0.21488480269908905 at epoch: 14 and batch_num: 551\n",
      "Loss of train set: 0.35233283042907715 at epoch: 14 and batch_num: 552\n",
      "Loss of train set: 0.24133513867855072 at epoch: 14 and batch_num: 553\n",
      "Loss of train set: 0.21210864186286926 at epoch: 14 and batch_num: 554\n",
      "Loss of train set: 0.27856266498565674 at epoch: 14 and batch_num: 555\n",
      "Loss of train set: 0.289220929145813 at epoch: 14 and batch_num: 556\n",
      "Loss of train set: 0.24451544880867004 at epoch: 14 and batch_num: 557\n",
      "Loss of train set: 0.3729422092437744 at epoch: 14 and batch_num: 558\n",
      "Loss of train set: 0.3016374707221985 at epoch: 14 and batch_num: 559\n",
      "Loss of train set: 0.22647030651569366 at epoch: 14 and batch_num: 560\n",
      "Loss of train set: 0.34488460421562195 at epoch: 14 and batch_num: 561\n",
      "Loss of train set: 0.2218242883682251 at epoch: 14 and batch_num: 562\n",
      "Loss of train set: 0.4503013789653778 at epoch: 14 and batch_num: 563\n",
      "Loss of train set: 0.2841217815876007 at epoch: 14 and batch_num: 564\n",
      "Loss of train set: 0.2701782286167145 at epoch: 14 and batch_num: 565\n",
      "Loss of train set: 0.3983941078186035 at epoch: 14 and batch_num: 566\n",
      "Loss of train set: 0.17054657638072968 at epoch: 14 and batch_num: 567\n",
      "Loss of train set: 0.22563724219799042 at epoch: 14 and batch_num: 568\n",
      "Loss of train set: 0.1779901385307312 at epoch: 14 and batch_num: 569\n",
      "Loss of train set: 0.2146783322095871 at epoch: 14 and batch_num: 570\n",
      "Loss of train set: 0.2513125240802765 at epoch: 14 and batch_num: 571\n",
      "Loss of train set: 0.33841055631637573 at epoch: 14 and batch_num: 572\n",
      "Loss of train set: 0.2820313274860382 at epoch: 14 and batch_num: 573\n",
      "Loss of train set: 0.1896088570356369 at epoch: 14 and batch_num: 574\n",
      "Loss of train set: 0.29854458570480347 at epoch: 14 and batch_num: 575\n",
      "Loss of train set: 0.30098986625671387 at epoch: 14 and batch_num: 576\n",
      "Loss of train set: 0.33086273074150085 at epoch: 14 and batch_num: 577\n",
      "Loss of train set: 0.23289281129837036 at epoch: 14 and batch_num: 578\n",
      "Loss of train set: 0.26877427101135254 at epoch: 14 and batch_num: 579\n",
      "Loss of train set: 0.1882653683423996 at epoch: 14 and batch_num: 580\n",
      "Loss of train set: 0.29278966784477234 at epoch: 14 and batch_num: 581\n",
      "Loss of train set: 0.32460296154022217 at epoch: 14 and batch_num: 582\n",
      "Loss of train set: 0.15767452120780945 at epoch: 14 and batch_num: 583\n",
      "Loss of train set: 0.4863256812095642 at epoch: 14 and batch_num: 584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3193724453449249 at epoch: 14 and batch_num: 585\n",
      "Loss of train set: 0.22633619606494904 at epoch: 14 and batch_num: 586\n",
      "Loss of train set: 0.30503734946250916 at epoch: 14 and batch_num: 587\n",
      "Loss of train set: 0.2652699649333954 at epoch: 14 and batch_num: 588\n",
      "Loss of train set: 0.19812875986099243 at epoch: 14 and batch_num: 589\n",
      "Loss of train set: 0.2724705934524536 at epoch: 14 and batch_num: 590\n",
      "Loss of train set: 0.4797295033931732 at epoch: 14 and batch_num: 591\n",
      "Loss of train set: 0.2677757143974304 at epoch: 14 and batch_num: 592\n",
      "Loss of train set: 0.436492919921875 at epoch: 14 and batch_num: 593\n",
      "Loss of train set: 0.44779250025749207 at epoch: 14 and batch_num: 594\n",
      "Loss of train set: 0.5373656749725342 at epoch: 14 and batch_num: 595\n",
      "Loss of train set: 0.5858746767044067 at epoch: 14 and batch_num: 596\n",
      "Loss of train set: 0.4495662748813629 at epoch: 14 and batch_num: 597\n",
      "Loss of train set: 0.39688411355018616 at epoch: 14 and batch_num: 598\n",
      "Loss of train set: 0.18218810856342316 at epoch: 14 and batch_num: 599\n",
      "Loss of train set: 0.45916393399238586 at epoch: 14 and batch_num: 600\n",
      "Loss of train set: 0.2690490484237671 at epoch: 14 and batch_num: 601\n",
      "Loss of train set: 0.16116929054260254 at epoch: 14 and batch_num: 602\n",
      "Loss of train set: 0.22395476698875427 at epoch: 14 and batch_num: 603\n",
      "Loss of train set: 0.1730990707874298 at epoch: 14 and batch_num: 604\n",
      "Loss of train set: 0.20832861959934235 at epoch: 14 and batch_num: 605\n",
      "Loss of train set: 0.41405946016311646 at epoch: 14 and batch_num: 606\n",
      "Loss of train set: 0.24190737307071686 at epoch: 14 and batch_num: 607\n",
      "Loss of train set: 0.272793173789978 at epoch: 14 and batch_num: 608\n",
      "Loss of train set: 0.31586015224456787 at epoch: 14 and batch_num: 609\n",
      "Loss of train set: 0.2673434317111969 at epoch: 14 and batch_num: 610\n",
      "Loss of train set: 0.16878247261047363 at epoch: 14 and batch_num: 611\n",
      "Loss of train set: 0.30014294385910034 at epoch: 14 and batch_num: 612\n",
      "Loss of train set: 0.27913063764572144 at epoch: 14 and batch_num: 613\n",
      "Loss of train set: 0.46468666195869446 at epoch: 14 and batch_num: 614\n",
      "Loss of train set: 0.3636171221733093 at epoch: 14 and batch_num: 615\n",
      "Loss of train set: 0.2387651950120926 at epoch: 14 and batch_num: 616\n",
      "Loss of train set: 0.21542879939079285 at epoch: 14 and batch_num: 617\n",
      "Loss of train set: 0.3042766749858856 at epoch: 14 and batch_num: 618\n",
      "Loss of train set: 0.4156378209590912 at epoch: 14 and batch_num: 619\n",
      "Loss of train set: 0.29611796140670776 at epoch: 14 and batch_num: 620\n",
      "Loss of train set: 0.367867648601532 at epoch: 14 and batch_num: 621\n",
      "Loss of train set: 0.3477630019187927 at epoch: 14 and batch_num: 622\n",
      "Loss of train set: 0.25481414794921875 at epoch: 14 and batch_num: 623\n",
      "Loss of train set: 0.20377954840660095 at epoch: 14 and batch_num: 624\n",
      "Loss of train set: 0.13335368037223816 at epoch: 14 and batch_num: 625\n",
      "Loss of train set: 0.28867194056510925 at epoch: 14 and batch_num: 626\n",
      "Loss of train set: 0.34052741527557373 at epoch: 14 and batch_num: 627\n",
      "Loss of train set: 0.3894945979118347 at epoch: 14 and batch_num: 628\n",
      "Loss of train set: 0.39546746015548706 at epoch: 14 and batch_num: 629\n",
      "Loss of train set: 0.1654079258441925 at epoch: 14 and batch_num: 630\n",
      "Loss of train set: 0.3543107211589813 at epoch: 14 and batch_num: 631\n",
      "Loss of train set: 0.18194785714149475 at epoch: 14 and batch_num: 632\n",
      "Loss of train set: 0.2753652036190033 at epoch: 14 and batch_num: 633\n",
      "Loss of train set: 0.20804403722286224 at epoch: 14 and batch_num: 634\n",
      "Loss of train set: 0.2642764449119568 at epoch: 14 and batch_num: 635\n",
      "Loss of train set: 0.2730119228363037 at epoch: 14 and batch_num: 636\n",
      "Loss of train set: 0.31863313913345337 at epoch: 14 and batch_num: 637\n",
      "Loss of train set: 0.39316731691360474 at epoch: 14 and batch_num: 638\n",
      "Loss of train set: 0.3087889552116394 at epoch: 14 and batch_num: 639\n",
      "Loss of train set: 0.3759536147117615 at epoch: 14 and batch_num: 640\n",
      "Loss of train set: 0.23301705718040466 at epoch: 14 and batch_num: 641\n",
      "Loss of train set: 0.3274809718132019 at epoch: 14 and batch_num: 642\n",
      "Loss of train set: 0.40799927711486816 at epoch: 14 and batch_num: 643\n",
      "Loss of train set: 0.2679596543312073 at epoch: 14 and batch_num: 644\n",
      "Loss of train set: 0.3986603915691376 at epoch: 14 and batch_num: 645\n",
      "Loss of train set: 0.3580884337425232 at epoch: 14 and batch_num: 646\n",
      "Loss of train set: 0.2462691366672516 at epoch: 14 and batch_num: 647\n",
      "Loss of train set: 0.1859624832868576 at epoch: 14 and batch_num: 648\n",
      "Loss of train set: 0.17602227628231049 at epoch: 14 and batch_num: 649\n",
      "Loss of train set: 0.27864548563957214 at epoch: 14 and batch_num: 650\n",
      "Loss of train set: 0.35850900411605835 at epoch: 14 and batch_num: 651\n",
      "Loss of train set: 0.3140001893043518 at epoch: 14 and batch_num: 652\n",
      "Loss of train set: 0.22055849432945251 at epoch: 14 and batch_num: 653\n",
      "Loss of train set: 0.34398093819618225 at epoch: 14 and batch_num: 654\n",
      "Loss of train set: 0.3325573205947876 at epoch: 14 and batch_num: 655\n",
      "Loss of train set: 0.3277222812175751 at epoch: 14 and batch_num: 656\n",
      "Loss of train set: 0.2878298759460449 at epoch: 14 and batch_num: 657\n",
      "Loss of train set: 0.3164541721343994 at epoch: 14 and batch_num: 658\n",
      "Loss of train set: 0.4316243529319763 at epoch: 14 and batch_num: 659\n",
      "Loss of train set: 0.30068206787109375 at epoch: 14 and batch_num: 660\n",
      "Loss of train set: 0.2225915491580963 at epoch: 14 and batch_num: 661\n",
      "Loss of train set: 0.20942126214504242 at epoch: 14 and batch_num: 662\n",
      "Loss of train set: 0.38617056608200073 at epoch: 14 and batch_num: 663\n",
      "Loss of train set: 0.3282102346420288 at epoch: 14 and batch_num: 664\n",
      "Loss of train set: 0.24089315533638 at epoch: 14 and batch_num: 665\n",
      "Loss of train set: 0.2641561031341553 at epoch: 14 and batch_num: 666\n",
      "Loss of train set: 0.18749186396598816 at epoch: 14 and batch_num: 667\n",
      "Loss of train set: 0.377718061208725 at epoch: 14 and batch_num: 668\n",
      "Loss of train set: 0.4944208860397339 at epoch: 14 and batch_num: 669\n",
      "Loss of train set: 0.4082188010215759 at epoch: 14 and batch_num: 670\n",
      "Loss of train set: 0.16070976853370667 at epoch: 14 and batch_num: 671\n",
      "Loss of train set: 0.2449457049369812 at epoch: 14 and batch_num: 672\n",
      "Loss of train set: 0.442399263381958 at epoch: 14 and batch_num: 673\n",
      "Loss of train set: 0.33782097697257996 at epoch: 14 and batch_num: 674\n",
      "Loss of train set: 0.3729095757007599 at epoch: 14 and batch_num: 675\n",
      "Loss of train set: 0.48846691846847534 at epoch: 14 and batch_num: 676\n",
      "Loss of train set: 0.2672090232372284 at epoch: 14 and batch_num: 677\n",
      "Loss of train set: 0.37026259303092957 at epoch: 14 and batch_num: 678\n",
      "Loss of train set: 0.4237311780452728 at epoch: 14 and batch_num: 679\n",
      "Loss of train set: 0.2743394672870636 at epoch: 14 and batch_num: 680\n",
      "Loss of train set: 0.29065489768981934 at epoch: 14 and batch_num: 681\n",
      "Loss of train set: 0.37942880392074585 at epoch: 14 and batch_num: 682\n",
      "Loss of train set: 0.3312032222747803 at epoch: 14 and batch_num: 683\n",
      "Loss of train set: 0.3199709951877594 at epoch: 14 and batch_num: 684\n",
      "Loss of train set: 0.2134372442960739 at epoch: 14 and batch_num: 685\n",
      "Loss of train set: 0.3161241412162781 at epoch: 14 and batch_num: 686\n",
      "Loss of train set: 0.2316984087228775 at epoch: 14 and batch_num: 687\n",
      "Loss of train set: 0.3368145227432251 at epoch: 14 and batch_num: 688\n",
      "Loss of train set: 0.4138784408569336 at epoch: 14 and batch_num: 689\n",
      "Loss of train set: 0.35562729835510254 at epoch: 14 and batch_num: 690\n",
      "Loss of train set: 0.3157583475112915 at epoch: 14 and batch_num: 691\n",
      "Loss of train set: 0.1991673707962036 at epoch: 14 and batch_num: 692\n",
      "Loss of train set: 0.31433945894241333 at epoch: 14 and batch_num: 693\n",
      "Loss of train set: 0.22158846259117126 at epoch: 14 and batch_num: 694\n",
      "Loss of train set: 0.3662017583847046 at epoch: 14 and batch_num: 695\n",
      "Loss of train set: 0.21938951313495636 at epoch: 14 and batch_num: 696\n",
      "Loss of train set: 0.3047969341278076 at epoch: 14 and batch_num: 697\n",
      "Loss of train set: 0.4212080240249634 at epoch: 14 and batch_num: 698\n",
      "Loss of train set: 0.3209097683429718 at epoch: 14 and batch_num: 699\n",
      "Loss of train set: 0.39847683906555176 at epoch: 14 and batch_num: 700\n",
      "Loss of train set: 0.3207729458808899 at epoch: 14 and batch_num: 701\n",
      "Loss of train set: 0.3001789450645447 at epoch: 14 and batch_num: 702\n",
      "Loss of train set: 0.16391317546367645 at epoch: 14 and batch_num: 703\n",
      "Loss of train set: 0.5435304045677185 at epoch: 14 and batch_num: 704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2863296866416931 at epoch: 14 and batch_num: 705\n",
      "Loss of train set: 0.14072591066360474 at epoch: 14 and batch_num: 706\n",
      "Loss of train set: 0.37561526894569397 at epoch: 14 and batch_num: 707\n",
      "Loss of train set: 0.22203202545642853 at epoch: 14 and batch_num: 708\n",
      "Loss of train set: 0.38730865716934204 at epoch: 14 and batch_num: 709\n",
      "Loss of train set: 0.41440093517303467 at epoch: 14 and batch_num: 710\n",
      "Loss of train set: 0.34450212121009827 at epoch: 14 and batch_num: 711\n",
      "Loss of train set: 0.22719456255435944 at epoch: 14 and batch_num: 712\n",
      "Loss of train set: 0.3254815936088562 at epoch: 14 and batch_num: 713\n",
      "Loss of train set: 0.44996869564056396 at epoch: 14 and batch_num: 714\n",
      "Loss of train set: 0.336870014667511 at epoch: 14 and batch_num: 715\n",
      "Loss of train set: 0.281475692987442 at epoch: 14 and batch_num: 716\n",
      "Loss of train set: 0.3110159933567047 at epoch: 14 and batch_num: 717\n",
      "Loss of train set: 0.4202668070793152 at epoch: 14 and batch_num: 718\n",
      "Loss of train set: 0.47698742151260376 at epoch: 14 and batch_num: 719\n",
      "Loss of train set: 0.34580355882644653 at epoch: 14 and batch_num: 720\n",
      "Loss of train set: 0.4071357846260071 at epoch: 14 and batch_num: 721\n",
      "Loss of train set: 0.3961471915245056 at epoch: 14 and batch_num: 722\n",
      "Loss of train set: 0.34757691621780396 at epoch: 14 and batch_num: 723\n",
      "Loss of train set: 0.17368946969509125 at epoch: 14 and batch_num: 724\n",
      "Loss of train set: 0.37513935565948486 at epoch: 14 and batch_num: 725\n",
      "Loss of train set: 0.30194538831710815 at epoch: 14 and batch_num: 726\n",
      "Loss of train set: 0.3415410816669464 at epoch: 14 and batch_num: 727\n",
      "Loss of train set: 0.18295139074325562 at epoch: 14 and batch_num: 728\n",
      "Loss of train set: 0.19607234001159668 at epoch: 14 and batch_num: 729\n",
      "Loss of train set: 0.20879963040351868 at epoch: 14 and batch_num: 730\n",
      "Loss of train set: 0.353518545627594 at epoch: 14 and batch_num: 731\n",
      "Loss of train set: 0.36496034264564514 at epoch: 14 and batch_num: 732\n",
      "Loss of train set: 0.23628202080726624 at epoch: 14 and batch_num: 733\n",
      "Loss of train set: 0.28930526971817017 at epoch: 14 and batch_num: 734\n",
      "Loss of train set: 0.26517704129219055 at epoch: 14 and batch_num: 735\n",
      "Loss of train set: 0.4370806813240051 at epoch: 14 and batch_num: 736\n",
      "Loss of train set: 0.2734885513782501 at epoch: 14 and batch_num: 737\n",
      "Loss of train set: 0.3221666216850281 at epoch: 14 and batch_num: 738\n",
      "Loss of train set: 0.2812771797180176 at epoch: 14 and batch_num: 739\n",
      "Loss of train set: 0.4329276382923126 at epoch: 14 and batch_num: 740\n",
      "Loss of train set: 0.2617529630661011 at epoch: 14 and batch_num: 741\n",
      "Loss of train set: 0.1353476643562317 at epoch: 14 and batch_num: 742\n",
      "Loss of train set: 0.305402934551239 at epoch: 14 and batch_num: 743\n",
      "Loss of train set: 0.3179403245449066 at epoch: 14 and batch_num: 744\n",
      "Loss of train set: 0.21558326482772827 at epoch: 14 and batch_num: 745\n",
      "Loss of train set: 0.40501880645751953 at epoch: 14 and batch_num: 746\n",
      "Loss of train set: 0.38490521907806396 at epoch: 14 and batch_num: 747\n",
      "Loss of train set: 0.28542739152908325 at epoch: 14 and batch_num: 748\n",
      "Loss of train set: 0.32445913553237915 at epoch: 14 and batch_num: 749\n",
      "Loss of train set: 0.3317020535469055 at epoch: 14 and batch_num: 750\n",
      "Loss of train set: 0.2052644044160843 at epoch: 14 and batch_num: 751\n",
      "Loss of train set: 0.3267640173435211 at epoch: 14 and batch_num: 752\n",
      "Loss of train set: 0.35248270630836487 at epoch: 14 and batch_num: 753\n",
      "Loss of train set: 0.2974511981010437 at epoch: 14 and batch_num: 754\n",
      "Loss of train set: 0.4384819269180298 at epoch: 14 and batch_num: 755\n",
      "Loss of train set: 0.15376245975494385 at epoch: 14 and batch_num: 756\n",
      "Loss of train set: 0.21403028070926666 at epoch: 14 and batch_num: 757\n",
      "Loss of train set: 0.23012375831604004 at epoch: 14 and batch_num: 758\n",
      "Loss of train set: 0.2806137800216675 at epoch: 14 and batch_num: 759\n",
      "Loss of train set: 0.23779845237731934 at epoch: 14 and batch_num: 760\n",
      "Loss of train set: 0.35055509209632874 at epoch: 14 and batch_num: 761\n",
      "Loss of train set: 0.26067501306533813 at epoch: 14 and batch_num: 762\n",
      "Loss of train set: 0.16847683489322662 at epoch: 14 and batch_num: 763\n",
      "Loss of train set: 0.1857236921787262 at epoch: 14 and batch_num: 764\n",
      "Loss of train set: 0.3425755500793457 at epoch: 14 and batch_num: 765\n",
      "Loss of train set: 0.2474927008152008 at epoch: 14 and batch_num: 766\n",
      "Loss of train set: 0.21649274230003357 at epoch: 14 and batch_num: 767\n",
      "Loss of train set: 0.2824115753173828 at epoch: 14 and batch_num: 768\n",
      "Loss of train set: 0.2930524945259094 at epoch: 14 and batch_num: 769\n",
      "Loss of train set: 0.17587679624557495 at epoch: 14 and batch_num: 770\n",
      "Loss of train set: 0.15192767977714539 at epoch: 14 and batch_num: 771\n",
      "Loss of train set: 0.25363051891326904 at epoch: 14 and batch_num: 772\n",
      "Loss of train set: 0.6128328442573547 at epoch: 14 and batch_num: 773\n",
      "Loss of train set: 0.2998301088809967 at epoch: 14 and batch_num: 774\n",
      "Loss of train set: 0.17013750970363617 at epoch: 14 and batch_num: 775\n",
      "Loss of train set: 0.19244343042373657 at epoch: 14 and batch_num: 776\n",
      "Loss of train set: 0.413424015045166 at epoch: 14 and batch_num: 777\n",
      "Loss of train set: 0.261587917804718 at epoch: 14 and batch_num: 778\n",
      "Loss of train set: 0.3044913411140442 at epoch: 14 and batch_num: 779\n",
      "Loss of train set: 0.44336801767349243 at epoch: 14 and batch_num: 780\n",
      "Loss of train set: 0.19397097826004028 at epoch: 14 and batch_num: 781\n",
      "Loss of train set: 0.3024721145629883 at epoch: 14 and batch_num: 782\n",
      "Loss of train set: 0.32637637853622437 at epoch: 14 and batch_num: 783\n",
      "Loss of train set: 0.3086419403553009 at epoch: 14 and batch_num: 784\n",
      "Loss of train set: 0.18092282116413116 at epoch: 14 and batch_num: 785\n",
      "Loss of train set: 0.43619364500045776 at epoch: 14 and batch_num: 786\n",
      "Loss of train set: 0.22669200599193573 at epoch: 14 and batch_num: 787\n",
      "Loss of train set: 0.26930487155914307 at epoch: 14 and batch_num: 788\n",
      "Loss of train set: 0.22761812806129456 at epoch: 14 and batch_num: 789\n",
      "Loss of train set: 0.2727269232273102 at epoch: 14 and batch_num: 790\n",
      "Loss of train set: 0.35472461581230164 at epoch: 14 and batch_num: 791\n",
      "Loss of train set: 0.30969417095184326 at epoch: 14 and batch_num: 792\n",
      "Loss of train set: 0.22601567208766937 at epoch: 14 and batch_num: 793\n",
      "Loss of train set: 0.2647138237953186 at epoch: 14 and batch_num: 794\n",
      "Loss of train set: 0.32571205496788025 at epoch: 14 and batch_num: 795\n",
      "Loss of train set: 0.2861965298652649 at epoch: 14 and batch_num: 796\n",
      "Loss of train set: 0.20466023683547974 at epoch: 14 and batch_num: 797\n",
      "Loss of train set: 0.11988019198179245 at epoch: 14 and batch_num: 798\n",
      "Loss of train set: 0.3331066071987152 at epoch: 14 and batch_num: 799\n",
      "Loss of train set: 0.24300624430179596 at epoch: 14 and batch_num: 800\n",
      "Loss of train set: 0.3398427367210388 at epoch: 14 and batch_num: 801\n",
      "Loss of train set: 0.34311938285827637 at epoch: 14 and batch_num: 802\n",
      "Loss of train set: 0.3399479389190674 at epoch: 14 and batch_num: 803\n",
      "Loss of train set: 0.17804652452468872 at epoch: 14 and batch_num: 804\n",
      "Loss of train set: 0.24042025208473206 at epoch: 14 and batch_num: 805\n",
      "Loss of train set: 0.3735242486000061 at epoch: 14 and batch_num: 806\n",
      "Loss of train set: 0.24650834500789642 at epoch: 14 and batch_num: 807\n",
      "Loss of train set: 0.16068415343761444 at epoch: 14 and batch_num: 808\n",
      "Loss of train set: 0.31878191232681274 at epoch: 14 and batch_num: 809\n",
      "Loss of train set: 0.3933483958244324 at epoch: 14 and batch_num: 810\n",
      "Loss of train set: 0.5721325874328613 at epoch: 14 and batch_num: 811\n",
      "Loss of train set: 0.14385484158992767 at epoch: 14 and batch_num: 812\n",
      "Loss of train set: 0.38023534417152405 at epoch: 14 and batch_num: 813\n",
      "Loss of train set: 0.22268061339855194 at epoch: 14 and batch_num: 814\n",
      "Loss of train set: 0.3946242332458496 at epoch: 14 and batch_num: 815\n",
      "Loss of train set: 0.43189066648483276 at epoch: 14 and batch_num: 816\n",
      "Loss of train set: 0.2310338318347931 at epoch: 14 and batch_num: 817\n",
      "Loss of train set: 0.23389261960983276 at epoch: 14 and batch_num: 818\n",
      "Loss of train set: 0.41288402676582336 at epoch: 14 and batch_num: 819\n",
      "Loss of train set: 0.2706788182258606 at epoch: 14 and batch_num: 820\n",
      "Loss of train set: 0.3226196765899658 at epoch: 14 and batch_num: 821\n",
      "Loss of train set: 0.499278724193573 at epoch: 14 and batch_num: 822\n",
      "Loss of train set: 0.3034387230873108 at epoch: 14 and batch_num: 823\n",
      "Loss of train set: 0.29383349418640137 at epoch: 14 and batch_num: 824\n",
      "Loss of train set: 0.33438700437545776 at epoch: 14 and batch_num: 825\n",
      "Loss of train set: 0.39071595668792725 at epoch: 14 and batch_num: 826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3118479251861572 at epoch: 14 and batch_num: 827\n",
      "Loss of train set: 0.19763144850730896 at epoch: 14 and batch_num: 828\n",
      "Loss of train set: 0.2582228183746338 at epoch: 14 and batch_num: 829\n",
      "Loss of train set: 0.2856482267379761 at epoch: 14 and batch_num: 830\n",
      "Loss of train set: 0.4809072017669678 at epoch: 14 and batch_num: 831\n",
      "Loss of train set: 0.19406522810459137 at epoch: 14 and batch_num: 832\n",
      "Loss of train set: 0.23181314766407013 at epoch: 14 and batch_num: 833\n",
      "Loss of train set: 0.4349963068962097 at epoch: 14 and batch_num: 834\n",
      "Loss of train set: 0.2838200628757477 at epoch: 14 and batch_num: 835\n",
      "Loss of train set: 0.31338635087013245 at epoch: 14 and batch_num: 836\n",
      "Loss of train set: 0.2892211079597473 at epoch: 14 and batch_num: 837\n",
      "Loss of train set: 0.3225838541984558 at epoch: 14 and batch_num: 838\n",
      "Loss of train set: 0.23309624195098877 at epoch: 14 and batch_num: 839\n",
      "Loss of train set: 0.36342525482177734 at epoch: 14 and batch_num: 840\n",
      "Loss of train set: 0.3609904944896698 at epoch: 14 and batch_num: 841\n",
      "Loss of train set: 0.2690367102622986 at epoch: 14 and batch_num: 842\n",
      "Loss of train set: 0.3048741817474365 at epoch: 14 and batch_num: 843\n",
      "Loss of train set: 0.3608613610267639 at epoch: 14 and batch_num: 844\n",
      "Loss of train set: 0.2183079719543457 at epoch: 14 and batch_num: 845\n",
      "Loss of train set: 0.1874692142009735 at epoch: 14 and batch_num: 846\n",
      "Loss of train set: 0.3731688857078552 at epoch: 14 and batch_num: 847\n",
      "Loss of train set: 0.4179888367652893 at epoch: 14 and batch_num: 848\n",
      "Loss of train set: 0.37451836466789246 at epoch: 14 and batch_num: 849\n",
      "Loss of train set: 0.352687269449234 at epoch: 14 and batch_num: 850\n",
      "Loss of train set: 0.28548726439476013 at epoch: 14 and batch_num: 851\n",
      "Loss of train set: 0.2844521403312683 at epoch: 14 and batch_num: 852\n",
      "Loss of train set: 0.26201343536376953 at epoch: 14 and batch_num: 853\n",
      "Loss of train set: 0.4472492039203644 at epoch: 14 and batch_num: 854\n",
      "Loss of train set: 0.41141024231910706 at epoch: 14 and batch_num: 855\n",
      "Loss of train set: 0.3759867548942566 at epoch: 14 and batch_num: 856\n",
      "Loss of train set: 0.2601572275161743 at epoch: 14 and batch_num: 857\n",
      "Loss of train set: 0.3717935085296631 at epoch: 14 and batch_num: 858\n",
      "Loss of train set: 0.3010881245136261 at epoch: 14 and batch_num: 859\n",
      "Loss of train set: 0.27991294860839844 at epoch: 14 and batch_num: 860\n",
      "Loss of train set: 0.38107672333717346 at epoch: 14 and batch_num: 861\n",
      "Loss of train set: 0.447365403175354 at epoch: 14 and batch_num: 862\n",
      "Loss of train set: 0.425790935754776 at epoch: 14 and batch_num: 863\n",
      "Loss of train set: 0.20805315673351288 at epoch: 14 and batch_num: 864\n",
      "Loss of train set: 0.26033860445022583 at epoch: 14 and batch_num: 865\n",
      "Loss of train set: 0.17884337902069092 at epoch: 14 and batch_num: 866\n",
      "Loss of train set: 0.17832034826278687 at epoch: 14 and batch_num: 867\n",
      "Loss of train set: 0.31321731209754944 at epoch: 14 and batch_num: 868\n",
      "Loss of train set: 0.1430225819349289 at epoch: 14 and batch_num: 869\n",
      "Loss of train set: 0.14267688989639282 at epoch: 14 and batch_num: 870\n",
      "Loss of train set: 0.30660879611968994 at epoch: 14 and batch_num: 871\n",
      "Loss of train set: 0.3110599219799042 at epoch: 14 and batch_num: 872\n",
      "Loss of train set: 0.3673933148384094 at epoch: 14 and batch_num: 873\n",
      "Loss of train set: 0.4545080065727234 at epoch: 14 and batch_num: 874\n",
      "Loss of train set: 0.252812922000885 at epoch: 14 and batch_num: 875\n",
      "Loss of train set: 0.197934091091156 at epoch: 14 and batch_num: 876\n",
      "Loss of train set: 0.4229978919029236 at epoch: 14 and batch_num: 877\n",
      "Loss of train set: 0.40405580401420593 at epoch: 14 and batch_num: 878\n",
      "Loss of train set: 0.30830150842666626 at epoch: 14 and batch_num: 879\n",
      "Loss of train set: 0.5147069096565247 at epoch: 14 and batch_num: 880\n",
      "Loss of train set: 0.3411005139350891 at epoch: 14 and batch_num: 881\n",
      "Loss of train set: 0.37683945894241333 at epoch: 14 and batch_num: 882\n",
      "Loss of train set: 0.2211863100528717 at epoch: 14 and batch_num: 883\n",
      "Loss of train set: 0.3206554055213928 at epoch: 14 and batch_num: 884\n",
      "Loss of train set: 0.255159854888916 at epoch: 14 and batch_num: 885\n",
      "Loss of train set: 0.13554635643959045 at epoch: 14 and batch_num: 886\n",
      "Loss of train set: 0.29559260606765747 at epoch: 14 and batch_num: 887\n",
      "Loss of train set: 0.3991907835006714 at epoch: 14 and batch_num: 888\n",
      "Loss of train set: 0.21933220326900482 at epoch: 14 and batch_num: 889\n",
      "Loss of train set: 0.15417294204235077 at epoch: 14 and batch_num: 890\n",
      "Loss of train set: 0.25877922773361206 at epoch: 14 and batch_num: 891\n",
      "Loss of train set: 0.3789325952529907 at epoch: 14 and batch_num: 892\n",
      "Loss of train set: 0.3569389879703522 at epoch: 14 and batch_num: 893\n",
      "Loss of train set: 0.29653313755989075 at epoch: 14 and batch_num: 894\n",
      "Loss of train set: 0.23905348777770996 at epoch: 14 and batch_num: 895\n",
      "Loss of train set: 0.2659411132335663 at epoch: 14 and batch_num: 896\n",
      "Loss of train set: 0.2588276267051697 at epoch: 14 and batch_num: 897\n",
      "Loss of train set: 0.26601895689964294 at epoch: 14 and batch_num: 898\n",
      "Loss of train set: 0.24032700061798096 at epoch: 14 and batch_num: 899\n",
      "Loss of train set: 0.3639116585254669 at epoch: 14 and batch_num: 900\n",
      "Loss of train set: 0.314023494720459 at epoch: 14 and batch_num: 901\n",
      "Loss of train set: 0.4760010242462158 at epoch: 14 and batch_num: 902\n",
      "Loss of train set: 0.28973719477653503 at epoch: 14 and batch_num: 903\n",
      "Loss of train set: 0.4043150842189789 at epoch: 14 and batch_num: 904\n",
      "Loss of train set: 0.22499704360961914 at epoch: 14 and batch_num: 905\n",
      "Loss of train set: 0.35989779233932495 at epoch: 14 and batch_num: 906\n",
      "Loss of train set: 0.14583560824394226 at epoch: 14 and batch_num: 907\n",
      "Loss of train set: 0.4273303747177124 at epoch: 14 and batch_num: 908\n",
      "Loss of train set: 0.312124639749527 at epoch: 14 and batch_num: 909\n",
      "Loss of train set: 0.23742493987083435 at epoch: 14 and batch_num: 910\n",
      "Loss of train set: 0.22875849902629852 at epoch: 14 and batch_num: 911\n",
      "Loss of train set: 0.2770870625972748 at epoch: 14 and batch_num: 912\n",
      "Loss of train set: 0.27139854431152344 at epoch: 14 and batch_num: 913\n",
      "Loss of train set: 0.3780154585838318 at epoch: 14 and batch_num: 914\n",
      "Loss of train set: 0.15883788466453552 at epoch: 14 and batch_num: 915\n",
      "Loss of train set: 0.2700241208076477 at epoch: 14 and batch_num: 916\n",
      "Loss of train set: 0.2160789668560028 at epoch: 14 and batch_num: 917\n",
      "Loss of train set: 0.3877965807914734 at epoch: 14 and batch_num: 918\n",
      "Loss of train set: 0.519966721534729 at epoch: 14 and batch_num: 919\n",
      "Loss of train set: 0.2925167679786682 at epoch: 14 and batch_num: 920\n",
      "Loss of train set: 0.4920262098312378 at epoch: 14 and batch_num: 921\n",
      "Loss of train set: 0.2791360318660736 at epoch: 14 and batch_num: 922\n",
      "Loss of train set: 0.2721574008464813 at epoch: 14 and batch_num: 923\n",
      "Loss of train set: 0.24823829531669617 at epoch: 14 and batch_num: 924\n",
      "Loss of train set: 0.33522918820381165 at epoch: 14 and batch_num: 925\n",
      "Loss of train set: 0.20609253644943237 at epoch: 14 and batch_num: 926\n",
      "Loss of train set: 0.31012386083602905 at epoch: 14 and batch_num: 927\n",
      "Loss of train set: 0.31229040026664734 at epoch: 14 and batch_num: 928\n",
      "Loss of train set: 0.21165752410888672 at epoch: 14 and batch_num: 929\n",
      "Loss of train set: 0.12319649755954742 at epoch: 14 and batch_num: 930\n",
      "Loss of train set: 0.29365700483322144 at epoch: 14 and batch_num: 931\n",
      "Loss of train set: 0.27652013301849365 at epoch: 14 and batch_num: 932\n",
      "Loss of train set: 0.2770808935165405 at epoch: 14 and batch_num: 933\n",
      "Loss of train set: 0.418983519077301 at epoch: 14 and batch_num: 934\n",
      "Loss of train set: 0.19994445145130157 at epoch: 14 and batch_num: 935\n",
      "Loss of train set: 0.21602264046669006 at epoch: 14 and batch_num: 936\n",
      "Loss of train set: 0.316974401473999 at epoch: 14 and batch_num: 937\n",
      "Accuracy of train set: 0.8912666666666667\n",
      "Loss of test set: 0.2776203751564026 at epoch: 14 and batch_num: 0\n",
      "Loss of test set: 0.3378654420375824 at epoch: 14 and batch_num: 1\n",
      "Loss of test set: 0.3733433485031128 at epoch: 14 and batch_num: 2\n",
      "Loss of test set: 0.363583505153656 at epoch: 14 and batch_num: 3\n",
      "Loss of test set: 0.31481480598449707 at epoch: 14 and batch_num: 4\n",
      "Loss of test set: 0.3972359001636505 at epoch: 14 and batch_num: 5\n",
      "Loss of test set: 0.41072243452072144 at epoch: 14 and batch_num: 6\n",
      "Loss of test set: 0.4836840331554413 at epoch: 14 and batch_num: 7\n",
      "Loss of test set: 0.26886266469955444 at epoch: 14 and batch_num: 8\n",
      "Loss of test set: 0.27035751938819885 at epoch: 14 and batch_num: 9\n",
      "Loss of test set: 0.3440936803817749 at epoch: 14 and batch_num: 10\n",
      "Loss of test set: 0.3099835515022278 at epoch: 14 and batch_num: 11\n",
      "Loss of test set: 0.37540552020072937 at epoch: 14 and batch_num: 12\n",
      "Loss of test set: 0.33153611421585083 at epoch: 14 and batch_num: 13\n",
      "Loss of test set: 0.5064295530319214 at epoch: 14 and batch_num: 14\n",
      "Loss of test set: 0.2718827724456787 at epoch: 14 and batch_num: 15\n",
      "Loss of test set: 0.3632381558418274 at epoch: 14 and batch_num: 16\n",
      "Loss of test set: 0.38659757375717163 at epoch: 14 and batch_num: 17\n",
      "Loss of test set: 0.30019766092300415 at epoch: 14 and batch_num: 18\n",
      "Loss of test set: 0.3801613450050354 at epoch: 14 and batch_num: 19\n",
      "Loss of test set: 0.5873907804489136 at epoch: 14 and batch_num: 20\n",
      "Loss of test set: 0.255582720041275 at epoch: 14 and batch_num: 21\n",
      "Loss of test set: 0.36573460698127747 at epoch: 14 and batch_num: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.2014232873916626 at epoch: 14 and batch_num: 23\n",
      "Loss of test set: 0.419854998588562 at epoch: 14 and batch_num: 24\n",
      "Loss of test set: 0.42224717140197754 at epoch: 14 and batch_num: 25\n",
      "Loss of test set: 0.3338714838027954 at epoch: 14 and batch_num: 26\n",
      "Loss of test set: 0.4575802683830261 at epoch: 14 and batch_num: 27\n",
      "Loss of test set: 0.4342685341835022 at epoch: 14 and batch_num: 28\n",
      "Loss of test set: 0.29818761348724365 at epoch: 14 and batch_num: 29\n",
      "Loss of test set: 0.34069547057151794 at epoch: 14 and batch_num: 30\n",
      "Loss of test set: 0.3427203595638275 at epoch: 14 and batch_num: 31\n",
      "Loss of test set: 0.3596981167793274 at epoch: 14 and batch_num: 32\n",
      "Loss of test set: 0.5257009267807007 at epoch: 14 and batch_num: 33\n",
      "Loss of test set: 0.2541721761226654 at epoch: 14 and batch_num: 34\n",
      "Loss of test set: 0.44049152731895447 at epoch: 14 and batch_num: 35\n",
      "Loss of test set: 0.6171755790710449 at epoch: 14 and batch_num: 36\n",
      "Loss of test set: 0.409992516040802 at epoch: 14 and batch_num: 37\n",
      "Loss of test set: 0.34551435708999634 at epoch: 14 and batch_num: 38\n",
      "Loss of test set: 0.23731417953968048 at epoch: 14 and batch_num: 39\n",
      "Loss of test set: 0.24560904502868652 at epoch: 14 and batch_num: 40\n",
      "Loss of test set: 0.5889714956283569 at epoch: 14 and batch_num: 41\n",
      "Loss of test set: 0.32095974683761597 at epoch: 14 and batch_num: 42\n",
      "Loss of test set: 0.49763596057891846 at epoch: 14 and batch_num: 43\n",
      "Loss of test set: 0.46783116459846497 at epoch: 14 and batch_num: 44\n",
      "Loss of test set: 0.4665214717388153 at epoch: 14 and batch_num: 45\n",
      "Loss of test set: 0.36024343967437744 at epoch: 14 and batch_num: 46\n",
      "Loss of test set: 0.18907862901687622 at epoch: 14 and batch_num: 47\n",
      "Loss of test set: 0.2643156349658966 at epoch: 14 and batch_num: 48\n",
      "Loss of test set: 0.3541397452354431 at epoch: 14 and batch_num: 49\n",
      "Loss of test set: 0.3499218821525574 at epoch: 14 and batch_num: 50\n",
      "Loss of test set: 0.6660169959068298 at epoch: 14 and batch_num: 51\n",
      "Loss of test set: 0.3541979491710663 at epoch: 14 and batch_num: 52\n",
      "Loss of test set: 0.530370831489563 at epoch: 14 and batch_num: 53\n",
      "Loss of test set: 0.44397205114364624 at epoch: 14 and batch_num: 54\n",
      "Loss of test set: 0.4062548279762268 at epoch: 14 and batch_num: 55\n",
      "Loss of test set: 0.3882905840873718 at epoch: 14 and batch_num: 56\n",
      "Loss of test set: 0.26859089732170105 at epoch: 14 and batch_num: 57\n",
      "Loss of test set: 0.2295779585838318 at epoch: 14 and batch_num: 58\n",
      "Loss of test set: 0.21810171008110046 at epoch: 14 and batch_num: 59\n",
      "Loss of test set: 0.29370933771133423 at epoch: 14 and batch_num: 60\n",
      "Loss of test set: 0.4416329860687256 at epoch: 14 and batch_num: 61\n",
      "Loss of test set: 0.2575959861278534 at epoch: 14 and batch_num: 62\n",
      "Loss of test set: 0.2433701604604721 at epoch: 14 and batch_num: 63\n",
      "Loss of test set: 0.24842816591262817 at epoch: 14 and batch_num: 64\n",
      "Loss of test set: 0.5116332769393921 at epoch: 14 and batch_num: 65\n",
      "Loss of test set: 0.30659055709838867 at epoch: 14 and batch_num: 66\n",
      "Loss of test set: 0.39422744512557983 at epoch: 14 and batch_num: 67\n",
      "Loss of test set: 0.26460468769073486 at epoch: 14 and batch_num: 68\n",
      "Loss of test set: 0.4445791244506836 at epoch: 14 and batch_num: 69\n",
      "Loss of test set: 0.494811087846756 at epoch: 14 and batch_num: 70\n",
      "Loss of test set: 0.29907843470573425 at epoch: 14 and batch_num: 71\n",
      "Loss of test set: 0.2674447298049927 at epoch: 14 and batch_num: 72\n",
      "Loss of test set: 0.32115399837493896 at epoch: 14 and batch_num: 73\n",
      "Loss of test set: 0.29621315002441406 at epoch: 14 and batch_num: 74\n",
      "Loss of test set: 0.5347518920898438 at epoch: 14 and batch_num: 75\n",
      "Loss of test set: 0.23730570077896118 at epoch: 14 and batch_num: 76\n",
      "Loss of test set: 0.27205759286880493 at epoch: 14 and batch_num: 77\n",
      "Loss of test set: 0.463131844997406 at epoch: 14 and batch_num: 78\n",
      "Loss of test set: 0.47352761030197144 at epoch: 14 and batch_num: 79\n",
      "Loss of test set: 0.26660221815109253 at epoch: 14 and batch_num: 80\n",
      "Loss of test set: 0.2736338675022125 at epoch: 14 and batch_num: 81\n",
      "Loss of test set: 0.6199305653572083 at epoch: 14 and batch_num: 82\n",
      "Loss of test set: 0.501380205154419 at epoch: 14 and batch_num: 83\n",
      "Loss of test set: 0.3977394998073578 at epoch: 14 and batch_num: 84\n",
      "Loss of test set: 0.1840834617614746 at epoch: 14 and batch_num: 85\n",
      "Loss of test set: 0.2914963960647583 at epoch: 14 and batch_num: 86\n",
      "Loss of test set: 0.5514183044433594 at epoch: 14 and batch_num: 87\n",
      "Loss of test set: 0.3652665615081787 at epoch: 14 and batch_num: 88\n",
      "Loss of test set: 0.17558807134628296 at epoch: 14 and batch_num: 89\n",
      "Loss of test set: 0.27307790517807007 at epoch: 14 and batch_num: 90\n",
      "Loss of test set: 0.48103243112564087 at epoch: 14 and batch_num: 91\n",
      "Loss of test set: 0.32814669609069824 at epoch: 14 and batch_num: 92\n",
      "Loss of test set: 0.31417572498321533 at epoch: 14 and batch_num: 93\n",
      "Loss of test set: 0.40935781598091125 at epoch: 14 and batch_num: 94\n",
      "Loss of test set: 0.39192909002304077 at epoch: 14 and batch_num: 95\n",
      "Loss of test set: 0.262382447719574 at epoch: 14 and batch_num: 96\n",
      "Loss of test set: 0.4296625852584839 at epoch: 14 and batch_num: 97\n",
      "Loss of test set: 0.3145909607410431 at epoch: 14 and batch_num: 98\n",
      "Loss of test set: 0.3891443610191345 at epoch: 14 and batch_num: 99\n",
      "Loss of test set: 0.330314040184021 at epoch: 14 and batch_num: 100\n",
      "Loss of test set: 0.3779018521308899 at epoch: 14 and batch_num: 101\n",
      "Loss of test set: 0.31387805938720703 at epoch: 14 and batch_num: 102\n",
      "Loss of test set: 0.47065287828445435 at epoch: 14 and batch_num: 103\n",
      "Loss of test set: 0.3207612633705139 at epoch: 14 and batch_num: 104\n",
      "Loss of test set: 0.5796142816543579 at epoch: 14 and batch_num: 105\n",
      "Loss of test set: 0.5112420916557312 at epoch: 14 and batch_num: 106\n",
      "Loss of test set: 0.43002206087112427 at epoch: 14 and batch_num: 107\n",
      "Loss of test set: 0.31072789430618286 at epoch: 14 and batch_num: 108\n",
      "Loss of test set: 0.3505111634731293 at epoch: 14 and batch_num: 109\n",
      "Loss of test set: 0.19165274500846863 at epoch: 14 and batch_num: 110\n",
      "Loss of test set: 0.419005423784256 at epoch: 14 and batch_num: 111\n",
      "Loss of test set: 0.3420354127883911 at epoch: 14 and batch_num: 112\n",
      "Loss of test set: 0.17309905588626862 at epoch: 14 and batch_num: 113\n",
      "Loss of test set: 0.3005051016807556 at epoch: 14 and batch_num: 114\n",
      "Loss of test set: 0.3405340909957886 at epoch: 14 and batch_num: 115\n",
      "Loss of test set: 0.28892356157302856 at epoch: 14 and batch_num: 116\n",
      "Loss of test set: 0.3439762592315674 at epoch: 14 and batch_num: 117\n",
      "Loss of test set: 0.4762335419654846 at epoch: 14 and batch_num: 118\n",
      "Loss of test set: 0.3227747678756714 at epoch: 14 and batch_num: 119\n",
      "Loss of test set: 0.3389700651168823 at epoch: 14 and batch_num: 120\n",
      "Loss of test set: 0.4143093228340149 at epoch: 14 and batch_num: 121\n",
      "Loss of test set: 0.3635266423225403 at epoch: 14 and batch_num: 122\n",
      "Loss of test set: 0.26356878876686096 at epoch: 14 and batch_num: 123\n",
      "Loss of test set: 0.28144314885139465 at epoch: 14 and batch_num: 124\n",
      "Loss of test set: 0.45894649624824524 at epoch: 14 and batch_num: 125\n",
      "Loss of test set: 0.18167901039123535 at epoch: 14 and batch_num: 126\n",
      "Loss of test set: 0.41103512048721313 at epoch: 14 and batch_num: 127\n",
      "Loss of test set: 0.4579434096813202 at epoch: 14 and batch_num: 128\n",
      "Loss of test set: 0.7134084701538086 at epoch: 14 and batch_num: 129\n",
      "Loss of test set: 0.36088114976882935 at epoch: 14 and batch_num: 130\n",
      "Loss of test set: 0.3577699065208435 at epoch: 14 and batch_num: 131\n",
      "Loss of test set: 0.39104539155960083 at epoch: 14 and batch_num: 132\n",
      "Loss of test set: 0.564700722694397 at epoch: 14 and batch_num: 133\n",
      "Loss of test set: 0.34289470314979553 at epoch: 14 and batch_num: 134\n",
      "Loss of test set: 0.5656553506851196 at epoch: 14 and batch_num: 135\n",
      "Loss of test set: 0.4497472941875458 at epoch: 14 and batch_num: 136\n",
      "Loss of test set: 0.3359169661998749 at epoch: 14 and batch_num: 137\n",
      "Loss of test set: 0.3933393359184265 at epoch: 14 and batch_num: 138\n",
      "Loss of test set: 0.3300350308418274 at epoch: 14 and batch_num: 139\n",
      "Loss of test set: 0.42165979743003845 at epoch: 14 and batch_num: 140\n",
      "Loss of test set: 0.4412349462509155 at epoch: 14 and batch_num: 141\n",
      "Loss of test set: 0.399023175239563 at epoch: 14 and batch_num: 142\n",
      "Loss of test set: 0.5623217821121216 at epoch: 14 and batch_num: 143\n",
      "Loss of test set: 0.6291804909706116 at epoch: 14 and batch_num: 144\n",
      "Loss of test set: 0.2792547643184662 at epoch: 14 and batch_num: 145\n",
      "Loss of test set: 0.22543644905090332 at epoch: 14 and batch_num: 146\n",
      "Loss of test set: 0.5135657787322998 at epoch: 14 and batch_num: 147\n",
      "Loss of test set: 0.2785053849220276 at epoch: 14 and batch_num: 148\n",
      "Loss of test set: 0.18462416529655457 at epoch: 14 and batch_num: 149\n",
      "Loss of test set: 0.2518576979637146 at epoch: 14 and batch_num: 150\n",
      "Loss of test set: 0.18860219419002533 at epoch: 14 and batch_num: 151\n",
      "Loss of test set: 0.3407038450241089 at epoch: 14 and batch_num: 152\n",
      "Loss of test set: 0.36727774143218994 at epoch: 14 and batch_num: 153\n",
      "Loss of test set: 0.2988973557949066 at epoch: 14 and batch_num: 154\n",
      "Loss of test set: 0.2984611392021179 at epoch: 14 and batch_num: 155\n",
      "Loss of test set: 0.26619893312454224 at epoch: 14 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8675\n",
      "Loss of train set: 0.35011184215545654 at epoch: 15 and batch_num: 0\n",
      "Loss of train set: 0.25450795888900757 at epoch: 15 and batch_num: 1\n",
      "Loss of train set: 0.1961098462343216 at epoch: 15 and batch_num: 2\n",
      "Loss of train set: 0.3280758857727051 at epoch: 15 and batch_num: 3\n",
      "Loss of train set: 0.22089830040931702 at epoch: 15 and batch_num: 4\n",
      "Loss of train set: 0.34156346321105957 at epoch: 15 and batch_num: 5\n",
      "Loss of train set: 0.3836376667022705 at epoch: 15 and batch_num: 6\n",
      "Loss of train set: 0.25802117586135864 at epoch: 15 and batch_num: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2535553574562073 at epoch: 15 and batch_num: 8\n",
      "Loss of train set: 0.3278690278530121 at epoch: 15 and batch_num: 9\n",
      "Loss of train set: 0.3298964202404022 at epoch: 15 and batch_num: 10\n",
      "Loss of train set: 0.403428316116333 at epoch: 15 and batch_num: 11\n",
      "Loss of train set: 0.3975702226161957 at epoch: 15 and batch_num: 12\n",
      "Loss of train set: 0.13714979588985443 at epoch: 15 and batch_num: 13\n",
      "Loss of train set: 0.2458823174238205 at epoch: 15 and batch_num: 14\n",
      "Loss of train set: 0.45602840185165405 at epoch: 15 and batch_num: 15\n",
      "Loss of train set: 0.4768918752670288 at epoch: 15 and batch_num: 16\n",
      "Loss of train set: 0.342365026473999 at epoch: 15 and batch_num: 17\n",
      "Loss of train set: 0.31652194261550903 at epoch: 15 and batch_num: 18\n",
      "Loss of train set: 0.30646491050720215 at epoch: 15 and batch_num: 19\n",
      "Loss of train set: 0.34246551990509033 at epoch: 15 and batch_num: 20\n",
      "Loss of train set: 0.4176771938800812 at epoch: 15 and batch_num: 21\n",
      "Loss of train set: 0.25881537795066833 at epoch: 15 and batch_num: 22\n",
      "Loss of train set: 0.26336759328842163 at epoch: 15 and batch_num: 23\n",
      "Loss of train set: 0.3989005386829376 at epoch: 15 and batch_num: 24\n",
      "Loss of train set: 0.24912890791893005 at epoch: 15 and batch_num: 25\n",
      "Loss of train set: 0.3521931767463684 at epoch: 15 and batch_num: 26\n",
      "Loss of train set: 0.24110707640647888 at epoch: 15 and batch_num: 27\n",
      "Loss of train set: 0.30301767587661743 at epoch: 15 and batch_num: 28\n",
      "Loss of train set: 0.34459948539733887 at epoch: 15 and batch_num: 29\n",
      "Loss of train set: 0.2809751033782959 at epoch: 15 and batch_num: 30\n",
      "Loss of train set: 0.2713233232498169 at epoch: 15 and batch_num: 31\n",
      "Loss of train set: 0.22403639554977417 at epoch: 15 and batch_num: 32\n",
      "Loss of train set: 0.2724495530128479 at epoch: 15 and batch_num: 33\n",
      "Loss of train set: 0.43845462799072266 at epoch: 15 and batch_num: 34\n",
      "Loss of train set: 0.22775422036647797 at epoch: 15 and batch_num: 35\n",
      "Loss of train set: 0.23473036289215088 at epoch: 15 and batch_num: 36\n",
      "Loss of train set: 0.22490164637565613 at epoch: 15 and batch_num: 37\n",
      "Loss of train set: 0.2750670313835144 at epoch: 15 and batch_num: 38\n",
      "Loss of train set: 0.335247278213501 at epoch: 15 and batch_num: 39\n",
      "Loss of train set: 0.2311391532421112 at epoch: 15 and batch_num: 40\n",
      "Loss of train set: 0.4459991753101349 at epoch: 15 and batch_num: 41\n",
      "Loss of train set: 0.38724714517593384 at epoch: 15 and batch_num: 42\n",
      "Loss of train set: 0.27935343980789185 at epoch: 15 and batch_num: 43\n",
      "Loss of train set: 0.13156141340732574 at epoch: 15 and batch_num: 44\n",
      "Loss of train set: 0.30192330479621887 at epoch: 15 and batch_num: 45\n",
      "Loss of train set: 0.4195564389228821 at epoch: 15 and batch_num: 46\n",
      "Loss of train set: 0.20268514752388 at epoch: 15 and batch_num: 47\n",
      "Loss of train set: 0.23264603316783905 at epoch: 15 and batch_num: 48\n",
      "Loss of train set: 0.3739500939846039 at epoch: 15 and batch_num: 49\n",
      "Loss of train set: 0.3550185561180115 at epoch: 15 and batch_num: 50\n",
      "Loss of train set: 0.28464213013648987 at epoch: 15 and batch_num: 51\n",
      "Loss of train set: 0.3476399779319763 at epoch: 15 and batch_num: 52\n",
      "Loss of train set: 0.2561381757259369 at epoch: 15 and batch_num: 53\n",
      "Loss of train set: 0.3160398602485657 at epoch: 15 and batch_num: 54\n",
      "Loss of train set: 0.43489158153533936 at epoch: 15 and batch_num: 55\n",
      "Loss of train set: 0.26429373025894165 at epoch: 15 and batch_num: 56\n",
      "Loss of train set: 0.30617979168891907 at epoch: 15 and batch_num: 57\n",
      "Loss of train set: 0.31120023131370544 at epoch: 15 and batch_num: 58\n",
      "Loss of train set: 0.3930633068084717 at epoch: 15 and batch_num: 59\n",
      "Loss of train set: 0.2326265126466751 at epoch: 15 and batch_num: 60\n",
      "Loss of train set: 0.20428182184696198 at epoch: 15 and batch_num: 61\n",
      "Loss of train set: 0.19883844256401062 at epoch: 15 and batch_num: 62\n",
      "Loss of train set: 0.4668843746185303 at epoch: 15 and batch_num: 63\n",
      "Loss of train set: 0.2727501094341278 at epoch: 15 and batch_num: 64\n",
      "Loss of train set: 0.2804132103919983 at epoch: 15 and batch_num: 65\n",
      "Loss of train set: 0.47740399837493896 at epoch: 15 and batch_num: 66\n",
      "Loss of train set: 0.3842625021934509 at epoch: 15 and batch_num: 67\n",
      "Loss of train set: 0.2525128424167633 at epoch: 15 and batch_num: 68\n",
      "Loss of train set: 0.3151550590991974 at epoch: 15 and batch_num: 69\n",
      "Loss of train set: 0.3120804727077484 at epoch: 15 and batch_num: 70\n",
      "Loss of train set: 0.31441906094551086 at epoch: 15 and batch_num: 71\n",
      "Loss of train set: 0.2677130699157715 at epoch: 15 and batch_num: 72\n",
      "Loss of train set: 0.2915598750114441 at epoch: 15 and batch_num: 73\n",
      "Loss of train set: 0.40244194865226746 at epoch: 15 and batch_num: 74\n",
      "Loss of train set: 0.27467572689056396 at epoch: 15 and batch_num: 75\n",
      "Loss of train set: 0.2920496463775635 at epoch: 15 and batch_num: 76\n",
      "Loss of train set: 0.29435622692108154 at epoch: 15 and batch_num: 77\n",
      "Loss of train set: 0.28701770305633545 at epoch: 15 and batch_num: 78\n",
      "Loss of train set: 0.23830747604370117 at epoch: 15 and batch_num: 79\n",
      "Loss of train set: 0.3113173842430115 at epoch: 15 and batch_num: 80\n",
      "Loss of train set: 0.13357862830162048 at epoch: 15 and batch_num: 81\n",
      "Loss of train set: 0.4986729323863983 at epoch: 15 and batch_num: 82\n",
      "Loss of train set: 0.3471304178237915 at epoch: 15 and batch_num: 83\n",
      "Loss of train set: 0.3762201964855194 at epoch: 15 and batch_num: 84\n",
      "Loss of train set: 0.3228236436843872 at epoch: 15 and batch_num: 85\n",
      "Loss of train set: 0.3886615037918091 at epoch: 15 and batch_num: 86\n",
      "Loss of train set: 0.308219850063324 at epoch: 15 and batch_num: 87\n",
      "Loss of train set: 0.2977162003517151 at epoch: 15 and batch_num: 88\n",
      "Loss of train set: 0.4042550325393677 at epoch: 15 and batch_num: 89\n",
      "Loss of train set: 0.43263399600982666 at epoch: 15 and batch_num: 90\n",
      "Loss of train set: 0.2727799415588379 at epoch: 15 and batch_num: 91\n",
      "Loss of train set: 0.2342662215232849 at epoch: 15 and batch_num: 92\n",
      "Loss of train set: 0.35175052285194397 at epoch: 15 and batch_num: 93\n",
      "Loss of train set: 0.2563348710536957 at epoch: 15 and batch_num: 94\n",
      "Loss of train set: 0.4128809869289398 at epoch: 15 and batch_num: 95\n",
      "Loss of train set: 0.3025969862937927 at epoch: 15 and batch_num: 96\n",
      "Loss of train set: 0.2582223117351532 at epoch: 15 and batch_num: 97\n",
      "Loss of train set: 0.46600571274757385 at epoch: 15 and batch_num: 98\n",
      "Loss of train set: 0.3173251152038574 at epoch: 15 and batch_num: 99\n",
      "Loss of train set: 0.3803901672363281 at epoch: 15 and batch_num: 100\n",
      "Loss of train set: 0.2460566759109497 at epoch: 15 and batch_num: 101\n",
      "Loss of train set: 0.4515115022659302 at epoch: 15 and batch_num: 102\n",
      "Loss of train set: 0.24874648451805115 at epoch: 15 and batch_num: 103\n",
      "Loss of train set: 0.36604952812194824 at epoch: 15 and batch_num: 104\n",
      "Loss of train set: 0.2533729672431946 at epoch: 15 and batch_num: 105\n",
      "Loss of train set: 0.19225597381591797 at epoch: 15 and batch_num: 106\n",
      "Loss of train set: 0.4969939887523651 at epoch: 15 and batch_num: 107\n",
      "Loss of train set: 0.17907953262329102 at epoch: 15 and batch_num: 108\n",
      "Loss of train set: 0.39013221859931946 at epoch: 15 and batch_num: 109\n",
      "Loss of train set: 0.497262179851532 at epoch: 15 and batch_num: 110\n",
      "Loss of train set: 0.36452943086624146 at epoch: 15 and batch_num: 111\n",
      "Loss of train set: 0.347836434841156 at epoch: 15 and batch_num: 112\n",
      "Loss of train set: 0.17439508438110352 at epoch: 15 and batch_num: 113\n",
      "Loss of train set: 0.2479967176914215 at epoch: 15 and batch_num: 114\n",
      "Loss of train set: 0.28377193212509155 at epoch: 15 and batch_num: 115\n",
      "Loss of train set: 0.28841733932495117 at epoch: 15 and batch_num: 116\n",
      "Loss of train set: 0.2978883981704712 at epoch: 15 and batch_num: 117\n",
      "Loss of train set: 0.26052892208099365 at epoch: 15 and batch_num: 118\n",
      "Loss of train set: 0.39445406198501587 at epoch: 15 and batch_num: 119\n",
      "Loss of train set: 0.19838765263557434 at epoch: 15 and batch_num: 120\n",
      "Loss of train set: 0.2579054534435272 at epoch: 15 and batch_num: 121\n",
      "Loss of train set: 0.37820178270339966 at epoch: 15 and batch_num: 122\n",
      "Loss of train set: 0.38875818252563477 at epoch: 15 and batch_num: 123\n",
      "Loss of train set: 0.18624064326286316 at epoch: 15 and batch_num: 124\n",
      "Loss of train set: 0.14865463972091675 at epoch: 15 and batch_num: 125\n",
      "Loss of train set: 0.28308817744255066 at epoch: 15 and batch_num: 126\n",
      "Loss of train set: 0.23090939223766327 at epoch: 15 and batch_num: 127\n",
      "Loss of train set: 0.21933169662952423 at epoch: 15 and batch_num: 128\n",
      "Loss of train set: 0.274170845746994 at epoch: 15 and batch_num: 129\n",
      "Loss of train set: 0.18018648028373718 at epoch: 15 and batch_num: 130\n",
      "Loss of train set: 0.24844951927661896 at epoch: 15 and batch_num: 131\n",
      "Loss of train set: 0.313641220331192 at epoch: 15 and batch_num: 132\n",
      "Loss of train set: 0.3594091832637787 at epoch: 15 and batch_num: 133\n",
      "Loss of train set: 0.14421847462654114 at epoch: 15 and batch_num: 134\n",
      "Loss of train set: 0.26195287704467773 at epoch: 15 and batch_num: 135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3044958710670471 at epoch: 15 and batch_num: 136\n",
      "Loss of train set: 0.21206516027450562 at epoch: 15 and batch_num: 137\n",
      "Loss of train set: 0.2562563121318817 at epoch: 15 and batch_num: 138\n",
      "Loss of train set: 0.2949553430080414 at epoch: 15 and batch_num: 139\n",
      "Loss of train set: 0.1968621462583542 at epoch: 15 and batch_num: 140\n",
      "Loss of train set: 0.37712228298187256 at epoch: 15 and batch_num: 141\n",
      "Loss of train set: 0.36625176668167114 at epoch: 15 and batch_num: 142\n",
      "Loss of train set: 0.30465078353881836 at epoch: 15 and batch_num: 143\n",
      "Loss of train set: 0.2850344181060791 at epoch: 15 and batch_num: 144\n",
      "Loss of train set: 0.27925851941108704 at epoch: 15 and batch_num: 145\n",
      "Loss of train set: 0.30438363552093506 at epoch: 15 and batch_num: 146\n",
      "Loss of train set: 0.41241198778152466 at epoch: 15 and batch_num: 147\n",
      "Loss of train set: 0.412569135427475 at epoch: 15 and batch_num: 148\n",
      "Loss of train set: 0.619056224822998 at epoch: 15 and batch_num: 149\n",
      "Loss of train set: 0.23076310753822327 at epoch: 15 and batch_num: 150\n",
      "Loss of train set: 0.3095483183860779 at epoch: 15 and batch_num: 151\n",
      "Loss of train set: 0.23748533427715302 at epoch: 15 and batch_num: 152\n",
      "Loss of train set: 0.2836832106113434 at epoch: 15 and batch_num: 153\n",
      "Loss of train set: 0.379756361246109 at epoch: 15 and batch_num: 154\n",
      "Loss of train set: 0.29097288846969604 at epoch: 15 and batch_num: 155\n",
      "Loss of train set: 0.5428029894828796 at epoch: 15 and batch_num: 156\n",
      "Loss of train set: 0.19777947664260864 at epoch: 15 and batch_num: 157\n",
      "Loss of train set: 0.34159067273139954 at epoch: 15 and batch_num: 158\n",
      "Loss of train set: 0.27929824590682983 at epoch: 15 and batch_num: 159\n",
      "Loss of train set: 0.22663451731204987 at epoch: 15 and batch_num: 160\n",
      "Loss of train set: 0.3800269365310669 at epoch: 15 and batch_num: 161\n",
      "Loss of train set: 0.37138283252716064 at epoch: 15 and batch_num: 162\n",
      "Loss of train set: 0.35945773124694824 at epoch: 15 and batch_num: 163\n",
      "Loss of train set: 0.24047699570655823 at epoch: 15 and batch_num: 164\n",
      "Loss of train set: 0.15876668691635132 at epoch: 15 and batch_num: 165\n",
      "Loss of train set: 0.4294182360172272 at epoch: 15 and batch_num: 166\n",
      "Loss of train set: 0.26560819149017334 at epoch: 15 and batch_num: 167\n",
      "Loss of train set: 0.3954545855522156 at epoch: 15 and batch_num: 168\n",
      "Loss of train set: 0.3356892168521881 at epoch: 15 and batch_num: 169\n",
      "Loss of train set: 0.25569698214530945 at epoch: 15 and batch_num: 170\n",
      "Loss of train set: 0.23569801449775696 at epoch: 15 and batch_num: 171\n",
      "Loss of train set: 0.21617566049098969 at epoch: 15 and batch_num: 172\n",
      "Loss of train set: 0.30143189430236816 at epoch: 15 and batch_num: 173\n",
      "Loss of train set: 0.2101101577281952 at epoch: 15 and batch_num: 174\n",
      "Loss of train set: 0.1827509105205536 at epoch: 15 and batch_num: 175\n",
      "Loss of train set: 0.32843348383903503 at epoch: 15 and batch_num: 176\n",
      "Loss of train set: 0.3158113956451416 at epoch: 15 and batch_num: 177\n",
      "Loss of train set: 0.29024505615234375 at epoch: 15 and batch_num: 178\n",
      "Loss of train set: 0.2670341730117798 at epoch: 15 and batch_num: 179\n",
      "Loss of train set: 0.3354753851890564 at epoch: 15 and batch_num: 180\n",
      "Loss of train set: 0.4945206642150879 at epoch: 15 and batch_num: 181\n",
      "Loss of train set: 0.43470925092697144 at epoch: 15 and batch_num: 182\n",
      "Loss of train set: 0.1868884265422821 at epoch: 15 and batch_num: 183\n",
      "Loss of train set: 0.22116462886333466 at epoch: 15 and batch_num: 184\n",
      "Loss of train set: 0.22927641868591309 at epoch: 15 and batch_num: 185\n",
      "Loss of train set: 0.34104305505752563 at epoch: 15 and batch_num: 186\n",
      "Loss of train set: 0.2646196484565735 at epoch: 15 and batch_num: 187\n",
      "Loss of train set: 0.19044536352157593 at epoch: 15 and batch_num: 188\n",
      "Loss of train set: 0.26042643189430237 at epoch: 15 and batch_num: 189\n",
      "Loss of train set: 0.3857806921005249 at epoch: 15 and batch_num: 190\n",
      "Loss of train set: 0.20323126018047333 at epoch: 15 and batch_num: 191\n",
      "Loss of train set: 0.31690841913223267 at epoch: 15 and batch_num: 192\n",
      "Loss of train set: 0.45224878191947937 at epoch: 15 and batch_num: 193\n",
      "Loss of train set: 0.20732726156711578 at epoch: 15 and batch_num: 194\n",
      "Loss of train set: 0.27625709772109985 at epoch: 15 and batch_num: 195\n",
      "Loss of train set: 0.40791264176368713 at epoch: 15 and batch_num: 196\n",
      "Loss of train set: 0.1683848351240158 at epoch: 15 and batch_num: 197\n",
      "Loss of train set: 0.22863703966140747 at epoch: 15 and batch_num: 198\n",
      "Loss of train set: 0.31580427289009094 at epoch: 15 and batch_num: 199\n",
      "Loss of train set: 0.3718128204345703 at epoch: 15 and batch_num: 200\n",
      "Loss of train set: 0.3570997714996338 at epoch: 15 and batch_num: 201\n",
      "Loss of train set: 0.26723045110702515 at epoch: 15 and batch_num: 202\n",
      "Loss of train set: 0.34494394063949585 at epoch: 15 and batch_num: 203\n",
      "Loss of train set: 0.20384655892848969 at epoch: 15 and batch_num: 204\n",
      "Loss of train set: 0.21702983975410461 at epoch: 15 and batch_num: 205\n",
      "Loss of train set: 0.4193190336227417 at epoch: 15 and batch_num: 206\n",
      "Loss of train set: 0.2655088007450104 at epoch: 15 and batch_num: 207\n",
      "Loss of train set: 0.29237666726112366 at epoch: 15 and batch_num: 208\n",
      "Loss of train set: 0.3976094722747803 at epoch: 15 and batch_num: 209\n",
      "Loss of train set: 0.3630611300468445 at epoch: 15 and batch_num: 210\n",
      "Loss of train set: 0.3663782477378845 at epoch: 15 and batch_num: 211\n",
      "Loss of train set: 0.47947537899017334 at epoch: 15 and batch_num: 212\n",
      "Loss of train set: 0.36938127875328064 at epoch: 15 and batch_num: 213\n",
      "Loss of train set: 0.37538647651672363 at epoch: 15 and batch_num: 214\n",
      "Loss of train set: 0.3613913655281067 at epoch: 15 and batch_num: 215\n",
      "Loss of train set: 0.32790887355804443 at epoch: 15 and batch_num: 216\n",
      "Loss of train set: 0.24493369460105896 at epoch: 15 and batch_num: 217\n",
      "Loss of train set: 0.26060566306114197 at epoch: 15 and batch_num: 218\n",
      "Loss of train set: 0.32896947860717773 at epoch: 15 and batch_num: 219\n",
      "Loss of train set: 0.37481915950775146 at epoch: 15 and batch_num: 220\n",
      "Loss of train set: 0.22793301939964294 at epoch: 15 and batch_num: 221\n",
      "Loss of train set: 0.3305356800556183 at epoch: 15 and batch_num: 222\n",
      "Loss of train set: 0.33451104164123535 at epoch: 15 and batch_num: 223\n",
      "Loss of train set: 0.33607620000839233 at epoch: 15 and batch_num: 224\n",
      "Loss of train set: 0.26741233468055725 at epoch: 15 and batch_num: 225\n",
      "Loss of train set: 0.495334267616272 at epoch: 15 and batch_num: 226\n",
      "Loss of train set: 0.24744608998298645 at epoch: 15 and batch_num: 227\n",
      "Loss of train set: 0.2497851550579071 at epoch: 15 and batch_num: 228\n",
      "Loss of train set: 0.33898037672042847 at epoch: 15 and batch_num: 229\n",
      "Loss of train set: 0.342958927154541 at epoch: 15 and batch_num: 230\n",
      "Loss of train set: 0.3375231921672821 at epoch: 15 and batch_num: 231\n",
      "Loss of train set: 0.3981951177120209 at epoch: 15 and batch_num: 232\n",
      "Loss of train set: 0.3080434799194336 at epoch: 15 and batch_num: 233\n",
      "Loss of train set: 0.2424037754535675 at epoch: 15 and batch_num: 234\n",
      "Loss of train set: 0.28390055894851685 at epoch: 15 and batch_num: 235\n",
      "Loss of train set: 0.24161316454410553 at epoch: 15 and batch_num: 236\n",
      "Loss of train set: 0.6242983341217041 at epoch: 15 and batch_num: 237\n",
      "Loss of train set: 0.23627245426177979 at epoch: 15 and batch_num: 238\n",
      "Loss of train set: 0.25245946645736694 at epoch: 15 and batch_num: 239\n",
      "Loss of train set: 0.35969871282577515 at epoch: 15 and batch_num: 240\n",
      "Loss of train set: 0.2773303687572479 at epoch: 15 and batch_num: 241\n",
      "Loss of train set: 0.22088560461997986 at epoch: 15 and batch_num: 242\n",
      "Loss of train set: 0.4606959819793701 at epoch: 15 and batch_num: 243\n",
      "Loss of train set: 0.3753707706928253 at epoch: 15 and batch_num: 244\n",
      "Loss of train set: 0.2295691967010498 at epoch: 15 and batch_num: 245\n",
      "Loss of train set: 0.3025659918785095 at epoch: 15 and batch_num: 246\n",
      "Loss of train set: 0.4304981827735901 at epoch: 15 and batch_num: 247\n",
      "Loss of train set: 0.22982309758663177 at epoch: 15 and batch_num: 248\n",
      "Loss of train set: 0.3662116527557373 at epoch: 15 and batch_num: 249\n",
      "Loss of train set: 0.5154533982276917 at epoch: 15 and batch_num: 250\n",
      "Loss of train set: 0.23224616050720215 at epoch: 15 and batch_num: 251\n",
      "Loss of train set: 0.24530795216560364 at epoch: 15 and batch_num: 252\n",
      "Loss of train set: 0.28751668334007263 at epoch: 15 and batch_num: 253\n",
      "Loss of train set: 0.24773958325386047 at epoch: 15 and batch_num: 254\n",
      "Loss of train set: 0.2184557318687439 at epoch: 15 and batch_num: 255\n",
      "Loss of train set: 0.3199671506881714 at epoch: 15 and batch_num: 256\n",
      "Loss of train set: 0.2522171437740326 at epoch: 15 and batch_num: 257\n",
      "Loss of train set: 0.17785970866680145 at epoch: 15 and batch_num: 258\n",
      "Loss of train set: 0.3067021071910858 at epoch: 15 and batch_num: 259\n",
      "Loss of train set: 0.27657240629196167 at epoch: 15 and batch_num: 260\n",
      "Loss of train set: 0.3130737841129303 at epoch: 15 and batch_num: 261\n",
      "Loss of train set: 0.2758236229419708 at epoch: 15 and batch_num: 262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2498396784067154 at epoch: 15 and batch_num: 263\n",
      "Loss of train set: 0.28795021772384644 at epoch: 15 and batch_num: 264\n",
      "Loss of train set: 0.2055625021457672 at epoch: 15 and batch_num: 265\n",
      "Loss of train set: 0.20193442702293396 at epoch: 15 and batch_num: 266\n",
      "Loss of train set: 0.20587176084518433 at epoch: 15 and batch_num: 267\n",
      "Loss of train set: 0.17568928003311157 at epoch: 15 and batch_num: 268\n",
      "Loss of train set: 0.18487364053726196 at epoch: 15 and batch_num: 269\n",
      "Loss of train set: 0.3827558755874634 at epoch: 15 and batch_num: 270\n",
      "Loss of train set: 0.21324686706066132 at epoch: 15 and batch_num: 271\n",
      "Loss of train set: 0.5027846693992615 at epoch: 15 and batch_num: 272\n",
      "Loss of train set: 0.4273190498352051 at epoch: 15 and batch_num: 273\n",
      "Loss of train set: 0.4433059096336365 at epoch: 15 and batch_num: 274\n",
      "Loss of train set: 0.16044987738132477 at epoch: 15 and batch_num: 275\n",
      "Loss of train set: 0.17318964004516602 at epoch: 15 and batch_num: 276\n",
      "Loss of train set: 0.3206864595413208 at epoch: 15 and batch_num: 277\n",
      "Loss of train set: 0.22760885953903198 at epoch: 15 and batch_num: 278\n",
      "Loss of train set: 0.3633653521537781 at epoch: 15 and batch_num: 279\n",
      "Loss of train set: 0.31427064538002014 at epoch: 15 and batch_num: 280\n",
      "Loss of train set: 0.3434045910835266 at epoch: 15 and batch_num: 281\n",
      "Loss of train set: 0.30492696166038513 at epoch: 15 and batch_num: 282\n",
      "Loss of train set: 0.2536785900592804 at epoch: 15 and batch_num: 283\n",
      "Loss of train set: 0.31934988498687744 at epoch: 15 and batch_num: 284\n",
      "Loss of train set: 0.25454944372177124 at epoch: 15 and batch_num: 285\n",
      "Loss of train set: 0.39273953437805176 at epoch: 15 and batch_num: 286\n",
      "Loss of train set: 0.2833016812801361 at epoch: 15 and batch_num: 287\n",
      "Loss of train set: 0.27565842866897583 at epoch: 15 and batch_num: 288\n",
      "Loss of train set: 0.1799381971359253 at epoch: 15 and batch_num: 289\n",
      "Loss of train set: 0.20834386348724365 at epoch: 15 and batch_num: 290\n",
      "Loss of train set: 0.2534920573234558 at epoch: 15 and batch_num: 291\n",
      "Loss of train set: 0.2710312306880951 at epoch: 15 and batch_num: 292\n",
      "Loss of train set: 0.41645264625549316 at epoch: 15 and batch_num: 293\n",
      "Loss of train set: 0.14577773213386536 at epoch: 15 and batch_num: 294\n",
      "Loss of train set: 0.3382924795150757 at epoch: 15 and batch_num: 295\n",
      "Loss of train set: 0.23729676008224487 at epoch: 15 and batch_num: 296\n",
      "Loss of train set: 0.3058135211467743 at epoch: 15 and batch_num: 297\n",
      "Loss of train set: 0.42243626713752747 at epoch: 15 and batch_num: 298\n",
      "Loss of train set: 0.3050648272037506 at epoch: 15 and batch_num: 299\n",
      "Loss of train set: 0.37570804357528687 at epoch: 15 and batch_num: 300\n",
      "Loss of train set: 0.3498997688293457 at epoch: 15 and batch_num: 301\n",
      "Loss of train set: 0.3961140811443329 at epoch: 15 and batch_num: 302\n",
      "Loss of train set: 0.32825830578804016 at epoch: 15 and batch_num: 303\n",
      "Loss of train set: 0.39799511432647705 at epoch: 15 and batch_num: 304\n",
      "Loss of train set: 0.22953665256500244 at epoch: 15 and batch_num: 305\n",
      "Loss of train set: 0.4262239933013916 at epoch: 15 and batch_num: 306\n",
      "Loss of train set: 0.2533920407295227 at epoch: 15 and batch_num: 307\n",
      "Loss of train set: 0.2871546149253845 at epoch: 15 and batch_num: 308\n",
      "Loss of train set: 0.21322137117385864 at epoch: 15 and batch_num: 309\n",
      "Loss of train set: 0.22973161935806274 at epoch: 15 and batch_num: 310\n",
      "Loss of train set: 0.3020467162132263 at epoch: 15 and batch_num: 311\n",
      "Loss of train set: 0.46415549516677856 at epoch: 15 and batch_num: 312\n",
      "Loss of train set: 0.260043740272522 at epoch: 15 and batch_num: 313\n",
      "Loss of train set: 0.2181946337223053 at epoch: 15 and batch_num: 314\n",
      "Loss of train set: 0.425473153591156 at epoch: 15 and batch_num: 315\n",
      "Loss of train set: 0.5076070427894592 at epoch: 15 and batch_num: 316\n",
      "Loss of train set: 0.2582099437713623 at epoch: 15 and batch_num: 317\n",
      "Loss of train set: 0.4355893135070801 at epoch: 15 and batch_num: 318\n",
      "Loss of train set: 0.48752450942993164 at epoch: 15 and batch_num: 319\n",
      "Loss of train set: 0.20060771703720093 at epoch: 15 and batch_num: 320\n",
      "Loss of train set: 0.518048107624054 at epoch: 15 and batch_num: 321\n",
      "Loss of train set: 0.15420342981815338 at epoch: 15 and batch_num: 322\n",
      "Loss of train set: 0.31620749831199646 at epoch: 15 and batch_num: 323\n",
      "Loss of train set: 0.3187189996242523 at epoch: 15 and batch_num: 324\n",
      "Loss of train set: 0.20774191617965698 at epoch: 15 and batch_num: 325\n",
      "Loss of train set: 0.3775419592857361 at epoch: 15 and batch_num: 326\n",
      "Loss of train set: 0.2435481995344162 at epoch: 15 and batch_num: 327\n",
      "Loss of train set: 0.23079554736614227 at epoch: 15 and batch_num: 328\n",
      "Loss of train set: 0.2378350794315338 at epoch: 15 and batch_num: 329\n",
      "Loss of train set: 0.3628438711166382 at epoch: 15 and batch_num: 330\n",
      "Loss of train set: 0.27161961793899536 at epoch: 15 and batch_num: 331\n",
      "Loss of train set: 0.22319436073303223 at epoch: 15 and batch_num: 332\n",
      "Loss of train set: 0.11957535147666931 at epoch: 15 and batch_num: 333\n",
      "Loss of train set: 0.36500006914138794 at epoch: 15 and batch_num: 334\n",
      "Loss of train set: 0.22104433178901672 at epoch: 15 and batch_num: 335\n",
      "Loss of train set: 0.24394641816616058 at epoch: 15 and batch_num: 336\n",
      "Loss of train set: 0.39478856325149536 at epoch: 15 and batch_num: 337\n",
      "Loss of train set: 0.3213709890842438 at epoch: 15 and batch_num: 338\n",
      "Loss of train set: 0.32822632789611816 at epoch: 15 and batch_num: 339\n",
      "Loss of train set: 0.2631412446498871 at epoch: 15 and batch_num: 340\n",
      "Loss of train set: 0.26492828130722046 at epoch: 15 and batch_num: 341\n",
      "Loss of train set: 0.300382137298584 at epoch: 15 and batch_num: 342\n",
      "Loss of train set: 0.22467809915542603 at epoch: 15 and batch_num: 343\n",
      "Loss of train set: 0.24393105506896973 at epoch: 15 and batch_num: 344\n",
      "Loss of train set: 0.4108414053916931 at epoch: 15 and batch_num: 345\n",
      "Loss of train set: 0.2998400926589966 at epoch: 15 and batch_num: 346\n",
      "Loss of train set: 0.29730719327926636 at epoch: 15 and batch_num: 347\n",
      "Loss of train set: 0.241794615983963 at epoch: 15 and batch_num: 348\n",
      "Loss of train set: 0.2677200436592102 at epoch: 15 and batch_num: 349\n",
      "Loss of train set: 0.22944802045822144 at epoch: 15 and batch_num: 350\n",
      "Loss of train set: 0.3077242076396942 at epoch: 15 and batch_num: 351\n",
      "Loss of train set: 0.29332464933395386 at epoch: 15 and batch_num: 352\n",
      "Loss of train set: 0.4511718451976776 at epoch: 15 and batch_num: 353\n",
      "Loss of train set: 0.3813501298427582 at epoch: 15 and batch_num: 354\n",
      "Loss of train set: 0.29698628187179565 at epoch: 15 and batch_num: 355\n",
      "Loss of train set: 0.29008355736732483 at epoch: 15 and batch_num: 356\n",
      "Loss of train set: 0.221499502658844 at epoch: 15 and batch_num: 357\n",
      "Loss of train set: 0.28354376554489136 at epoch: 15 and batch_num: 358\n",
      "Loss of train set: 0.3537582755088806 at epoch: 15 and batch_num: 359\n",
      "Loss of train set: 0.36108899116516113 at epoch: 15 and batch_num: 360\n",
      "Loss of train set: 0.3553428053855896 at epoch: 15 and batch_num: 361\n",
      "Loss of train set: 0.2918459177017212 at epoch: 15 and batch_num: 362\n",
      "Loss of train set: 0.21819230914115906 at epoch: 15 and batch_num: 363\n",
      "Loss of train set: 0.25416600704193115 at epoch: 15 and batch_num: 364\n",
      "Loss of train set: 0.3578089475631714 at epoch: 15 and batch_num: 365\n",
      "Loss of train set: 0.21557873487472534 at epoch: 15 and batch_num: 366\n",
      "Loss of train set: 0.239578515291214 at epoch: 15 and batch_num: 367\n",
      "Loss of train set: 0.27069756388664246 at epoch: 15 and batch_num: 368\n",
      "Loss of train set: 0.19252419471740723 at epoch: 15 and batch_num: 369\n",
      "Loss of train set: 0.364610880613327 at epoch: 15 and batch_num: 370\n",
      "Loss of train set: 0.36629191040992737 at epoch: 15 and batch_num: 371\n",
      "Loss of train set: 0.22311528027057648 at epoch: 15 and batch_num: 372\n",
      "Loss of train set: 0.46773263812065125 at epoch: 15 and batch_num: 373\n",
      "Loss of train set: 0.20681752264499664 at epoch: 15 and batch_num: 374\n",
      "Loss of train set: 0.23528596758842468 at epoch: 15 and batch_num: 375\n",
      "Loss of train set: 0.31282198429107666 at epoch: 15 and batch_num: 376\n",
      "Loss of train set: 0.3637775480747223 at epoch: 15 and batch_num: 377\n",
      "Loss of train set: 0.36060312390327454 at epoch: 15 and batch_num: 378\n",
      "Loss of train set: 0.23422178626060486 at epoch: 15 and batch_num: 379\n",
      "Loss of train set: 0.24719607830047607 at epoch: 15 and batch_num: 380\n",
      "Loss of train set: 0.4046156406402588 at epoch: 15 and batch_num: 381\n",
      "Loss of train set: 0.22399355471134186 at epoch: 15 and batch_num: 382\n",
      "Loss of train set: 0.1411362737417221 at epoch: 15 and batch_num: 383\n",
      "Loss of train set: 0.2521659731864929 at epoch: 15 and batch_num: 384\n",
      "Loss of train set: 0.19060268998146057 at epoch: 15 and batch_num: 385\n",
      "Loss of train set: 0.3042650818824768 at epoch: 15 and batch_num: 386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.1754675656557083 at epoch: 15 and batch_num: 387\n",
      "Loss of train set: 0.4283273220062256 at epoch: 15 and batch_num: 388\n",
      "Loss of train set: 0.37851250171661377 at epoch: 15 and batch_num: 389\n",
      "Loss of train set: 0.28270429372787476 at epoch: 15 and batch_num: 390\n",
      "Loss of train set: 0.2143268585205078 at epoch: 15 and batch_num: 391\n",
      "Loss of train set: 0.26534709334373474 at epoch: 15 and batch_num: 392\n",
      "Loss of train set: 0.34440624713897705 at epoch: 15 and batch_num: 393\n",
      "Loss of train set: 0.3612367510795593 at epoch: 15 and batch_num: 394\n",
      "Loss of train set: 0.3771175146102905 at epoch: 15 and batch_num: 395\n",
      "Loss of train set: 0.388200581073761 at epoch: 15 and batch_num: 396\n",
      "Loss of train set: 0.24736569821834564 at epoch: 15 and batch_num: 397\n",
      "Loss of train set: 0.28568035364151 at epoch: 15 and batch_num: 398\n",
      "Loss of train set: 0.25828495621681213 at epoch: 15 and batch_num: 399\n",
      "Loss of train set: 0.3382471799850464 at epoch: 15 and batch_num: 400\n",
      "Loss of train set: 0.5243494510650635 at epoch: 15 and batch_num: 401\n",
      "Loss of train set: 0.28122150897979736 at epoch: 15 and batch_num: 402\n",
      "Loss of train set: 0.3012448251247406 at epoch: 15 and batch_num: 403\n",
      "Loss of train set: 0.1750817894935608 at epoch: 15 and batch_num: 404\n",
      "Loss of train set: 0.1532796323299408 at epoch: 15 and batch_num: 405\n",
      "Loss of train set: 0.40899917483329773 at epoch: 15 and batch_num: 406\n",
      "Loss of train set: 0.2926231622695923 at epoch: 15 and batch_num: 407\n",
      "Loss of train set: 0.27461108565330505 at epoch: 15 and batch_num: 408\n",
      "Loss of train set: 0.3540117144584656 at epoch: 15 and batch_num: 409\n",
      "Loss of train set: 0.4163997769355774 at epoch: 15 and batch_num: 410\n",
      "Loss of train set: 0.2873210608959198 at epoch: 15 and batch_num: 411\n",
      "Loss of train set: 0.3310823440551758 at epoch: 15 and batch_num: 412\n",
      "Loss of train set: 0.24418063461780548 at epoch: 15 and batch_num: 413\n",
      "Loss of train set: 0.2368602305650711 at epoch: 15 and batch_num: 414\n",
      "Loss of train set: 0.44467321038246155 at epoch: 15 and batch_num: 415\n",
      "Loss of train set: 0.17867237329483032 at epoch: 15 and batch_num: 416\n",
      "Loss of train set: 0.46599698066711426 at epoch: 15 and batch_num: 417\n",
      "Loss of train set: 0.3057055175304413 at epoch: 15 and batch_num: 418\n",
      "Loss of train set: 0.42241185903549194 at epoch: 15 and batch_num: 419\n",
      "Loss of train set: 0.20730575919151306 at epoch: 15 and batch_num: 420\n",
      "Loss of train set: 0.27633029222488403 at epoch: 15 and batch_num: 421\n",
      "Loss of train set: 0.31872254610061646 at epoch: 15 and batch_num: 422\n",
      "Loss of train set: 0.28834062814712524 at epoch: 15 and batch_num: 423\n",
      "Loss of train set: 0.214631587266922 at epoch: 15 and batch_num: 424\n",
      "Loss of train set: 0.2469145953655243 at epoch: 15 and batch_num: 425\n",
      "Loss of train set: 0.16978171467781067 at epoch: 15 and batch_num: 426\n",
      "Loss of train set: 0.31807348132133484 at epoch: 15 and batch_num: 427\n",
      "Loss of train set: 0.315702885389328 at epoch: 15 and batch_num: 428\n",
      "Loss of train set: 0.25377726554870605 at epoch: 15 and batch_num: 429\n",
      "Loss of train set: 0.23078429698944092 at epoch: 15 and batch_num: 430\n",
      "Loss of train set: 0.25234508514404297 at epoch: 15 and batch_num: 431\n",
      "Loss of train set: 0.2447250783443451 at epoch: 15 and batch_num: 432\n",
      "Loss of train set: 0.3374329209327698 at epoch: 15 and batch_num: 433\n",
      "Loss of train set: 0.25437986850738525 at epoch: 15 and batch_num: 434\n",
      "Loss of train set: 0.11793424189090729 at epoch: 15 and batch_num: 435\n",
      "Loss of train set: 0.2509245276451111 at epoch: 15 and batch_num: 436\n",
      "Loss of train set: 0.43868476152420044 at epoch: 15 and batch_num: 437\n",
      "Loss of train set: 0.3500538468360901 at epoch: 15 and batch_num: 438\n",
      "Loss of train set: 0.2672148644924164 at epoch: 15 and batch_num: 439\n",
      "Loss of train set: 0.1438087821006775 at epoch: 15 and batch_num: 440\n",
      "Loss of train set: 0.5151580572128296 at epoch: 15 and batch_num: 441\n",
      "Loss of train set: 0.3045834004878998 at epoch: 15 and batch_num: 442\n",
      "Loss of train set: 0.19464099407196045 at epoch: 15 and batch_num: 443\n",
      "Loss of train set: 0.20935264229774475 at epoch: 15 and batch_num: 444\n",
      "Loss of train set: 0.29292112588882446 at epoch: 15 and batch_num: 445\n",
      "Loss of train set: 0.2486116588115692 at epoch: 15 and batch_num: 446\n",
      "Loss of train set: 0.3086094260215759 at epoch: 15 and batch_num: 447\n",
      "Loss of train set: 0.20621554553508759 at epoch: 15 and batch_num: 448\n",
      "Loss of train set: 0.18255040049552917 at epoch: 15 and batch_num: 449\n",
      "Loss of train set: 0.13008630275726318 at epoch: 15 and batch_num: 450\n",
      "Loss of train set: 0.41103947162628174 at epoch: 15 and batch_num: 451\n",
      "Loss of train set: 0.17795918881893158 at epoch: 15 and batch_num: 452\n",
      "Loss of train set: 0.393218994140625 at epoch: 15 and batch_num: 453\n",
      "Loss of train set: 0.21035216748714447 at epoch: 15 and batch_num: 454\n",
      "Loss of train set: 0.6035088300704956 at epoch: 15 and batch_num: 455\n",
      "Loss of train set: 0.2817310094833374 at epoch: 15 and batch_num: 456\n",
      "Loss of train set: 0.32351332902908325 at epoch: 15 and batch_num: 457\n",
      "Loss of train set: 0.24442672729492188 at epoch: 15 and batch_num: 458\n",
      "Loss of train set: 0.2770833969116211 at epoch: 15 and batch_num: 459\n",
      "Loss of train set: 0.473355770111084 at epoch: 15 and batch_num: 460\n",
      "Loss of train set: 0.3117907643318176 at epoch: 15 and batch_num: 461\n",
      "Loss of train set: 0.25153622031211853 at epoch: 15 and batch_num: 462\n",
      "Loss of train set: 0.3671916723251343 at epoch: 15 and batch_num: 463\n",
      "Loss of train set: 0.3969230055809021 at epoch: 15 and batch_num: 464\n",
      "Loss of train set: 0.368959903717041 at epoch: 15 and batch_num: 465\n",
      "Loss of train set: 0.31505775451660156 at epoch: 15 and batch_num: 466\n",
      "Loss of train set: 0.28428834676742554 at epoch: 15 and batch_num: 467\n",
      "Loss of train set: 0.3586198091506958 at epoch: 15 and batch_num: 468\n",
      "Loss of train set: 0.12960529327392578 at epoch: 15 and batch_num: 469\n",
      "Loss of train set: 0.30770766735076904 at epoch: 15 and batch_num: 470\n",
      "Loss of train set: 0.2694436013698578 at epoch: 15 and batch_num: 471\n",
      "Loss of train set: 0.31779325008392334 at epoch: 15 and batch_num: 472\n",
      "Loss of train set: 0.20145383477210999 at epoch: 15 and batch_num: 473\n",
      "Loss of train set: 0.31962406635284424 at epoch: 15 and batch_num: 474\n",
      "Loss of train set: 0.23421256244182587 at epoch: 15 and batch_num: 475\n",
      "Loss of train set: 0.34282776713371277 at epoch: 15 and batch_num: 476\n",
      "Loss of train set: 0.2957538068294525 at epoch: 15 and batch_num: 477\n",
      "Loss of train set: 0.20058037340641022 at epoch: 15 and batch_num: 478\n",
      "Loss of train set: 0.1664842814207077 at epoch: 15 and batch_num: 479\n",
      "Loss of train set: 0.24048176407814026 at epoch: 15 and batch_num: 480\n",
      "Loss of train set: 0.2427128255367279 at epoch: 15 and batch_num: 481\n",
      "Loss of train set: 0.388799250125885 at epoch: 15 and batch_num: 482\n",
      "Loss of train set: 0.2525223195552826 at epoch: 15 and batch_num: 483\n",
      "Loss of train set: 0.25408345460891724 at epoch: 15 and batch_num: 484\n",
      "Loss of train set: 0.3598196804523468 at epoch: 15 and batch_num: 485\n",
      "Loss of train set: 0.19420690834522247 at epoch: 15 and batch_num: 486\n",
      "Loss of train set: 0.32607966661453247 at epoch: 15 and batch_num: 487\n",
      "Loss of train set: 0.31476283073425293 at epoch: 15 and batch_num: 488\n",
      "Loss of train set: 0.26547104120254517 at epoch: 15 and batch_num: 489\n",
      "Loss of train set: 0.2667401432991028 at epoch: 15 and batch_num: 490\n",
      "Loss of train set: 0.2991030812263489 at epoch: 15 and batch_num: 491\n",
      "Loss of train set: 0.3509984016418457 at epoch: 15 and batch_num: 492\n",
      "Loss of train set: 0.39539992809295654 at epoch: 15 and batch_num: 493\n",
      "Loss of train set: 0.2922406792640686 at epoch: 15 and batch_num: 494\n",
      "Loss of train set: 0.3043249249458313 at epoch: 15 and batch_num: 495\n",
      "Loss of train set: 0.6269866824150085 at epoch: 15 and batch_num: 496\n",
      "Loss of train set: 0.2638394832611084 at epoch: 15 and batch_num: 497\n",
      "Loss of train set: 0.42813849449157715 at epoch: 15 and batch_num: 498\n",
      "Loss of train set: 0.40667450428009033 at epoch: 15 and batch_num: 499\n",
      "Loss of train set: 0.25391310453414917 at epoch: 15 and batch_num: 500\n",
      "Loss of train set: 0.14119791984558105 at epoch: 15 and batch_num: 501\n",
      "Loss of train set: 0.21284371614456177 at epoch: 15 and batch_num: 502\n",
      "Loss of train set: 0.491818368434906 at epoch: 15 and batch_num: 503\n",
      "Loss of train set: 0.3024856448173523 at epoch: 15 and batch_num: 504\n",
      "Loss of train set: 0.24197718501091003 at epoch: 15 and batch_num: 505\n",
      "Loss of train set: 0.20922914147377014 at epoch: 15 and batch_num: 506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.21281924843788147 at epoch: 15 and batch_num: 507\n",
      "Loss of train set: 0.2647508680820465 at epoch: 15 and batch_num: 508\n",
      "Loss of train set: 0.3089209198951721 at epoch: 15 and batch_num: 509\n",
      "Loss of train set: 0.21178987622261047 at epoch: 15 and batch_num: 510\n",
      "Loss of train set: 0.35075539350509644 at epoch: 15 and batch_num: 511\n",
      "Loss of train set: 0.261770635843277 at epoch: 15 and batch_num: 512\n",
      "Loss of train set: 0.21838098764419556 at epoch: 15 and batch_num: 513\n",
      "Loss of train set: 0.29276251792907715 at epoch: 15 and batch_num: 514\n",
      "Loss of train set: 0.3076243996620178 at epoch: 15 and batch_num: 515\n",
      "Loss of train set: 0.21169555187225342 at epoch: 15 and batch_num: 516\n",
      "Loss of train set: 0.30241259932518005 at epoch: 15 and batch_num: 517\n",
      "Loss of train set: 0.29024896025657654 at epoch: 15 and batch_num: 518\n",
      "Loss of train set: 0.3040659427642822 at epoch: 15 and batch_num: 519\n",
      "Loss of train set: 0.47763797640800476 at epoch: 15 and batch_num: 520\n",
      "Loss of train set: 0.2595771253108978 at epoch: 15 and batch_num: 521\n",
      "Loss of train set: 0.23638780415058136 at epoch: 15 and batch_num: 522\n",
      "Loss of train set: 0.2396877259016037 at epoch: 15 and batch_num: 523\n",
      "Loss of train set: 0.25153428316116333 at epoch: 15 and batch_num: 524\n",
      "Loss of train set: 0.2532561719417572 at epoch: 15 and batch_num: 525\n",
      "Loss of train set: 0.3225981593132019 at epoch: 15 and batch_num: 526\n",
      "Loss of train set: 0.343791127204895 at epoch: 15 and batch_num: 527\n",
      "Loss of train set: 0.42947542667388916 at epoch: 15 and batch_num: 528\n",
      "Loss of train set: 0.3399125039577484 at epoch: 15 and batch_num: 529\n",
      "Loss of train set: 0.28518950939178467 at epoch: 15 and batch_num: 530\n",
      "Loss of train set: 0.21471667289733887 at epoch: 15 and batch_num: 531\n",
      "Loss of train set: 0.2936425805091858 at epoch: 15 and batch_num: 532\n",
      "Loss of train set: 0.27007314562797546 at epoch: 15 and batch_num: 533\n",
      "Loss of train set: 0.4111064076423645 at epoch: 15 and batch_num: 534\n",
      "Loss of train set: 0.31279534101486206 at epoch: 15 and batch_num: 535\n",
      "Loss of train set: 0.23409472405910492 at epoch: 15 and batch_num: 536\n",
      "Loss of train set: 0.20891395211219788 at epoch: 15 and batch_num: 537\n",
      "Loss of train set: 0.3130151033401489 at epoch: 15 and batch_num: 538\n",
      "Loss of train set: 0.2887408137321472 at epoch: 15 and batch_num: 539\n",
      "Loss of train set: 0.33288663625717163 at epoch: 15 and batch_num: 540\n",
      "Loss of train set: 0.42236050963401794 at epoch: 15 and batch_num: 541\n",
      "Loss of train set: 0.2606911361217499 at epoch: 15 and batch_num: 542\n",
      "Loss of train set: 0.29164689779281616 at epoch: 15 and batch_num: 543\n",
      "Loss of train set: 0.32554492354393005 at epoch: 15 and batch_num: 544\n",
      "Loss of train set: 0.3437420725822449 at epoch: 15 and batch_num: 545\n",
      "Loss of train set: 0.251596063375473 at epoch: 15 and batch_num: 546\n",
      "Loss of train set: 0.39872509241104126 at epoch: 15 and batch_num: 547\n",
      "Loss of train set: 0.24636875092983246 at epoch: 15 and batch_num: 548\n",
      "Loss of train set: 0.3292185664176941 at epoch: 15 and batch_num: 549\n",
      "Loss of train set: 0.3815385699272156 at epoch: 15 and batch_num: 550\n",
      "Loss of train set: 0.34404128789901733 at epoch: 15 and batch_num: 551\n",
      "Loss of train set: 0.449853777885437 at epoch: 15 and batch_num: 552\n",
      "Loss of train set: 0.34895649552345276 at epoch: 15 and batch_num: 553\n",
      "Loss of train set: 0.3323093056678772 at epoch: 15 and batch_num: 554\n",
      "Loss of train set: 0.311716765165329 at epoch: 15 and batch_num: 555\n",
      "Loss of train set: 0.29570236802101135 at epoch: 15 and batch_num: 556\n",
      "Loss of train set: 0.2431664764881134 at epoch: 15 and batch_num: 557\n",
      "Loss of train set: 0.5419633388519287 at epoch: 15 and batch_num: 558\n",
      "Loss of train set: 0.23508568108081818 at epoch: 15 and batch_num: 559\n",
      "Loss of train set: 0.35169535875320435 at epoch: 15 and batch_num: 560\n",
      "Loss of train set: 0.19630999863147736 at epoch: 15 and batch_num: 561\n",
      "Loss of train set: 0.3465697169303894 at epoch: 15 and batch_num: 562\n",
      "Loss of train set: 0.34863483905792236 at epoch: 15 and batch_num: 563\n",
      "Loss of train set: 0.3865468204021454 at epoch: 15 and batch_num: 564\n",
      "Loss of train set: 0.19362035393714905 at epoch: 15 and batch_num: 565\n",
      "Loss of train set: 0.20017623901367188 at epoch: 15 and batch_num: 566\n",
      "Loss of train set: 0.23915202915668488 at epoch: 15 and batch_num: 567\n",
      "Loss of train set: 0.29529544711112976 at epoch: 15 and batch_num: 568\n",
      "Loss of train set: 0.3015320897102356 at epoch: 15 and batch_num: 569\n",
      "Loss of train set: 0.34499871730804443 at epoch: 15 and batch_num: 570\n",
      "Loss of train set: 0.3593297600746155 at epoch: 15 and batch_num: 571\n",
      "Loss of train set: 0.39477628469467163 at epoch: 15 and batch_num: 572\n",
      "Loss of train set: 0.3068014681339264 at epoch: 15 and batch_num: 573\n",
      "Loss of train set: 0.23236392438411713 at epoch: 15 and batch_num: 574\n",
      "Loss of train set: 0.4449464678764343 at epoch: 15 and batch_num: 575\n",
      "Loss of train set: 0.3689520061016083 at epoch: 15 and batch_num: 576\n",
      "Loss of train set: 0.4036210775375366 at epoch: 15 and batch_num: 577\n",
      "Loss of train set: 0.2950904965400696 at epoch: 15 and batch_num: 578\n",
      "Loss of train set: 0.18185032904148102 at epoch: 15 and batch_num: 579\n",
      "Loss of train set: 0.22993095219135284 at epoch: 15 and batch_num: 580\n",
      "Loss of train set: 0.2765675485134125 at epoch: 15 and batch_num: 581\n",
      "Loss of train set: 0.3184991478919983 at epoch: 15 and batch_num: 582\n",
      "Loss of train set: 0.25510916113853455 at epoch: 15 and batch_num: 583\n",
      "Loss of train set: 0.2191253900527954 at epoch: 15 and batch_num: 584\n",
      "Loss of train set: 0.19607014954090118 at epoch: 15 and batch_num: 585\n",
      "Loss of train set: 0.35919612646102905 at epoch: 15 and batch_num: 586\n",
      "Loss of train set: 0.29143619537353516 at epoch: 15 and batch_num: 587\n",
      "Loss of train set: 0.3935157060623169 at epoch: 15 and batch_num: 588\n",
      "Loss of train set: 0.3823392987251282 at epoch: 15 and batch_num: 589\n",
      "Loss of train set: 0.1579282283782959 at epoch: 15 and batch_num: 590\n",
      "Loss of train set: 0.3441964387893677 at epoch: 15 and batch_num: 591\n",
      "Loss of train set: 0.4858301877975464 at epoch: 15 and batch_num: 592\n",
      "Loss of train set: 0.3290987014770508 at epoch: 15 and batch_num: 593\n",
      "Loss of train set: 0.25121983885765076 at epoch: 15 and batch_num: 594\n",
      "Loss of train set: 0.24009272456169128 at epoch: 15 and batch_num: 595\n",
      "Loss of train set: 0.294458270072937 at epoch: 15 and batch_num: 596\n",
      "Loss of train set: 0.1745356321334839 at epoch: 15 and batch_num: 597\n",
      "Loss of train set: 0.3339434862136841 at epoch: 15 and batch_num: 598\n",
      "Loss of train set: 0.38995161652565 at epoch: 15 and batch_num: 599\n",
      "Loss of train set: 0.41621410846710205 at epoch: 15 and batch_num: 600\n",
      "Loss of train set: 0.4304371476173401 at epoch: 15 and batch_num: 601\n",
      "Loss of train set: 0.30092209577560425 at epoch: 15 and batch_num: 602\n",
      "Loss of train set: 0.30913686752319336 at epoch: 15 and batch_num: 603\n",
      "Loss of train set: 0.34642404317855835 at epoch: 15 and batch_num: 604\n",
      "Loss of train set: 0.31863707304000854 at epoch: 15 and batch_num: 605\n",
      "Loss of train set: 0.24171128869056702 at epoch: 15 and batch_num: 606\n",
      "Loss of train set: 0.3148832321166992 at epoch: 15 and batch_num: 607\n",
      "Loss of train set: 0.12157708406448364 at epoch: 15 and batch_num: 608\n",
      "Loss of train set: 0.38928669691085815 at epoch: 15 and batch_num: 609\n",
      "Loss of train set: 0.29331302642822266 at epoch: 15 and batch_num: 610\n",
      "Loss of train set: 0.24254289269447327 at epoch: 15 and batch_num: 611\n",
      "Loss of train set: 0.27435439825057983 at epoch: 15 and batch_num: 612\n",
      "Loss of train set: 0.23723283410072327 at epoch: 15 and batch_num: 613\n",
      "Loss of train set: 0.15909253060817719 at epoch: 15 and batch_num: 614\n",
      "Loss of train set: 0.3025360107421875 at epoch: 15 and batch_num: 615\n",
      "Loss of train set: 0.3351293206214905 at epoch: 15 and batch_num: 616\n",
      "Loss of train set: 0.37658989429473877 at epoch: 15 and batch_num: 617\n",
      "Loss of train set: 0.23082537949085236 at epoch: 15 and batch_num: 618\n",
      "Loss of train set: 0.286332368850708 at epoch: 15 and batch_num: 619\n",
      "Loss of train set: 0.2953546643257141 at epoch: 15 and batch_num: 620\n",
      "Loss of train set: 0.4197179973125458 at epoch: 15 and batch_num: 621\n",
      "Loss of train set: 0.1836566925048828 at epoch: 15 and batch_num: 622\n",
      "Loss of train set: 0.2527177035808563 at epoch: 15 and batch_num: 623\n",
      "Loss of train set: 0.13249966502189636 at epoch: 15 and batch_num: 624\n",
      "Loss of train set: 0.20056340098381042 at epoch: 15 and batch_num: 625\n",
      "Loss of train set: 0.36088237166404724 at epoch: 15 and batch_num: 626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.368122398853302 at epoch: 15 and batch_num: 627\n",
      "Loss of train set: 0.2880384922027588 at epoch: 15 and batch_num: 628\n",
      "Loss of train set: 0.20240366458892822 at epoch: 15 and batch_num: 629\n",
      "Loss of train set: 0.2828162908554077 at epoch: 15 and batch_num: 630\n",
      "Loss of train set: 0.21772266924381256 at epoch: 15 and batch_num: 631\n",
      "Loss of train set: 0.37799519300460815 at epoch: 15 and batch_num: 632\n",
      "Loss of train set: 0.2992395758628845 at epoch: 15 and batch_num: 633\n",
      "Loss of train set: 0.21063683927059174 at epoch: 15 and batch_num: 634\n",
      "Loss of train set: 0.35090816020965576 at epoch: 15 and batch_num: 635\n",
      "Loss of train set: 0.2612195611000061 at epoch: 15 and batch_num: 636\n",
      "Loss of train set: 0.16440032422542572 at epoch: 15 and batch_num: 637\n",
      "Loss of train set: 0.2187303602695465 at epoch: 15 and batch_num: 638\n",
      "Loss of train set: 0.28000253438949585 at epoch: 15 and batch_num: 639\n",
      "Loss of train set: 0.28126809000968933 at epoch: 15 and batch_num: 640\n",
      "Loss of train set: 0.28896939754486084 at epoch: 15 and batch_num: 641\n",
      "Loss of train set: 0.3404094874858856 at epoch: 15 and batch_num: 642\n",
      "Loss of train set: 0.31427839398384094 at epoch: 15 and batch_num: 643\n",
      "Loss of train set: 0.21859760582447052 at epoch: 15 and batch_num: 644\n",
      "Loss of train set: 0.29696887731552124 at epoch: 15 and batch_num: 645\n",
      "Loss of train set: 0.3167605996131897 at epoch: 15 and batch_num: 646\n",
      "Loss of train set: 0.2350926250219345 at epoch: 15 and batch_num: 647\n",
      "Loss of train set: 0.2867598831653595 at epoch: 15 and batch_num: 648\n",
      "Loss of train set: 0.30119699239730835 at epoch: 15 and batch_num: 649\n",
      "Loss of train set: 0.1584736406803131 at epoch: 15 and batch_num: 650\n",
      "Loss of train set: 0.4236097037792206 at epoch: 15 and batch_num: 651\n",
      "Loss of train set: 0.232444167137146 at epoch: 15 and batch_num: 652\n",
      "Loss of train set: 0.293688029050827 at epoch: 15 and batch_num: 653\n",
      "Loss of train set: 0.21962833404541016 at epoch: 15 and batch_num: 654\n",
      "Loss of train set: 0.17641565203666687 at epoch: 15 and batch_num: 655\n",
      "Loss of train set: 0.19278928637504578 at epoch: 15 and batch_num: 656\n",
      "Loss of train set: 0.23635870218276978 at epoch: 15 and batch_num: 657\n",
      "Loss of train set: 0.31632760167121887 at epoch: 15 and batch_num: 658\n",
      "Loss of train set: 0.35376882553100586 at epoch: 15 and batch_num: 659\n",
      "Loss of train set: 0.3262431025505066 at epoch: 15 and batch_num: 660\n",
      "Loss of train set: 0.35131001472473145 at epoch: 15 and batch_num: 661\n",
      "Loss of train set: 0.32856059074401855 at epoch: 15 and batch_num: 662\n",
      "Loss of train set: 0.44548583030700684 at epoch: 15 and batch_num: 663\n",
      "Loss of train set: 0.25635915994644165 at epoch: 15 and batch_num: 664\n",
      "Loss of train set: 0.3825240731239319 at epoch: 15 and batch_num: 665\n",
      "Loss of train set: 0.2860630750656128 at epoch: 15 and batch_num: 666\n",
      "Loss of train set: 0.662236213684082 at epoch: 15 and batch_num: 667\n",
      "Loss of train set: 0.1639949530363083 at epoch: 15 and batch_num: 668\n",
      "Loss of train set: 0.25841566920280457 at epoch: 15 and batch_num: 669\n",
      "Loss of train set: 0.42944103479385376 at epoch: 15 and batch_num: 670\n",
      "Loss of train set: 0.3115343451499939 at epoch: 15 and batch_num: 671\n",
      "Loss of train set: 0.25416603684425354 at epoch: 15 and batch_num: 672\n",
      "Loss of train set: 0.2395363301038742 at epoch: 15 and batch_num: 673\n",
      "Loss of train set: 0.3185999393463135 at epoch: 15 and batch_num: 674\n",
      "Loss of train set: 0.5307035446166992 at epoch: 15 and batch_num: 675\n",
      "Loss of train set: 0.2955250144004822 at epoch: 15 and batch_num: 676\n",
      "Loss of train set: 0.2250843346118927 at epoch: 15 and batch_num: 677\n",
      "Loss of train set: 0.27221986651420593 at epoch: 15 and batch_num: 678\n",
      "Loss of train set: 0.27959883213043213 at epoch: 15 and batch_num: 679\n",
      "Loss of train set: 0.3402688503265381 at epoch: 15 and batch_num: 680\n",
      "Loss of train set: 0.29302147030830383 at epoch: 15 and batch_num: 681\n",
      "Loss of train set: 0.18147292733192444 at epoch: 15 and batch_num: 682\n",
      "Loss of train set: 0.2541595697402954 at epoch: 15 and batch_num: 683\n",
      "Loss of train set: 0.5134562849998474 at epoch: 15 and batch_num: 684\n",
      "Loss of train set: 0.3920191526412964 at epoch: 15 and batch_num: 685\n",
      "Loss of train set: 0.21180135011672974 at epoch: 15 and batch_num: 686\n",
      "Loss of train set: 0.27292436361312866 at epoch: 15 and batch_num: 687\n",
      "Loss of train set: 0.22062215209007263 at epoch: 15 and batch_num: 688\n",
      "Loss of train set: 0.2824345827102661 at epoch: 15 and batch_num: 689\n",
      "Loss of train set: 0.24352654814720154 at epoch: 15 and batch_num: 690\n",
      "Loss of train set: 0.28973710536956787 at epoch: 15 and batch_num: 691\n",
      "Loss of train set: 0.26632606983184814 at epoch: 15 and batch_num: 692\n",
      "Loss of train set: 0.2622401714324951 at epoch: 15 and batch_num: 693\n",
      "Loss of train set: 0.24677759408950806 at epoch: 15 and batch_num: 694\n",
      "Loss of train set: 0.39078059792518616 at epoch: 15 and batch_num: 695\n",
      "Loss of train set: 0.26462358236312866 at epoch: 15 and batch_num: 696\n",
      "Loss of train set: 0.4058977961540222 at epoch: 15 and batch_num: 697\n",
      "Loss of train set: 0.29120177030563354 at epoch: 15 and batch_num: 698\n",
      "Loss of train set: 0.5702974796295166 at epoch: 15 and batch_num: 699\n",
      "Loss of train set: 0.30398374795913696 at epoch: 15 and batch_num: 700\n",
      "Loss of train set: 0.34845882654190063 at epoch: 15 and batch_num: 701\n",
      "Loss of train set: 0.3519611358642578 at epoch: 15 and batch_num: 702\n",
      "Loss of train set: 0.14448051154613495 at epoch: 15 and batch_num: 703\n",
      "Loss of train set: 0.2677527070045471 at epoch: 15 and batch_num: 704\n",
      "Loss of train set: 0.44787365198135376 at epoch: 15 and batch_num: 705\n",
      "Loss of train set: 0.291057288646698 at epoch: 15 and batch_num: 706\n",
      "Loss of train set: 0.2920289635658264 at epoch: 15 and batch_num: 707\n",
      "Loss of train set: 0.31826263666152954 at epoch: 15 and batch_num: 708\n",
      "Loss of train set: 0.2282450944185257 at epoch: 15 and batch_num: 709\n",
      "Loss of train set: 0.2803647220134735 at epoch: 15 and batch_num: 710\n",
      "Loss of train set: 0.1435498148202896 at epoch: 15 and batch_num: 711\n",
      "Loss of train set: 0.3005148470401764 at epoch: 15 and batch_num: 712\n",
      "Loss of train set: 0.23053672909736633 at epoch: 15 and batch_num: 713\n",
      "Loss of train set: 0.19250598549842834 at epoch: 15 and batch_num: 714\n",
      "Loss of train set: 0.4091784358024597 at epoch: 15 and batch_num: 715\n",
      "Loss of train set: 0.3062446117401123 at epoch: 15 and batch_num: 716\n",
      "Loss of train set: 0.36592215299606323 at epoch: 15 and batch_num: 717\n",
      "Loss of train set: 0.32685181498527527 at epoch: 15 and batch_num: 718\n",
      "Loss of train set: 0.19177719950675964 at epoch: 15 and batch_num: 719\n",
      "Loss of train set: 0.21649298071861267 at epoch: 15 and batch_num: 720\n",
      "Loss of train set: 0.4236553907394409 at epoch: 15 and batch_num: 721\n",
      "Loss of train set: 0.24711333215236664 at epoch: 15 and batch_num: 722\n",
      "Loss of train set: 0.3776998221874237 at epoch: 15 and batch_num: 723\n",
      "Loss of train set: 0.24568873643875122 at epoch: 15 and batch_num: 724\n",
      "Loss of train set: 0.19900894165039062 at epoch: 15 and batch_num: 725\n",
      "Loss of train set: 0.28710994124412537 at epoch: 15 and batch_num: 726\n",
      "Loss of train set: 0.2256220132112503 at epoch: 15 and batch_num: 727\n",
      "Loss of train set: 0.15829172730445862 at epoch: 15 and batch_num: 728\n",
      "Loss of train set: 0.3362155556678772 at epoch: 15 and batch_num: 729\n",
      "Loss of train set: 0.22133387625217438 at epoch: 15 and batch_num: 730\n",
      "Loss of train set: 0.3641156256198883 at epoch: 15 and batch_num: 731\n",
      "Loss of train set: 0.3763978183269501 at epoch: 15 and batch_num: 732\n",
      "Loss of train set: 0.18670012056827545 at epoch: 15 and batch_num: 733\n",
      "Loss of train set: 0.32994866371154785 at epoch: 15 and batch_num: 734\n",
      "Loss of train set: 0.3111943006515503 at epoch: 15 and batch_num: 735\n",
      "Loss of train set: 0.4351522922515869 at epoch: 15 and batch_num: 736\n",
      "Loss of train set: 0.42044007778167725 at epoch: 15 and batch_num: 737\n",
      "Loss of train set: 0.17899775505065918 at epoch: 15 and batch_num: 738\n",
      "Loss of train set: 0.24066060781478882 at epoch: 15 and batch_num: 739\n",
      "Loss of train set: 0.3880963623523712 at epoch: 15 and batch_num: 740\n",
      "Loss of train set: 0.3678169250488281 at epoch: 15 and batch_num: 741\n",
      "Loss of train set: 0.5698124170303345 at epoch: 15 and batch_num: 742\n",
      "Loss of train set: 0.3169328570365906 at epoch: 15 and batch_num: 743\n",
      "Loss of train set: 0.21326354146003723 at epoch: 15 and batch_num: 744\n",
      "Loss of train set: 0.2752414047718048 at epoch: 15 and batch_num: 745\n",
      "Loss of train set: 0.3607094883918762 at epoch: 15 and batch_num: 746\n",
      "Loss of train set: 0.25213682651519775 at epoch: 15 and batch_num: 747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.31632131338119507 at epoch: 15 and batch_num: 748\n",
      "Loss of train set: 0.3299720287322998 at epoch: 15 and batch_num: 749\n",
      "Loss of train set: 0.3049020767211914 at epoch: 15 and batch_num: 750\n",
      "Loss of train set: 0.28534799814224243 at epoch: 15 and batch_num: 751\n",
      "Loss of train set: 0.20378200709819794 at epoch: 15 and batch_num: 752\n",
      "Loss of train set: 0.32084593176841736 at epoch: 15 and batch_num: 753\n",
      "Loss of train set: 0.3351402282714844 at epoch: 15 and batch_num: 754\n",
      "Loss of train set: 0.4067176580429077 at epoch: 15 and batch_num: 755\n",
      "Loss of train set: 0.3547118008136749 at epoch: 15 and batch_num: 756\n",
      "Loss of train set: 0.32023531198501587 at epoch: 15 and batch_num: 757\n",
      "Loss of train set: 0.3751622438430786 at epoch: 15 and batch_num: 758\n",
      "Loss of train set: 0.315324068069458 at epoch: 15 and batch_num: 759\n",
      "Loss of train set: 0.6531216502189636 at epoch: 15 and batch_num: 760\n",
      "Loss of train set: 0.22033993899822235 at epoch: 15 and batch_num: 761\n",
      "Loss of train set: 0.20159870386123657 at epoch: 15 and batch_num: 762\n",
      "Loss of train set: 0.28480100631713867 at epoch: 15 and batch_num: 763\n",
      "Loss of train set: 0.2895681858062744 at epoch: 15 and batch_num: 764\n",
      "Loss of train set: 0.22840847074985504 at epoch: 15 and batch_num: 765\n",
      "Loss of train set: 0.13605454564094543 at epoch: 15 and batch_num: 766\n",
      "Loss of train set: 0.14107519388198853 at epoch: 15 and batch_num: 767\n",
      "Loss of train set: 0.2246936708688736 at epoch: 15 and batch_num: 768\n",
      "Loss of train set: 0.35038846731185913 at epoch: 15 and batch_num: 769\n",
      "Loss of train set: 0.25531303882598877 at epoch: 15 and batch_num: 770\n",
      "Loss of train set: 0.35246995091438293 at epoch: 15 and batch_num: 771\n",
      "Loss of train set: 0.26908236742019653 at epoch: 15 and batch_num: 772\n",
      "Loss of train set: 0.4757169187068939 at epoch: 15 and batch_num: 773\n",
      "Loss of train set: 0.23194755613803864 at epoch: 15 and batch_num: 774\n",
      "Loss of train set: 0.2961975336074829 at epoch: 15 and batch_num: 775\n",
      "Loss of train set: 0.43164128065109253 at epoch: 15 and batch_num: 776\n",
      "Loss of train set: 0.13969245553016663 at epoch: 15 and batch_num: 777\n",
      "Loss of train set: 0.38808125257492065 at epoch: 15 and batch_num: 778\n",
      "Loss of train set: 0.36204856634140015 at epoch: 15 and batch_num: 779\n",
      "Loss of train set: 0.2791043519973755 at epoch: 15 and batch_num: 780\n",
      "Loss of train set: 0.22683927416801453 at epoch: 15 and batch_num: 781\n",
      "Loss of train set: 0.31600767374038696 at epoch: 15 and batch_num: 782\n",
      "Loss of train set: 0.4413796067237854 at epoch: 15 and batch_num: 783\n",
      "Loss of train set: 0.1956174373626709 at epoch: 15 and batch_num: 784\n",
      "Loss of train set: 0.30272504687309265 at epoch: 15 and batch_num: 785\n",
      "Loss of train set: 0.2526823878288269 at epoch: 15 and batch_num: 786\n",
      "Loss of train set: 0.2508925199508667 at epoch: 15 and batch_num: 787\n",
      "Loss of train set: 0.3096620440483093 at epoch: 15 and batch_num: 788\n",
      "Loss of train set: 0.29986846446990967 at epoch: 15 and batch_num: 789\n",
      "Loss of train set: 0.28222233057022095 at epoch: 15 and batch_num: 790\n",
      "Loss of train set: 0.40830355882644653 at epoch: 15 and batch_num: 791\n",
      "Loss of train set: 0.4228711724281311 at epoch: 15 and batch_num: 792\n",
      "Loss of train set: 0.2636081278324127 at epoch: 15 and batch_num: 793\n",
      "Loss of train set: 0.41428864002227783 at epoch: 15 and batch_num: 794\n",
      "Loss of train set: 0.37261366844177246 at epoch: 15 and batch_num: 795\n",
      "Loss of train set: 0.2118743360042572 at epoch: 15 and batch_num: 796\n",
      "Loss of train set: 0.24649396538734436 at epoch: 15 and batch_num: 797\n",
      "Loss of train set: 0.2537176012992859 at epoch: 15 and batch_num: 798\n",
      "Loss of train set: 0.32255232334136963 at epoch: 15 and batch_num: 799\n",
      "Loss of train set: 0.538874089717865 at epoch: 15 and batch_num: 800\n",
      "Loss of train set: 0.29898518323898315 at epoch: 15 and batch_num: 801\n",
      "Loss of train set: 0.10399362444877625 at epoch: 15 and batch_num: 802\n",
      "Loss of train set: 0.3161330223083496 at epoch: 15 and batch_num: 803\n",
      "Loss of train set: 0.3791245222091675 at epoch: 15 and batch_num: 804\n",
      "Loss of train set: 0.2152348756790161 at epoch: 15 and batch_num: 805\n",
      "Loss of train set: 0.3830268979072571 at epoch: 15 and batch_num: 806\n",
      "Loss of train set: 0.3241193890571594 at epoch: 15 and batch_num: 807\n",
      "Loss of train set: 0.24273715913295746 at epoch: 15 and batch_num: 808\n",
      "Loss of train set: 0.4466249942779541 at epoch: 15 and batch_num: 809\n",
      "Loss of train set: 0.164815753698349 at epoch: 15 and batch_num: 810\n",
      "Loss of train set: 0.27561473846435547 at epoch: 15 and batch_num: 811\n",
      "Loss of train set: 0.41637930274009705 at epoch: 15 and batch_num: 812\n",
      "Loss of train set: 0.23061195015907288 at epoch: 15 and batch_num: 813\n",
      "Loss of train set: 0.20981495082378387 at epoch: 15 and batch_num: 814\n",
      "Loss of train set: 0.2694042921066284 at epoch: 15 and batch_num: 815\n",
      "Loss of train set: 0.2794581651687622 at epoch: 15 and batch_num: 816\n",
      "Loss of train set: 0.38342785835266113 at epoch: 15 and batch_num: 817\n",
      "Loss of train set: 0.3152494430541992 at epoch: 15 and batch_num: 818\n",
      "Loss of train set: 0.3015274405479431 at epoch: 15 and batch_num: 819\n",
      "Loss of train set: 0.4047057032585144 at epoch: 15 and batch_num: 820\n",
      "Loss of train set: 0.5145732760429382 at epoch: 15 and batch_num: 821\n",
      "Loss of train set: 0.3488158881664276 at epoch: 15 and batch_num: 822\n",
      "Loss of train set: 0.18783357739448547 at epoch: 15 and batch_num: 823\n",
      "Loss of train set: 0.23420685529708862 at epoch: 15 and batch_num: 824\n",
      "Loss of train set: 0.2393970787525177 at epoch: 15 and batch_num: 825\n",
      "Loss of train set: 0.13185125589370728 at epoch: 15 and batch_num: 826\n",
      "Loss of train set: 0.34587040543556213 at epoch: 15 and batch_num: 827\n",
      "Loss of train set: 0.3123694658279419 at epoch: 15 and batch_num: 828\n",
      "Loss of train set: 0.3042106032371521 at epoch: 15 and batch_num: 829\n",
      "Loss of train set: 0.28416329622268677 at epoch: 15 and batch_num: 830\n",
      "Loss of train set: 0.5025275945663452 at epoch: 15 and batch_num: 831\n",
      "Loss of train set: 0.2206214964389801 at epoch: 15 and batch_num: 832\n",
      "Loss of train set: 0.27748942375183105 at epoch: 15 and batch_num: 833\n",
      "Loss of train set: 0.3720136284828186 at epoch: 15 and batch_num: 834\n",
      "Loss of train set: 0.2895175516605377 at epoch: 15 and batch_num: 835\n",
      "Loss of train set: 0.19543969631195068 at epoch: 15 and batch_num: 836\n",
      "Loss of train set: 0.2620003819465637 at epoch: 15 and batch_num: 837\n",
      "Loss of train set: 0.4739440083503723 at epoch: 15 and batch_num: 838\n",
      "Loss of train set: 0.3216622769832611 at epoch: 15 and batch_num: 839\n",
      "Loss of train set: 0.2715741693973541 at epoch: 15 and batch_num: 840\n",
      "Loss of train set: 0.3791986107826233 at epoch: 15 and batch_num: 841\n",
      "Loss of train set: 0.3227842152118683 at epoch: 15 and batch_num: 842\n",
      "Loss of train set: 0.1492946445941925 at epoch: 15 and batch_num: 843\n",
      "Loss of train set: 0.2879728078842163 at epoch: 15 and batch_num: 844\n",
      "Loss of train set: 0.31617847084999084 at epoch: 15 and batch_num: 845\n",
      "Loss of train set: 0.37610703706741333 at epoch: 15 and batch_num: 846\n",
      "Loss of train set: 0.3552994728088379 at epoch: 15 and batch_num: 847\n",
      "Loss of train set: 0.2678448557853699 at epoch: 15 and batch_num: 848\n",
      "Loss of train set: 0.2811308205127716 at epoch: 15 and batch_num: 849\n",
      "Loss of train set: 0.20007741451263428 at epoch: 15 and batch_num: 850\n",
      "Loss of train set: 0.3367621600627899 at epoch: 15 and batch_num: 851\n",
      "Loss of train set: 0.24207048118114471 at epoch: 15 and batch_num: 852\n",
      "Loss of train set: 0.44610539078712463 at epoch: 15 and batch_num: 853\n",
      "Loss of train set: 0.2245730310678482 at epoch: 15 and batch_num: 854\n",
      "Loss of train set: 0.3283836245536804 at epoch: 15 and batch_num: 855\n",
      "Loss of train set: 0.3980584144592285 at epoch: 15 and batch_num: 856\n",
      "Loss of train set: 0.33261245489120483 at epoch: 15 and batch_num: 857\n",
      "Loss of train set: 0.46236276626586914 at epoch: 15 and batch_num: 858\n",
      "Loss of train set: 0.22490426898002625 at epoch: 15 and batch_num: 859\n",
      "Loss of train set: 0.24596628546714783 at epoch: 15 and batch_num: 860\n",
      "Loss of train set: 0.42392873764038086 at epoch: 15 and batch_num: 861\n",
      "Loss of train set: 0.25802505016326904 at epoch: 15 and batch_num: 862\n",
      "Loss of train set: 0.4123246669769287 at epoch: 15 and batch_num: 863\n",
      "Loss of train set: 0.13953065872192383 at epoch: 15 and batch_num: 864\n",
      "Loss of train set: 0.2310858517885208 at epoch: 15 and batch_num: 865\n",
      "Loss of train set: 0.18996362388134003 at epoch: 15 and batch_num: 866\n",
      "Loss of train set: 0.1818292737007141 at epoch: 15 and batch_num: 867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.24822789430618286 at epoch: 15 and batch_num: 868\n",
      "Loss of train set: 0.389739990234375 at epoch: 15 and batch_num: 869\n",
      "Loss of train set: 0.24287042021751404 at epoch: 15 and batch_num: 870\n",
      "Loss of train set: 0.4891052842140198 at epoch: 15 and batch_num: 871\n",
      "Loss of train set: 0.17706218361854553 at epoch: 15 and batch_num: 872\n",
      "Loss of train set: 0.32565099000930786 at epoch: 15 and batch_num: 873\n",
      "Loss of train set: 0.20454438030719757 at epoch: 15 and batch_num: 874\n",
      "Loss of train set: 0.36270248889923096 at epoch: 15 and batch_num: 875\n",
      "Loss of train set: 0.32558485865592957 at epoch: 15 and batch_num: 876\n",
      "Loss of train set: 0.2891363501548767 at epoch: 15 and batch_num: 877\n",
      "Loss of train set: 0.34519848227500916 at epoch: 15 and batch_num: 878\n",
      "Loss of train set: 0.30569255352020264 at epoch: 15 and batch_num: 879\n",
      "Loss of train set: 0.1339089721441269 at epoch: 15 and batch_num: 880\n",
      "Loss of train set: 0.23677483201026917 at epoch: 15 and batch_num: 881\n",
      "Loss of train set: 0.19735220074653625 at epoch: 15 and batch_num: 882\n",
      "Loss of train set: 0.32744869589805603 at epoch: 15 and batch_num: 883\n",
      "Loss of train set: 0.3819029629230499 at epoch: 15 and batch_num: 884\n",
      "Loss of train set: 0.27754613757133484 at epoch: 15 and batch_num: 885\n",
      "Loss of train set: 0.364480197429657 at epoch: 15 and batch_num: 886\n",
      "Loss of train set: 0.27606692910194397 at epoch: 15 and batch_num: 887\n",
      "Loss of train set: 0.14111429452896118 at epoch: 15 and batch_num: 888\n",
      "Loss of train set: 0.39421746134757996 at epoch: 15 and batch_num: 889\n",
      "Loss of train set: 0.3617570698261261 at epoch: 15 and batch_num: 890\n",
      "Loss of train set: 0.4141717255115509 at epoch: 15 and batch_num: 891\n",
      "Loss of train set: 0.43361806869506836 at epoch: 15 and batch_num: 892\n",
      "Loss of train set: 0.1947416067123413 at epoch: 15 and batch_num: 893\n",
      "Loss of train set: 0.42322972416877747 at epoch: 15 and batch_num: 894\n",
      "Loss of train set: 0.29109734296798706 at epoch: 15 and batch_num: 895\n",
      "Loss of train set: 0.4415760636329651 at epoch: 15 and batch_num: 896\n",
      "Loss of train set: 0.2865478992462158 at epoch: 15 and batch_num: 897\n",
      "Loss of train set: 0.2160196453332901 at epoch: 15 and batch_num: 898\n",
      "Loss of train set: 0.22622336447238922 at epoch: 15 and batch_num: 899\n",
      "Loss of train set: 0.19515660405158997 at epoch: 15 and batch_num: 900\n",
      "Loss of train set: 0.16852836310863495 at epoch: 15 and batch_num: 901\n",
      "Loss of train set: 0.224265918135643 at epoch: 15 and batch_num: 902\n",
      "Loss of train set: 0.3467332720756531 at epoch: 15 and batch_num: 903\n",
      "Loss of train set: 0.33324331045150757 at epoch: 15 and batch_num: 904\n",
      "Loss of train set: 0.41768962144851685 at epoch: 15 and batch_num: 905\n",
      "Loss of train set: 0.28421691060066223 at epoch: 15 and batch_num: 906\n",
      "Loss of train set: 0.39660245180130005 at epoch: 15 and batch_num: 907\n",
      "Loss of train set: 0.4881535470485687 at epoch: 15 and batch_num: 908\n",
      "Loss of train set: 0.3146814703941345 at epoch: 15 and batch_num: 909\n",
      "Loss of train set: 0.33833348751068115 at epoch: 15 and batch_num: 910\n",
      "Loss of train set: 0.4658642113208771 at epoch: 15 and batch_num: 911\n",
      "Loss of train set: 0.3314306437969208 at epoch: 15 and batch_num: 912\n",
      "Loss of train set: 0.3215961456298828 at epoch: 15 and batch_num: 913\n",
      "Loss of train set: 0.34080010652542114 at epoch: 15 and batch_num: 914\n",
      "Loss of train set: 0.1836700588464737 at epoch: 15 and batch_num: 915\n",
      "Loss of train set: 0.23833388090133667 at epoch: 15 and batch_num: 916\n",
      "Loss of train set: 0.318813681602478 at epoch: 15 and batch_num: 917\n",
      "Loss of train set: 0.3308389484882355 at epoch: 15 and batch_num: 918\n",
      "Loss of train set: 0.20058295130729675 at epoch: 15 and batch_num: 919\n",
      "Loss of train set: 0.2151331603527069 at epoch: 15 and batch_num: 920\n",
      "Loss of train set: 0.24823793768882751 at epoch: 15 and batch_num: 921\n",
      "Loss of train set: 0.3978825807571411 at epoch: 15 and batch_num: 922\n",
      "Loss of train set: 0.28087252378463745 at epoch: 15 and batch_num: 923\n",
      "Loss of train set: 0.19056987762451172 at epoch: 15 and batch_num: 924\n",
      "Loss of train set: 0.28773176670074463 at epoch: 15 and batch_num: 925\n",
      "Loss of train set: 0.17236876487731934 at epoch: 15 and batch_num: 926\n",
      "Loss of train set: 0.37022489309310913 at epoch: 15 and batch_num: 927\n",
      "Loss of train set: 0.28387200832366943 at epoch: 15 and batch_num: 928\n",
      "Loss of train set: 0.2555270791053772 at epoch: 15 and batch_num: 929\n",
      "Loss of train set: 0.34044787287712097 at epoch: 15 and batch_num: 930\n",
      "Loss of train set: 0.27593034505844116 at epoch: 15 and batch_num: 931\n",
      "Loss of train set: 0.20959807932376862 at epoch: 15 and batch_num: 932\n",
      "Loss of train set: 0.4148895740509033 at epoch: 15 and batch_num: 933\n",
      "Loss of train set: 0.334133505821228 at epoch: 15 and batch_num: 934\n",
      "Loss of train set: 0.33730870485305786 at epoch: 15 and batch_num: 935\n",
      "Loss of train set: 0.29248589277267456 at epoch: 15 and batch_num: 936\n",
      "Loss of train set: 0.2868759334087372 at epoch: 15 and batch_num: 937\n",
      "Accuracy of train set: 0.8907166666666667\n",
      "Loss of test set: 0.4224315285682678 at epoch: 15 and batch_num: 0\n",
      "Loss of test set: 0.3917163014411926 at epoch: 15 and batch_num: 1\n",
      "Loss of test set: 0.49667662382125854 at epoch: 15 and batch_num: 2\n",
      "Loss of test set: 0.4105900824069977 at epoch: 15 and batch_num: 3\n",
      "Loss of test set: 0.5563521981239319 at epoch: 15 and batch_num: 4\n",
      "Loss of test set: 0.6769305467605591 at epoch: 15 and batch_num: 5\n",
      "Loss of test set: 0.46359577775001526 at epoch: 15 and batch_num: 6\n",
      "Loss of test set: 0.6244670152664185 at epoch: 15 and batch_num: 7\n",
      "Loss of test set: 0.501057505607605 at epoch: 15 and batch_num: 8\n",
      "Loss of test set: 0.5698408484458923 at epoch: 15 and batch_num: 9\n",
      "Loss of test set: 0.39326876401901245 at epoch: 15 and batch_num: 10\n",
      "Loss of test set: 0.3248771131038666 at epoch: 15 and batch_num: 11\n",
      "Loss of test set: 0.44738155603408813 at epoch: 15 and batch_num: 12\n",
      "Loss of test set: 0.3190751373767853 at epoch: 15 and batch_num: 13\n",
      "Loss of test set: 0.38607504963874817 at epoch: 15 and batch_num: 14\n",
      "Loss of test set: 0.7022669911384583 at epoch: 15 and batch_num: 15\n",
      "Loss of test set: 0.5031094551086426 at epoch: 15 and batch_num: 16\n",
      "Loss of test set: 0.294153094291687 at epoch: 15 and batch_num: 17\n",
      "Loss of test set: 0.5682429075241089 at epoch: 15 and batch_num: 18\n",
      "Loss of test set: 0.2979443073272705 at epoch: 15 and batch_num: 19\n",
      "Loss of test set: 0.4517781734466553 at epoch: 15 and batch_num: 20\n",
      "Loss of test set: 0.46878883242607117 at epoch: 15 and batch_num: 21\n",
      "Loss of test set: 0.4326637089252472 at epoch: 15 and batch_num: 22\n",
      "Loss of test set: 0.3093188405036926 at epoch: 15 and batch_num: 23\n",
      "Loss of test set: 0.41644108295440674 at epoch: 15 and batch_num: 24\n",
      "Loss of test set: 0.36720895767211914 at epoch: 15 and batch_num: 25\n",
      "Loss of test set: 0.42685747146606445 at epoch: 15 and batch_num: 26\n",
      "Loss of test set: 0.35392943024635315 at epoch: 15 and batch_num: 27\n",
      "Loss of test set: 0.3832817077636719 at epoch: 15 and batch_num: 28\n",
      "Loss of test set: 0.3789706230163574 at epoch: 15 and batch_num: 29\n",
      "Loss of test set: 0.5259087085723877 at epoch: 15 and batch_num: 30\n",
      "Loss of test set: 0.3873775601387024 at epoch: 15 and batch_num: 31\n",
      "Loss of test set: 0.5492714643478394 at epoch: 15 and batch_num: 32\n",
      "Loss of test set: 0.5087561011314392 at epoch: 15 and batch_num: 33\n",
      "Loss of test set: 0.5754967927932739 at epoch: 15 and batch_num: 34\n",
      "Loss of test set: 0.456900417804718 at epoch: 15 and batch_num: 35\n",
      "Loss of test set: 0.7870794534683228 at epoch: 15 and batch_num: 36\n",
      "Loss of test set: 0.4835570454597473 at epoch: 15 and batch_num: 37\n",
      "Loss of test set: 0.553281307220459 at epoch: 15 and batch_num: 38\n",
      "Loss of test set: 0.7234646081924438 at epoch: 15 and batch_num: 39\n",
      "Loss of test set: 0.65814208984375 at epoch: 15 and batch_num: 40\n",
      "Loss of test set: 0.39096158742904663 at epoch: 15 and batch_num: 41\n",
      "Loss of test set: 0.40733760595321655 at epoch: 15 and batch_num: 42\n",
      "Loss of test set: 0.3978838622570038 at epoch: 15 and batch_num: 43\n",
      "Loss of test set: 0.4191310405731201 at epoch: 15 and batch_num: 44\n",
      "Loss of test set: 0.5536155104637146 at epoch: 15 and batch_num: 45\n",
      "Loss of test set: 0.5033215880393982 at epoch: 15 and batch_num: 46\n",
      "Loss of test set: 0.4258943200111389 at epoch: 15 and batch_num: 47\n",
      "Loss of test set: 0.37397921085357666 at epoch: 15 and batch_num: 48\n",
      "Loss of test set: 0.46640610694885254 at epoch: 15 and batch_num: 49\n",
      "Loss of test set: 0.4019651412963867 at epoch: 15 and batch_num: 50\n",
      "Loss of test set: 0.42325592041015625 at epoch: 15 and batch_num: 51\n",
      "Loss of test set: 0.502702534198761 at epoch: 15 and batch_num: 52\n",
      "Loss of test set: 0.4228588938713074 at epoch: 15 and batch_num: 53\n",
      "Loss of test set: 0.49028635025024414 at epoch: 15 and batch_num: 54\n",
      "Loss of test set: 0.24481600522994995 at epoch: 15 and batch_num: 55\n",
      "Loss of test set: 0.29359105229377747 at epoch: 15 and batch_num: 56\n",
      "Loss of test set: 0.6659846305847168 at epoch: 15 and batch_num: 57\n",
      "Loss of test set: 0.5891715288162231 at epoch: 15 and batch_num: 58\n",
      "Loss of test set: 0.20095163583755493 at epoch: 15 and batch_num: 59\n",
      "Loss of test set: 0.30238157510757446 at epoch: 15 and batch_num: 60\n",
      "Loss of test set: 0.31790584325790405 at epoch: 15 and batch_num: 61\n",
      "Loss of test set: 0.3887123763561249 at epoch: 15 and batch_num: 62\n",
      "Loss of test set: 0.5547672510147095 at epoch: 15 and batch_num: 63\n",
      "Loss of test set: 0.42154860496520996 at epoch: 15 and batch_num: 64\n",
      "Loss of test set: 0.400473415851593 at epoch: 15 and batch_num: 65\n",
      "Loss of test set: 0.5313331484794617 at epoch: 15 and batch_num: 66\n",
      "Loss of test set: 0.43604201078414917 at epoch: 15 and batch_num: 67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.40382686257362366 at epoch: 15 and batch_num: 68\n",
      "Loss of test set: 0.38200870156288147 at epoch: 15 and batch_num: 69\n",
      "Loss of test set: 0.37806111574172974 at epoch: 15 and batch_num: 70\n",
      "Loss of test set: 0.3144441246986389 at epoch: 15 and batch_num: 71\n",
      "Loss of test set: 0.6249487400054932 at epoch: 15 and batch_num: 72\n",
      "Loss of test set: 0.3901793360710144 at epoch: 15 and batch_num: 73\n",
      "Loss of test set: 0.4178847074508667 at epoch: 15 and batch_num: 74\n",
      "Loss of test set: 0.38744819164276123 at epoch: 15 and batch_num: 75\n",
      "Loss of test set: 0.5164232850074768 at epoch: 15 and batch_num: 76\n",
      "Loss of test set: 0.4486792981624603 at epoch: 15 and batch_num: 77\n",
      "Loss of test set: 0.4556131362915039 at epoch: 15 and batch_num: 78\n",
      "Loss of test set: 0.4571078419685364 at epoch: 15 and batch_num: 79\n",
      "Loss of test set: 0.43409988284111023 at epoch: 15 and batch_num: 80\n",
      "Loss of test set: 0.3085065484046936 at epoch: 15 and batch_num: 81\n",
      "Loss of test set: 0.2794305086135864 at epoch: 15 and batch_num: 82\n",
      "Loss of test set: 0.5318756699562073 at epoch: 15 and batch_num: 83\n",
      "Loss of test set: 0.5423210859298706 at epoch: 15 and batch_num: 84\n",
      "Loss of test set: 0.38906702399253845 at epoch: 15 and batch_num: 85\n",
      "Loss of test set: 0.2144693285226822 at epoch: 15 and batch_num: 86\n",
      "Loss of test set: 0.3713221848011017 at epoch: 15 and batch_num: 87\n",
      "Loss of test set: 0.458636075258255 at epoch: 15 and batch_num: 88\n",
      "Loss of test set: 0.2480890452861786 at epoch: 15 and batch_num: 89\n",
      "Loss of test set: 0.3986990451812744 at epoch: 15 and batch_num: 90\n",
      "Loss of test set: 0.31505322456359863 at epoch: 15 and batch_num: 91\n",
      "Loss of test set: 0.5803240537643433 at epoch: 15 and batch_num: 92\n",
      "Loss of test set: 0.25306782126426697 at epoch: 15 and batch_num: 93\n",
      "Loss of test set: 0.4062395691871643 at epoch: 15 and batch_num: 94\n",
      "Loss of test set: 0.2653997540473938 at epoch: 15 and batch_num: 95\n",
      "Loss of test set: 0.5207847952842712 at epoch: 15 and batch_num: 96\n",
      "Loss of test set: 0.28903159499168396 at epoch: 15 and batch_num: 97\n",
      "Loss of test set: 0.4089939594268799 at epoch: 15 and batch_num: 98\n",
      "Loss of test set: 0.33930474519729614 at epoch: 15 and batch_num: 99\n",
      "Loss of test set: 0.3414236307144165 at epoch: 15 and batch_num: 100\n",
      "Loss of test set: 0.8142170906066895 at epoch: 15 and batch_num: 101\n",
      "Loss of test set: 0.4507116675376892 at epoch: 15 and batch_num: 102\n",
      "Loss of test set: 0.5159759521484375 at epoch: 15 and batch_num: 103\n",
      "Loss of test set: 0.26351064443588257 at epoch: 15 and batch_num: 104\n",
      "Loss of test set: 0.4173159897327423 at epoch: 15 and batch_num: 105\n",
      "Loss of test set: 0.18336215615272522 at epoch: 15 and batch_num: 106\n",
      "Loss of test set: 0.440521776676178 at epoch: 15 and batch_num: 107\n",
      "Loss of test set: 0.4795069396495819 at epoch: 15 and batch_num: 108\n",
      "Loss of test set: 0.4739431142807007 at epoch: 15 and batch_num: 109\n",
      "Loss of test set: 0.4379330277442932 at epoch: 15 and batch_num: 110\n",
      "Loss of test set: 0.4119817912578583 at epoch: 15 and batch_num: 111\n",
      "Loss of test set: 0.46835407614707947 at epoch: 15 and batch_num: 112\n",
      "Loss of test set: 0.5435484051704407 at epoch: 15 and batch_num: 113\n",
      "Loss of test set: 0.3916624188423157 at epoch: 15 and batch_num: 114\n",
      "Loss of test set: 0.4784485697746277 at epoch: 15 and batch_num: 115\n",
      "Loss of test set: 0.37715858221054077 at epoch: 15 and batch_num: 116\n",
      "Loss of test set: 0.39105695486068726 at epoch: 15 and batch_num: 117\n",
      "Loss of test set: 0.4867156744003296 at epoch: 15 and batch_num: 118\n",
      "Loss of test set: 0.3472801446914673 at epoch: 15 and batch_num: 119\n",
      "Loss of test set: 0.4962812662124634 at epoch: 15 and batch_num: 120\n",
      "Loss of test set: 0.3519541919231415 at epoch: 15 and batch_num: 121\n",
      "Loss of test set: 0.4444483518600464 at epoch: 15 and batch_num: 122\n",
      "Loss of test set: 0.5274607539176941 at epoch: 15 and batch_num: 123\n",
      "Loss of test set: 0.49522894620895386 at epoch: 15 and batch_num: 124\n",
      "Loss of test set: 0.39285603165626526 at epoch: 15 and batch_num: 125\n",
      "Loss of test set: 0.40151894092559814 at epoch: 15 and batch_num: 126\n",
      "Loss of test set: 0.5915523767471313 at epoch: 15 and batch_num: 127\n",
      "Loss of test set: 0.4437825381755829 at epoch: 15 and batch_num: 128\n",
      "Loss of test set: 0.4216077923774719 at epoch: 15 and batch_num: 129\n",
      "Loss of test set: 0.42284637689590454 at epoch: 15 and batch_num: 130\n",
      "Loss of test set: 0.3821355104446411 at epoch: 15 and batch_num: 131\n",
      "Loss of test set: 0.4033395051956177 at epoch: 15 and batch_num: 132\n",
      "Loss of test set: 0.43212461471557617 at epoch: 15 and batch_num: 133\n",
      "Loss of test set: 0.5435553193092346 at epoch: 15 and batch_num: 134\n",
      "Loss of test set: 0.5587369203567505 at epoch: 15 and batch_num: 135\n",
      "Loss of test set: 0.5936182737350464 at epoch: 15 and batch_num: 136\n",
      "Loss of test set: 0.4943751394748688 at epoch: 15 and batch_num: 137\n",
      "Loss of test set: 0.4666711688041687 at epoch: 15 and batch_num: 138\n",
      "Loss of test set: 0.2985527515411377 at epoch: 15 and batch_num: 139\n",
      "Loss of test set: 0.6018632650375366 at epoch: 15 and batch_num: 140\n",
      "Loss of test set: 0.34969547390937805 at epoch: 15 and batch_num: 141\n",
      "Loss of test set: 0.29442137479782104 at epoch: 15 and batch_num: 142\n",
      "Loss of test set: 0.5628252029418945 at epoch: 15 and batch_num: 143\n",
      "Loss of test set: 0.4101956784725189 at epoch: 15 and batch_num: 144\n",
      "Loss of test set: 0.29770487546920776 at epoch: 15 and batch_num: 145\n",
      "Loss of test set: 0.4991271495819092 at epoch: 15 and batch_num: 146\n",
      "Loss of test set: 0.25570183992385864 at epoch: 15 and batch_num: 147\n",
      "Loss of test set: 0.42208266258239746 at epoch: 15 and batch_num: 148\n",
      "Loss of test set: 0.34699931740760803 at epoch: 15 and batch_num: 149\n",
      "Loss of test set: 0.28082218766212463 at epoch: 15 and batch_num: 150\n",
      "Loss of test set: 0.3560928702354431 at epoch: 15 and batch_num: 151\n",
      "Loss of test set: 0.29332250356674194 at epoch: 15 and batch_num: 152\n",
      "Loss of test set: 0.5416162610054016 at epoch: 15 and batch_num: 153\n",
      "Loss of test set: 0.6353066563606262 at epoch: 15 and batch_num: 154\n",
      "Loss of test set: 0.3321491479873657 at epoch: 15 and batch_num: 155\n",
      "Loss of test set: 1.139366865158081 at epoch: 15 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8395\n",
      "Loss of train set: 0.22763212025165558 at epoch: 16 and batch_num: 0\n",
      "Loss of train set: 0.15033236145973206 at epoch: 16 and batch_num: 1\n",
      "Loss of train set: 0.32233601808547974 at epoch: 16 and batch_num: 2\n",
      "Loss of train set: 0.21020540595054626 at epoch: 16 and batch_num: 3\n",
      "Loss of train set: 0.363402396440506 at epoch: 16 and batch_num: 4\n",
      "Loss of train set: 0.24675914645195007 at epoch: 16 and batch_num: 5\n",
      "Loss of train set: 0.32714107632637024 at epoch: 16 and batch_num: 6\n",
      "Loss of train set: 0.2813754379749298 at epoch: 16 and batch_num: 7\n",
      "Loss of train set: 0.19603481888771057 at epoch: 16 and batch_num: 8\n",
      "Loss of train set: 0.26462751626968384 at epoch: 16 and batch_num: 9\n",
      "Loss of train set: 0.26711034774780273 at epoch: 16 and batch_num: 10\n",
      "Loss of train set: 0.49102675914764404 at epoch: 16 and batch_num: 11\n",
      "Loss of train set: 0.3794877827167511 at epoch: 16 and batch_num: 12\n",
      "Loss of train set: 0.38755613565444946 at epoch: 16 and batch_num: 13\n",
      "Loss of train set: 0.2629418671131134 at epoch: 16 and batch_num: 14\n",
      "Loss of train set: 0.4524998068809509 at epoch: 16 and batch_num: 15\n",
      "Loss of train set: 0.1710374355316162 at epoch: 16 and batch_num: 16\n",
      "Loss of train set: 0.2887973189353943 at epoch: 16 and batch_num: 17\n",
      "Loss of train set: 0.27936333417892456 at epoch: 16 and batch_num: 18\n",
      "Loss of train set: 0.31267112493515015 at epoch: 16 and batch_num: 19\n",
      "Loss of train set: 0.2787601351737976 at epoch: 16 and batch_num: 20\n",
      "Loss of train set: 0.4575497508049011 at epoch: 16 and batch_num: 21\n",
      "Loss of train set: 0.3368035554885864 at epoch: 16 and batch_num: 22\n",
      "Loss of train set: 0.589583158493042 at epoch: 16 and batch_num: 23\n",
      "Loss of train set: 0.3418882489204407 at epoch: 16 and batch_num: 24\n",
      "Loss of train set: 0.3029351234436035 at epoch: 16 and batch_num: 25\n",
      "Loss of train set: 0.4796987771987915 at epoch: 16 and batch_num: 26\n",
      "Loss of train set: 0.43506112694740295 at epoch: 16 and batch_num: 27\n",
      "Loss of train set: 0.3875916004180908 at epoch: 16 and batch_num: 28\n",
      "Loss of train set: 0.3367740213871002 at epoch: 16 and batch_num: 29\n",
      "Loss of train set: 0.2998664975166321 at epoch: 16 and batch_num: 30\n",
      "Loss of train set: 0.37364357709884644 at epoch: 16 and batch_num: 31\n",
      "Loss of train set: 0.27052372694015503 at epoch: 16 and batch_num: 32\n",
      "Loss of train set: 0.3374500572681427 at epoch: 16 and batch_num: 33\n",
      "Loss of train set: 0.22674143314361572 at epoch: 16 and batch_num: 34\n",
      "Loss of train set: 0.18137939274311066 at epoch: 16 and batch_num: 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3066211938858032 at epoch: 16 and batch_num: 36\n",
      "Loss of train set: 0.417452871799469 at epoch: 16 and batch_num: 37\n",
      "Loss of train set: 0.13477155566215515 at epoch: 16 and batch_num: 38\n",
      "Loss of train set: 0.24601596593856812 at epoch: 16 and batch_num: 39\n",
      "Loss of train set: 0.319255530834198 at epoch: 16 and batch_num: 40\n",
      "Loss of train set: 0.3080602288246155 at epoch: 16 and batch_num: 41\n",
      "Loss of train set: 0.20202136039733887 at epoch: 16 and batch_num: 42\n",
      "Loss of train set: 0.37876319885253906 at epoch: 16 and batch_num: 43\n",
      "Loss of train set: 0.2932990789413452 at epoch: 16 and batch_num: 44\n",
      "Loss of train set: 0.28828129172325134 at epoch: 16 and batch_num: 45\n",
      "Loss of train set: 0.2628627121448517 at epoch: 16 and batch_num: 46\n",
      "Loss of train set: 0.47307416796684265 at epoch: 16 and batch_num: 47\n",
      "Loss of train set: 0.2322998195886612 at epoch: 16 and batch_num: 48\n",
      "Loss of train set: 0.265896737575531 at epoch: 16 and batch_num: 49\n",
      "Loss of train set: 0.22312042117118835 at epoch: 16 and batch_num: 50\n",
      "Loss of train set: 0.2812609374523163 at epoch: 16 and batch_num: 51\n",
      "Loss of train set: 0.13902482390403748 at epoch: 16 and batch_num: 52\n",
      "Loss of train set: 0.3247023820877075 at epoch: 16 and batch_num: 53\n",
      "Loss of train set: 0.27078789472579956 at epoch: 16 and batch_num: 54\n",
      "Loss of train set: 0.2500927448272705 at epoch: 16 and batch_num: 55\n",
      "Loss of train set: 0.40286868810653687 at epoch: 16 and batch_num: 56\n",
      "Loss of train set: 0.26107218861579895 at epoch: 16 and batch_num: 57\n",
      "Loss of train set: 0.26261597871780396 at epoch: 16 and batch_num: 58\n",
      "Loss of train set: 0.3498985171318054 at epoch: 16 and batch_num: 59\n",
      "Loss of train set: 0.1803261935710907 at epoch: 16 and batch_num: 60\n",
      "Loss of train set: 0.47566017508506775 at epoch: 16 and batch_num: 61\n",
      "Loss of train set: 0.42847558856010437 at epoch: 16 and batch_num: 62\n",
      "Loss of train set: 0.440377414226532 at epoch: 16 and batch_num: 63\n",
      "Loss of train set: 0.32078975439071655 at epoch: 16 and batch_num: 64\n",
      "Loss of train set: 0.26199278235435486 at epoch: 16 and batch_num: 65\n",
      "Loss of train set: 0.45845577120780945 at epoch: 16 and batch_num: 66\n",
      "Loss of train set: 0.4136373996734619 at epoch: 16 and batch_num: 67\n",
      "Loss of train set: 0.4237750768661499 at epoch: 16 and batch_num: 68\n",
      "Loss of train set: 0.3775075376033783 at epoch: 16 and batch_num: 69\n",
      "Loss of train set: 0.27436041831970215 at epoch: 16 and batch_num: 70\n",
      "Loss of train set: 0.21112936735153198 at epoch: 16 and batch_num: 71\n",
      "Loss of train set: 0.34671902656555176 at epoch: 16 and batch_num: 72\n",
      "Loss of train set: 0.3056500554084778 at epoch: 16 and batch_num: 73\n",
      "Loss of train set: 0.24375450611114502 at epoch: 16 and batch_num: 74\n",
      "Loss of train set: 0.2793678641319275 at epoch: 16 and batch_num: 75\n",
      "Loss of train set: 0.28575029969215393 at epoch: 16 and batch_num: 76\n",
      "Loss of train set: 0.40491175651550293 at epoch: 16 and batch_num: 77\n",
      "Loss of train set: 0.2076210081577301 at epoch: 16 and batch_num: 78\n",
      "Loss of train set: 0.18180625140666962 at epoch: 16 and batch_num: 79\n",
      "Loss of train set: 0.30212652683258057 at epoch: 16 and batch_num: 80\n",
      "Loss of train set: 0.19669947028160095 at epoch: 16 and batch_num: 81\n",
      "Loss of train set: 0.28609374165534973 at epoch: 16 and batch_num: 82\n",
      "Loss of train set: 0.22808122634887695 at epoch: 16 and batch_num: 83\n",
      "Loss of train set: 0.26140445470809937 at epoch: 16 and batch_num: 84\n",
      "Loss of train set: 0.2796822190284729 at epoch: 16 and batch_num: 85\n",
      "Loss of train set: 0.19123709201812744 at epoch: 16 and batch_num: 86\n",
      "Loss of train set: 0.20326310396194458 at epoch: 16 and batch_num: 87\n",
      "Loss of train set: 0.4425520896911621 at epoch: 16 and batch_num: 88\n",
      "Loss of train set: 0.26802659034729004 at epoch: 16 and batch_num: 89\n",
      "Loss of train set: 0.10351104289293289 at epoch: 16 and batch_num: 90\n",
      "Loss of train set: 0.30964839458465576 at epoch: 16 and batch_num: 91\n",
      "Loss of train set: 0.3096444606781006 at epoch: 16 and batch_num: 92\n",
      "Loss of train set: 0.18892179429531097 at epoch: 16 and batch_num: 93\n",
      "Loss of train set: 0.3594052493572235 at epoch: 16 and batch_num: 94\n",
      "Loss of train set: 0.4404759407043457 at epoch: 16 and batch_num: 95\n",
      "Loss of train set: 0.22927291691303253 at epoch: 16 and batch_num: 96\n",
      "Loss of train set: 0.26840707659721375 at epoch: 16 and batch_num: 97\n",
      "Loss of train set: 0.33475369215011597 at epoch: 16 and batch_num: 98\n",
      "Loss of train set: 0.36414623260498047 at epoch: 16 and batch_num: 99\n",
      "Loss of train set: 0.2883819341659546 at epoch: 16 and batch_num: 100\n",
      "Loss of train set: 0.2510323226451874 at epoch: 16 and batch_num: 101\n",
      "Loss of train set: 0.3322417438030243 at epoch: 16 and batch_num: 102\n",
      "Loss of train set: 0.2008010298013687 at epoch: 16 and batch_num: 103\n",
      "Loss of train set: 0.20483160018920898 at epoch: 16 and batch_num: 104\n",
      "Loss of train set: 0.2921660244464874 at epoch: 16 and batch_num: 105\n",
      "Loss of train set: 0.32728761434555054 at epoch: 16 and batch_num: 106\n",
      "Loss of train set: 0.3749794065952301 at epoch: 16 and batch_num: 107\n",
      "Loss of train set: 0.34697163105010986 at epoch: 16 and batch_num: 108\n",
      "Loss of train set: 0.4428872764110565 at epoch: 16 and batch_num: 109\n",
      "Loss of train set: 0.3662143349647522 at epoch: 16 and batch_num: 110\n",
      "Loss of train set: 0.3063012361526489 at epoch: 16 and batch_num: 111\n",
      "Loss of train set: 0.21450193226337433 at epoch: 16 and batch_num: 112\n",
      "Loss of train set: 0.19525136053562164 at epoch: 16 and batch_num: 113\n",
      "Loss of train set: 0.20519885420799255 at epoch: 16 and batch_num: 114\n",
      "Loss of train set: 0.18017075955867767 at epoch: 16 and batch_num: 115\n",
      "Loss of train set: 0.19059854745864868 at epoch: 16 and batch_num: 116\n",
      "Loss of train set: 0.22915829718112946 at epoch: 16 and batch_num: 117\n",
      "Loss of train set: 0.24634391069412231 at epoch: 16 and batch_num: 118\n",
      "Loss of train set: 0.31995630264282227 at epoch: 16 and batch_num: 119\n",
      "Loss of train set: 0.4110521376132965 at epoch: 16 and batch_num: 120\n",
      "Loss of train set: 0.3011665344238281 at epoch: 16 and batch_num: 121\n",
      "Loss of train set: 0.17716392874717712 at epoch: 16 and batch_num: 122\n",
      "Loss of train set: 0.3667103052139282 at epoch: 16 and batch_num: 123\n",
      "Loss of train set: 0.3468180298805237 at epoch: 16 and batch_num: 124\n",
      "Loss of train set: 0.38156959414482117 at epoch: 16 and batch_num: 125\n",
      "Loss of train set: 0.3492075204849243 at epoch: 16 and batch_num: 126\n",
      "Loss of train set: 0.304313600063324 at epoch: 16 and batch_num: 127\n",
      "Loss of train set: 0.25371071696281433 at epoch: 16 and batch_num: 128\n",
      "Loss of train set: 0.32645875215530396 at epoch: 16 and batch_num: 129\n",
      "Loss of train set: 0.15689459443092346 at epoch: 16 and batch_num: 130\n",
      "Loss of train set: 0.33988162875175476 at epoch: 16 and batch_num: 131\n",
      "Loss of train set: 0.26234179735183716 at epoch: 16 and batch_num: 132\n",
      "Loss of train set: 0.27576401829719543 at epoch: 16 and batch_num: 133\n",
      "Loss of train set: 0.22875000536441803 at epoch: 16 and batch_num: 134\n",
      "Loss of train set: 0.24876847863197327 at epoch: 16 and batch_num: 135\n",
      "Loss of train set: 0.25908559560775757 at epoch: 16 and batch_num: 136\n",
      "Loss of train set: 0.24037201702594757 at epoch: 16 and batch_num: 137\n",
      "Loss of train set: 0.31714195013046265 at epoch: 16 and batch_num: 138\n",
      "Loss of train set: 0.3039434552192688 at epoch: 16 and batch_num: 139\n",
      "Loss of train set: 0.34144580364227295 at epoch: 16 and batch_num: 140\n",
      "Loss of train set: 0.2583848834037781 at epoch: 16 and batch_num: 141\n",
      "Loss of train set: 0.239824116230011 at epoch: 16 and batch_num: 142\n",
      "Loss of train set: 0.35391050577163696 at epoch: 16 and batch_num: 143\n",
      "Loss of train set: 0.14757461845874786 at epoch: 16 and batch_num: 144\n",
      "Loss of train set: 0.28342124819755554 at epoch: 16 and batch_num: 145\n",
      "Loss of train set: 0.30704551935195923 at epoch: 16 and batch_num: 146\n",
      "Loss of train set: 0.37670356035232544 at epoch: 16 and batch_num: 147\n",
      "Loss of train set: 0.38750916719436646 at epoch: 16 and batch_num: 148\n",
      "Loss of train set: 0.3291066288948059 at epoch: 16 and batch_num: 149\n",
      "Loss of train set: 0.4687119722366333 at epoch: 16 and batch_num: 150\n",
      "Loss of train set: 0.09305182844400406 at epoch: 16 and batch_num: 151\n",
      "Loss of train set: 0.28677308559417725 at epoch: 16 and batch_num: 152\n",
      "Loss of train set: 0.18405160307884216 at epoch: 16 and batch_num: 153\n",
      "Loss of train set: 0.33208930492401123 at epoch: 16 and batch_num: 154\n",
      "Loss of train set: 0.39527392387390137 at epoch: 16 and batch_num: 155\n",
      "Loss of train set: 0.27096712589263916 at epoch: 16 and batch_num: 156\n",
      "Loss of train set: 0.36504045128822327 at epoch: 16 and batch_num: 157\n",
      "Loss of train set: 0.37619495391845703 at epoch: 16 and batch_num: 158\n",
      "Loss of train set: 0.16238614916801453 at epoch: 16 and batch_num: 159\n",
      "Loss of train set: 0.18908381462097168 at epoch: 16 and batch_num: 160\n",
      "Loss of train set: 0.31473201513290405 at epoch: 16 and batch_num: 161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.18435639142990112 at epoch: 16 and batch_num: 162\n",
      "Loss of train set: 0.256590336561203 at epoch: 16 and batch_num: 163\n",
      "Loss of train set: 0.27550119161605835 at epoch: 16 and batch_num: 164\n",
      "Loss of train set: 0.19098582863807678 at epoch: 16 and batch_num: 165\n",
      "Loss of train set: 0.26046478748321533 at epoch: 16 and batch_num: 166\n",
      "Loss of train set: 0.16873495280742645 at epoch: 16 and batch_num: 167\n",
      "Loss of train set: 0.17668431997299194 at epoch: 16 and batch_num: 168\n",
      "Loss of train set: 0.21840566396713257 at epoch: 16 and batch_num: 169\n",
      "Loss of train set: 0.38996583223342896 at epoch: 16 and batch_num: 170\n",
      "Loss of train set: 0.27561256289482117 at epoch: 16 and batch_num: 171\n",
      "Loss of train set: 0.24262772500514984 at epoch: 16 and batch_num: 172\n",
      "Loss of train set: 0.23627668619155884 at epoch: 16 and batch_num: 173\n",
      "Loss of train set: 0.3339056968688965 at epoch: 16 and batch_num: 174\n",
      "Loss of train set: 0.2978874146938324 at epoch: 16 and batch_num: 175\n",
      "Loss of train set: 0.30497804284095764 at epoch: 16 and batch_num: 176\n",
      "Loss of train set: 0.28895699977874756 at epoch: 16 and batch_num: 177\n",
      "Loss of train set: 0.24924977123737335 at epoch: 16 and batch_num: 178\n",
      "Loss of train set: 0.38109564781188965 at epoch: 16 and batch_num: 179\n",
      "Loss of train set: 0.36145472526550293 at epoch: 16 and batch_num: 180\n",
      "Loss of train set: 0.4751499891281128 at epoch: 16 and batch_num: 181\n",
      "Loss of train set: 0.3282274007797241 at epoch: 16 and batch_num: 182\n",
      "Loss of train set: 0.21507379412651062 at epoch: 16 and batch_num: 183\n",
      "Loss of train set: 0.22830168902873993 at epoch: 16 and batch_num: 184\n",
      "Loss of train set: 0.22904349863529205 at epoch: 16 and batch_num: 185\n",
      "Loss of train set: 0.27580052614212036 at epoch: 16 and batch_num: 186\n",
      "Loss of train set: 0.4366474151611328 at epoch: 16 and batch_num: 187\n",
      "Loss of train set: 0.23488998413085938 at epoch: 16 and batch_num: 188\n",
      "Loss of train set: 0.3486987352371216 at epoch: 16 and batch_num: 189\n",
      "Loss of train set: 0.4809401035308838 at epoch: 16 and batch_num: 190\n",
      "Loss of train set: 0.44137877225875854 at epoch: 16 and batch_num: 191\n",
      "Loss of train set: 0.3391569256782532 at epoch: 16 and batch_num: 192\n",
      "Loss of train set: 0.2402953952550888 at epoch: 16 and batch_num: 193\n",
      "Loss of train set: 0.22998887300491333 at epoch: 16 and batch_num: 194\n",
      "Loss of train set: 0.27381908893585205 at epoch: 16 and batch_num: 195\n",
      "Loss of train set: 0.31314539909362793 at epoch: 16 and batch_num: 196\n",
      "Loss of train set: 0.29513850808143616 at epoch: 16 and batch_num: 197\n",
      "Loss of train set: 0.24366414546966553 at epoch: 16 and batch_num: 198\n",
      "Loss of train set: 0.23722296953201294 at epoch: 16 and batch_num: 199\n",
      "Loss of train set: 0.19913874566555023 at epoch: 16 and batch_num: 200\n",
      "Loss of train set: 0.2633110284805298 at epoch: 16 and batch_num: 201\n",
      "Loss of train set: 0.6017510890960693 at epoch: 16 and batch_num: 202\n",
      "Loss of train set: 0.4859071373939514 at epoch: 16 and batch_num: 203\n",
      "Loss of train set: 0.1970716416835785 at epoch: 16 and batch_num: 204\n",
      "Loss of train set: 0.32090383768081665 at epoch: 16 and batch_num: 205\n",
      "Loss of train set: 0.26188915967941284 at epoch: 16 and batch_num: 206\n",
      "Loss of train set: 0.3017197847366333 at epoch: 16 and batch_num: 207\n",
      "Loss of train set: 0.37729668617248535 at epoch: 16 and batch_num: 208\n",
      "Loss of train set: 0.2513483464717865 at epoch: 16 and batch_num: 209\n",
      "Loss of train set: 0.15762579441070557 at epoch: 16 and batch_num: 210\n",
      "Loss of train set: 0.2783491611480713 at epoch: 16 and batch_num: 211\n",
      "Loss of train set: 0.21646898984909058 at epoch: 16 and batch_num: 212\n",
      "Loss of train set: 0.3059723675251007 at epoch: 16 and batch_num: 213\n",
      "Loss of train set: 0.33627086877822876 at epoch: 16 and batch_num: 214\n",
      "Loss of train set: 0.3685767352581024 at epoch: 16 and batch_num: 215\n",
      "Loss of train set: 0.35390496253967285 at epoch: 16 and batch_num: 216\n",
      "Loss of train set: 0.2629924416542053 at epoch: 16 and batch_num: 217\n",
      "Loss of train set: 0.26309332251548767 at epoch: 16 and batch_num: 218\n",
      "Loss of train set: 0.3212255835533142 at epoch: 16 and batch_num: 219\n",
      "Loss of train set: 0.4405098557472229 at epoch: 16 and batch_num: 220\n",
      "Loss of train set: 0.14968955516815186 at epoch: 16 and batch_num: 221\n",
      "Loss of train set: 0.21005265414714813 at epoch: 16 and batch_num: 222\n",
      "Loss of train set: 0.2828018367290497 at epoch: 16 and batch_num: 223\n",
      "Loss of train set: 0.23495253920555115 at epoch: 16 and batch_num: 224\n",
      "Loss of train set: 0.21504655480384827 at epoch: 16 and batch_num: 225\n",
      "Loss of train set: 0.3729957938194275 at epoch: 16 and batch_num: 226\n",
      "Loss of train set: 0.2729797661304474 at epoch: 16 and batch_num: 227\n",
      "Loss of train set: 0.2967511713504791 at epoch: 16 and batch_num: 228\n",
      "Loss of train set: 0.2909512519836426 at epoch: 16 and batch_num: 229\n",
      "Loss of train set: 0.1974930763244629 at epoch: 16 and batch_num: 230\n",
      "Loss of train set: 0.27713561058044434 at epoch: 16 and batch_num: 231\n",
      "Loss of train set: 0.4142839312553406 at epoch: 16 and batch_num: 232\n",
      "Loss of train set: 0.5153034925460815 at epoch: 16 and batch_num: 233\n",
      "Loss of train set: 0.3304649889469147 at epoch: 16 and batch_num: 234\n",
      "Loss of train set: 0.3794543743133545 at epoch: 16 and batch_num: 235\n",
      "Loss of train set: 0.3023983836174011 at epoch: 16 and batch_num: 236\n",
      "Loss of train set: 0.22797884047031403 at epoch: 16 and batch_num: 237\n",
      "Loss of train set: 0.22314274311065674 at epoch: 16 and batch_num: 238\n",
      "Loss of train set: 0.20966173708438873 at epoch: 16 and batch_num: 239\n",
      "Loss of train set: 0.24386678636074066 at epoch: 16 and batch_num: 240\n",
      "Loss of train set: 0.31732216477394104 at epoch: 16 and batch_num: 241\n",
      "Loss of train set: 0.29285240173339844 at epoch: 16 and batch_num: 242\n",
      "Loss of train set: 0.3405682444572449 at epoch: 16 and batch_num: 243\n",
      "Loss of train set: 0.3735812306404114 at epoch: 16 and batch_num: 244\n",
      "Loss of train set: 0.24734556674957275 at epoch: 16 and batch_num: 245\n",
      "Loss of train set: 0.5054452419281006 at epoch: 16 and batch_num: 246\n",
      "Loss of train set: 0.27030229568481445 at epoch: 16 and batch_num: 247\n",
      "Loss of train set: 0.2690320611000061 at epoch: 16 and batch_num: 248\n",
      "Loss of train set: 0.4214240312576294 at epoch: 16 and batch_num: 249\n",
      "Loss of train set: 0.34461408853530884 at epoch: 16 and batch_num: 250\n",
      "Loss of train set: 0.23861145973205566 at epoch: 16 and batch_num: 251\n",
      "Loss of train set: 0.26912349462509155 at epoch: 16 and batch_num: 252\n",
      "Loss of train set: 0.3058675527572632 at epoch: 16 and batch_num: 253\n",
      "Loss of train set: 0.13410837948322296 at epoch: 16 and batch_num: 254\n",
      "Loss of train set: 0.1261785626411438 at epoch: 16 and batch_num: 255\n",
      "Loss of train set: 0.16245663166046143 at epoch: 16 and batch_num: 256\n",
      "Loss of train set: 0.42365914583206177 at epoch: 16 and batch_num: 257\n",
      "Loss of train set: 0.3155633211135864 at epoch: 16 and batch_num: 258\n",
      "Loss of train set: 0.3083101511001587 at epoch: 16 and batch_num: 259\n",
      "Loss of train set: 0.2450900673866272 at epoch: 16 and batch_num: 260\n",
      "Loss of train set: 0.3164259195327759 at epoch: 16 and batch_num: 261\n",
      "Loss of train set: 0.29249247908592224 at epoch: 16 and batch_num: 262\n",
      "Loss of train set: 0.2815771996974945 at epoch: 16 and batch_num: 263\n",
      "Loss of train set: 0.2956315875053406 at epoch: 16 and batch_num: 264\n",
      "Loss of train set: 0.36456412076950073 at epoch: 16 and batch_num: 265\n",
      "Loss of train set: 0.25621968507766724 at epoch: 16 and batch_num: 266\n",
      "Loss of train set: 0.19573651254177094 at epoch: 16 and batch_num: 267\n",
      "Loss of train set: 0.24110373854637146 at epoch: 16 and batch_num: 268\n",
      "Loss of train set: 0.25257372856140137 at epoch: 16 and batch_num: 269\n",
      "Loss of train set: 0.306922972202301 at epoch: 16 and batch_num: 270\n",
      "Loss of train set: 0.29788294434547424 at epoch: 16 and batch_num: 271\n",
      "Loss of train set: 0.37469056248664856 at epoch: 16 and batch_num: 272\n",
      "Loss of train set: 0.15615373849868774 at epoch: 16 and batch_num: 273\n",
      "Loss of train set: 0.396097332239151 at epoch: 16 and batch_num: 274\n",
      "Loss of train set: 0.1945888102054596 at epoch: 16 and batch_num: 275\n",
      "Loss of train set: 0.2912597060203552 at epoch: 16 and batch_num: 276\n",
      "Loss of train set: 0.21424198150634766 at epoch: 16 and batch_num: 277\n",
      "Loss of train set: 0.24139198660850525 at epoch: 16 and batch_num: 278\n",
      "Loss of train set: 0.1456611007452011 at epoch: 16 and batch_num: 279\n",
      "Loss of train set: 0.3604034185409546 at epoch: 16 and batch_num: 280\n",
      "Loss of train set: 0.3348254859447479 at epoch: 16 and batch_num: 281\n",
      "Loss of train set: 0.3293952941894531 at epoch: 16 and batch_num: 282\n",
      "Loss of train set: 0.33697310090065 at epoch: 16 and batch_num: 283\n",
      "Loss of train set: 0.2620967626571655 at epoch: 16 and batch_num: 284\n",
      "Loss of train set: 0.33360207080841064 at epoch: 16 and batch_num: 285\n",
      "Loss of train set: 0.47514623403549194 at epoch: 16 and batch_num: 286\n",
      "Loss of train set: 0.20073673129081726 at epoch: 16 and batch_num: 287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.19472737610340118 at epoch: 16 and batch_num: 288\n",
      "Loss of train set: 0.44286048412323 at epoch: 16 and batch_num: 289\n",
      "Loss of train set: 0.19014085829257965 at epoch: 16 and batch_num: 290\n",
      "Loss of train set: 0.20843328535556793 at epoch: 16 and batch_num: 291\n",
      "Loss of train set: 0.26406973600387573 at epoch: 16 and batch_num: 292\n",
      "Loss of train set: 0.4107261300086975 at epoch: 16 and batch_num: 293\n",
      "Loss of train set: 0.1766301989555359 at epoch: 16 and batch_num: 294\n",
      "Loss of train set: 0.27872228622436523 at epoch: 16 and batch_num: 295\n",
      "Loss of train set: 0.14474347233772278 at epoch: 16 and batch_num: 296\n",
      "Loss of train set: 0.2970713973045349 at epoch: 16 and batch_num: 297\n",
      "Loss of train set: 0.2948317229747772 at epoch: 16 and batch_num: 298\n",
      "Loss of train set: 0.2921716272830963 at epoch: 16 and batch_num: 299\n",
      "Loss of train set: 0.28672775626182556 at epoch: 16 and batch_num: 300\n",
      "Loss of train set: 0.24732650816440582 at epoch: 16 and batch_num: 301\n",
      "Loss of train set: 0.3203878700733185 at epoch: 16 and batch_num: 302\n",
      "Loss of train set: 0.33668267726898193 at epoch: 16 and batch_num: 303\n",
      "Loss of train set: 0.2530668079853058 at epoch: 16 and batch_num: 304\n",
      "Loss of train set: 0.4448017179965973 at epoch: 16 and batch_num: 305\n",
      "Loss of train set: 0.2692340910434723 at epoch: 16 and batch_num: 306\n",
      "Loss of train set: 0.241539865732193 at epoch: 16 and batch_num: 307\n",
      "Loss of train set: 0.28023773431777954 at epoch: 16 and batch_num: 308\n",
      "Loss of train set: 0.4149380922317505 at epoch: 16 and batch_num: 309\n",
      "Loss of train set: 0.3842388987541199 at epoch: 16 and batch_num: 310\n",
      "Loss of train set: 0.19131731986999512 at epoch: 16 and batch_num: 311\n",
      "Loss of train set: 0.25551730394363403 at epoch: 16 and batch_num: 312\n",
      "Loss of train set: 0.46139514446258545 at epoch: 16 and batch_num: 313\n",
      "Loss of train set: 0.33699125051498413 at epoch: 16 and batch_num: 314\n",
      "Loss of train set: 0.37967365980148315 at epoch: 16 and batch_num: 315\n",
      "Loss of train set: 0.24677202105522156 at epoch: 16 and batch_num: 316\n",
      "Loss of train set: 0.277834415435791 at epoch: 16 and batch_num: 317\n",
      "Loss of train set: 0.30443328619003296 at epoch: 16 and batch_num: 318\n",
      "Loss of train set: 0.37541481852531433 at epoch: 16 and batch_num: 319\n",
      "Loss of train set: 0.3045899271965027 at epoch: 16 and batch_num: 320\n",
      "Loss of train set: 0.2833929657936096 at epoch: 16 and batch_num: 321\n",
      "Loss of train set: 0.21632049977779388 at epoch: 16 and batch_num: 322\n",
      "Loss of train set: 0.2814595401287079 at epoch: 16 and batch_num: 323\n",
      "Loss of train set: 0.3157230615615845 at epoch: 16 and batch_num: 324\n",
      "Loss of train set: 0.1152852326631546 at epoch: 16 and batch_num: 325\n",
      "Loss of train set: 0.23413267731666565 at epoch: 16 and batch_num: 326\n",
      "Loss of train set: 0.5390148758888245 at epoch: 16 and batch_num: 327\n",
      "Loss of train set: 0.314412921667099 at epoch: 16 and batch_num: 328\n",
      "Loss of train set: 0.35187840461730957 at epoch: 16 and batch_num: 329\n",
      "Loss of train set: 0.27191483974456787 at epoch: 16 and batch_num: 330\n",
      "Loss of train set: 0.28769317269325256 at epoch: 16 and batch_num: 331\n",
      "Loss of train set: 0.18165206909179688 at epoch: 16 and batch_num: 332\n",
      "Loss of train set: 0.26233330368995667 at epoch: 16 and batch_num: 333\n",
      "Loss of train set: 0.429017037153244 at epoch: 16 and batch_num: 334\n",
      "Loss of train set: 0.29027044773101807 at epoch: 16 and batch_num: 335\n",
      "Loss of train set: 0.19225241243839264 at epoch: 16 and batch_num: 336\n",
      "Loss of train set: 0.3904382884502411 at epoch: 16 and batch_num: 337\n",
      "Loss of train set: 0.27726680040359497 at epoch: 16 and batch_num: 338\n",
      "Loss of train set: 0.2484993040561676 at epoch: 16 and batch_num: 339\n",
      "Loss of train set: 0.26577627658843994 at epoch: 16 and batch_num: 340\n",
      "Loss of train set: 0.35640591382980347 at epoch: 16 and batch_num: 341\n",
      "Loss of train set: 0.269726037979126 at epoch: 16 and batch_num: 342\n",
      "Loss of train set: 0.24469636380672455 at epoch: 16 and batch_num: 343\n",
      "Loss of train set: 0.29069629311561584 at epoch: 16 and batch_num: 344\n",
      "Loss of train set: 0.3385710120201111 at epoch: 16 and batch_num: 345\n",
      "Loss of train set: 0.2601841390132904 at epoch: 16 and batch_num: 346\n",
      "Loss of train set: 0.28332972526550293 at epoch: 16 and batch_num: 347\n",
      "Loss of train set: 0.23129794001579285 at epoch: 16 and batch_num: 348\n",
      "Loss of train set: 0.34964311122894287 at epoch: 16 and batch_num: 349\n",
      "Loss of train set: 0.2355138063430786 at epoch: 16 and batch_num: 350\n",
      "Loss of train set: 0.3191494345664978 at epoch: 16 and batch_num: 351\n",
      "Loss of train set: 0.2935716509819031 at epoch: 16 and batch_num: 352\n",
      "Loss of train set: 0.3236886262893677 at epoch: 16 and batch_num: 353\n",
      "Loss of train set: 0.30566662549972534 at epoch: 16 and batch_num: 354\n",
      "Loss of train set: 0.246056467294693 at epoch: 16 and batch_num: 355\n",
      "Loss of train set: 0.3737867772579193 at epoch: 16 and batch_num: 356\n",
      "Loss of train set: 0.36937063932418823 at epoch: 16 and batch_num: 357\n",
      "Loss of train set: 0.3788924515247345 at epoch: 16 and batch_num: 358\n",
      "Loss of train set: 0.3609048128128052 at epoch: 16 and batch_num: 359\n",
      "Loss of train set: 0.18650765717029572 at epoch: 16 and batch_num: 360\n",
      "Loss of train set: 0.28904736042022705 at epoch: 16 and batch_num: 361\n",
      "Loss of train set: 0.26984065771102905 at epoch: 16 and batch_num: 362\n",
      "Loss of train set: 0.3617640733718872 at epoch: 16 and batch_num: 363\n",
      "Loss of train set: 0.35494112968444824 at epoch: 16 and batch_num: 364\n",
      "Loss of train set: 0.33219796419143677 at epoch: 16 and batch_num: 365\n",
      "Loss of train set: 0.31665802001953125 at epoch: 16 and batch_num: 366\n",
      "Loss of train set: 0.22555318474769592 at epoch: 16 and batch_num: 367\n",
      "Loss of train set: 0.2583602964878082 at epoch: 16 and batch_num: 368\n",
      "Loss of train set: 0.3142033815383911 at epoch: 16 and batch_num: 369\n",
      "Loss of train set: 0.33925703167915344 at epoch: 16 and batch_num: 370\n",
      "Loss of train set: 0.2590714991092682 at epoch: 16 and batch_num: 371\n",
      "Loss of train set: 0.34206169843673706 at epoch: 16 and batch_num: 372\n",
      "Loss of train set: 0.19790349900722504 at epoch: 16 and batch_num: 373\n",
      "Loss of train set: 0.2068082094192505 at epoch: 16 and batch_num: 374\n",
      "Loss of train set: 0.3290005326271057 at epoch: 16 and batch_num: 375\n",
      "Loss of train set: 0.3310282826423645 at epoch: 16 and batch_num: 376\n",
      "Loss of train set: 0.27404236793518066 at epoch: 16 and batch_num: 377\n",
      "Loss of train set: 0.29051685333251953 at epoch: 16 and batch_num: 378\n",
      "Loss of train set: 0.2789326012134552 at epoch: 16 and batch_num: 379\n",
      "Loss of train set: 0.3090200424194336 at epoch: 16 and batch_num: 380\n",
      "Loss of train set: 0.29553455114364624 at epoch: 16 and batch_num: 381\n",
      "Loss of train set: 0.22533082962036133 at epoch: 16 and batch_num: 382\n",
      "Loss of train set: 0.41636037826538086 at epoch: 16 and batch_num: 383\n",
      "Loss of train set: 0.2720361053943634 at epoch: 16 and batch_num: 384\n",
      "Loss of train set: 0.451560914516449 at epoch: 16 and batch_num: 385\n",
      "Loss of train set: 0.35188233852386475 at epoch: 16 and batch_num: 386\n",
      "Loss of train set: 0.32386356592178345 at epoch: 16 and batch_num: 387\n",
      "Loss of train set: 0.39137089252471924 at epoch: 16 and batch_num: 388\n",
      "Loss of train set: 0.3289512097835541 at epoch: 16 and batch_num: 389\n",
      "Loss of train set: 0.32823461294174194 at epoch: 16 and batch_num: 390\n",
      "Loss of train set: 0.3726901710033417 at epoch: 16 and batch_num: 391\n",
      "Loss of train set: 0.34747469425201416 at epoch: 16 and batch_num: 392\n",
      "Loss of train set: 0.3290409445762634 at epoch: 16 and batch_num: 393\n",
      "Loss of train set: 0.3327331840991974 at epoch: 16 and batch_num: 394\n",
      "Loss of train set: 0.19658902287483215 at epoch: 16 and batch_num: 395\n",
      "Loss of train set: 0.21198193728923798 at epoch: 16 and batch_num: 396\n",
      "Loss of train set: 0.25621095299720764 at epoch: 16 and batch_num: 397\n",
      "Loss of train set: 0.3223809003829956 at epoch: 16 and batch_num: 398\n",
      "Loss of train set: 0.3865368068218231 at epoch: 16 and batch_num: 399\n",
      "Loss of train set: 0.23460647463798523 at epoch: 16 and batch_num: 400\n",
      "Loss of train set: 0.18541991710662842 at epoch: 16 and batch_num: 401\n",
      "Loss of train set: 0.17864759266376495 at epoch: 16 and batch_num: 402\n",
      "Loss of train set: 0.21670877933502197 at epoch: 16 and batch_num: 403\n",
      "Loss of train set: 0.2417973279953003 at epoch: 16 and batch_num: 404\n",
      "Loss of train set: 0.19014185667037964 at epoch: 16 and batch_num: 405\n",
      "Loss of train set: 0.5253335237503052 at epoch: 16 and batch_num: 406\n",
      "Loss of train set: 0.21860939264297485 at epoch: 16 and batch_num: 407\n",
      "Loss of train set: 0.24886071681976318 at epoch: 16 and batch_num: 408\n",
      "Loss of train set: 0.2822939455509186 at epoch: 16 and batch_num: 409\n",
      "Loss of train set: 0.2947196662425995 at epoch: 16 and batch_num: 410\n",
      "Loss of train set: 0.3771899342536926 at epoch: 16 and batch_num: 411\n",
      "Loss of train set: 0.2800476849079132 at epoch: 16 and batch_num: 412\n",
      "Loss of train set: 0.4731628894805908 at epoch: 16 and batch_num: 413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.24437692761421204 at epoch: 16 and batch_num: 414\n",
      "Loss of train set: 0.27067652344703674 at epoch: 16 and batch_num: 415\n",
      "Loss of train set: 0.23655471205711365 at epoch: 16 and batch_num: 416\n",
      "Loss of train set: 0.1515900194644928 at epoch: 16 and batch_num: 417\n",
      "Loss of train set: 0.47187232971191406 at epoch: 16 and batch_num: 418\n",
      "Loss of train set: 0.2687540650367737 at epoch: 16 and batch_num: 419\n",
      "Loss of train set: 0.2973913252353668 at epoch: 16 and batch_num: 420\n",
      "Loss of train set: 0.2244676798582077 at epoch: 16 and batch_num: 421\n",
      "Loss of train set: 0.29651808738708496 at epoch: 16 and batch_num: 422\n",
      "Loss of train set: 0.18932396173477173 at epoch: 16 and batch_num: 423\n",
      "Loss of train set: 0.3691122531890869 at epoch: 16 and batch_num: 424\n",
      "Loss of train set: 0.2647470533847809 at epoch: 16 and batch_num: 425\n",
      "Loss of train set: 0.40840333700180054 at epoch: 16 and batch_num: 426\n",
      "Loss of train set: 0.30781877040863037 at epoch: 16 and batch_num: 427\n",
      "Loss of train set: 0.2741897702217102 at epoch: 16 and batch_num: 428\n",
      "Loss of train set: 0.31379416584968567 at epoch: 16 and batch_num: 429\n",
      "Loss of train set: 0.30280250310897827 at epoch: 16 and batch_num: 430\n",
      "Loss of train set: 0.25221771001815796 at epoch: 16 and batch_num: 431\n",
      "Loss of train set: 0.280107319355011 at epoch: 16 and batch_num: 432\n",
      "Loss of train set: 0.35512515902519226 at epoch: 16 and batch_num: 433\n",
      "Loss of train set: 0.3829781413078308 at epoch: 16 and batch_num: 434\n",
      "Loss of train set: 0.4729599952697754 at epoch: 16 and batch_num: 435\n",
      "Loss of train set: 0.15884152054786682 at epoch: 16 and batch_num: 436\n",
      "Loss of train set: 0.3197764754295349 at epoch: 16 and batch_num: 437\n",
      "Loss of train set: 0.2712211608886719 at epoch: 16 and batch_num: 438\n",
      "Loss of train set: 0.21030473709106445 at epoch: 16 and batch_num: 439\n",
      "Loss of train set: 0.3358997106552124 at epoch: 16 and batch_num: 440\n",
      "Loss of train set: 0.26836150884628296 at epoch: 16 and batch_num: 441\n",
      "Loss of train set: 0.14544542133808136 at epoch: 16 and batch_num: 442\n",
      "Loss of train set: 0.40348055958747864 at epoch: 16 and batch_num: 443\n",
      "Loss of train set: 0.3551587462425232 at epoch: 16 and batch_num: 444\n",
      "Loss of train set: 0.188481867313385 at epoch: 16 and batch_num: 445\n",
      "Loss of train set: 0.3463026285171509 at epoch: 16 and batch_num: 446\n",
      "Loss of train set: 0.14351363480091095 at epoch: 16 and batch_num: 447\n",
      "Loss of train set: 0.16980934143066406 at epoch: 16 and batch_num: 448\n",
      "Loss of train set: 0.20529133081436157 at epoch: 16 and batch_num: 449\n",
      "Loss of train set: 0.31788867712020874 at epoch: 16 and batch_num: 450\n",
      "Loss of train set: 0.6106618046760559 at epoch: 16 and batch_num: 451\n",
      "Loss of train set: 0.19471777975559235 at epoch: 16 and batch_num: 452\n",
      "Loss of train set: 0.24389128386974335 at epoch: 16 and batch_num: 453\n",
      "Loss of train set: 0.3837718963623047 at epoch: 16 and batch_num: 454\n",
      "Loss of train set: 0.24617302417755127 at epoch: 16 and batch_num: 455\n",
      "Loss of train set: 0.3007885217666626 at epoch: 16 and batch_num: 456\n",
      "Loss of train set: 0.27912431955337524 at epoch: 16 and batch_num: 457\n",
      "Loss of train set: 0.34491461515426636 at epoch: 16 and batch_num: 458\n",
      "Loss of train set: 0.26266974210739136 at epoch: 16 and batch_num: 459\n",
      "Loss of train set: 0.22757327556610107 at epoch: 16 and batch_num: 460\n",
      "Loss of train set: 0.22149556875228882 at epoch: 16 and batch_num: 461\n",
      "Loss of train set: 0.38926899433135986 at epoch: 16 and batch_num: 462\n",
      "Loss of train set: 0.2712957561016083 at epoch: 16 and batch_num: 463\n",
      "Loss of train set: 0.22398515045642853 at epoch: 16 and batch_num: 464\n",
      "Loss of train set: 0.23887202143669128 at epoch: 16 and batch_num: 465\n",
      "Loss of train set: 0.4135172665119171 at epoch: 16 and batch_num: 466\n",
      "Loss of train set: 0.25668612122535706 at epoch: 16 and batch_num: 467\n",
      "Loss of train set: 0.33316770195961 at epoch: 16 and batch_num: 468\n",
      "Loss of train set: 0.4180898070335388 at epoch: 16 and batch_num: 469\n",
      "Loss of train set: 0.2195562869310379 at epoch: 16 and batch_num: 470\n",
      "Loss of train set: 0.37111756205558777 at epoch: 16 and batch_num: 471\n",
      "Loss of train set: 0.37289535999298096 at epoch: 16 and batch_num: 472\n",
      "Loss of train set: 0.3043458163738251 at epoch: 16 and batch_num: 473\n",
      "Loss of train set: 0.28714972734451294 at epoch: 16 and batch_num: 474\n",
      "Loss of train set: 0.3678300678730011 at epoch: 16 and batch_num: 475\n",
      "Loss of train set: 0.10018274933099747 at epoch: 16 and batch_num: 476\n",
      "Loss of train set: 0.23924198746681213 at epoch: 16 and batch_num: 477\n",
      "Loss of train set: 0.35191231966018677 at epoch: 16 and batch_num: 478\n",
      "Loss of train set: 0.21056292951107025 at epoch: 16 and batch_num: 479\n",
      "Loss of train set: 0.22258248925209045 at epoch: 16 and batch_num: 480\n",
      "Loss of train set: 0.22176536917686462 at epoch: 16 and batch_num: 481\n",
      "Loss of train set: 0.3109515607357025 at epoch: 16 and batch_num: 482\n",
      "Loss of train set: 0.29840534925460815 at epoch: 16 and batch_num: 483\n",
      "Loss of train set: 0.6191431283950806 at epoch: 16 and batch_num: 484\n",
      "Loss of train set: 0.28777435421943665 at epoch: 16 and batch_num: 485\n",
      "Loss of train set: 0.2597777843475342 at epoch: 16 and batch_num: 486\n",
      "Loss of train set: 0.4131232500076294 at epoch: 16 and batch_num: 487\n",
      "Loss of train set: 0.33705446124076843 at epoch: 16 and batch_num: 488\n",
      "Loss of train set: 0.15449179708957672 at epoch: 16 and batch_num: 489\n",
      "Loss of train set: 0.2221154421567917 at epoch: 16 and batch_num: 490\n",
      "Loss of train set: 0.36684733629226685 at epoch: 16 and batch_num: 491\n",
      "Loss of train set: 0.29007259011268616 at epoch: 16 and batch_num: 492\n",
      "Loss of train set: 0.2148120403289795 at epoch: 16 and batch_num: 493\n",
      "Loss of train set: 0.3992292582988739 at epoch: 16 and batch_num: 494\n",
      "Loss of train set: 0.3308992385864258 at epoch: 16 and batch_num: 495\n",
      "Loss of train set: 0.26308673620224 at epoch: 16 and batch_num: 496\n",
      "Loss of train set: 0.1938297152519226 at epoch: 16 and batch_num: 497\n",
      "Loss of train set: 0.207875058054924 at epoch: 16 and batch_num: 498\n",
      "Loss of train set: 0.2212931364774704 at epoch: 16 and batch_num: 499\n",
      "Loss of train set: 0.27693936228752136 at epoch: 16 and batch_num: 500\n",
      "Loss of train set: 0.22089558839797974 at epoch: 16 and batch_num: 501\n",
      "Loss of train set: 0.31114012002944946 at epoch: 16 and batch_num: 502\n",
      "Loss of train set: 0.30958792567253113 at epoch: 16 and batch_num: 503\n",
      "Loss of train set: 0.23820848762989044 at epoch: 16 and batch_num: 504\n",
      "Loss of train set: 0.24450182914733887 at epoch: 16 and batch_num: 505\n",
      "Loss of train set: 0.3909526467323303 at epoch: 16 and batch_num: 506\n",
      "Loss of train set: 0.2021588236093521 at epoch: 16 and batch_num: 507\n",
      "Loss of train set: 0.3040573000907898 at epoch: 16 and batch_num: 508\n",
      "Loss of train set: 0.4156090319156647 at epoch: 16 and batch_num: 509\n",
      "Loss of train set: 0.3056298792362213 at epoch: 16 and batch_num: 510\n",
      "Loss of train set: 0.20791292190551758 at epoch: 16 and batch_num: 511\n",
      "Loss of train set: 0.24337701499462128 at epoch: 16 and batch_num: 512\n",
      "Loss of train set: 0.3230770230293274 at epoch: 16 and batch_num: 513\n",
      "Loss of train set: 0.24014058709144592 at epoch: 16 and batch_num: 514\n",
      "Loss of train set: 0.30734243988990784 at epoch: 16 and batch_num: 515\n",
      "Loss of train set: 0.31712043285369873 at epoch: 16 and batch_num: 516\n",
      "Loss of train set: 0.3527449667453766 at epoch: 16 and batch_num: 517\n",
      "Loss of train set: 0.3079584836959839 at epoch: 16 and batch_num: 518\n",
      "Loss of train set: 0.3672648072242737 at epoch: 16 and batch_num: 519\n",
      "Loss of train set: 0.20113515853881836 at epoch: 16 and batch_num: 520\n",
      "Loss of train set: 0.2525927722454071 at epoch: 16 and batch_num: 521\n",
      "Loss of train set: 0.28403204679489136 at epoch: 16 and batch_num: 522\n",
      "Loss of train set: 0.46622976660728455 at epoch: 16 and batch_num: 523\n",
      "Loss of train set: 0.19758491218090057 at epoch: 16 and batch_num: 524\n",
      "Loss of train set: 0.41805338859558105 at epoch: 16 and batch_num: 525\n",
      "Loss of train set: 0.28778669238090515 at epoch: 16 and batch_num: 526\n",
      "Loss of train set: 0.29209959506988525 at epoch: 16 and batch_num: 527\n",
      "Loss of train set: 0.4234493374824524 at epoch: 16 and batch_num: 528\n",
      "Loss of train set: 0.24185864627361298 at epoch: 16 and batch_num: 529\n",
      "Loss of train set: 0.33647680282592773 at epoch: 16 and batch_num: 530\n",
      "Loss of train set: 0.17168524861335754 at epoch: 16 and batch_num: 531\n",
      "Loss of train set: 0.31395423412323 at epoch: 16 and batch_num: 532\n",
      "Loss of train set: 0.266143262386322 at epoch: 16 and batch_num: 533\n",
      "Loss of train set: 0.24770352244377136 at epoch: 16 and batch_num: 534\n",
      "Loss of train set: 0.28832435607910156 at epoch: 16 and batch_num: 535\n",
      "Loss of train set: 0.5531051754951477 at epoch: 16 and batch_num: 536\n",
      "Loss of train set: 0.2764052152633667 at epoch: 16 and batch_num: 537\n",
      "Loss of train set: 0.2476237416267395 at epoch: 16 and batch_num: 538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3077848553657532 at epoch: 16 and batch_num: 539\n",
      "Loss of train set: 0.5885700583457947 at epoch: 16 and batch_num: 540\n",
      "Loss of train set: 0.25104039907455444 at epoch: 16 and batch_num: 541\n",
      "Loss of train set: 0.3677886128425598 at epoch: 16 and batch_num: 542\n",
      "Loss of train set: 0.3111308813095093 at epoch: 16 and batch_num: 543\n",
      "Loss of train set: 0.33620548248291016 at epoch: 16 and batch_num: 544\n",
      "Loss of train set: 0.2103312611579895 at epoch: 16 and batch_num: 545\n",
      "Loss of train set: 0.48621484637260437 at epoch: 16 and batch_num: 546\n",
      "Loss of train set: 0.2180899977684021 at epoch: 16 and batch_num: 547\n",
      "Loss of train set: 0.28109025955200195 at epoch: 16 and batch_num: 548\n",
      "Loss of train set: 0.23216059803962708 at epoch: 16 and batch_num: 549\n",
      "Loss of train set: 0.25431400537490845 at epoch: 16 and batch_num: 550\n",
      "Loss of train set: 0.23075611889362335 at epoch: 16 and batch_num: 551\n",
      "Loss of train set: 0.3793122172355652 at epoch: 16 and batch_num: 552\n",
      "Loss of train set: 0.294891357421875 at epoch: 16 and batch_num: 553\n",
      "Loss of train set: 0.15195226669311523 at epoch: 16 and batch_num: 554\n",
      "Loss of train set: 0.26165056228637695 at epoch: 16 and batch_num: 555\n",
      "Loss of train set: 0.20320621132850647 at epoch: 16 and batch_num: 556\n",
      "Loss of train set: 0.19896277785301208 at epoch: 16 and batch_num: 557\n",
      "Loss of train set: 0.24435478448867798 at epoch: 16 and batch_num: 558\n",
      "Loss of train set: 0.5541156530380249 at epoch: 16 and batch_num: 559\n",
      "Loss of train set: 0.3726409375667572 at epoch: 16 and batch_num: 560\n",
      "Loss of train set: 0.2700616717338562 at epoch: 16 and batch_num: 561\n",
      "Loss of train set: 0.47236770391464233 at epoch: 16 and batch_num: 562\n",
      "Loss of train set: 0.4749343991279602 at epoch: 16 and batch_num: 563\n",
      "Loss of train set: 0.18464991450309753 at epoch: 16 and batch_num: 564\n",
      "Loss of train set: 0.544090986251831 at epoch: 16 and batch_num: 565\n",
      "Loss of train set: 0.2808702290058136 at epoch: 16 and batch_num: 566\n",
      "Loss of train set: 0.36546850204467773 at epoch: 16 and batch_num: 567\n",
      "Loss of train set: 0.3559642434120178 at epoch: 16 and batch_num: 568\n",
      "Loss of train set: 0.21806037425994873 at epoch: 16 and batch_num: 569\n",
      "Loss of train set: 0.26558005809783936 at epoch: 16 and batch_num: 570\n",
      "Loss of train set: 0.2271154522895813 at epoch: 16 and batch_num: 571\n",
      "Loss of train set: 0.3315671980381012 at epoch: 16 and batch_num: 572\n",
      "Loss of train set: 0.19204354286193848 at epoch: 16 and batch_num: 573\n",
      "Loss of train set: 0.37354740500450134 at epoch: 16 and batch_num: 574\n",
      "Loss of train set: 0.2649722695350647 at epoch: 16 and batch_num: 575\n",
      "Loss of train set: 0.18136891722679138 at epoch: 16 and batch_num: 576\n",
      "Loss of train set: 0.4191768169403076 at epoch: 16 and batch_num: 577\n",
      "Loss of train set: 0.3102225661277771 at epoch: 16 and batch_num: 578\n",
      "Loss of train set: 0.336548775434494 at epoch: 16 and batch_num: 579\n",
      "Loss of train set: 0.29646044969558716 at epoch: 16 and batch_num: 580\n",
      "Loss of train set: 0.2538316249847412 at epoch: 16 and batch_num: 581\n",
      "Loss of train set: 0.22178077697753906 at epoch: 16 and batch_num: 582\n",
      "Loss of train set: 0.2772752344608307 at epoch: 16 and batch_num: 583\n",
      "Loss of train set: 0.17178386449813843 at epoch: 16 and batch_num: 584\n",
      "Loss of train set: 0.247651606798172 at epoch: 16 and batch_num: 585\n",
      "Loss of train set: 0.49784812331199646 at epoch: 16 and batch_num: 586\n",
      "Loss of train set: 0.37441062927246094 at epoch: 16 and batch_num: 587\n",
      "Loss of train set: 0.32300010323524475 at epoch: 16 and batch_num: 588\n",
      "Loss of train set: 0.4928460121154785 at epoch: 16 and batch_num: 589\n",
      "Loss of train set: 0.3536858558654785 at epoch: 16 and batch_num: 590\n",
      "Loss of train set: 0.3593258559703827 at epoch: 16 and batch_num: 591\n",
      "Loss of train set: 0.43527448177337646 at epoch: 16 and batch_num: 592\n",
      "Loss of train set: 0.46012890338897705 at epoch: 16 and batch_num: 593\n",
      "Loss of train set: 0.3448410630226135 at epoch: 16 and batch_num: 594\n",
      "Loss of train set: 0.22020681202411652 at epoch: 16 and batch_num: 595\n",
      "Loss of train set: 0.35302412509918213 at epoch: 16 and batch_num: 596\n",
      "Loss of train set: 0.3799890875816345 at epoch: 16 and batch_num: 597\n",
      "Loss of train set: 0.2579641044139862 at epoch: 16 and batch_num: 598\n",
      "Loss of train set: 0.45343196392059326 at epoch: 16 and batch_num: 599\n",
      "Loss of train set: 0.5163862705230713 at epoch: 16 and batch_num: 600\n",
      "Loss of train set: 0.41724711656570435 at epoch: 16 and batch_num: 601\n",
      "Loss of train set: 0.24217191338539124 at epoch: 16 and batch_num: 602\n",
      "Loss of train set: 0.32582682371139526 at epoch: 16 and batch_num: 603\n",
      "Loss of train set: 0.1722050905227661 at epoch: 16 and batch_num: 604\n",
      "Loss of train set: 0.32211607694625854 at epoch: 16 and batch_num: 605\n",
      "Loss of train set: 0.18216367065906525 at epoch: 16 and batch_num: 606\n",
      "Loss of train set: 0.3929716944694519 at epoch: 16 and batch_num: 607\n",
      "Loss of train set: 0.22804245352745056 at epoch: 16 and batch_num: 608\n",
      "Loss of train set: 0.4095446765422821 at epoch: 16 and batch_num: 609\n",
      "Loss of train set: 0.25974342226982117 at epoch: 16 and batch_num: 610\n",
      "Loss of train set: 0.23943698406219482 at epoch: 16 and batch_num: 611\n",
      "Loss of train set: 0.3984828591346741 at epoch: 16 and batch_num: 612\n",
      "Loss of train set: 0.16770237684249878 at epoch: 16 and batch_num: 613\n",
      "Loss of train set: 0.253418505191803 at epoch: 16 and batch_num: 614\n",
      "Loss of train set: 0.3166958689689636 at epoch: 16 and batch_num: 615\n",
      "Loss of train set: 0.2404518723487854 at epoch: 16 and batch_num: 616\n",
      "Loss of train set: 0.23002278804779053 at epoch: 16 and batch_num: 617\n",
      "Loss of train set: 0.3797294795513153 at epoch: 16 and batch_num: 618\n",
      "Loss of train set: 0.1905204802751541 at epoch: 16 and batch_num: 619\n",
      "Loss of train set: 0.24004146456718445 at epoch: 16 and batch_num: 620\n",
      "Loss of train set: 0.249620258808136 at epoch: 16 and batch_num: 621\n",
      "Loss of train set: 0.28018659353256226 at epoch: 16 and batch_num: 622\n",
      "Loss of train set: 0.2467731237411499 at epoch: 16 and batch_num: 623\n",
      "Loss of train set: 0.22054103016853333 at epoch: 16 and batch_num: 624\n",
      "Loss of train set: 0.32547229528427124 at epoch: 16 and batch_num: 625\n",
      "Loss of train set: 0.37432149052619934 at epoch: 16 and batch_num: 626\n",
      "Loss of train set: 0.1940152794122696 at epoch: 16 and batch_num: 627\n",
      "Loss of train set: 0.3839699923992157 at epoch: 16 and batch_num: 628\n",
      "Loss of train set: 0.3270043730735779 at epoch: 16 and batch_num: 629\n",
      "Loss of train set: 0.37041136622428894 at epoch: 16 and batch_num: 630\n",
      "Loss of train set: 0.2499736249446869 at epoch: 16 and batch_num: 631\n",
      "Loss of train set: 0.31344690918922424 at epoch: 16 and batch_num: 632\n",
      "Loss of train set: 0.2362762838602066 at epoch: 16 and batch_num: 633\n",
      "Loss of train set: 0.28511843085289 at epoch: 16 and batch_num: 634\n",
      "Loss of train set: 0.2512051463127136 at epoch: 16 and batch_num: 635\n",
      "Loss of train set: 0.22376951575279236 at epoch: 16 and batch_num: 636\n",
      "Loss of train set: 0.2750965356826782 at epoch: 16 and batch_num: 637\n",
      "Loss of train set: 0.31267309188842773 at epoch: 16 and batch_num: 638\n",
      "Loss of train set: 0.4647844433784485 at epoch: 16 and batch_num: 639\n",
      "Loss of train set: 0.341979444026947 at epoch: 16 and batch_num: 640\n",
      "Loss of train set: 0.31759655475616455 at epoch: 16 and batch_num: 641\n",
      "Loss of train set: 0.34456902742385864 at epoch: 16 and batch_num: 642\n",
      "Loss of train set: 0.20617380738258362 at epoch: 16 and batch_num: 643\n",
      "Loss of train set: 0.31590020656585693 at epoch: 16 and batch_num: 644\n",
      "Loss of train set: 0.32913631200790405 at epoch: 16 and batch_num: 645\n",
      "Loss of train set: 0.29892027378082275 at epoch: 16 and batch_num: 646\n",
      "Loss of train set: 0.29986923933029175 at epoch: 16 and batch_num: 647\n",
      "Loss of train set: 0.21649706363677979 at epoch: 16 and batch_num: 648\n",
      "Loss of train set: 0.2836272418498993 at epoch: 16 and batch_num: 649\n",
      "Loss of train set: 0.26778149604797363 at epoch: 16 and batch_num: 650\n",
      "Loss of train set: 0.336974561214447 at epoch: 16 and batch_num: 651\n",
      "Loss of train set: 0.34675246477127075 at epoch: 16 and batch_num: 652\n",
      "Loss of train set: 0.18037337064743042 at epoch: 16 and batch_num: 653\n",
      "Loss of train set: 0.394738107919693 at epoch: 16 and batch_num: 654\n",
      "Loss of train set: 0.3026273846626282 at epoch: 16 and batch_num: 655\n",
      "Loss of train set: 0.3605404496192932 at epoch: 16 and batch_num: 656\n",
      "Loss of train set: 0.2741278111934662 at epoch: 16 and batch_num: 657\n",
      "Loss of train set: 0.2316839098930359 at epoch: 16 and batch_num: 658\n",
      "Loss of train set: 0.14718173444271088 at epoch: 16 and batch_num: 659\n",
      "Loss of train set: 0.23271125555038452 at epoch: 16 and batch_num: 660\n",
      "Loss of train set: 0.2324647158384323 at epoch: 16 and batch_num: 661\n",
      "Loss of train set: 0.30502572655677795 at epoch: 16 and batch_num: 662\n",
      "Loss of train set: 0.24102254211902618 at epoch: 16 and batch_num: 663\n",
      "Loss of train set: 0.24494609236717224 at epoch: 16 and batch_num: 664\n",
      "Loss of train set: 0.2211799919605255 at epoch: 16 and batch_num: 665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.42587989568710327 at epoch: 16 and batch_num: 666\n",
      "Loss of train set: 0.2230554223060608 at epoch: 16 and batch_num: 667\n",
      "Loss of train set: 0.32821962237358093 at epoch: 16 and batch_num: 668\n",
      "Loss of train set: 0.22038787603378296 at epoch: 16 and batch_num: 669\n",
      "Loss of train set: 0.28533363342285156 at epoch: 16 and batch_num: 670\n",
      "Loss of train set: 0.31085309386253357 at epoch: 16 and batch_num: 671\n",
      "Loss of train set: 0.2694256901741028 at epoch: 16 and batch_num: 672\n",
      "Loss of train set: 0.26116394996643066 at epoch: 16 and batch_num: 673\n",
      "Loss of train set: 0.21983030438423157 at epoch: 16 and batch_num: 674\n",
      "Loss of train set: 0.3000492453575134 at epoch: 16 and batch_num: 675\n",
      "Loss of train set: 0.2263495922088623 at epoch: 16 and batch_num: 676\n",
      "Loss of train set: 0.1829398274421692 at epoch: 16 and batch_num: 677\n",
      "Loss of train set: 0.2976112365722656 at epoch: 16 and batch_num: 678\n",
      "Loss of train set: 0.33757448196411133 at epoch: 16 and batch_num: 679\n",
      "Loss of train set: 0.23211628198623657 at epoch: 16 and batch_num: 680\n",
      "Loss of train set: 0.34552961587905884 at epoch: 16 and batch_num: 681\n",
      "Loss of train set: 0.37714123725891113 at epoch: 16 and batch_num: 682\n",
      "Loss of train set: 0.37949684262275696 at epoch: 16 and batch_num: 683\n",
      "Loss of train set: 0.23252543807029724 at epoch: 16 and batch_num: 684\n",
      "Loss of train set: 0.2715657651424408 at epoch: 16 and batch_num: 685\n",
      "Loss of train set: 0.32606059312820435 at epoch: 16 and batch_num: 686\n",
      "Loss of train set: 0.3043595552444458 at epoch: 16 and batch_num: 687\n",
      "Loss of train set: 0.1547170877456665 at epoch: 16 and batch_num: 688\n",
      "Loss of train set: 0.36616554856300354 at epoch: 16 and batch_num: 689\n",
      "Loss of train set: 0.21036085486412048 at epoch: 16 and batch_num: 690\n",
      "Loss of train set: 0.31780576705932617 at epoch: 16 and batch_num: 691\n",
      "Loss of train set: 0.2749790549278259 at epoch: 16 and batch_num: 692\n",
      "Loss of train set: 0.26772773265838623 at epoch: 16 and batch_num: 693\n",
      "Loss of train set: 0.2343570739030838 at epoch: 16 and batch_num: 694\n",
      "Loss of train set: 0.34479397535324097 at epoch: 16 and batch_num: 695\n",
      "Loss of train set: 0.24488258361816406 at epoch: 16 and batch_num: 696\n",
      "Loss of train set: 0.283342570066452 at epoch: 16 and batch_num: 697\n",
      "Loss of train set: 0.5526243448257446 at epoch: 16 and batch_num: 698\n",
      "Loss of train set: 0.20989438891410828 at epoch: 16 and batch_num: 699\n",
      "Loss of train set: 0.41558992862701416 at epoch: 16 and batch_num: 700\n",
      "Loss of train set: 0.25519341230392456 at epoch: 16 and batch_num: 701\n",
      "Loss of train set: 0.34749579429626465 at epoch: 16 and batch_num: 702\n",
      "Loss of train set: 0.2696640193462372 at epoch: 16 and batch_num: 703\n",
      "Loss of train set: 0.5205472111701965 at epoch: 16 and batch_num: 704\n",
      "Loss of train set: 0.21134138107299805 at epoch: 16 and batch_num: 705\n",
      "Loss of train set: 0.15122690796852112 at epoch: 16 and batch_num: 706\n",
      "Loss of train set: 0.1455230563879013 at epoch: 16 and batch_num: 707\n",
      "Loss of train set: 0.446565717458725 at epoch: 16 and batch_num: 708\n",
      "Loss of train set: 0.3661472797393799 at epoch: 16 and batch_num: 709\n",
      "Loss of train set: 0.5698075294494629 at epoch: 16 and batch_num: 710\n",
      "Loss of train set: 0.2620564103126526 at epoch: 16 and batch_num: 711\n",
      "Loss of train set: 0.12460975348949432 at epoch: 16 and batch_num: 712\n",
      "Loss of train set: 0.3019057512283325 at epoch: 16 and batch_num: 713\n",
      "Loss of train set: 0.24684563279151917 at epoch: 16 and batch_num: 714\n",
      "Loss of train set: 0.46848803758621216 at epoch: 16 and batch_num: 715\n",
      "Loss of train set: 0.22424548864364624 at epoch: 16 and batch_num: 716\n",
      "Loss of train set: 0.29641610383987427 at epoch: 16 and batch_num: 717\n",
      "Loss of train set: 0.25058701634407043 at epoch: 16 and batch_num: 718\n",
      "Loss of train set: 0.33403488993644714 at epoch: 16 and batch_num: 719\n",
      "Loss of train set: 0.3370923399925232 at epoch: 16 and batch_num: 720\n",
      "Loss of train set: 0.39651983976364136 at epoch: 16 and batch_num: 721\n",
      "Loss of train set: 0.32329243421554565 at epoch: 16 and batch_num: 722\n",
      "Loss of train set: 0.20306441187858582 at epoch: 16 and batch_num: 723\n",
      "Loss of train set: 0.23516643047332764 at epoch: 16 and batch_num: 724\n",
      "Loss of train set: 0.23188598453998566 at epoch: 16 and batch_num: 725\n",
      "Loss of train set: 0.3937914967536926 at epoch: 16 and batch_num: 726\n",
      "Loss of train set: 0.23762699961662292 at epoch: 16 and batch_num: 727\n",
      "Loss of train set: 0.3071064352989197 at epoch: 16 and batch_num: 728\n",
      "Loss of train set: 0.16519178450107574 at epoch: 16 and batch_num: 729\n",
      "Loss of train set: 0.3758838176727295 at epoch: 16 and batch_num: 730\n",
      "Loss of train set: 0.3709494173526764 at epoch: 16 and batch_num: 731\n",
      "Loss of train set: 0.5018365979194641 at epoch: 16 and batch_num: 732\n",
      "Loss of train set: 0.2737874984741211 at epoch: 16 and batch_num: 733\n",
      "Loss of train set: 0.2732076942920685 at epoch: 16 and batch_num: 734\n",
      "Loss of train set: 0.5412741303443909 at epoch: 16 and batch_num: 735\n",
      "Loss of train set: 0.4533122777938843 at epoch: 16 and batch_num: 736\n",
      "Loss of train set: 0.34337425231933594 at epoch: 16 and batch_num: 737\n",
      "Loss of train set: 0.31100204586982727 at epoch: 16 and batch_num: 738\n",
      "Loss of train set: 0.22949248552322388 at epoch: 16 and batch_num: 739\n",
      "Loss of train set: 0.2723981738090515 at epoch: 16 and batch_num: 740\n",
      "Loss of train set: 0.38631054759025574 at epoch: 16 and batch_num: 741\n",
      "Loss of train set: 0.3673868179321289 at epoch: 16 and batch_num: 742\n",
      "Loss of train set: 0.2648973762989044 at epoch: 16 and batch_num: 743\n",
      "Loss of train set: 0.3660627603530884 at epoch: 16 and batch_num: 744\n",
      "Loss of train set: 0.44254070520401 at epoch: 16 and batch_num: 745\n",
      "Loss of train set: 0.416156530380249 at epoch: 16 and batch_num: 746\n",
      "Loss of train set: 0.29802456498146057 at epoch: 16 and batch_num: 747\n",
      "Loss of train set: 0.28625375032424927 at epoch: 16 and batch_num: 748\n",
      "Loss of train set: 0.2831062972545624 at epoch: 16 and batch_num: 749\n",
      "Loss of train set: 0.2018798291683197 at epoch: 16 and batch_num: 750\n",
      "Loss of train set: 0.328281432390213 at epoch: 16 and batch_num: 751\n",
      "Loss of train set: 0.45047709345817566 at epoch: 16 and batch_num: 752\n",
      "Loss of train set: 0.33486855030059814 at epoch: 16 and batch_num: 753\n",
      "Loss of train set: 0.4260455369949341 at epoch: 16 and batch_num: 754\n",
      "Loss of train set: 0.14947831630706787 at epoch: 16 and batch_num: 755\n",
      "Loss of train set: 0.3913392126560211 at epoch: 16 and batch_num: 756\n",
      "Loss of train set: 0.22552360594272614 at epoch: 16 and batch_num: 757\n",
      "Loss of train set: 0.21777945756912231 at epoch: 16 and batch_num: 758\n",
      "Loss of train set: 0.3677327632904053 at epoch: 16 and batch_num: 759\n",
      "Loss of train set: 0.1568664163351059 at epoch: 16 and batch_num: 760\n",
      "Loss of train set: 0.23601816594600677 at epoch: 16 and batch_num: 761\n",
      "Loss of train set: 0.1759188175201416 at epoch: 16 and batch_num: 762\n",
      "Loss of train set: 0.3255254626274109 at epoch: 16 and batch_num: 763\n",
      "Loss of train set: 0.2047286033630371 at epoch: 16 and batch_num: 764\n",
      "Loss of train set: 0.18291209638118744 at epoch: 16 and batch_num: 765\n",
      "Loss of train set: 0.26470619440078735 at epoch: 16 and batch_num: 766\n",
      "Loss of train set: 0.13232094049453735 at epoch: 16 and batch_num: 767\n",
      "Loss of train set: 0.3360328674316406 at epoch: 16 and batch_num: 768\n",
      "Loss of train set: 0.35238951444625854 at epoch: 16 and batch_num: 769\n",
      "Loss of train set: 0.30812498927116394 at epoch: 16 and batch_num: 770\n",
      "Loss of train set: 0.41564130783081055 at epoch: 16 and batch_num: 771\n",
      "Loss of train set: 0.39952772855758667 at epoch: 16 and batch_num: 772\n",
      "Loss of train set: 0.2824717164039612 at epoch: 16 and batch_num: 773\n",
      "Loss of train set: 0.48898038268089294 at epoch: 16 and batch_num: 774\n",
      "Loss of train set: 0.30613917112350464 at epoch: 16 and batch_num: 775\n",
      "Loss of train set: 0.23780682682991028 at epoch: 16 and batch_num: 776\n",
      "Loss of train set: 0.3196674585342407 at epoch: 16 and batch_num: 777\n",
      "Loss of train set: 0.23305031657218933 at epoch: 16 and batch_num: 778\n",
      "Loss of train set: 0.3090628385543823 at epoch: 16 and batch_num: 779\n",
      "Loss of train set: 0.2561867833137512 at epoch: 16 and batch_num: 780\n",
      "Loss of train set: 0.3510958254337311 at epoch: 16 and batch_num: 781\n",
      "Loss of train set: 0.1695992648601532 at epoch: 16 and batch_num: 782\n",
      "Loss of train set: 0.32256385684013367 at epoch: 16 and batch_num: 783\n",
      "Loss of train set: 0.192365825176239 at epoch: 16 and batch_num: 784\n",
      "Loss of train set: 0.28903406858444214 at epoch: 16 and batch_num: 785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2669886648654938 at epoch: 16 and batch_num: 786\n",
      "Loss of train set: 0.22331702709197998 at epoch: 16 and batch_num: 787\n",
      "Loss of train set: 0.34560108184814453 at epoch: 16 and batch_num: 788\n",
      "Loss of train set: 0.1874988079071045 at epoch: 16 and batch_num: 789\n",
      "Loss of train set: 0.1874150037765503 at epoch: 16 and batch_num: 790\n",
      "Loss of train set: 0.16899147629737854 at epoch: 16 and batch_num: 791\n",
      "Loss of train set: 0.25796419382095337 at epoch: 16 and batch_num: 792\n",
      "Loss of train set: 0.2027200311422348 at epoch: 16 and batch_num: 793\n",
      "Loss of train set: 0.3373028635978699 at epoch: 16 and batch_num: 794\n",
      "Loss of train set: 0.48641759157180786 at epoch: 16 and batch_num: 795\n",
      "Loss of train set: 0.4877591133117676 at epoch: 16 and batch_num: 796\n",
      "Loss of train set: 0.335751473903656 at epoch: 16 and batch_num: 797\n",
      "Loss of train set: 0.4083014130592346 at epoch: 16 and batch_num: 798\n",
      "Loss of train set: 0.2491203248500824 at epoch: 16 and batch_num: 799\n",
      "Loss of train set: 0.3025134205818176 at epoch: 16 and batch_num: 800\n",
      "Loss of train set: 0.2462427318096161 at epoch: 16 and batch_num: 801\n",
      "Loss of train set: 0.334966778755188 at epoch: 16 and batch_num: 802\n",
      "Loss of train set: 0.38444194197654724 at epoch: 16 and batch_num: 803\n",
      "Loss of train set: 0.333601176738739 at epoch: 16 and batch_num: 804\n",
      "Loss of train set: 0.34622102975845337 at epoch: 16 and batch_num: 805\n",
      "Loss of train set: 0.23060014843940735 at epoch: 16 and batch_num: 806\n",
      "Loss of train set: 0.19590085744857788 at epoch: 16 and batch_num: 807\n",
      "Loss of train set: 0.3104487657546997 at epoch: 16 and batch_num: 808\n",
      "Loss of train set: 0.20534928143024445 at epoch: 16 and batch_num: 809\n",
      "Loss of train set: 0.29461973905563354 at epoch: 16 and batch_num: 810\n",
      "Loss of train set: 0.46319282054901123 at epoch: 16 and batch_num: 811\n",
      "Loss of train set: 0.352154016494751 at epoch: 16 and batch_num: 812\n",
      "Loss of train set: 0.1292589157819748 at epoch: 16 and batch_num: 813\n",
      "Loss of train set: 0.38776659965515137 at epoch: 16 and batch_num: 814\n",
      "Loss of train set: 0.3563976585865021 at epoch: 16 and batch_num: 815\n",
      "Loss of train set: 0.32812970876693726 at epoch: 16 and batch_num: 816\n",
      "Loss of train set: 0.2558833956718445 at epoch: 16 and batch_num: 817\n",
      "Loss of train set: 0.19036203622817993 at epoch: 16 and batch_num: 818\n",
      "Loss of train set: 0.23280437290668488 at epoch: 16 and batch_num: 819\n",
      "Loss of train set: 0.22717057168483734 at epoch: 16 and batch_num: 820\n",
      "Loss of train set: 0.325869619846344 at epoch: 16 and batch_num: 821\n",
      "Loss of train set: 0.388100266456604 at epoch: 16 and batch_num: 822\n",
      "Loss of train set: 0.23002579808235168 at epoch: 16 and batch_num: 823\n",
      "Loss of train set: 0.21367000043392181 at epoch: 16 and batch_num: 824\n",
      "Loss of train set: 0.22897088527679443 at epoch: 16 and batch_num: 825\n",
      "Loss of train set: 0.1910681426525116 at epoch: 16 and batch_num: 826\n",
      "Loss of train set: 0.30162328481674194 at epoch: 16 and batch_num: 827\n",
      "Loss of train set: 0.3850325345993042 at epoch: 16 and batch_num: 828\n",
      "Loss of train set: 0.3890961706638336 at epoch: 16 and batch_num: 829\n",
      "Loss of train set: 0.16027021408081055 at epoch: 16 and batch_num: 830\n",
      "Loss of train set: 0.229964017868042 at epoch: 16 and batch_num: 831\n",
      "Loss of train set: 0.180009663105011 at epoch: 16 and batch_num: 832\n",
      "Loss of train set: 0.3683812618255615 at epoch: 16 and batch_num: 833\n",
      "Loss of train set: 0.18680071830749512 at epoch: 16 and batch_num: 834\n",
      "Loss of train set: 0.13420379161834717 at epoch: 16 and batch_num: 835\n",
      "Loss of train set: 0.2561548948287964 at epoch: 16 and batch_num: 836\n",
      "Loss of train set: 0.1895636022090912 at epoch: 16 and batch_num: 837\n",
      "Loss of train set: 0.3612919747829437 at epoch: 16 and batch_num: 838\n",
      "Loss of train set: 0.2894730567932129 at epoch: 16 and batch_num: 839\n",
      "Loss of train set: 0.2643800973892212 at epoch: 16 and batch_num: 840\n",
      "Loss of train set: 0.29001542925834656 at epoch: 16 and batch_num: 841\n",
      "Loss of train set: 0.35556530952453613 at epoch: 16 and batch_num: 842\n",
      "Loss of train set: 0.2758219838142395 at epoch: 16 and batch_num: 843\n",
      "Loss of train set: 0.2133159041404724 at epoch: 16 and batch_num: 844\n",
      "Loss of train set: 0.30150043964385986 at epoch: 16 and batch_num: 845\n",
      "Loss of train set: 0.16226214170455933 at epoch: 16 and batch_num: 846\n",
      "Loss of train set: 0.32355934381484985 at epoch: 16 and batch_num: 847\n",
      "Loss of train set: 0.41152775287628174 at epoch: 16 and batch_num: 848\n",
      "Loss of train set: 0.3723081350326538 at epoch: 16 and batch_num: 849\n",
      "Loss of train set: 0.33550208806991577 at epoch: 16 and batch_num: 850\n",
      "Loss of train set: 0.20067596435546875 at epoch: 16 and batch_num: 851\n",
      "Loss of train set: 0.41547873616218567 at epoch: 16 and batch_num: 852\n",
      "Loss of train set: 0.44398823380470276 at epoch: 16 and batch_num: 853\n",
      "Loss of train set: 0.24101731181144714 at epoch: 16 and batch_num: 854\n",
      "Loss of train set: 0.28767329454421997 at epoch: 16 and batch_num: 855\n",
      "Loss of train set: 0.186770498752594 at epoch: 16 and batch_num: 856\n",
      "Loss of train set: 0.288373202085495 at epoch: 16 and batch_num: 857\n",
      "Loss of train set: 0.3113066852092743 at epoch: 16 and batch_num: 858\n",
      "Loss of train set: 0.3969426155090332 at epoch: 16 and batch_num: 859\n",
      "Loss of train set: 0.44295555353164673 at epoch: 16 and batch_num: 860\n",
      "Loss of train set: 0.2174670398235321 at epoch: 16 and batch_num: 861\n",
      "Loss of train set: 0.2903413772583008 at epoch: 16 and batch_num: 862\n",
      "Loss of train set: 0.15158097445964813 at epoch: 16 and batch_num: 863\n",
      "Loss of train set: 0.3907034397125244 at epoch: 16 and batch_num: 864\n",
      "Loss of train set: 0.3457622528076172 at epoch: 16 and batch_num: 865\n",
      "Loss of train set: 0.3184603452682495 at epoch: 16 and batch_num: 866\n",
      "Loss of train set: 0.5795499682426453 at epoch: 16 and batch_num: 867\n",
      "Loss of train set: 0.5216255784034729 at epoch: 16 and batch_num: 868\n",
      "Loss of train set: 0.23518140614032745 at epoch: 16 and batch_num: 869\n",
      "Loss of train set: 0.24170751869678497 at epoch: 16 and batch_num: 870\n",
      "Loss of train set: 0.4518176317214966 at epoch: 16 and batch_num: 871\n",
      "Loss of train set: 0.4121764600276947 at epoch: 16 and batch_num: 872\n",
      "Loss of train set: 0.3235326409339905 at epoch: 16 and batch_num: 873\n",
      "Loss of train set: 0.27897733449935913 at epoch: 16 and batch_num: 874\n",
      "Loss of train set: 0.21926839649677277 at epoch: 16 and batch_num: 875\n",
      "Loss of train set: 0.3078106939792633 at epoch: 16 and batch_num: 876\n",
      "Loss of train set: 0.3674054741859436 at epoch: 16 and batch_num: 877\n",
      "Loss of train set: 0.29476386308670044 at epoch: 16 and batch_num: 878\n",
      "Loss of train set: 0.24999335408210754 at epoch: 16 and batch_num: 879\n",
      "Loss of train set: 0.37229761481285095 at epoch: 16 and batch_num: 880\n",
      "Loss of train set: 0.23651300370693207 at epoch: 16 and batch_num: 881\n",
      "Loss of train set: 0.11544552445411682 at epoch: 16 and batch_num: 882\n",
      "Loss of train set: 0.25641998648643494 at epoch: 16 and batch_num: 883\n",
      "Loss of train set: 0.34333038330078125 at epoch: 16 and batch_num: 884\n",
      "Loss of train set: 0.23581641912460327 at epoch: 16 and batch_num: 885\n",
      "Loss of train set: 0.3699883222579956 at epoch: 16 and batch_num: 886\n",
      "Loss of train set: 0.23421624302864075 at epoch: 16 and batch_num: 887\n",
      "Loss of train set: 0.2814173996448517 at epoch: 16 and batch_num: 888\n",
      "Loss of train set: 0.33352091908454895 at epoch: 16 and batch_num: 889\n",
      "Loss of train set: 0.28617292642593384 at epoch: 16 and batch_num: 890\n",
      "Loss of train set: 0.26702821254730225 at epoch: 16 and batch_num: 891\n",
      "Loss of train set: 0.2216567099094391 at epoch: 16 and batch_num: 892\n",
      "Loss of train set: 0.28338682651519775 at epoch: 16 and batch_num: 893\n",
      "Loss of train set: 0.2720928490161896 at epoch: 16 and batch_num: 894\n",
      "Loss of train set: 0.4659530818462372 at epoch: 16 and batch_num: 895\n",
      "Loss of train set: 0.30665701627731323 at epoch: 16 and batch_num: 896\n",
      "Loss of train set: 0.2554636299610138 at epoch: 16 and batch_num: 897\n",
      "Loss of train set: 0.4925077259540558 at epoch: 16 and batch_num: 898\n",
      "Loss of train set: 0.2533797025680542 at epoch: 16 and batch_num: 899\n",
      "Loss of train set: 0.48534107208251953 at epoch: 16 and batch_num: 900\n",
      "Loss of train set: 0.10930344462394714 at epoch: 16 and batch_num: 901\n",
      "Loss of train set: 0.3632661700248718 at epoch: 16 and batch_num: 902\n",
      "Loss of train set: 0.17502962052822113 at epoch: 16 and batch_num: 903\n",
      "Loss of train set: 0.2832186222076416 at epoch: 16 and batch_num: 904\n",
      "Loss of train set: 0.28637105226516724 at epoch: 16 and batch_num: 905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3066977262496948 at epoch: 16 and batch_num: 906\n",
      "Loss of train set: 0.567466676235199 at epoch: 16 and batch_num: 907\n",
      "Loss of train set: 0.2759243845939636 at epoch: 16 and batch_num: 908\n",
      "Loss of train set: 0.214964359998703 at epoch: 16 and batch_num: 909\n",
      "Loss of train set: 0.33906131982803345 at epoch: 16 and batch_num: 910\n",
      "Loss of train set: 0.14991533756256104 at epoch: 16 and batch_num: 911\n",
      "Loss of train set: 0.24892811477184296 at epoch: 16 and batch_num: 912\n",
      "Loss of train set: 0.23734134435653687 at epoch: 16 and batch_num: 913\n",
      "Loss of train set: 0.24329936504364014 at epoch: 16 and batch_num: 914\n",
      "Loss of train set: 0.20587408542633057 at epoch: 16 and batch_num: 915\n",
      "Loss of train set: 0.3331937789916992 at epoch: 16 and batch_num: 916\n",
      "Loss of train set: 0.3574240207672119 at epoch: 16 and batch_num: 917\n",
      "Loss of train set: 0.3314421772956848 at epoch: 16 and batch_num: 918\n",
      "Loss of train set: 0.5673454999923706 at epoch: 16 and batch_num: 919\n",
      "Loss of train set: 0.2101694643497467 at epoch: 16 and batch_num: 920\n",
      "Loss of train set: 0.3958505392074585 at epoch: 16 and batch_num: 921\n",
      "Loss of train set: 0.3314061164855957 at epoch: 16 and batch_num: 922\n",
      "Loss of train set: 0.18974436819553375 at epoch: 16 and batch_num: 923\n",
      "Loss of train set: 0.24038513004779816 at epoch: 16 and batch_num: 924\n",
      "Loss of train set: 0.3678296208381653 at epoch: 16 and batch_num: 925\n",
      "Loss of train set: 0.160329207777977 at epoch: 16 and batch_num: 926\n",
      "Loss of train set: 0.38192734122276306 at epoch: 16 and batch_num: 927\n",
      "Loss of train set: 0.3528890609741211 at epoch: 16 and batch_num: 928\n",
      "Loss of train set: 0.22333800792694092 at epoch: 16 and batch_num: 929\n",
      "Loss of train set: 0.28792935609817505 at epoch: 16 and batch_num: 930\n",
      "Loss of train set: 0.41244781017303467 at epoch: 16 and batch_num: 931\n",
      "Loss of train set: 0.17720606923103333 at epoch: 16 and batch_num: 932\n",
      "Loss of train set: 0.2699427008628845 at epoch: 16 and batch_num: 933\n",
      "Loss of train set: 0.4291227459907532 at epoch: 16 and batch_num: 934\n",
      "Loss of train set: 0.4071786105632782 at epoch: 16 and batch_num: 935\n",
      "Loss of train set: 0.5324959754943848 at epoch: 16 and batch_num: 936\n",
      "Loss of train set: 0.18325668573379517 at epoch: 16 and batch_num: 937\n",
      "Accuracy of train set: 0.89275\n",
      "Loss of test set: 0.24096834659576416 at epoch: 16 and batch_num: 0\n",
      "Loss of test set: 0.336123526096344 at epoch: 16 and batch_num: 1\n",
      "Loss of test set: 0.3598819375038147 at epoch: 16 and batch_num: 2\n",
      "Loss of test set: 0.502382218837738 at epoch: 16 and batch_num: 3\n",
      "Loss of test set: 0.4087238311767578 at epoch: 16 and batch_num: 4\n",
      "Loss of test set: 0.24397408962249756 at epoch: 16 and batch_num: 5\n",
      "Loss of test set: 0.28476959466934204 at epoch: 16 and batch_num: 6\n",
      "Loss of test set: 0.2868364453315735 at epoch: 16 and batch_num: 7\n",
      "Loss of test set: 0.44053158164024353 at epoch: 16 and batch_num: 8\n",
      "Loss of test set: 0.2878607213497162 at epoch: 16 and batch_num: 9\n",
      "Loss of test set: 0.4567902684211731 at epoch: 16 and batch_num: 10\n",
      "Loss of test set: 0.5027539730072021 at epoch: 16 and batch_num: 11\n",
      "Loss of test set: 0.3021293580532074 at epoch: 16 and batch_num: 12\n",
      "Loss of test set: 0.5200655460357666 at epoch: 16 and batch_num: 13\n",
      "Loss of test set: 0.530013382434845 at epoch: 16 and batch_num: 14\n",
      "Loss of test set: 0.28098195791244507 at epoch: 16 and batch_num: 15\n",
      "Loss of test set: 0.23275834321975708 at epoch: 16 and batch_num: 16\n",
      "Loss of test set: 0.3167066276073456 at epoch: 16 and batch_num: 17\n",
      "Loss of test set: 0.35233044624328613 at epoch: 16 and batch_num: 18\n",
      "Loss of test set: 0.37890762090682983 at epoch: 16 and batch_num: 19\n",
      "Loss of test set: 0.38506650924682617 at epoch: 16 and batch_num: 20\n",
      "Loss of test set: 0.37935227155685425 at epoch: 16 and batch_num: 21\n",
      "Loss of test set: 0.24118074774742126 at epoch: 16 and batch_num: 22\n",
      "Loss of test set: 0.37927260994911194 at epoch: 16 and batch_num: 23\n",
      "Loss of test set: 0.24660392105579376 at epoch: 16 and batch_num: 24\n",
      "Loss of test set: 0.3434259295463562 at epoch: 16 and batch_num: 25\n",
      "Loss of test set: 0.30253034830093384 at epoch: 16 and batch_num: 26\n",
      "Loss of test set: 0.4396645128726959 at epoch: 16 and batch_num: 27\n",
      "Loss of test set: 0.5253458023071289 at epoch: 16 and batch_num: 28\n",
      "Loss of test set: 0.36295053362846375 at epoch: 16 and batch_num: 29\n",
      "Loss of test set: 0.3586055040359497 at epoch: 16 and batch_num: 30\n",
      "Loss of test set: 0.44957518577575684 at epoch: 16 and batch_num: 31\n",
      "Loss of test set: 0.3916891813278198 at epoch: 16 and batch_num: 32\n",
      "Loss of test set: 0.4426876902580261 at epoch: 16 and batch_num: 33\n",
      "Loss of test set: 0.48118269443511963 at epoch: 16 and batch_num: 34\n",
      "Loss of test set: 0.29064834117889404 at epoch: 16 and batch_num: 35\n",
      "Loss of test set: 0.48965173959732056 at epoch: 16 and batch_num: 36\n",
      "Loss of test set: 0.33347976207733154 at epoch: 16 and batch_num: 37\n",
      "Loss of test set: 0.28019440174102783 at epoch: 16 and batch_num: 38\n",
      "Loss of test set: 0.3305791914463043 at epoch: 16 and batch_num: 39\n",
      "Loss of test set: 0.5641745328903198 at epoch: 16 and batch_num: 40\n",
      "Loss of test set: 0.3503076434135437 at epoch: 16 and batch_num: 41\n",
      "Loss of test set: 0.191792294383049 at epoch: 16 and batch_num: 42\n",
      "Loss of test set: 0.20208437740802765 at epoch: 16 and batch_num: 43\n",
      "Loss of test set: 0.3207662105560303 at epoch: 16 and batch_num: 44\n",
      "Loss of test set: 0.29406145215034485 at epoch: 16 and batch_num: 45\n",
      "Loss of test set: 0.1784312129020691 at epoch: 16 and batch_num: 46\n",
      "Loss of test set: 0.5110445022583008 at epoch: 16 and batch_num: 47\n",
      "Loss of test set: 0.2728564739227295 at epoch: 16 and batch_num: 48\n",
      "Loss of test set: 0.17374886572360992 at epoch: 16 and batch_num: 49\n",
      "Loss of test set: 0.35033535957336426 at epoch: 16 and batch_num: 50\n",
      "Loss of test set: 0.265608012676239 at epoch: 16 and batch_num: 51\n",
      "Loss of test set: 0.3254924416542053 at epoch: 16 and batch_num: 52\n",
      "Loss of test set: 0.39047250151634216 at epoch: 16 and batch_num: 53\n",
      "Loss of test set: 0.3454464077949524 at epoch: 16 and batch_num: 54\n",
      "Loss of test set: 0.2955487370491028 at epoch: 16 and batch_num: 55\n",
      "Loss of test set: 0.40629062056541443 at epoch: 16 and batch_num: 56\n",
      "Loss of test set: 0.3351489007472992 at epoch: 16 and batch_num: 57\n",
      "Loss of test set: 0.21890577673912048 at epoch: 16 and batch_num: 58\n",
      "Loss of test set: 0.2784121632575989 at epoch: 16 and batch_num: 59\n",
      "Loss of test set: 0.36107853055000305 at epoch: 16 and batch_num: 60\n",
      "Loss of test set: 0.36523720622062683 at epoch: 16 and batch_num: 61\n",
      "Loss of test set: 0.33699437975883484 at epoch: 16 and batch_num: 62\n",
      "Loss of test set: 0.2584325075149536 at epoch: 16 and batch_num: 63\n",
      "Loss of test set: 0.3744036555290222 at epoch: 16 and batch_num: 64\n",
      "Loss of test set: 0.5238145589828491 at epoch: 16 and batch_num: 65\n",
      "Loss of test set: 0.408488929271698 at epoch: 16 and batch_num: 66\n",
      "Loss of test set: 0.4184761047363281 at epoch: 16 and batch_num: 67\n",
      "Loss of test set: 0.16026882827281952 at epoch: 16 and batch_num: 68\n",
      "Loss of test set: 0.3009459674358368 at epoch: 16 and batch_num: 69\n",
      "Loss of test set: 0.45997726917266846 at epoch: 16 and batch_num: 70\n",
      "Loss of test set: 0.2890627682209015 at epoch: 16 and batch_num: 71\n",
      "Loss of test set: 0.2677689492702484 at epoch: 16 and batch_num: 72\n",
      "Loss of test set: 0.2732381820678711 at epoch: 16 and batch_num: 73\n",
      "Loss of test set: 0.5271002054214478 at epoch: 16 and batch_num: 74\n",
      "Loss of test set: 0.2632027864456177 at epoch: 16 and batch_num: 75\n",
      "Loss of test set: 0.229275643825531 at epoch: 16 and batch_num: 76\n",
      "Loss of test set: 0.38173580169677734 at epoch: 16 and batch_num: 77\n",
      "Loss of test set: 0.22148844599723816 at epoch: 16 and batch_num: 78\n",
      "Loss of test set: 0.23681636154651642 at epoch: 16 and batch_num: 79\n",
      "Loss of test set: 0.6722307205200195 at epoch: 16 and batch_num: 80\n",
      "Loss of test set: 0.3010818362236023 at epoch: 16 and batch_num: 81\n",
      "Loss of test set: 0.2440474033355713 at epoch: 16 and batch_num: 82\n",
      "Loss of test set: 0.4089670181274414 at epoch: 16 and batch_num: 83\n",
      "Loss of test set: 0.40957286953926086 at epoch: 16 and batch_num: 84\n",
      "Loss of test set: 0.3737402856349945 at epoch: 16 and batch_num: 85\n",
      "Loss of test set: 0.12454517185688019 at epoch: 16 and batch_num: 86\n",
      "Loss of test set: 0.4285426735877991 at epoch: 16 and batch_num: 87\n",
      "Loss of test set: 0.592646598815918 at epoch: 16 and batch_num: 88\n",
      "Loss of test set: 0.37940526008605957 at epoch: 16 and batch_num: 89\n",
      "Loss of test set: 0.4773006737232208 at epoch: 16 and batch_num: 90\n",
      "Loss of test set: 0.368593692779541 at epoch: 16 and batch_num: 91\n",
      "Loss of test set: 0.3301718831062317 at epoch: 16 and batch_num: 92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.3401579260826111 at epoch: 16 and batch_num: 93\n",
      "Loss of test set: 0.44222575426101685 at epoch: 16 and batch_num: 94\n",
      "Loss of test set: 0.2821476459503174 at epoch: 16 and batch_num: 95\n",
      "Loss of test set: 0.6600195169448853 at epoch: 16 and batch_num: 96\n",
      "Loss of test set: 0.2088320255279541 at epoch: 16 and batch_num: 97\n",
      "Loss of test set: 0.3413323163986206 at epoch: 16 and batch_num: 98\n",
      "Loss of test set: 0.2488679438829422 at epoch: 16 and batch_num: 99\n",
      "Loss of test set: 0.33136647939682007 at epoch: 16 and batch_num: 100\n",
      "Loss of test set: 0.47895383834838867 at epoch: 16 and batch_num: 101\n",
      "Loss of test set: 0.3820744752883911 at epoch: 16 and batch_num: 102\n",
      "Loss of test set: 0.29859066009521484 at epoch: 16 and batch_num: 103\n",
      "Loss of test set: 0.2998133897781372 at epoch: 16 and batch_num: 104\n",
      "Loss of test set: 0.360586941242218 at epoch: 16 and batch_num: 105\n",
      "Loss of test set: 0.3041248321533203 at epoch: 16 and batch_num: 106\n",
      "Loss of test set: 0.5329009294509888 at epoch: 16 and batch_num: 107\n",
      "Loss of test set: 0.3226856589317322 at epoch: 16 and batch_num: 108\n",
      "Loss of test set: 0.3473508954048157 at epoch: 16 and batch_num: 109\n",
      "Loss of test set: 0.17427581548690796 at epoch: 16 and batch_num: 110\n",
      "Loss of test set: 0.2928452789783478 at epoch: 16 and batch_num: 111\n",
      "Loss of test set: 0.4094346761703491 at epoch: 16 and batch_num: 112\n",
      "Loss of test set: 0.44178882241249084 at epoch: 16 and batch_num: 113\n",
      "Loss of test set: 0.29260575771331787 at epoch: 16 and batch_num: 114\n",
      "Loss of test set: 0.4662895202636719 at epoch: 16 and batch_num: 115\n",
      "Loss of test set: 0.260631263256073 at epoch: 16 and batch_num: 116\n",
      "Loss of test set: 0.2788769602775574 at epoch: 16 and batch_num: 117\n",
      "Loss of test set: 0.39177778363227844 at epoch: 16 and batch_num: 118\n",
      "Loss of test set: 0.2696729004383087 at epoch: 16 and batch_num: 119\n",
      "Loss of test set: 0.3624872863292694 at epoch: 16 and batch_num: 120\n",
      "Loss of test set: 0.2256557047367096 at epoch: 16 and batch_num: 121\n",
      "Loss of test set: 0.17606058716773987 at epoch: 16 and batch_num: 122\n",
      "Loss of test set: 0.2508928179740906 at epoch: 16 and batch_num: 123\n",
      "Loss of test set: 0.43734806776046753 at epoch: 16 and batch_num: 124\n",
      "Loss of test set: 0.31673258543014526 at epoch: 16 and batch_num: 125\n",
      "Loss of test set: 0.4376451373100281 at epoch: 16 and batch_num: 126\n",
      "Loss of test set: 0.22564995288848877 at epoch: 16 and batch_num: 127\n",
      "Loss of test set: 0.37152376770973206 at epoch: 16 and batch_num: 128\n",
      "Loss of test set: 0.42969584465026855 at epoch: 16 and batch_num: 129\n",
      "Loss of test set: 0.37499210238456726 at epoch: 16 and batch_num: 130\n",
      "Loss of test set: 0.4023156464099884 at epoch: 16 and batch_num: 131\n",
      "Loss of test set: 0.47171103954315186 at epoch: 16 and batch_num: 132\n",
      "Loss of test set: 0.4294637441635132 at epoch: 16 and batch_num: 133\n",
      "Loss of test set: 0.35644564032554626 at epoch: 16 and batch_num: 134\n",
      "Loss of test set: 0.3549387454986572 at epoch: 16 and batch_num: 135\n",
      "Loss of test set: 0.3916509747505188 at epoch: 16 and batch_num: 136\n",
      "Loss of test set: 0.42816656827926636 at epoch: 16 and batch_num: 137\n",
      "Loss of test set: 0.22668839991092682 at epoch: 16 and batch_num: 138\n",
      "Loss of test set: 0.26739975810050964 at epoch: 16 and batch_num: 139\n",
      "Loss of test set: 0.37857648730278015 at epoch: 16 and batch_num: 140\n",
      "Loss of test set: 0.2229975461959839 at epoch: 16 and batch_num: 141\n",
      "Loss of test set: 0.28974902629852295 at epoch: 16 and batch_num: 142\n",
      "Loss of test set: 0.2504355013370514 at epoch: 16 and batch_num: 143\n",
      "Loss of test set: 0.4237734079360962 at epoch: 16 and batch_num: 144\n",
      "Loss of test set: 0.24681523442268372 at epoch: 16 and batch_num: 145\n",
      "Loss of test set: 0.23809094727039337 at epoch: 16 and batch_num: 146\n",
      "Loss of test set: 0.23137113451957703 at epoch: 16 and batch_num: 147\n",
      "Loss of test set: 0.5198054313659668 at epoch: 16 and batch_num: 148\n",
      "Loss of test set: 0.2734033465385437 at epoch: 16 and batch_num: 149\n",
      "Loss of test set: 0.44537073373794556 at epoch: 16 and batch_num: 150\n",
      "Loss of test set: 0.35442787408828735 at epoch: 16 and batch_num: 151\n",
      "Loss of test set: 0.6512913703918457 at epoch: 16 and batch_num: 152\n",
      "Loss of test set: 0.3568883538246155 at epoch: 16 and batch_num: 153\n",
      "Loss of test set: 0.45154207944869995 at epoch: 16 and batch_num: 154\n",
      "Loss of test set: 0.37099844217300415 at epoch: 16 and batch_num: 155\n",
      "Loss of test set: 0.6752341985702515 at epoch: 16 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8718\n",
      "Loss of train set: 0.22454151511192322 at epoch: 17 and batch_num: 0\n",
      "Loss of train set: 0.23864158987998962 at epoch: 17 and batch_num: 1\n",
      "Loss of train set: 0.23205193877220154 at epoch: 17 and batch_num: 2\n",
      "Loss of train set: 0.2736373841762543 at epoch: 17 and batch_num: 3\n",
      "Loss of train set: 0.31353551149368286 at epoch: 17 and batch_num: 4\n",
      "Loss of train set: 0.2389337122440338 at epoch: 17 and batch_num: 5\n",
      "Loss of train set: 0.27954691648483276 at epoch: 17 and batch_num: 6\n",
      "Loss of train set: 0.24776428937911987 at epoch: 17 and batch_num: 7\n",
      "Loss of train set: 0.332599014043808 at epoch: 17 and batch_num: 8\n",
      "Loss of train set: 0.18107523024082184 at epoch: 17 and batch_num: 9\n",
      "Loss of train set: 0.22695183753967285 at epoch: 17 and batch_num: 10\n",
      "Loss of train set: 0.1966744065284729 at epoch: 17 and batch_num: 11\n",
      "Loss of train set: 0.19359725713729858 at epoch: 17 and batch_num: 12\n",
      "Loss of train set: 0.3818000257015228 at epoch: 17 and batch_num: 13\n",
      "Loss of train set: 0.3054652512073517 at epoch: 17 and batch_num: 14\n",
      "Loss of train set: 0.2796623110771179 at epoch: 17 and batch_num: 15\n",
      "Loss of train set: 0.2811115086078644 at epoch: 17 and batch_num: 16\n",
      "Loss of train set: 0.23450730741024017 at epoch: 17 and batch_num: 17\n",
      "Loss of train set: 0.34938642382621765 at epoch: 17 and batch_num: 18\n",
      "Loss of train set: 0.18793371319770813 at epoch: 17 and batch_num: 19\n",
      "Loss of train set: 0.39862728118896484 at epoch: 17 and batch_num: 20\n",
      "Loss of train set: 0.3721991181373596 at epoch: 17 and batch_num: 21\n",
      "Loss of train set: 0.1662423014640808 at epoch: 17 and batch_num: 22\n",
      "Loss of train set: 0.4374145567417145 at epoch: 17 and batch_num: 23\n",
      "Loss of train set: 0.277887761592865 at epoch: 17 and batch_num: 24\n",
      "Loss of train set: 0.36745989322662354 at epoch: 17 and batch_num: 25\n",
      "Loss of train set: 0.22016242146492004 at epoch: 17 and batch_num: 26\n",
      "Loss of train set: 0.26539719104766846 at epoch: 17 and batch_num: 27\n",
      "Loss of train set: 0.23762796819210052 at epoch: 17 and batch_num: 28\n",
      "Loss of train set: 0.20909284055233002 at epoch: 17 and batch_num: 29\n",
      "Loss of train set: 0.26015812158584595 at epoch: 17 and batch_num: 30\n",
      "Loss of train set: 0.38537925481796265 at epoch: 17 and batch_num: 31\n",
      "Loss of train set: 0.28725385665893555 at epoch: 17 and batch_num: 32\n",
      "Loss of train set: 0.19913208484649658 at epoch: 17 and batch_num: 33\n",
      "Loss of train set: 0.22452913224697113 at epoch: 17 and batch_num: 34\n",
      "Loss of train set: 0.15830829739570618 at epoch: 17 and batch_num: 35\n",
      "Loss of train set: 0.3110489845275879 at epoch: 17 and batch_num: 36\n",
      "Loss of train set: 0.29162511229515076 at epoch: 17 and batch_num: 37\n",
      "Loss of train set: 0.32764551043510437 at epoch: 17 and batch_num: 38\n",
      "Loss of train set: 0.2104904055595398 at epoch: 17 and batch_num: 39\n",
      "Loss of train set: 0.23245897889137268 at epoch: 17 and batch_num: 40\n",
      "Loss of train set: 0.11532540619373322 at epoch: 17 and batch_num: 41\n",
      "Loss of train set: 0.3409970700740814 at epoch: 17 and batch_num: 42\n",
      "Loss of train set: 0.40352052450180054 at epoch: 17 and batch_num: 43\n",
      "Loss of train set: 0.3836416006088257 at epoch: 17 and batch_num: 44\n",
      "Loss of train set: 0.34012851119041443 at epoch: 17 and batch_num: 45\n",
      "Loss of train set: 0.6146370768547058 at epoch: 17 and batch_num: 46\n",
      "Loss of train set: 0.20446990430355072 at epoch: 17 and batch_num: 47\n",
      "Loss of train set: 0.3460872173309326 at epoch: 17 and batch_num: 48\n",
      "Loss of train set: 0.25342610478401184 at epoch: 17 and batch_num: 49\n",
      "Loss of train set: 0.2644958198070526 at epoch: 17 and batch_num: 50\n",
      "Loss of train set: 0.25006741285324097 at epoch: 17 and batch_num: 51\n",
      "Loss of train set: 0.45818817615509033 at epoch: 17 and batch_num: 52\n",
      "Loss of train set: 0.2135639786720276 at epoch: 17 and batch_num: 53\n",
      "Loss of train set: 0.39625853300094604 at epoch: 17 and batch_num: 54\n",
      "Loss of train set: 0.2519517242908478 at epoch: 17 and batch_num: 55\n",
      "Loss of train set: 0.2033107578754425 at epoch: 17 and batch_num: 56\n",
      "Loss of train set: 0.286084920167923 at epoch: 17 and batch_num: 57\n",
      "Loss of train set: 0.4760669469833374 at epoch: 17 and batch_num: 58\n",
      "Loss of train set: 0.2213413268327713 at epoch: 17 and batch_num: 59\n",
      "Loss of train set: 0.33195221424102783 at epoch: 17 and batch_num: 60\n",
      "Loss of train set: 0.46321815252304077 at epoch: 17 and batch_num: 61\n",
      "Loss of train set: 0.37992626428604126 at epoch: 17 and batch_num: 62\n",
      "Loss of train set: 0.33096444606781006 at epoch: 17 and batch_num: 63\n",
      "Loss of train set: 0.2690433859825134 at epoch: 17 and batch_num: 64\n",
      "Loss of train set: 0.23676353693008423 at epoch: 17 and batch_num: 65\n",
      "Loss of train set: 0.2781374454498291 at epoch: 17 and batch_num: 66\n",
      "Loss of train set: 0.302402138710022 at epoch: 17 and batch_num: 67\n",
      "Loss of train set: 0.2509131133556366 at epoch: 17 and batch_num: 68\n",
      "Loss of train set: 0.19602932035923004 at epoch: 17 and batch_num: 69\n",
      "Loss of train set: 0.3117408752441406 at epoch: 17 and batch_num: 70\n",
      "Loss of train set: 0.32714712619781494 at epoch: 17 and batch_num: 71\n",
      "Loss of train set: 0.27511805295944214 at epoch: 17 and batch_num: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3804270625114441 at epoch: 17 and batch_num: 73\n",
      "Loss of train set: 0.30347108840942383 at epoch: 17 and batch_num: 74\n",
      "Loss of train set: 0.14132419228553772 at epoch: 17 and batch_num: 75\n",
      "Loss of train set: 0.2876940071582794 at epoch: 17 and batch_num: 76\n",
      "Loss of train set: 0.3869470953941345 at epoch: 17 and batch_num: 77\n",
      "Loss of train set: 0.20642675459384918 at epoch: 17 and batch_num: 78\n",
      "Loss of train set: 0.2701753079891205 at epoch: 17 and batch_num: 79\n",
      "Loss of train set: 0.28191250562667847 at epoch: 17 and batch_num: 80\n",
      "Loss of train set: 0.3253050446510315 at epoch: 17 and batch_num: 81\n",
      "Loss of train set: 0.43519800901412964 at epoch: 17 and batch_num: 82\n",
      "Loss of train set: 0.44857311248779297 at epoch: 17 and batch_num: 83\n",
      "Loss of train set: 0.37172675132751465 at epoch: 17 and batch_num: 84\n",
      "Loss of train set: 0.25414013862609863 at epoch: 17 and batch_num: 85\n",
      "Loss of train set: 0.23704904317855835 at epoch: 17 and batch_num: 86\n",
      "Loss of train set: 0.21974942088127136 at epoch: 17 and batch_num: 87\n",
      "Loss of train set: 0.23476389050483704 at epoch: 17 and batch_num: 88\n",
      "Loss of train set: 0.37709787487983704 at epoch: 17 and batch_num: 89\n",
      "Loss of train set: 0.24432313442230225 at epoch: 17 and batch_num: 90\n",
      "Loss of train set: 0.41037631034851074 at epoch: 17 and batch_num: 91\n",
      "Loss of train set: 0.2666293978691101 at epoch: 17 and batch_num: 92\n",
      "Loss of train set: 0.2440336048603058 at epoch: 17 and batch_num: 93\n",
      "Loss of train set: 0.38474905490875244 at epoch: 17 and batch_num: 94\n",
      "Loss of train set: 0.2560805082321167 at epoch: 17 and batch_num: 95\n",
      "Loss of train set: 0.3102564513683319 at epoch: 17 and batch_num: 96\n",
      "Loss of train set: 0.25002023577690125 at epoch: 17 and batch_num: 97\n",
      "Loss of train set: 0.26685428619384766 at epoch: 17 and batch_num: 98\n",
      "Loss of train set: 0.3341015875339508 at epoch: 17 and batch_num: 99\n",
      "Loss of train set: 0.2363288551568985 at epoch: 17 and batch_num: 100\n",
      "Loss of train set: 0.3624476194381714 at epoch: 17 and batch_num: 101\n",
      "Loss of train set: 0.23888476192951202 at epoch: 17 and batch_num: 102\n",
      "Loss of train set: 0.4417962431907654 at epoch: 17 and batch_num: 103\n",
      "Loss of train set: 0.2874852418899536 at epoch: 17 and batch_num: 104\n",
      "Loss of train set: 0.30466949939727783 at epoch: 17 and batch_num: 105\n",
      "Loss of train set: 0.33793652057647705 at epoch: 17 and batch_num: 106\n",
      "Loss of train set: 0.3142368495464325 at epoch: 17 and batch_num: 107\n",
      "Loss of train set: 0.33864784240722656 at epoch: 17 and batch_num: 108\n",
      "Loss of train set: 0.30643704533576965 at epoch: 17 and batch_num: 109\n",
      "Loss of train set: 0.35517358779907227 at epoch: 17 and batch_num: 110\n",
      "Loss of train set: 0.3250521123409271 at epoch: 17 and batch_num: 111\n",
      "Loss of train set: 0.1464485377073288 at epoch: 17 and batch_num: 112\n",
      "Loss of train set: 0.1776026487350464 at epoch: 17 and batch_num: 113\n",
      "Loss of train set: 0.3327082395553589 at epoch: 17 and batch_num: 114\n",
      "Loss of train set: 0.17980659008026123 at epoch: 17 and batch_num: 115\n",
      "Loss of train set: 0.14873336255550385 at epoch: 17 and batch_num: 116\n",
      "Loss of train set: 0.5060136914253235 at epoch: 17 and batch_num: 117\n",
      "Loss of train set: 0.2876881957054138 at epoch: 17 and batch_num: 118\n",
      "Loss of train set: 0.31198233366012573 at epoch: 17 and batch_num: 119\n",
      "Loss of train set: 0.329944372177124 at epoch: 17 and batch_num: 120\n",
      "Loss of train set: 0.46825265884399414 at epoch: 17 and batch_num: 121\n",
      "Loss of train set: 0.48549389839172363 at epoch: 17 and batch_num: 122\n",
      "Loss of train set: 0.30174559354782104 at epoch: 17 and batch_num: 123\n",
      "Loss of train set: 0.4756583571434021 at epoch: 17 and batch_num: 124\n",
      "Loss of train set: 0.29542237520217896 at epoch: 17 and batch_num: 125\n",
      "Loss of train set: 0.2957627475261688 at epoch: 17 and batch_num: 126\n",
      "Loss of train set: 0.34788838028907776 at epoch: 17 and batch_num: 127\n",
      "Loss of train set: 0.40787220001220703 at epoch: 17 and batch_num: 128\n",
      "Loss of train set: 0.3290570378303528 at epoch: 17 and batch_num: 129\n",
      "Loss of train set: 0.3343845009803772 at epoch: 17 and batch_num: 130\n",
      "Loss of train set: 0.3084179759025574 at epoch: 17 and batch_num: 131\n",
      "Loss of train set: 0.2777925431728363 at epoch: 17 and batch_num: 132\n",
      "Loss of train set: 0.5279547572135925 at epoch: 17 and batch_num: 133\n",
      "Loss of train set: 0.29722511768341064 at epoch: 17 and batch_num: 134\n",
      "Loss of train set: 0.41649216413497925 at epoch: 17 and batch_num: 135\n",
      "Loss of train set: 0.21962924301624298 at epoch: 17 and batch_num: 136\n",
      "Loss of train set: 0.4802347421646118 at epoch: 17 and batch_num: 137\n",
      "Loss of train set: 0.24919958412647247 at epoch: 17 and batch_num: 138\n",
      "Loss of train set: 0.4130256772041321 at epoch: 17 and batch_num: 139\n",
      "Loss of train set: 0.3738032579421997 at epoch: 17 and batch_num: 140\n",
      "Loss of train set: 0.27938419580459595 at epoch: 17 and batch_num: 141\n",
      "Loss of train set: 0.13990971446037292 at epoch: 17 and batch_num: 142\n",
      "Loss of train set: 0.2908579707145691 at epoch: 17 and batch_num: 143\n",
      "Loss of train set: 0.25765931606292725 at epoch: 17 and batch_num: 144\n",
      "Loss of train set: 0.4100190997123718 at epoch: 17 and batch_num: 145\n",
      "Loss of train set: 0.264311820268631 at epoch: 17 and batch_num: 146\n",
      "Loss of train set: 0.30827510356903076 at epoch: 17 and batch_num: 147\n",
      "Loss of train set: 0.31398940086364746 at epoch: 17 and batch_num: 148\n",
      "Loss of train set: 0.30941110849380493 at epoch: 17 and batch_num: 149\n",
      "Loss of train set: 0.2723478674888611 at epoch: 17 and batch_num: 150\n",
      "Loss of train set: 0.23587752878665924 at epoch: 17 and batch_num: 151\n",
      "Loss of train set: 0.30461180210113525 at epoch: 17 and batch_num: 152\n",
      "Loss of train set: 0.3007969260215759 at epoch: 17 and batch_num: 153\n",
      "Loss of train set: 0.23121377825737 at epoch: 17 and batch_num: 154\n",
      "Loss of train set: 0.2787487506866455 at epoch: 17 and batch_num: 155\n",
      "Loss of train set: 0.22162115573883057 at epoch: 17 and batch_num: 156\n",
      "Loss of train set: 0.3536781072616577 at epoch: 17 and batch_num: 157\n",
      "Loss of train set: 0.349251925945282 at epoch: 17 and batch_num: 158\n",
      "Loss of train set: 0.3938504755496979 at epoch: 17 and batch_num: 159\n",
      "Loss of train set: 0.25597718358039856 at epoch: 17 and batch_num: 160\n",
      "Loss of train set: 0.33846014738082886 at epoch: 17 and batch_num: 161\n",
      "Loss of train set: 0.39875879883766174 at epoch: 17 and batch_num: 162\n",
      "Loss of train set: 0.3170555830001831 at epoch: 17 and batch_num: 163\n",
      "Loss of train set: 0.3977707028388977 at epoch: 17 and batch_num: 164\n",
      "Loss of train set: 0.3609147071838379 at epoch: 17 and batch_num: 165\n",
      "Loss of train set: 0.196939155459404 at epoch: 17 and batch_num: 166\n",
      "Loss of train set: 0.421960711479187 at epoch: 17 and batch_num: 167\n",
      "Loss of train set: 0.19465553760528564 at epoch: 17 and batch_num: 168\n",
      "Loss of train set: 0.2413237988948822 at epoch: 17 and batch_num: 169\n",
      "Loss of train set: 0.24190053343772888 at epoch: 17 and batch_num: 170\n",
      "Loss of train set: 0.29459336400032043 at epoch: 17 and batch_num: 171\n",
      "Loss of train set: 0.30675044655799866 at epoch: 17 and batch_num: 172\n",
      "Loss of train set: 0.19446924328804016 at epoch: 17 and batch_num: 173\n",
      "Loss of train set: 0.25631287693977356 at epoch: 17 and batch_num: 174\n",
      "Loss of train set: 0.3383086919784546 at epoch: 17 and batch_num: 175\n",
      "Loss of train set: 0.24389570951461792 at epoch: 17 and batch_num: 176\n",
      "Loss of train set: 0.2938001751899719 at epoch: 17 and batch_num: 177\n",
      "Loss of train set: 0.2290748953819275 at epoch: 17 and batch_num: 178\n",
      "Loss of train set: 0.31223002076148987 at epoch: 17 and batch_num: 179\n",
      "Loss of train set: 0.1875520646572113 at epoch: 17 and batch_num: 180\n",
      "Loss of train set: 0.11214634776115417 at epoch: 17 and batch_num: 181\n",
      "Loss of train set: 0.3593594431877136 at epoch: 17 and batch_num: 182\n",
      "Loss of train set: 0.3059072196483612 at epoch: 17 and batch_num: 183\n",
      "Loss of train set: 0.3017308712005615 at epoch: 17 and batch_num: 184\n",
      "Loss of train set: 0.3534611463546753 at epoch: 17 and batch_num: 185\n",
      "Loss of train set: 0.3910994529724121 at epoch: 17 and batch_num: 186\n",
      "Loss of train set: 0.32508373260498047 at epoch: 17 and batch_num: 187\n",
      "Loss of train set: 0.40157607197761536 at epoch: 17 and batch_num: 188\n",
      "Loss of train set: 0.31610119342803955 at epoch: 17 and batch_num: 189\n",
      "Loss of train set: 0.3454210162162781 at epoch: 17 and batch_num: 190\n",
      "Loss of train set: 0.25078797340393066 at epoch: 17 and batch_num: 191\n",
      "Loss of train set: 0.16604693233966827 at epoch: 17 and batch_num: 192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.26127463579177856 at epoch: 17 and batch_num: 193\n",
      "Loss of train set: 0.3723158836364746 at epoch: 17 and batch_num: 194\n",
      "Loss of train set: 0.2558632791042328 at epoch: 17 and batch_num: 195\n",
      "Loss of train set: 0.4760451018810272 at epoch: 17 and batch_num: 196\n",
      "Loss of train set: 0.1827218234539032 at epoch: 17 and batch_num: 197\n",
      "Loss of train set: 0.20976611971855164 at epoch: 17 and batch_num: 198\n",
      "Loss of train set: 0.23161034286022186 at epoch: 17 and batch_num: 199\n",
      "Loss of train set: 0.2997417449951172 at epoch: 17 and batch_num: 200\n",
      "Loss of train set: 0.20634347200393677 at epoch: 17 and batch_num: 201\n",
      "Loss of train set: 0.25444328784942627 at epoch: 17 and batch_num: 202\n",
      "Loss of train set: 0.309303343296051 at epoch: 17 and batch_num: 203\n",
      "Loss of train set: 0.22164127230644226 at epoch: 17 and batch_num: 204\n",
      "Loss of train set: 0.1133977547287941 at epoch: 17 and batch_num: 205\n",
      "Loss of train set: 0.4318754971027374 at epoch: 17 and batch_num: 206\n",
      "Loss of train set: 0.25124889612197876 at epoch: 17 and batch_num: 207\n",
      "Loss of train set: 0.2062218338251114 at epoch: 17 and batch_num: 208\n",
      "Loss of train set: 0.2890874147415161 at epoch: 17 and batch_num: 209\n",
      "Loss of train set: 0.26265424489974976 at epoch: 17 and batch_num: 210\n",
      "Loss of train set: 0.2783190906047821 at epoch: 17 and batch_num: 211\n",
      "Loss of train set: 0.28622227907180786 at epoch: 17 and batch_num: 212\n",
      "Loss of train set: 0.4839375913143158 at epoch: 17 and batch_num: 213\n",
      "Loss of train set: 0.21518392860889435 at epoch: 17 and batch_num: 214\n",
      "Loss of train set: 0.41927412152290344 at epoch: 17 and batch_num: 215\n",
      "Loss of train set: 0.2620110511779785 at epoch: 17 and batch_num: 216\n",
      "Loss of train set: 0.2619735896587372 at epoch: 17 and batch_num: 217\n",
      "Loss of train set: 0.22443711757659912 at epoch: 17 and batch_num: 218\n",
      "Loss of train set: 0.29810187220573425 at epoch: 17 and batch_num: 219\n",
      "Loss of train set: 0.22817309200763702 at epoch: 17 and batch_num: 220\n",
      "Loss of train set: 0.2102431058883667 at epoch: 17 and batch_num: 221\n",
      "Loss of train set: 0.183714359998703 at epoch: 17 and batch_num: 222\n",
      "Loss of train set: 0.32450515031814575 at epoch: 17 and batch_num: 223\n",
      "Loss of train set: 0.30690592527389526 at epoch: 17 and batch_num: 224\n",
      "Loss of train set: 0.2692166864871979 at epoch: 17 and batch_num: 225\n",
      "Loss of train set: 0.25355517864227295 at epoch: 17 and batch_num: 226\n",
      "Loss of train set: 0.31861844658851624 at epoch: 17 and batch_num: 227\n",
      "Loss of train set: 0.3405891954898834 at epoch: 17 and batch_num: 228\n",
      "Loss of train set: 0.3425180912017822 at epoch: 17 and batch_num: 229\n",
      "Loss of train set: 0.21142882108688354 at epoch: 17 and batch_num: 230\n",
      "Loss of train set: 0.36996960639953613 at epoch: 17 and batch_num: 231\n",
      "Loss of train set: 0.2015196681022644 at epoch: 17 and batch_num: 232\n",
      "Loss of train set: 0.3220541775226593 at epoch: 17 and batch_num: 233\n",
      "Loss of train set: 0.23815593123435974 at epoch: 17 and batch_num: 234\n",
      "Loss of train set: 0.3230467140674591 at epoch: 17 and batch_num: 235\n",
      "Loss of train set: 0.40036895871162415 at epoch: 17 and batch_num: 236\n",
      "Loss of train set: 0.4731757938861847 at epoch: 17 and batch_num: 237\n",
      "Loss of train set: 0.3201192617416382 at epoch: 17 and batch_num: 238\n",
      "Loss of train set: 0.2728774845600128 at epoch: 17 and batch_num: 239\n",
      "Loss of train set: 0.3301571309566498 at epoch: 17 and batch_num: 240\n",
      "Loss of train set: 0.3291734457015991 at epoch: 17 and batch_num: 241\n",
      "Loss of train set: 0.23793500661849976 at epoch: 17 and batch_num: 242\n",
      "Loss of train set: 0.11440838873386383 at epoch: 17 and batch_num: 243\n",
      "Loss of train set: 0.3663146495819092 at epoch: 17 and batch_num: 244\n",
      "Loss of train set: 0.2078324854373932 at epoch: 17 and batch_num: 245\n",
      "Loss of train set: 0.389290452003479 at epoch: 17 and batch_num: 246\n",
      "Loss of train set: 0.3174958825111389 at epoch: 17 and batch_num: 247\n",
      "Loss of train set: 0.2488366961479187 at epoch: 17 and batch_num: 248\n",
      "Loss of train set: 0.2468678206205368 at epoch: 17 and batch_num: 249\n",
      "Loss of train set: 0.23603221774101257 at epoch: 17 and batch_num: 250\n",
      "Loss of train set: 0.252908319234848 at epoch: 17 and batch_num: 251\n",
      "Loss of train set: 0.37323468923568726 at epoch: 17 and batch_num: 252\n",
      "Loss of train set: 0.2555190324783325 at epoch: 17 and batch_num: 253\n",
      "Loss of train set: 0.3842161297798157 at epoch: 17 and batch_num: 254\n",
      "Loss of train set: 0.21611203253269196 at epoch: 17 and batch_num: 255\n",
      "Loss of train set: 0.26365986466407776 at epoch: 17 and batch_num: 256\n",
      "Loss of train set: 0.31806740164756775 at epoch: 17 and batch_num: 257\n",
      "Loss of train set: 0.3125050663948059 at epoch: 17 and batch_num: 258\n",
      "Loss of train set: 0.36298710107803345 at epoch: 17 and batch_num: 259\n",
      "Loss of train set: 0.1848817765712738 at epoch: 17 and batch_num: 260\n",
      "Loss of train set: 0.2689034044742584 at epoch: 17 and batch_num: 261\n",
      "Loss of train set: 0.19937445223331451 at epoch: 17 and batch_num: 262\n",
      "Loss of train set: 0.16580811142921448 at epoch: 17 and batch_num: 263\n",
      "Loss of train set: 0.29890158772468567 at epoch: 17 and batch_num: 264\n",
      "Loss of train set: 0.17135867476463318 at epoch: 17 and batch_num: 265\n",
      "Loss of train set: 0.2810175120830536 at epoch: 17 and batch_num: 266\n",
      "Loss of train set: 0.29422879219055176 at epoch: 17 and batch_num: 267\n",
      "Loss of train set: 0.3796007037162781 at epoch: 17 and batch_num: 268\n",
      "Loss of train set: 0.3304375410079956 at epoch: 17 and batch_num: 269\n",
      "Loss of train set: 0.29732489585876465 at epoch: 17 and batch_num: 270\n",
      "Loss of train set: 0.19291245937347412 at epoch: 17 and batch_num: 271\n",
      "Loss of train set: 0.39791491627693176 at epoch: 17 and batch_num: 272\n",
      "Loss of train set: 0.16997548937797546 at epoch: 17 and batch_num: 273\n",
      "Loss of train set: 0.3173046112060547 at epoch: 17 and batch_num: 274\n",
      "Loss of train set: 0.4253029227256775 at epoch: 17 and batch_num: 275\n",
      "Loss of train set: 0.12280214577913284 at epoch: 17 and batch_num: 276\n",
      "Loss of train set: 0.2847394347190857 at epoch: 17 and batch_num: 277\n",
      "Loss of train set: 0.3375927209854126 at epoch: 17 and batch_num: 278\n",
      "Loss of train set: 0.24786503612995148 at epoch: 17 and batch_num: 279\n",
      "Loss of train set: 0.3377036452293396 at epoch: 17 and batch_num: 280\n",
      "Loss of train set: 0.2703508138656616 at epoch: 17 and batch_num: 281\n",
      "Loss of train set: 0.23703955113887787 at epoch: 17 and batch_num: 282\n",
      "Loss of train set: 0.24476966261863708 at epoch: 17 and batch_num: 283\n",
      "Loss of train set: 0.10976676642894745 at epoch: 17 and batch_num: 284\n",
      "Loss of train set: 0.23332557082176208 at epoch: 17 and batch_num: 285\n",
      "Loss of train set: 0.14604724943637848 at epoch: 17 and batch_num: 286\n",
      "Loss of train set: 0.3289027214050293 at epoch: 17 and batch_num: 287\n",
      "Loss of train set: 0.19893105328083038 at epoch: 17 and batch_num: 288\n",
      "Loss of train set: 0.16135162115097046 at epoch: 17 and batch_num: 289\n",
      "Loss of train set: 0.5258634686470032 at epoch: 17 and batch_num: 290\n",
      "Loss of train set: 0.2974175214767456 at epoch: 17 and batch_num: 291\n",
      "Loss of train set: 0.2728769779205322 at epoch: 17 and batch_num: 292\n",
      "Loss of train set: 0.3315911293029785 at epoch: 17 and batch_num: 293\n",
      "Loss of train set: 0.4115234613418579 at epoch: 17 and batch_num: 294\n",
      "Loss of train set: 0.2857015132904053 at epoch: 17 and batch_num: 295\n",
      "Loss of train set: 0.25203731656074524 at epoch: 17 and batch_num: 296\n",
      "Loss of train set: 0.32067495584487915 at epoch: 17 and batch_num: 297\n",
      "Loss of train set: 0.3487459421157837 at epoch: 17 and batch_num: 298\n",
      "Loss of train set: 0.2528752088546753 at epoch: 17 and batch_num: 299\n",
      "Loss of train set: 0.1820836067199707 at epoch: 17 and batch_num: 300\n",
      "Loss of train set: 0.19577351212501526 at epoch: 17 and batch_num: 301\n",
      "Loss of train set: 0.32647719979286194 at epoch: 17 and batch_num: 302\n",
      "Loss of train set: 0.27451813220977783 at epoch: 17 and batch_num: 303\n",
      "Loss of train set: 0.2521752119064331 at epoch: 17 and batch_num: 304\n",
      "Loss of train set: 0.2017800211906433 at epoch: 17 and batch_num: 305\n",
      "Loss of train set: 0.28049302101135254 at epoch: 17 and batch_num: 306\n",
      "Loss of train set: 0.31337878108024597 at epoch: 17 and batch_num: 307\n",
      "Loss of train set: 0.29537856578826904 at epoch: 17 and batch_num: 308\n",
      "Loss of train set: 0.28197163343429565 at epoch: 17 and batch_num: 309\n",
      "Loss of train set: 0.462343692779541 at epoch: 17 and batch_num: 310\n",
      "Loss of train set: 0.31494471430778503 at epoch: 17 and batch_num: 311\n",
      "Loss of train set: 0.3652893900871277 at epoch: 17 and batch_num: 312\n",
      "Loss of train set: 0.36789917945861816 at epoch: 17 and batch_num: 313\n",
      "Loss of train set: 0.32447338104248047 at epoch: 17 and batch_num: 314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.35655635595321655 at epoch: 17 and batch_num: 315\n",
      "Loss of train set: 0.43027839064598083 at epoch: 17 and batch_num: 316\n",
      "Loss of train set: 0.17548976838588715 at epoch: 17 and batch_num: 317\n",
      "Loss of train set: 0.4108496904373169 at epoch: 17 and batch_num: 318\n",
      "Loss of train set: 0.25727033615112305 at epoch: 17 and batch_num: 319\n",
      "Loss of train set: 0.18325133621692657 at epoch: 17 and batch_num: 320\n",
      "Loss of train set: 0.45271962881088257 at epoch: 17 and batch_num: 321\n",
      "Loss of train set: 0.22466203570365906 at epoch: 17 and batch_num: 322\n",
      "Loss of train set: 0.3406412601470947 at epoch: 17 and batch_num: 323\n",
      "Loss of train set: 0.35222095251083374 at epoch: 17 and batch_num: 324\n",
      "Loss of train set: 0.4601806700229645 at epoch: 17 and batch_num: 325\n",
      "Loss of train set: 0.22220997512340546 at epoch: 17 and batch_num: 326\n",
      "Loss of train set: 0.316495805978775 at epoch: 17 and batch_num: 327\n",
      "Loss of train set: 0.2620491683483124 at epoch: 17 and batch_num: 328\n",
      "Loss of train set: 0.25710803270339966 at epoch: 17 and batch_num: 329\n",
      "Loss of train set: 0.12504668533802032 at epoch: 17 and batch_num: 330\n",
      "Loss of train set: 0.19079530239105225 at epoch: 17 and batch_num: 331\n",
      "Loss of train set: 0.2711429297924042 at epoch: 17 and batch_num: 332\n",
      "Loss of train set: 0.3251834213733673 at epoch: 17 and batch_num: 333\n",
      "Loss of train set: 0.2793588638305664 at epoch: 17 and batch_num: 334\n",
      "Loss of train set: 0.27079734206199646 at epoch: 17 and batch_num: 335\n",
      "Loss of train set: 0.3999824821949005 at epoch: 17 and batch_num: 336\n",
      "Loss of train set: 0.1876261681318283 at epoch: 17 and batch_num: 337\n",
      "Loss of train set: 0.1795928180217743 at epoch: 17 and batch_num: 338\n",
      "Loss of train set: 0.2784603536128998 at epoch: 17 and batch_num: 339\n",
      "Loss of train set: 0.417838454246521 at epoch: 17 and batch_num: 340\n",
      "Loss of train set: 0.21349535882472992 at epoch: 17 and batch_num: 341\n",
      "Loss of train set: 0.2568613290786743 at epoch: 17 and batch_num: 342\n",
      "Loss of train set: 0.41665688157081604 at epoch: 17 and batch_num: 343\n",
      "Loss of train set: 0.23977315425872803 at epoch: 17 and batch_num: 344\n",
      "Loss of train set: 0.2153300642967224 at epoch: 17 and batch_num: 345\n",
      "Loss of train set: 0.2985050082206726 at epoch: 17 and batch_num: 346\n",
      "Loss of train set: 0.3937881588935852 at epoch: 17 and batch_num: 347\n",
      "Loss of train set: 0.26333731412887573 at epoch: 17 and batch_num: 348\n",
      "Loss of train set: 0.34323304891586304 at epoch: 17 and batch_num: 349\n",
      "Loss of train set: 0.44152742624282837 at epoch: 17 and batch_num: 350\n",
      "Loss of train set: 0.2561788260936737 at epoch: 17 and batch_num: 351\n",
      "Loss of train set: 0.2297680377960205 at epoch: 17 and batch_num: 352\n",
      "Loss of train set: 0.17946991324424744 at epoch: 17 and batch_num: 353\n",
      "Loss of train set: 0.2462995946407318 at epoch: 17 and batch_num: 354\n",
      "Loss of train set: 0.23978884518146515 at epoch: 17 and batch_num: 355\n",
      "Loss of train set: 0.6291857957839966 at epoch: 17 and batch_num: 356\n",
      "Loss of train set: 0.3758576214313507 at epoch: 17 and batch_num: 357\n",
      "Loss of train set: 0.3932875394821167 at epoch: 17 and batch_num: 358\n",
      "Loss of train set: 0.21461641788482666 at epoch: 17 and batch_num: 359\n",
      "Loss of train set: 0.2577843964099884 at epoch: 17 and batch_num: 360\n",
      "Loss of train set: 0.34410202503204346 at epoch: 17 and batch_num: 361\n",
      "Loss of train set: 0.2803666293621063 at epoch: 17 and batch_num: 362\n",
      "Loss of train set: 0.15957868099212646 at epoch: 17 and batch_num: 363\n",
      "Loss of train set: 0.24582728743553162 at epoch: 17 and batch_num: 364\n",
      "Loss of train set: 0.35290461778640747 at epoch: 17 and batch_num: 365\n",
      "Loss of train set: 0.17232710123062134 at epoch: 17 and batch_num: 366\n",
      "Loss of train set: 0.1750348061323166 at epoch: 17 and batch_num: 367\n",
      "Loss of train set: 0.1987987756729126 at epoch: 17 and batch_num: 368\n",
      "Loss of train set: 0.36662933230400085 at epoch: 17 and batch_num: 369\n",
      "Loss of train set: 0.278247207403183 at epoch: 17 and batch_num: 370\n",
      "Loss of train set: 0.1562102735042572 at epoch: 17 and batch_num: 371\n",
      "Loss of train set: 0.29652175307273865 at epoch: 17 and batch_num: 372\n",
      "Loss of train set: 0.3221700191497803 at epoch: 17 and batch_num: 373\n",
      "Loss of train set: 0.3493983745574951 at epoch: 17 and batch_num: 374\n",
      "Loss of train set: 0.32584136724472046 at epoch: 17 and batch_num: 375\n",
      "Loss of train set: 0.4607335925102234 at epoch: 17 and batch_num: 376\n",
      "Loss of train set: 0.3943619728088379 at epoch: 17 and batch_num: 377\n",
      "Loss of train set: 0.49778252840042114 at epoch: 17 and batch_num: 378\n",
      "Loss of train set: 0.46909797191619873 at epoch: 17 and batch_num: 379\n",
      "Loss of train set: 0.1653720736503601 at epoch: 17 and batch_num: 380\n",
      "Loss of train set: 0.3417041599750519 at epoch: 17 and batch_num: 381\n",
      "Loss of train set: 0.35824549198150635 at epoch: 17 and batch_num: 382\n",
      "Loss of train set: 0.4694492816925049 at epoch: 17 and batch_num: 383\n",
      "Loss of train set: 0.2528080642223358 at epoch: 17 and batch_num: 384\n",
      "Loss of train set: 0.3290734887123108 at epoch: 17 and batch_num: 385\n",
      "Loss of train set: 0.26296958327293396 at epoch: 17 and batch_num: 386\n",
      "Loss of train set: 0.0820145308971405 at epoch: 17 and batch_num: 387\n",
      "Loss of train set: 0.2814578711986542 at epoch: 17 and batch_num: 388\n",
      "Loss of train set: 0.27800145745277405 at epoch: 17 and batch_num: 389\n",
      "Loss of train set: 0.3325396478176117 at epoch: 17 and batch_num: 390\n",
      "Loss of train set: 0.15219679474830627 at epoch: 17 and batch_num: 391\n",
      "Loss of train set: 0.1461157649755478 at epoch: 17 and batch_num: 392\n",
      "Loss of train set: 0.3176891803741455 at epoch: 17 and batch_num: 393\n",
      "Loss of train set: 0.4421374499797821 at epoch: 17 and batch_num: 394\n",
      "Loss of train set: 0.33304092288017273 at epoch: 17 and batch_num: 395\n",
      "Loss of train set: 0.28873246908187866 at epoch: 17 and batch_num: 396\n",
      "Loss of train set: 0.4386468231678009 at epoch: 17 and batch_num: 397\n",
      "Loss of train set: 0.3830336034297943 at epoch: 17 and batch_num: 398\n",
      "Loss of train set: 0.1520402878522873 at epoch: 17 and batch_num: 399\n",
      "Loss of train set: 0.39741507172584534 at epoch: 17 and batch_num: 400\n",
      "Loss of train set: 0.44599273800849915 at epoch: 17 and batch_num: 401\n",
      "Loss of train set: 0.34018999338150024 at epoch: 17 and batch_num: 402\n",
      "Loss of train set: 0.2577420175075531 at epoch: 17 and batch_num: 403\n",
      "Loss of train set: 0.2193119078874588 at epoch: 17 and batch_num: 404\n",
      "Loss of train set: 0.21169863641262054 at epoch: 17 and batch_num: 405\n",
      "Loss of train set: 0.07739032059907913 at epoch: 17 and batch_num: 406\n",
      "Loss of train set: 0.3828178644180298 at epoch: 17 and batch_num: 407\n",
      "Loss of train set: 0.37013810873031616 at epoch: 17 and batch_num: 408\n",
      "Loss of train set: 0.36016929149627686 at epoch: 17 and batch_num: 409\n",
      "Loss of train set: 0.29491695761680603 at epoch: 17 and batch_num: 410\n",
      "Loss of train set: 0.40372002124786377 at epoch: 17 and batch_num: 411\n",
      "Loss of train set: 0.2259664237499237 at epoch: 17 and batch_num: 412\n",
      "Loss of train set: 0.2976507544517517 at epoch: 17 and batch_num: 413\n",
      "Loss of train set: 0.3606303632259369 at epoch: 17 and batch_num: 414\n",
      "Loss of train set: 0.23810707032680511 at epoch: 17 and batch_num: 415\n",
      "Loss of train set: 0.2297961562871933 at epoch: 17 and batch_num: 416\n",
      "Loss of train set: 0.20136255025863647 at epoch: 17 and batch_num: 417\n",
      "Loss of train set: 0.5185234546661377 at epoch: 17 and batch_num: 418\n",
      "Loss of train set: 0.3596838712692261 at epoch: 17 and batch_num: 419\n",
      "Loss of train set: 0.2695373296737671 at epoch: 17 and batch_num: 420\n",
      "Loss of train set: 0.43090641498565674 at epoch: 17 and batch_num: 421\n",
      "Loss of train set: 0.1933782398700714 at epoch: 17 and batch_num: 422\n",
      "Loss of train set: 0.18738926947116852 at epoch: 17 and batch_num: 423\n",
      "Loss of train set: 0.3263711929321289 at epoch: 17 and batch_num: 424\n",
      "Loss of train set: 0.2897084355354309 at epoch: 17 and batch_num: 425\n",
      "Loss of train set: 0.297817200422287 at epoch: 17 and batch_num: 426\n",
      "Loss of train set: 0.16175024211406708 at epoch: 17 and batch_num: 427\n",
      "Loss of train set: 0.3747560977935791 at epoch: 17 and batch_num: 428\n",
      "Loss of train set: 0.4088079333305359 at epoch: 17 and batch_num: 429\n",
      "Loss of train set: 0.270786851644516 at epoch: 17 and batch_num: 430\n",
      "Loss of train set: 0.25182071328163147 at epoch: 17 and batch_num: 431\n",
      "Loss of train set: 0.2448914349079132 at epoch: 17 and batch_num: 432\n",
      "Loss of train set: 0.39007481932640076 at epoch: 17 and batch_num: 433\n",
      "Loss of train set: 0.2717663049697876 at epoch: 17 and batch_num: 434\n",
      "Loss of train set: 0.2520824670791626 at epoch: 17 and batch_num: 435\n",
      "Loss of train set: 0.373765766620636 at epoch: 17 and batch_num: 436\n",
      "Loss of train set: 0.3299296200275421 at epoch: 17 and batch_num: 437\n",
      "Loss of train set: 0.2629673480987549 at epoch: 17 and batch_num: 438\n",
      "Loss of train set: 0.27749261260032654 at epoch: 17 and batch_num: 439\n",
      "Loss of train set: 0.1972837895154953 at epoch: 17 and batch_num: 440\n",
      "Loss of train set: 0.3276362121105194 at epoch: 17 and batch_num: 441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.37235990166664124 at epoch: 17 and batch_num: 442\n",
      "Loss of train set: 0.2263173907995224 at epoch: 17 and batch_num: 443\n",
      "Loss of train set: 0.2883460521697998 at epoch: 17 and batch_num: 444\n",
      "Loss of train set: 0.3220345079898834 at epoch: 17 and batch_num: 445\n",
      "Loss of train set: 0.27269744873046875 at epoch: 17 and batch_num: 446\n",
      "Loss of train set: 0.1908278614282608 at epoch: 17 and batch_num: 447\n",
      "Loss of train set: 0.2993263006210327 at epoch: 17 and batch_num: 448\n",
      "Loss of train set: 0.3967844545841217 at epoch: 17 and batch_num: 449\n",
      "Loss of train set: 0.34329327940940857 at epoch: 17 and batch_num: 450\n",
      "Loss of train set: 0.41094905138015747 at epoch: 17 and batch_num: 451\n",
      "Loss of train set: 0.26435595750808716 at epoch: 17 and batch_num: 452\n",
      "Loss of train set: 0.2933109998703003 at epoch: 17 and batch_num: 453\n",
      "Loss of train set: 0.34668901562690735 at epoch: 17 and batch_num: 454\n",
      "Loss of train set: 0.3096565902233124 at epoch: 17 and batch_num: 455\n",
      "Loss of train set: 0.4282669425010681 at epoch: 17 and batch_num: 456\n",
      "Loss of train set: 0.3742477297782898 at epoch: 17 and batch_num: 457\n",
      "Loss of train set: 0.4411090910434723 at epoch: 17 and batch_num: 458\n",
      "Loss of train set: 0.2821838855743408 at epoch: 17 and batch_num: 459\n",
      "Loss of train set: 0.3084425628185272 at epoch: 17 and batch_num: 460\n",
      "Loss of train set: 0.20573639869689941 at epoch: 17 and batch_num: 461\n",
      "Loss of train set: 0.3778001666069031 at epoch: 17 and batch_num: 462\n",
      "Loss of train set: 0.3092270493507385 at epoch: 17 and batch_num: 463\n",
      "Loss of train set: 0.34094417095184326 at epoch: 17 and batch_num: 464\n",
      "Loss of train set: 0.25784793496131897 at epoch: 17 and batch_num: 465\n",
      "Loss of train set: 0.24462568759918213 at epoch: 17 and batch_num: 466\n",
      "Loss of train set: 0.46072712540626526 at epoch: 17 and batch_num: 467\n",
      "Loss of train set: 0.36354002356529236 at epoch: 17 and batch_num: 468\n",
      "Loss of train set: 0.14211663603782654 at epoch: 17 and batch_num: 469\n",
      "Loss of train set: 0.253553569316864 at epoch: 17 and batch_num: 470\n",
      "Loss of train set: 0.3195796310901642 at epoch: 17 and batch_num: 471\n",
      "Loss of train set: 0.3613353669643402 at epoch: 17 and batch_num: 472\n",
      "Loss of train set: 0.22110727429389954 at epoch: 17 and batch_num: 473\n",
      "Loss of train set: 0.3601688742637634 at epoch: 17 and batch_num: 474\n",
      "Loss of train set: 0.36503198742866516 at epoch: 17 and batch_num: 475\n",
      "Loss of train set: 0.20722419023513794 at epoch: 17 and batch_num: 476\n",
      "Loss of train set: 0.1975206434726715 at epoch: 17 and batch_num: 477\n",
      "Loss of train set: 0.3298487961292267 at epoch: 17 and batch_num: 478\n",
      "Loss of train set: 0.22579902410507202 at epoch: 17 and batch_num: 479\n",
      "Loss of train set: 0.17506659030914307 at epoch: 17 and batch_num: 480\n",
      "Loss of train set: 0.23919743299484253 at epoch: 17 and batch_num: 481\n",
      "Loss of train set: 0.2661566734313965 at epoch: 17 and batch_num: 482\n",
      "Loss of train set: 0.1672210842370987 at epoch: 17 and batch_num: 483\n",
      "Loss of train set: 0.41861772537231445 at epoch: 17 and batch_num: 484\n",
      "Loss of train set: 0.3777827024459839 at epoch: 17 and batch_num: 485\n",
      "Loss of train set: 0.2729624807834625 at epoch: 17 and batch_num: 486\n",
      "Loss of train set: 0.18379217386245728 at epoch: 17 and batch_num: 487\n",
      "Loss of train set: 0.23419326543807983 at epoch: 17 and batch_num: 488\n",
      "Loss of train set: 0.30743008852005005 at epoch: 17 and batch_num: 489\n",
      "Loss of train set: 0.30846187472343445 at epoch: 17 and batch_num: 490\n",
      "Loss of train set: 0.3312895894050598 at epoch: 17 and batch_num: 491\n",
      "Loss of train set: 0.3072342872619629 at epoch: 17 and batch_num: 492\n",
      "Loss of train set: 0.3606582283973694 at epoch: 17 and batch_num: 493\n",
      "Loss of train set: 0.28078383207321167 at epoch: 17 and batch_num: 494\n",
      "Loss of train set: 0.33112531900405884 at epoch: 17 and batch_num: 495\n",
      "Loss of train set: 0.24330389499664307 at epoch: 17 and batch_num: 496\n",
      "Loss of train set: 0.18625298142433167 at epoch: 17 and batch_num: 497\n",
      "Loss of train set: 0.34166860580444336 at epoch: 17 and batch_num: 498\n",
      "Loss of train set: 0.5490686297416687 at epoch: 17 and batch_num: 499\n",
      "Loss of train set: 0.3359939157962799 at epoch: 17 and batch_num: 500\n",
      "Loss of train set: 0.430476576089859 at epoch: 17 and batch_num: 501\n",
      "Loss of train set: 0.2883877158164978 at epoch: 17 and batch_num: 502\n",
      "Loss of train set: 0.4292420744895935 at epoch: 17 and batch_num: 503\n",
      "Loss of train set: 0.43602803349494934 at epoch: 17 and batch_num: 504\n",
      "Loss of train set: 0.47764578461647034 at epoch: 17 and batch_num: 505\n",
      "Loss of train set: 0.4223838150501251 at epoch: 17 and batch_num: 506\n",
      "Loss of train set: 0.3664241433143616 at epoch: 17 and batch_num: 507\n",
      "Loss of train set: 0.3073195815086365 at epoch: 17 and batch_num: 508\n",
      "Loss of train set: 0.32869452238082886 at epoch: 17 and batch_num: 509\n",
      "Loss of train set: 0.36153751611709595 at epoch: 17 and batch_num: 510\n",
      "Loss of train set: 0.14829233288764954 at epoch: 17 and batch_num: 511\n",
      "Loss of train set: 0.3929884433746338 at epoch: 17 and batch_num: 512\n",
      "Loss of train set: 0.3137863278388977 at epoch: 17 and batch_num: 513\n",
      "Loss of train set: 0.32620376348495483 at epoch: 17 and batch_num: 514\n",
      "Loss of train set: 0.33592522144317627 at epoch: 17 and batch_num: 515\n",
      "Loss of train set: 0.1731007993221283 at epoch: 17 and batch_num: 516\n",
      "Loss of train set: 0.36808687448501587 at epoch: 17 and batch_num: 517\n",
      "Loss of train set: 0.2545240819454193 at epoch: 17 and batch_num: 518\n",
      "Loss of train set: 0.22989428043365479 at epoch: 17 and batch_num: 519\n",
      "Loss of train set: 0.22164201736450195 at epoch: 17 and batch_num: 520\n",
      "Loss of train set: 0.19553440809249878 at epoch: 17 and batch_num: 521\n",
      "Loss of train set: 0.36738133430480957 at epoch: 17 and batch_num: 522\n",
      "Loss of train set: 0.20541507005691528 at epoch: 17 and batch_num: 523\n",
      "Loss of train set: 0.28411591053009033 at epoch: 17 and batch_num: 524\n",
      "Loss of train set: 0.20320531725883484 at epoch: 17 and batch_num: 525\n",
      "Loss of train set: 0.3752594590187073 at epoch: 17 and batch_num: 526\n",
      "Loss of train set: 0.4874283969402313 at epoch: 17 and batch_num: 527\n",
      "Loss of train set: 0.32482466101646423 at epoch: 17 and batch_num: 528\n",
      "Loss of train set: 0.18215271830558777 at epoch: 17 and batch_num: 529\n",
      "Loss of train set: 0.24751494824886322 at epoch: 17 and batch_num: 530\n",
      "Loss of train set: 0.22025315463542938 at epoch: 17 and batch_num: 531\n",
      "Loss of train set: 0.2664962410926819 at epoch: 17 and batch_num: 532\n",
      "Loss of train set: 0.28708893060684204 at epoch: 17 and batch_num: 533\n",
      "Loss of train set: 0.19520936906337738 at epoch: 17 and batch_num: 534\n",
      "Loss of train set: 0.27800488471984863 at epoch: 17 and batch_num: 535\n",
      "Loss of train set: 0.15034620463848114 at epoch: 17 and batch_num: 536\n",
      "Loss of train set: 0.22649303078651428 at epoch: 17 and batch_num: 537\n",
      "Loss of train set: 0.2071651965379715 at epoch: 17 and batch_num: 538\n",
      "Loss of train set: 0.21295659244060516 at epoch: 17 and batch_num: 539\n",
      "Loss of train set: 0.3709932863712311 at epoch: 17 and batch_num: 540\n",
      "Loss of train set: 0.2640697956085205 at epoch: 17 and batch_num: 541\n",
      "Loss of train set: 0.29515793919563293 at epoch: 17 and batch_num: 542\n",
      "Loss of train set: 0.3140280246734619 at epoch: 17 and batch_num: 543\n",
      "Loss of train set: 0.5523605942726135 at epoch: 17 and batch_num: 544\n",
      "Loss of train set: 0.5071974992752075 at epoch: 17 and batch_num: 545\n",
      "Loss of train set: 0.268538236618042 at epoch: 17 and batch_num: 546\n",
      "Loss of train set: 0.21209552884101868 at epoch: 17 and batch_num: 547\n",
      "Loss of train set: 0.3116825819015503 at epoch: 17 and batch_num: 548\n",
      "Loss of train set: 0.320395290851593 at epoch: 17 and batch_num: 549\n",
      "Loss of train set: 0.3934501111507416 at epoch: 17 and batch_num: 550\n",
      "Loss of train set: 0.1564313918352127 at epoch: 17 and batch_num: 551\n",
      "Loss of train set: 0.31944870948791504 at epoch: 17 and batch_num: 552\n",
      "Loss of train set: 0.47061121463775635 at epoch: 17 and batch_num: 553\n",
      "Loss of train set: 0.3211311399936676 at epoch: 17 and batch_num: 554\n",
      "Loss of train set: 0.40705063939094543 at epoch: 17 and batch_num: 555\n",
      "Loss of train set: 0.2855771780014038 at epoch: 17 and batch_num: 556\n",
      "Loss of train set: 0.33678674697875977 at epoch: 17 and batch_num: 557\n",
      "Loss of train set: 0.2772212326526642 at epoch: 17 and batch_num: 558\n",
      "Loss of train set: 0.26913657784461975 at epoch: 17 and batch_num: 559\n",
      "Loss of train set: 0.2996788024902344 at epoch: 17 and batch_num: 560\n",
      "Loss of train set: 0.22794348001480103 at epoch: 17 and batch_num: 561\n",
      "Loss of train set: 0.25646281242370605 at epoch: 17 and batch_num: 562\n",
      "Loss of train set: 0.31324705481529236 at epoch: 17 and batch_num: 563\n",
      "Loss of train set: 0.33504679799079895 at epoch: 17 and batch_num: 564\n",
      "Loss of train set: 0.2016923427581787 at epoch: 17 and batch_num: 565\n",
      "Loss of train set: 0.17781952023506165 at epoch: 17 and batch_num: 566\n",
      "Loss of train set: 0.2824406027793884 at epoch: 17 and batch_num: 567\n",
      "Loss of train set: 0.2894720435142517 at epoch: 17 and batch_num: 568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3464401066303253 at epoch: 17 and batch_num: 569\n",
      "Loss of train set: 0.27743810415267944 at epoch: 17 and batch_num: 570\n",
      "Loss of train set: 0.2717512547969818 at epoch: 17 and batch_num: 571\n",
      "Loss of train set: 0.3908761143684387 at epoch: 17 and batch_num: 572\n",
      "Loss of train set: 0.4026295244693756 at epoch: 17 and batch_num: 573\n",
      "Loss of train set: 0.20932376384735107 at epoch: 17 and batch_num: 574\n",
      "Loss of train set: 0.3540957570075989 at epoch: 17 and batch_num: 575\n",
      "Loss of train set: 0.38708245754241943 at epoch: 17 and batch_num: 576\n",
      "Loss of train set: 0.3044120669364929 at epoch: 17 and batch_num: 577\n",
      "Loss of train set: 0.21786631643772125 at epoch: 17 and batch_num: 578\n",
      "Loss of train set: 0.30466705560684204 at epoch: 17 and batch_num: 579\n",
      "Loss of train set: 0.2824515104293823 at epoch: 17 and batch_num: 580\n",
      "Loss of train set: 0.27496591210365295 at epoch: 17 and batch_num: 581\n",
      "Loss of train set: 0.2727544903755188 at epoch: 17 and batch_num: 582\n",
      "Loss of train set: 0.29688823223114014 at epoch: 17 and batch_num: 583\n",
      "Loss of train set: 0.3898412883281708 at epoch: 17 and batch_num: 584\n",
      "Loss of train set: 0.16942036151885986 at epoch: 17 and batch_num: 585\n",
      "Loss of train set: 0.1949569433927536 at epoch: 17 and batch_num: 586\n",
      "Loss of train set: 0.2935638129711151 at epoch: 17 and batch_num: 587\n",
      "Loss of train set: 0.2688395380973816 at epoch: 17 and batch_num: 588\n",
      "Loss of train set: 0.26672542095184326 at epoch: 17 and batch_num: 589\n",
      "Loss of train set: 0.4494155943393707 at epoch: 17 and batch_num: 590\n",
      "Loss of train set: 0.25451281666755676 at epoch: 17 and batch_num: 591\n",
      "Loss of train set: 0.26151764392852783 at epoch: 17 and batch_num: 592\n",
      "Loss of train set: 0.22356456518173218 at epoch: 17 and batch_num: 593\n",
      "Loss of train set: 0.22559338808059692 at epoch: 17 and batch_num: 594\n",
      "Loss of train set: 0.37074342370033264 at epoch: 17 and batch_num: 595\n",
      "Loss of train set: 0.437709242105484 at epoch: 17 and batch_num: 596\n",
      "Loss of train set: 0.507473886013031 at epoch: 17 and batch_num: 597\n",
      "Loss of train set: 0.266217440366745 at epoch: 17 and batch_num: 598\n",
      "Loss of train set: 0.13021314144134521 at epoch: 17 and batch_num: 599\n",
      "Loss of train set: 0.3782716989517212 at epoch: 17 and batch_num: 600\n",
      "Loss of train set: 0.24542832374572754 at epoch: 17 and batch_num: 601\n",
      "Loss of train set: 0.20506078004837036 at epoch: 17 and batch_num: 602\n",
      "Loss of train set: 0.2276824712753296 at epoch: 17 and batch_num: 603\n",
      "Loss of train set: 0.20408038794994354 at epoch: 17 and batch_num: 604\n",
      "Loss of train set: 0.3890835642814636 at epoch: 17 and batch_num: 605\n",
      "Loss of train set: 0.37435489892959595 at epoch: 17 and batch_num: 606\n",
      "Loss of train set: 0.2142266482114792 at epoch: 17 and batch_num: 607\n",
      "Loss of train set: 0.2360297441482544 at epoch: 17 and batch_num: 608\n",
      "Loss of train set: 0.2823562026023865 at epoch: 17 and batch_num: 609\n",
      "Loss of train set: 0.2951018810272217 at epoch: 17 and batch_num: 610\n",
      "Loss of train set: 0.16514500975608826 at epoch: 17 and batch_num: 611\n",
      "Loss of train set: 0.27983397245407104 at epoch: 17 and batch_num: 612\n",
      "Loss of train set: 0.2893003821372986 at epoch: 17 and batch_num: 613\n",
      "Loss of train set: 0.3937039077281952 at epoch: 17 and batch_num: 614\n",
      "Loss of train set: 0.2936798930168152 at epoch: 17 and batch_num: 615\n",
      "Loss of train set: 0.26092058420181274 at epoch: 17 and batch_num: 616\n",
      "Loss of train set: 0.2901855409145355 at epoch: 17 and batch_num: 617\n",
      "Loss of train set: 0.18012066185474396 at epoch: 17 and batch_num: 618\n",
      "Loss of train set: 0.3586084246635437 at epoch: 17 and batch_num: 619\n",
      "Loss of train set: 0.3245348334312439 at epoch: 17 and batch_num: 620\n",
      "Loss of train set: 0.25313377380371094 at epoch: 17 and batch_num: 621\n",
      "Loss of train set: 0.27734774351119995 at epoch: 17 and batch_num: 622\n",
      "Loss of train set: 0.21087542176246643 at epoch: 17 and batch_num: 623\n",
      "Loss of train set: 0.1459696888923645 at epoch: 17 and batch_num: 624\n",
      "Loss of train set: 0.2312544882297516 at epoch: 17 and batch_num: 625\n",
      "Loss of train set: 0.1882392317056656 at epoch: 17 and batch_num: 626\n",
      "Loss of train set: 0.25816869735717773 at epoch: 17 and batch_num: 627\n",
      "Loss of train set: 0.5642505884170532 at epoch: 17 and batch_num: 628\n",
      "Loss of train set: 0.3139530420303345 at epoch: 17 and batch_num: 629\n",
      "Loss of train set: 0.2101464420557022 at epoch: 17 and batch_num: 630\n",
      "Loss of train set: 0.20734351873397827 at epoch: 17 and batch_num: 631\n",
      "Loss of train set: 0.2706495523452759 at epoch: 17 and batch_num: 632\n",
      "Loss of train set: 0.2756984233856201 at epoch: 17 and batch_num: 633\n",
      "Loss of train set: 0.3246229588985443 at epoch: 17 and batch_num: 634\n",
      "Loss of train set: 0.2082744538784027 at epoch: 17 and batch_num: 635\n",
      "Loss of train set: 0.31305408477783203 at epoch: 17 and batch_num: 636\n",
      "Loss of train set: 0.3607600927352905 at epoch: 17 and batch_num: 637\n",
      "Loss of train set: 0.25940853357315063 at epoch: 17 and batch_num: 638\n",
      "Loss of train set: 0.3668200969696045 at epoch: 17 and batch_num: 639\n",
      "Loss of train set: 0.27595341205596924 at epoch: 17 and batch_num: 640\n",
      "Loss of train set: 0.32470500469207764 at epoch: 17 and batch_num: 641\n",
      "Loss of train set: 0.31165874004364014 at epoch: 17 and batch_num: 642\n",
      "Loss of train set: 0.1776396632194519 at epoch: 17 and batch_num: 643\n",
      "Loss of train set: 0.3346526026725769 at epoch: 17 and batch_num: 644\n",
      "Loss of train set: 0.34556150436401367 at epoch: 17 and batch_num: 645\n",
      "Loss of train set: 0.45755261182785034 at epoch: 17 and batch_num: 646\n",
      "Loss of train set: 0.4631078243255615 at epoch: 17 and batch_num: 647\n",
      "Loss of train set: 0.2559238076210022 at epoch: 17 and batch_num: 648\n",
      "Loss of train set: 0.33928966522216797 at epoch: 17 and batch_num: 649\n",
      "Loss of train set: 0.29521235823631287 at epoch: 17 and batch_num: 650\n",
      "Loss of train set: 0.20641151070594788 at epoch: 17 and batch_num: 651\n",
      "Loss of train set: 0.43177491426467896 at epoch: 17 and batch_num: 652\n",
      "Loss of train set: 0.2525784969329834 at epoch: 17 and batch_num: 653\n",
      "Loss of train set: 0.2588294744491577 at epoch: 17 and batch_num: 654\n",
      "Loss of train set: 0.2464553564786911 at epoch: 17 and batch_num: 655\n",
      "Loss of train set: 0.35217082500457764 at epoch: 17 and batch_num: 656\n",
      "Loss of train set: 0.3654773533344269 at epoch: 17 and batch_num: 657\n",
      "Loss of train set: 0.3991042375564575 at epoch: 17 and batch_num: 658\n",
      "Loss of train set: 0.2787621021270752 at epoch: 17 and batch_num: 659\n",
      "Loss of train set: 0.15945345163345337 at epoch: 17 and batch_num: 660\n",
      "Loss of train set: 0.33184462785720825 at epoch: 17 and batch_num: 661\n",
      "Loss of train set: 0.1742038130760193 at epoch: 17 and batch_num: 662\n",
      "Loss of train set: 0.1735335886478424 at epoch: 17 and batch_num: 663\n",
      "Loss of train set: 0.354028582572937 at epoch: 17 and batch_num: 664\n",
      "Loss of train set: 0.1505255401134491 at epoch: 17 and batch_num: 665\n",
      "Loss of train set: 0.561417818069458 at epoch: 17 and batch_num: 666\n",
      "Loss of train set: 0.2577560842037201 at epoch: 17 and batch_num: 667\n",
      "Loss of train set: 0.4526559114456177 at epoch: 17 and batch_num: 668\n",
      "Loss of train set: 0.43202394247055054 at epoch: 17 and batch_num: 669\n",
      "Loss of train set: 0.24668771028518677 at epoch: 17 and batch_num: 670\n",
      "Loss of train set: 0.41549932956695557 at epoch: 17 and batch_num: 671\n",
      "Loss of train set: 0.32486599683761597 at epoch: 17 and batch_num: 672\n",
      "Loss of train set: 0.6026595830917358 at epoch: 17 and batch_num: 673\n",
      "Loss of train set: 0.4639906585216522 at epoch: 17 and batch_num: 674\n",
      "Loss of train set: 0.303799569606781 at epoch: 17 and batch_num: 675\n",
      "Loss of train set: 0.23908308148384094 at epoch: 17 and batch_num: 676\n",
      "Loss of train set: 0.22644942998886108 at epoch: 17 and batch_num: 677\n",
      "Loss of train set: 0.30072689056396484 at epoch: 17 and batch_num: 678\n",
      "Loss of train set: 0.26532185077667236 at epoch: 17 and batch_num: 679\n",
      "Loss of train set: 0.3346105217933655 at epoch: 17 and batch_num: 680\n",
      "Loss of train set: 0.20972949266433716 at epoch: 17 and batch_num: 681\n",
      "Loss of train set: 0.46214759349823 at epoch: 17 and batch_num: 682\n",
      "Loss of train set: 0.16895648837089539 at epoch: 17 and batch_num: 683\n",
      "Loss of train set: 0.3127169907093048 at epoch: 17 and batch_num: 684\n",
      "Loss of train set: 0.4187239408493042 at epoch: 17 and batch_num: 685\n",
      "Loss of train set: 0.3246687650680542 at epoch: 17 and batch_num: 686\n",
      "Loss of train set: 0.19335006177425385 at epoch: 17 and batch_num: 687\n",
      "Loss of train set: 0.2604537606239319 at epoch: 17 and batch_num: 688\n",
      "Loss of train set: 0.3343427777290344 at epoch: 17 and batch_num: 689\n",
      "Loss of train set: 0.2898012101650238 at epoch: 17 and batch_num: 690\n",
      "Loss of train set: 0.20390784740447998 at epoch: 17 and batch_num: 691\n",
      "Loss of train set: 0.12618687748908997 at epoch: 17 and batch_num: 692\n",
      "Loss of train set: 0.284686803817749 at epoch: 17 and batch_num: 693\n",
      "Loss of train set: 0.33965015411376953 at epoch: 17 and batch_num: 694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2882939279079437 at epoch: 17 and batch_num: 695\n",
      "Loss of train set: 0.227086141705513 at epoch: 17 and batch_num: 696\n",
      "Loss of train set: 0.211337149143219 at epoch: 17 and batch_num: 697\n",
      "Loss of train set: 0.2707379460334778 at epoch: 17 and batch_num: 698\n",
      "Loss of train set: 0.26057732105255127 at epoch: 17 and batch_num: 699\n",
      "Loss of train set: 0.24550367891788483 at epoch: 17 and batch_num: 700\n",
      "Loss of train set: 0.2704933285713196 at epoch: 17 and batch_num: 701\n",
      "Loss of train set: 0.22379322350025177 at epoch: 17 and batch_num: 702\n",
      "Loss of train set: 0.3117600083351135 at epoch: 17 and batch_num: 703\n",
      "Loss of train set: 0.47987860441207886 at epoch: 17 and batch_num: 704\n",
      "Loss of train set: 0.23227593302726746 at epoch: 17 and batch_num: 705\n",
      "Loss of train set: 0.2227165848016739 at epoch: 17 and batch_num: 706\n",
      "Loss of train set: 0.2923407256603241 at epoch: 17 and batch_num: 707\n",
      "Loss of train set: 0.23093818128108978 at epoch: 17 and batch_num: 708\n",
      "Loss of train set: 0.5098611116409302 at epoch: 17 and batch_num: 709\n",
      "Loss of train set: 0.31140434741973877 at epoch: 17 and batch_num: 710\n",
      "Loss of train set: 0.14407333731651306 at epoch: 17 and batch_num: 711\n",
      "Loss of train set: 0.3595634698867798 at epoch: 17 and batch_num: 712\n",
      "Loss of train set: 0.33507490158081055 at epoch: 17 and batch_num: 713\n",
      "Loss of train set: 0.3029646873474121 at epoch: 17 and batch_num: 714\n",
      "Loss of train set: 0.2716766595840454 at epoch: 17 and batch_num: 715\n",
      "Loss of train set: 0.35327911376953125 at epoch: 17 and batch_num: 716\n",
      "Loss of train set: 0.29348158836364746 at epoch: 17 and batch_num: 717\n",
      "Loss of train set: 0.31343376636505127 at epoch: 17 and batch_num: 718\n",
      "Loss of train set: 0.15426108241081238 at epoch: 17 and batch_num: 719\n",
      "Loss of train set: 0.22128765285015106 at epoch: 17 and batch_num: 720\n",
      "Loss of train set: 0.5189507603645325 at epoch: 17 and batch_num: 721\n",
      "Loss of train set: 0.21145330369472504 at epoch: 17 and batch_num: 722\n",
      "Loss of train set: 0.2094673365354538 at epoch: 17 and batch_num: 723\n",
      "Loss of train set: 0.3183620870113373 at epoch: 17 and batch_num: 724\n",
      "Loss of train set: 0.15512722730636597 at epoch: 17 and batch_num: 725\n",
      "Loss of train set: 0.1627940833568573 at epoch: 17 and batch_num: 726\n",
      "Loss of train set: 0.4178253412246704 at epoch: 17 and batch_num: 727\n",
      "Loss of train set: 0.4595864415168762 at epoch: 17 and batch_num: 728\n",
      "Loss of train set: 0.2552560865879059 at epoch: 17 and batch_num: 729\n",
      "Loss of train set: 0.35826221108436584 at epoch: 17 and batch_num: 730\n",
      "Loss of train set: 0.40359848737716675 at epoch: 17 and batch_num: 731\n",
      "Loss of train set: 0.1770889014005661 at epoch: 17 and batch_num: 732\n",
      "Loss of train set: 0.23812857270240784 at epoch: 17 and batch_num: 733\n",
      "Loss of train set: 0.1921609789133072 at epoch: 17 and batch_num: 734\n",
      "Loss of train set: 0.2613600194454193 at epoch: 17 and batch_num: 735\n",
      "Loss of train set: 0.2783292233943939 at epoch: 17 and batch_num: 736\n",
      "Loss of train set: 0.2671739161014557 at epoch: 17 and batch_num: 737\n",
      "Loss of train set: 0.3372442424297333 at epoch: 17 and batch_num: 738\n",
      "Loss of train set: 0.3622753918170929 at epoch: 17 and batch_num: 739\n",
      "Loss of train set: 0.2330564558506012 at epoch: 17 and batch_num: 740\n",
      "Loss of train set: 0.13772255182266235 at epoch: 17 and batch_num: 741\n",
      "Loss of train set: 0.34615641832351685 at epoch: 17 and batch_num: 742\n",
      "Loss of train set: 0.2545924782752991 at epoch: 17 and batch_num: 743\n",
      "Loss of train set: 0.28233039379119873 at epoch: 17 and batch_num: 744\n",
      "Loss of train set: 0.2430734634399414 at epoch: 17 and batch_num: 745\n",
      "Loss of train set: 0.34150421619415283 at epoch: 17 and batch_num: 746\n",
      "Loss of train set: 0.2700651288032532 at epoch: 17 and batch_num: 747\n",
      "Loss of train set: 0.2788954973220825 at epoch: 17 and batch_num: 748\n",
      "Loss of train set: 0.3512580990791321 at epoch: 17 and batch_num: 749\n",
      "Loss of train set: 0.2629441022872925 at epoch: 17 and batch_num: 750\n",
      "Loss of train set: 0.47020938992500305 at epoch: 17 and batch_num: 751\n",
      "Loss of train set: 0.3043675422668457 at epoch: 17 and batch_num: 752\n",
      "Loss of train set: 0.45909053087234497 at epoch: 17 and batch_num: 753\n",
      "Loss of train set: 0.26584503054618835 at epoch: 17 and batch_num: 754\n",
      "Loss of train set: 0.31646573543548584 at epoch: 17 and batch_num: 755\n",
      "Loss of train set: 0.4151982367038727 at epoch: 17 and batch_num: 756\n",
      "Loss of train set: 0.22952871024608612 at epoch: 17 and batch_num: 757\n",
      "Loss of train set: 0.37098684906959534 at epoch: 17 and batch_num: 758\n",
      "Loss of train set: 0.26463115215301514 at epoch: 17 and batch_num: 759\n",
      "Loss of train set: 0.23290333151817322 at epoch: 17 and batch_num: 760\n",
      "Loss of train set: 0.28045764565467834 at epoch: 17 and batch_num: 761\n",
      "Loss of train set: 0.24139438569545746 at epoch: 17 and batch_num: 762\n",
      "Loss of train set: 0.20137886703014374 at epoch: 17 and batch_num: 763\n",
      "Loss of train set: 0.40936386585235596 at epoch: 17 and batch_num: 764\n",
      "Loss of train set: 0.24971218407154083 at epoch: 17 and batch_num: 765\n",
      "Loss of train set: 0.21780042350292206 at epoch: 17 and batch_num: 766\n",
      "Loss of train set: 0.2800292372703552 at epoch: 17 and batch_num: 767\n",
      "Loss of train set: 0.23186367750167847 at epoch: 17 and batch_num: 768\n",
      "Loss of train set: 0.3166256248950958 at epoch: 17 and batch_num: 769\n",
      "Loss of train set: 0.38613376021385193 at epoch: 17 and batch_num: 770\n",
      "Loss of train set: 0.31421512365341187 at epoch: 17 and batch_num: 771\n",
      "Loss of train set: 0.2801086902618408 at epoch: 17 and batch_num: 772\n",
      "Loss of train set: 0.48350363969802856 at epoch: 17 and batch_num: 773\n",
      "Loss of train set: 0.2543345093727112 at epoch: 17 and batch_num: 774\n",
      "Loss of train set: 0.42263656854629517 at epoch: 17 and batch_num: 775\n",
      "Loss of train set: 0.33454227447509766 at epoch: 17 and batch_num: 776\n",
      "Loss of train set: 0.3161398768424988 at epoch: 17 and batch_num: 777\n",
      "Loss of train set: 0.15338227152824402 at epoch: 17 and batch_num: 778\n",
      "Loss of train set: 0.3425675928592682 at epoch: 17 and batch_num: 779\n",
      "Loss of train set: 0.31738054752349854 at epoch: 17 and batch_num: 780\n",
      "Loss of train set: 0.3445226550102234 at epoch: 17 and batch_num: 781\n",
      "Loss of train set: 0.21430066227912903 at epoch: 17 and batch_num: 782\n",
      "Loss of train set: 0.20456881821155548 at epoch: 17 and batch_num: 783\n",
      "Loss of train set: 0.24844661355018616 at epoch: 17 and batch_num: 784\n",
      "Loss of train set: 0.26588237285614014 at epoch: 17 and batch_num: 785\n",
      "Loss of train set: 0.215165376663208 at epoch: 17 and batch_num: 786\n",
      "Loss of train set: 0.26842042803764343 at epoch: 17 and batch_num: 787\n",
      "Loss of train set: 0.2212841808795929 at epoch: 17 and batch_num: 788\n",
      "Loss of train set: 0.3021194636821747 at epoch: 17 and batch_num: 789\n",
      "Loss of train set: 0.23745980858802795 at epoch: 17 and batch_num: 790\n",
      "Loss of train set: 0.2220781445503235 at epoch: 17 and batch_num: 791\n",
      "Loss of train set: 0.32983696460723877 at epoch: 17 and batch_num: 792\n",
      "Loss of train set: 0.2653624713420868 at epoch: 17 and batch_num: 793\n",
      "Loss of train set: 0.4014202952384949 at epoch: 17 and batch_num: 794\n",
      "Loss of train set: 0.42647379636764526 at epoch: 17 and batch_num: 795\n",
      "Loss of train set: 0.538661777973175 at epoch: 17 and batch_num: 796\n",
      "Loss of train set: 0.41706323623657227 at epoch: 17 and batch_num: 797\n",
      "Loss of train set: 0.2369072139263153 at epoch: 17 and batch_num: 798\n",
      "Loss of train set: 0.4935864508152008 at epoch: 17 and batch_num: 799\n",
      "Loss of train set: 0.2405659258365631 at epoch: 17 and batch_num: 800\n",
      "Loss of train set: 0.2632589340209961 at epoch: 17 and batch_num: 801\n",
      "Loss of train set: 0.38379621505737305 at epoch: 17 and batch_num: 802\n",
      "Loss of train set: 0.32639503479003906 at epoch: 17 and batch_num: 803\n",
      "Loss of train set: 0.4231550097465515 at epoch: 17 and batch_num: 804\n",
      "Loss of train set: 0.09238304197788239 at epoch: 17 and batch_num: 805\n",
      "Loss of train set: 0.1476060450077057 at epoch: 17 and batch_num: 806\n",
      "Loss of train set: 0.24504414200782776 at epoch: 17 and batch_num: 807\n",
      "Loss of train set: 0.40426650643348694 at epoch: 17 and batch_num: 808\n",
      "Loss of train set: 0.33223873376846313 at epoch: 17 and batch_num: 809\n",
      "Loss of train set: 0.26643499732017517 at epoch: 17 and batch_num: 810\n",
      "Loss of train set: 0.2635495066642761 at epoch: 17 and batch_num: 811\n",
      "Loss of train set: 0.5479061007499695 at epoch: 17 and batch_num: 812\n",
      "Loss of train set: 0.24172328412532806 at epoch: 17 and batch_num: 813\n",
      "Loss of train set: 0.2794967293739319 at epoch: 17 and batch_num: 814\n",
      "Loss of train set: 0.40404272079467773 at epoch: 17 and batch_num: 815\n",
      "Loss of train set: 0.24064776301383972 at epoch: 17 and batch_num: 816\n",
      "Loss of train set: 0.2889488935470581 at epoch: 17 and batch_num: 817\n",
      "Loss of train set: 0.37788844108581543 at epoch: 17 and batch_num: 818\n",
      "Loss of train set: 0.19738562405109406 at epoch: 17 and batch_num: 819\n",
      "Loss of train set: 0.3057624101638794 at epoch: 17 and batch_num: 820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.26088497042655945 at epoch: 17 and batch_num: 821\n",
      "Loss of train set: 0.3590949773788452 at epoch: 17 and batch_num: 822\n",
      "Loss of train set: 0.23465754091739655 at epoch: 17 and batch_num: 823\n",
      "Loss of train set: 0.3182905614376068 at epoch: 17 and batch_num: 824\n",
      "Loss of train set: 0.3293132781982422 at epoch: 17 and batch_num: 825\n",
      "Loss of train set: 0.22072570025920868 at epoch: 17 and batch_num: 826\n",
      "Loss of train set: 0.20479516685009003 at epoch: 17 and batch_num: 827\n",
      "Loss of train set: 0.23094826936721802 at epoch: 17 and batch_num: 828\n",
      "Loss of train set: 0.17911288142204285 at epoch: 17 and batch_num: 829\n",
      "Loss of train set: 0.33333051204681396 at epoch: 17 and batch_num: 830\n",
      "Loss of train set: 0.3158293068408966 at epoch: 17 and batch_num: 831\n",
      "Loss of train set: 0.40780192613601685 at epoch: 17 and batch_num: 832\n",
      "Loss of train set: 0.34600910544395447 at epoch: 17 and batch_num: 833\n",
      "Loss of train set: 0.22371552884578705 at epoch: 17 and batch_num: 834\n",
      "Loss of train set: 0.2697168290615082 at epoch: 17 and batch_num: 835\n",
      "Loss of train set: 0.21608701348304749 at epoch: 17 and batch_num: 836\n",
      "Loss of train set: 0.28642258048057556 at epoch: 17 and batch_num: 837\n",
      "Loss of train set: 0.3620039224624634 at epoch: 17 and batch_num: 838\n",
      "Loss of train set: 0.37296706438064575 at epoch: 17 and batch_num: 839\n",
      "Loss of train set: 0.23152035474777222 at epoch: 17 and batch_num: 840\n",
      "Loss of train set: 0.23447443544864655 at epoch: 17 and batch_num: 841\n",
      "Loss of train set: 0.1805589199066162 at epoch: 17 and batch_num: 842\n",
      "Loss of train set: 0.15457728505134583 at epoch: 17 and batch_num: 843\n",
      "Loss of train set: 0.11489707231521606 at epoch: 17 and batch_num: 844\n",
      "Loss of train set: 0.21140769124031067 at epoch: 17 and batch_num: 845\n",
      "Loss of train set: 0.3375503718852997 at epoch: 17 and batch_num: 846\n",
      "Loss of train set: 0.41860780119895935 at epoch: 17 and batch_num: 847\n",
      "Loss of train set: 0.1403980255126953 at epoch: 17 and batch_num: 848\n",
      "Loss of train set: 0.2849167585372925 at epoch: 17 and batch_num: 849\n",
      "Loss of train set: 0.25199955701828003 at epoch: 17 and batch_num: 850\n",
      "Loss of train set: 0.22120967507362366 at epoch: 17 and batch_num: 851\n",
      "Loss of train set: 0.23792634904384613 at epoch: 17 and batch_num: 852\n",
      "Loss of train set: 0.3022316098213196 at epoch: 17 and batch_num: 853\n",
      "Loss of train set: 0.5195061564445496 at epoch: 17 and batch_num: 854\n",
      "Loss of train set: 0.34247708320617676 at epoch: 17 and batch_num: 855\n",
      "Loss of train set: 0.31823456287384033 at epoch: 17 and batch_num: 856\n",
      "Loss of train set: 0.2516661286354065 at epoch: 17 and batch_num: 857\n",
      "Loss of train set: 0.20280864834785461 at epoch: 17 and batch_num: 858\n",
      "Loss of train set: 0.27179354429244995 at epoch: 17 and batch_num: 859\n",
      "Loss of train set: 0.3360515236854553 at epoch: 17 and batch_num: 860\n",
      "Loss of train set: 0.3154125511646271 at epoch: 17 and batch_num: 861\n",
      "Loss of train set: 0.2912113666534424 at epoch: 17 and batch_num: 862\n",
      "Loss of train set: 0.24445851147174835 at epoch: 17 and batch_num: 863\n",
      "Loss of train set: 0.17614877223968506 at epoch: 17 and batch_num: 864\n",
      "Loss of train set: 0.249198779463768 at epoch: 17 and batch_num: 865\n",
      "Loss of train set: 0.3201324939727783 at epoch: 17 and batch_num: 866\n",
      "Loss of train set: 0.14588966965675354 at epoch: 17 and batch_num: 867\n",
      "Loss of train set: 0.19491857290267944 at epoch: 17 and batch_num: 868\n",
      "Loss of train set: 0.2937178611755371 at epoch: 17 and batch_num: 869\n",
      "Loss of train set: 0.36546486616134644 at epoch: 17 and batch_num: 870\n",
      "Loss of train set: 0.4459298551082611 at epoch: 17 and batch_num: 871\n",
      "Loss of train set: 0.16941264271736145 at epoch: 17 and batch_num: 872\n",
      "Loss of train set: 0.17526617646217346 at epoch: 17 and batch_num: 873\n",
      "Loss of train set: 0.21990463137626648 at epoch: 17 and batch_num: 874\n",
      "Loss of train set: 0.12412264198064804 at epoch: 17 and batch_num: 875\n",
      "Loss of train set: 0.28757011890411377 at epoch: 17 and batch_num: 876\n",
      "Loss of train set: 0.2345089614391327 at epoch: 17 and batch_num: 877\n",
      "Loss of train set: 0.292502760887146 at epoch: 17 and batch_num: 878\n",
      "Loss of train set: 0.3727554678916931 at epoch: 17 and batch_num: 879\n",
      "Loss of train set: 0.31483131647109985 at epoch: 17 and batch_num: 880\n",
      "Loss of train set: 0.10729297995567322 at epoch: 17 and batch_num: 881\n",
      "Loss of train set: 0.29741740226745605 at epoch: 17 and batch_num: 882\n",
      "Loss of train set: 0.1901267170906067 at epoch: 17 and batch_num: 883\n",
      "Loss of train set: 0.18477173149585724 at epoch: 17 and batch_num: 884\n",
      "Loss of train set: 0.21776235103607178 at epoch: 17 and batch_num: 885\n",
      "Loss of train set: 0.3075867295265198 at epoch: 17 and batch_num: 886\n",
      "Loss of train set: 0.17883163690567017 at epoch: 17 and batch_num: 887\n",
      "Loss of train set: 0.23097975552082062 at epoch: 17 and batch_num: 888\n",
      "Loss of train set: 0.21845310926437378 at epoch: 17 and batch_num: 889\n",
      "Loss of train set: 0.29517537355422974 at epoch: 17 and batch_num: 890\n",
      "Loss of train set: 0.24888059496879578 at epoch: 17 and batch_num: 891\n",
      "Loss of train set: 0.24460653960704803 at epoch: 17 and batch_num: 892\n",
      "Loss of train set: 0.22362534701824188 at epoch: 17 and batch_num: 893\n",
      "Loss of train set: 0.49595797061920166 at epoch: 17 and batch_num: 894\n",
      "Loss of train set: 0.2968112528324127 at epoch: 17 and batch_num: 895\n",
      "Loss of train set: 0.28782376646995544 at epoch: 17 and batch_num: 896\n",
      "Loss of train set: 0.24192869663238525 at epoch: 17 and batch_num: 897\n",
      "Loss of train set: 0.28809165954589844 at epoch: 17 and batch_num: 898\n",
      "Loss of train set: 0.37749576568603516 at epoch: 17 and batch_num: 899\n",
      "Loss of train set: 0.30374467372894287 at epoch: 17 and batch_num: 900\n",
      "Loss of train set: 0.3424062132835388 at epoch: 17 and batch_num: 901\n",
      "Loss of train set: 0.27957022190093994 at epoch: 17 and batch_num: 902\n",
      "Loss of train set: 0.23555350303649902 at epoch: 17 and batch_num: 903\n",
      "Loss of train set: 0.36883872747421265 at epoch: 17 and batch_num: 904\n",
      "Loss of train set: 0.40583908557891846 at epoch: 17 and batch_num: 905\n",
      "Loss of train set: 0.2723933756351471 at epoch: 17 and batch_num: 906\n",
      "Loss of train set: 0.29993966221809387 at epoch: 17 and batch_num: 907\n",
      "Loss of train set: 0.237294003367424 at epoch: 17 and batch_num: 908\n",
      "Loss of train set: 0.2478104829788208 at epoch: 17 and batch_num: 909\n",
      "Loss of train set: 0.41290557384490967 at epoch: 17 and batch_num: 910\n",
      "Loss of train set: 0.2792392671108246 at epoch: 17 and batch_num: 911\n",
      "Loss of train set: 0.26321008801460266 at epoch: 17 and batch_num: 912\n",
      "Loss of train set: 0.4806833863258362 at epoch: 17 and batch_num: 913\n",
      "Loss of train set: 0.28548189997673035 at epoch: 17 and batch_num: 914\n",
      "Loss of train set: 0.1662561297416687 at epoch: 17 and batch_num: 915\n",
      "Loss of train set: 0.3451548218727112 at epoch: 17 and batch_num: 916\n",
      "Loss of train set: 0.4490695595741272 at epoch: 17 and batch_num: 917\n",
      "Loss of train set: 0.3159542977809906 at epoch: 17 and batch_num: 918\n",
      "Loss of train set: 0.2053382396697998 at epoch: 17 and batch_num: 919\n",
      "Loss of train set: 0.4300830364227295 at epoch: 17 and batch_num: 920\n",
      "Loss of train set: 0.38813766837120056 at epoch: 17 and batch_num: 921\n",
      "Loss of train set: 0.3043389618396759 at epoch: 17 and batch_num: 922\n",
      "Loss of train set: 0.2403569221496582 at epoch: 17 and batch_num: 923\n",
      "Loss of train set: 0.31916046142578125 at epoch: 17 and batch_num: 924\n",
      "Loss of train set: 0.21456798911094666 at epoch: 17 and batch_num: 925\n",
      "Loss of train set: 0.31700748205184937 at epoch: 17 and batch_num: 926\n",
      "Loss of train set: 0.16964221000671387 at epoch: 17 and batch_num: 927\n",
      "Loss of train set: 0.36620187759399414 at epoch: 17 and batch_num: 928\n",
      "Loss of train set: 0.20848309993743896 at epoch: 17 and batch_num: 929\n",
      "Loss of train set: 0.19780407845973969 at epoch: 17 and batch_num: 930\n",
      "Loss of train set: 0.4600302577018738 at epoch: 17 and batch_num: 931\n",
      "Loss of train set: 0.2845539450645447 at epoch: 17 and batch_num: 932\n",
      "Loss of train set: 0.2670518755912781 at epoch: 17 and batch_num: 933\n",
      "Loss of train set: 0.26118993759155273 at epoch: 17 and batch_num: 934\n",
      "Loss of train set: 0.44407957792282104 at epoch: 17 and batch_num: 935\n",
      "Loss of train set: 0.19457031786441803 at epoch: 17 and batch_num: 936\n",
      "Loss of train set: 0.3363463282585144 at epoch: 17 and batch_num: 937\n",
      "Accuracy of train set: 0.8940333333333333\n",
      "Loss of test set: 0.19215631484985352 at epoch: 17 and batch_num: 0\n",
      "Loss of test set: 0.3691340982913971 at epoch: 17 and batch_num: 1\n",
      "Loss of test set: 0.28655606508255005 at epoch: 17 and batch_num: 2\n",
      "Loss of test set: 0.3474496006965637 at epoch: 17 and batch_num: 3\n",
      "Loss of test set: 0.5599997043609619 at epoch: 17 and batch_num: 4\n",
      "Loss of test set: 0.36112451553344727 at epoch: 17 and batch_num: 5\n",
      "Loss of test set: 0.1837582290172577 at epoch: 17 and batch_num: 6\n",
      "Loss of test set: 0.285782128572464 at epoch: 17 and batch_num: 7\n",
      "Loss of test set: 0.5765578746795654 at epoch: 17 and batch_num: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.24115268886089325 at epoch: 17 and batch_num: 9\n",
      "Loss of test set: 0.3546231687068939 at epoch: 17 and batch_num: 10\n",
      "Loss of test set: 0.3378046154975891 at epoch: 17 and batch_num: 11\n",
      "Loss of test set: 0.3546852469444275 at epoch: 17 and batch_num: 12\n",
      "Loss of test set: 0.3330863118171692 at epoch: 17 and batch_num: 13\n",
      "Loss of test set: 0.39012566208839417 at epoch: 17 and batch_num: 14\n",
      "Loss of test set: 0.3089933693408966 at epoch: 17 and batch_num: 15\n",
      "Loss of test set: 0.2535645365715027 at epoch: 17 and batch_num: 16\n",
      "Loss of test set: 0.2521982192993164 at epoch: 17 and batch_num: 17\n",
      "Loss of test set: 0.4466405212879181 at epoch: 17 and batch_num: 18\n",
      "Loss of test set: 0.2614515423774719 at epoch: 17 and batch_num: 19\n",
      "Loss of test set: 0.30297616124153137 at epoch: 17 and batch_num: 20\n",
      "Loss of test set: 0.5216882228851318 at epoch: 17 and batch_num: 21\n",
      "Loss of test set: 0.3598715364933014 at epoch: 17 and batch_num: 22\n",
      "Loss of test set: 0.1772448718547821 at epoch: 17 and batch_num: 23\n",
      "Loss of test set: 0.23245860636234283 at epoch: 17 and batch_num: 24\n",
      "Loss of test set: 0.34909677505493164 at epoch: 17 and batch_num: 25\n",
      "Loss of test set: 0.5330865383148193 at epoch: 17 and batch_num: 26\n",
      "Loss of test set: 0.4347584843635559 at epoch: 17 and batch_num: 27\n",
      "Loss of test set: 0.3854566216468811 at epoch: 17 and batch_num: 28\n",
      "Loss of test set: 0.37969398498535156 at epoch: 17 and batch_num: 29\n",
      "Loss of test set: 0.37784647941589355 at epoch: 17 and batch_num: 30\n",
      "Loss of test set: 0.5774921774864197 at epoch: 17 and batch_num: 31\n",
      "Loss of test set: 0.32423272728919983 at epoch: 17 and batch_num: 32\n",
      "Loss of test set: 0.6761517524719238 at epoch: 17 and batch_num: 33\n",
      "Loss of test set: 0.34372371435165405 at epoch: 17 and batch_num: 34\n",
      "Loss of test set: 0.4635552167892456 at epoch: 17 and batch_num: 35\n",
      "Loss of test set: 0.4166546165943146 at epoch: 17 and batch_num: 36\n",
      "Loss of test set: 0.25218456983566284 at epoch: 17 and batch_num: 37\n",
      "Loss of test set: 0.25496989488601685 at epoch: 17 and batch_num: 38\n",
      "Loss of test set: 0.2687317132949829 at epoch: 17 and batch_num: 39\n",
      "Loss of test set: 0.28890755772590637 at epoch: 17 and batch_num: 40\n",
      "Loss of test set: 0.4288601577281952 at epoch: 17 and batch_num: 41\n",
      "Loss of test set: 0.4670965075492859 at epoch: 17 and batch_num: 42\n",
      "Loss of test set: 0.29667818546295166 at epoch: 17 and batch_num: 43\n",
      "Loss of test set: 0.22190508246421814 at epoch: 17 and batch_num: 44\n",
      "Loss of test set: 0.25988590717315674 at epoch: 17 and batch_num: 45\n",
      "Loss of test set: 0.21667225658893585 at epoch: 17 and batch_num: 46\n",
      "Loss of test set: 0.3192315995693207 at epoch: 17 and batch_num: 47\n",
      "Loss of test set: 0.30814221501350403 at epoch: 17 and batch_num: 48\n",
      "Loss of test set: 0.45922034978866577 at epoch: 17 and batch_num: 49\n",
      "Loss of test set: 0.2036069929599762 at epoch: 17 and batch_num: 50\n",
      "Loss of test set: 0.3700929284095764 at epoch: 17 and batch_num: 51\n",
      "Loss of test set: 0.4422772228717804 at epoch: 17 and batch_num: 52\n",
      "Loss of test set: 0.5421913266181946 at epoch: 17 and batch_num: 53\n",
      "Loss of test set: 0.3100526034832001 at epoch: 17 and batch_num: 54\n",
      "Loss of test set: 0.25749263167381287 at epoch: 17 and batch_num: 55\n",
      "Loss of test set: 0.3183959424495697 at epoch: 17 and batch_num: 56\n",
      "Loss of test set: 0.3959171772003174 at epoch: 17 and batch_num: 57\n",
      "Loss of test set: 0.6021726727485657 at epoch: 17 and batch_num: 58\n",
      "Loss of test set: 0.5052627325057983 at epoch: 17 and batch_num: 59\n",
      "Loss of test set: 0.3391677141189575 at epoch: 17 and batch_num: 60\n",
      "Loss of test set: 0.3076481819152832 at epoch: 17 and batch_num: 61\n",
      "Loss of test set: 0.3112003207206726 at epoch: 17 and batch_num: 62\n",
      "Loss of test set: 0.5733246207237244 at epoch: 17 and batch_num: 63\n",
      "Loss of test set: 0.28334853053092957 at epoch: 17 and batch_num: 64\n",
      "Loss of test set: 0.3155122399330139 at epoch: 17 and batch_num: 65\n",
      "Loss of test set: 0.3193140923976898 at epoch: 17 and batch_num: 66\n",
      "Loss of test set: 0.3982682526111603 at epoch: 17 and batch_num: 67\n",
      "Loss of test set: 0.40500301122665405 at epoch: 17 and batch_num: 68\n",
      "Loss of test set: 0.31046247482299805 at epoch: 17 and batch_num: 69\n",
      "Loss of test set: 0.40703150629997253 at epoch: 17 and batch_num: 70\n",
      "Loss of test set: 0.23250460624694824 at epoch: 17 and batch_num: 71\n",
      "Loss of test set: 0.41931819915771484 at epoch: 17 and batch_num: 72\n",
      "Loss of test set: 0.4032973647117615 at epoch: 17 and batch_num: 73\n",
      "Loss of test set: 0.31136444211006165 at epoch: 17 and batch_num: 74\n",
      "Loss of test set: 0.35218560695648193 at epoch: 17 and batch_num: 75\n",
      "Loss of test set: 0.4761803448200226 at epoch: 17 and batch_num: 76\n",
      "Loss of test set: 0.1609102189540863 at epoch: 17 and batch_num: 77\n",
      "Loss of test set: 0.27629852294921875 at epoch: 17 and batch_num: 78\n",
      "Loss of test set: 0.43387991189956665 at epoch: 17 and batch_num: 79\n",
      "Loss of test set: 0.17071954905986786 at epoch: 17 and batch_num: 80\n",
      "Loss of test set: 0.4323498606681824 at epoch: 17 and batch_num: 81\n",
      "Loss of test set: 0.5337835550308228 at epoch: 17 and batch_num: 82\n",
      "Loss of test set: 0.39274677634239197 at epoch: 17 and batch_num: 83\n",
      "Loss of test set: 0.4082251489162445 at epoch: 17 and batch_num: 84\n",
      "Loss of test set: 0.4764825701713562 at epoch: 17 and batch_num: 85\n",
      "Loss of test set: 0.2616366744041443 at epoch: 17 and batch_num: 86\n",
      "Loss of test set: 0.6775965690612793 at epoch: 17 and batch_num: 87\n",
      "Loss of test set: 0.30679023265838623 at epoch: 17 and batch_num: 88\n",
      "Loss of test set: 0.5675965547561646 at epoch: 17 and batch_num: 89\n",
      "Loss of test set: 0.2917967438697815 at epoch: 17 and batch_num: 90\n",
      "Loss of test set: 0.33552393317222595 at epoch: 17 and batch_num: 91\n",
      "Loss of test set: 0.4390837550163269 at epoch: 17 and batch_num: 92\n",
      "Loss of test set: 0.4012534022331238 at epoch: 17 and batch_num: 93\n",
      "Loss of test set: 0.2812104821205139 at epoch: 17 and batch_num: 94\n",
      "Loss of test set: 0.4445748031139374 at epoch: 17 and batch_num: 95\n",
      "Loss of test set: 0.415292352437973 at epoch: 17 and batch_num: 96\n",
      "Loss of test set: 0.3360789716243744 at epoch: 17 and batch_num: 97\n",
      "Loss of test set: 0.19535031914710999 at epoch: 17 and batch_num: 98\n",
      "Loss of test set: 0.3978883624076843 at epoch: 17 and batch_num: 99\n",
      "Loss of test set: 0.3161727488040924 at epoch: 17 and batch_num: 100\n",
      "Loss of test set: 0.5384526252746582 at epoch: 17 and batch_num: 101\n",
      "Loss of test set: 0.2681303024291992 at epoch: 17 and batch_num: 102\n",
      "Loss of test set: 0.47219324111938477 at epoch: 17 and batch_num: 103\n",
      "Loss of test set: 0.36178532242774963 at epoch: 17 and batch_num: 104\n",
      "Loss of test set: 0.539043128490448 at epoch: 17 and batch_num: 105\n",
      "Loss of test set: 0.24027082324028015 at epoch: 17 and batch_num: 106\n",
      "Loss of test set: 0.4106740951538086 at epoch: 17 and batch_num: 107\n",
      "Loss of test set: 0.37249556183815 at epoch: 17 and batch_num: 108\n",
      "Loss of test set: 0.22100770473480225 at epoch: 17 and batch_num: 109\n",
      "Loss of test set: 0.20972475409507751 at epoch: 17 and batch_num: 110\n",
      "Loss of test set: 0.5580026507377625 at epoch: 17 and batch_num: 111\n",
      "Loss of test set: 0.4897454082965851 at epoch: 17 and batch_num: 112\n",
      "Loss of test set: 0.582514762878418 at epoch: 17 and batch_num: 113\n",
      "Loss of test set: 0.4608253240585327 at epoch: 17 and batch_num: 114\n",
      "Loss of test set: 0.6049461960792542 at epoch: 17 and batch_num: 115\n",
      "Loss of test set: 0.36838382482528687 at epoch: 17 and batch_num: 116\n",
      "Loss of test set: 0.3763187527656555 at epoch: 17 and batch_num: 117\n",
      "Loss of test set: 0.4456949830055237 at epoch: 17 and batch_num: 118\n",
      "Loss of test set: 0.26024529337882996 at epoch: 17 and batch_num: 119\n",
      "Loss of test set: 0.36683449149131775 at epoch: 17 and batch_num: 120\n",
      "Loss of test set: 0.32379794120788574 at epoch: 17 and batch_num: 121\n",
      "Loss of test set: 0.32890403270721436 at epoch: 17 and batch_num: 122\n",
      "Loss of test set: 0.3574661314487457 at epoch: 17 and batch_num: 123\n",
      "Loss of test set: 0.41504478454589844 at epoch: 17 and batch_num: 124\n",
      "Loss of test set: 0.3012300729751587 at epoch: 17 and batch_num: 125\n",
      "Loss of test set: 0.4814265966415405 at epoch: 17 and batch_num: 126\n",
      "Loss of test set: 0.4210822284221649 at epoch: 17 and batch_num: 127\n",
      "Loss of test set: 0.27559083700180054 at epoch: 17 and batch_num: 128\n",
      "Loss of test set: 0.43691903352737427 at epoch: 17 and batch_num: 129\n",
      "Loss of test set: 0.22268187999725342 at epoch: 17 and batch_num: 130\n",
      "Loss of test set: 0.22872662544250488 at epoch: 17 and batch_num: 131\n",
      "Loss of test set: 0.3225112557411194 at epoch: 17 and batch_num: 132\n",
      "Loss of test set: 0.27640649676322937 at epoch: 17 and batch_num: 133\n",
      "Loss of test set: 0.3107048273086548 at epoch: 17 and batch_num: 134\n",
      "Loss of test set: 0.27764230966567993 at epoch: 17 and batch_num: 135\n",
      "Loss of test set: 0.4077657461166382 at epoch: 17 and batch_num: 136\n",
      "Loss of test set: 0.4320542514324188 at epoch: 17 and batch_num: 137\n",
      "Loss of test set: 0.47941160202026367 at epoch: 17 and batch_num: 138\n",
      "Loss of test set: 0.3572053909301758 at epoch: 17 and batch_num: 139\n",
      "Loss of test set: 0.2652079463005066 at epoch: 17 and batch_num: 140\n",
      "Loss of test set: 0.24413450062274933 at epoch: 17 and batch_num: 141\n",
      "Loss of test set: 0.28143346309661865 at epoch: 17 and batch_num: 142\n",
      "Loss of test set: 0.4436446726322174 at epoch: 17 and batch_num: 143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.44957855343818665 at epoch: 17 and batch_num: 144\n",
      "Loss of test set: 0.3959540128707886 at epoch: 17 and batch_num: 145\n",
      "Loss of test set: 0.37922629714012146 at epoch: 17 and batch_num: 146\n",
      "Loss of test set: 0.43960535526275635 at epoch: 17 and batch_num: 147\n",
      "Loss of test set: 0.3213545083999634 at epoch: 17 and batch_num: 148\n",
      "Loss of test set: 0.3046995997428894 at epoch: 17 and batch_num: 149\n",
      "Loss of test set: 0.5054017901420593 at epoch: 17 and batch_num: 150\n",
      "Loss of test set: 0.4978710114955902 at epoch: 17 and batch_num: 151\n",
      "Loss of test set: 0.2258564531803131 at epoch: 17 and batch_num: 152\n",
      "Loss of test set: 0.2815275490283966 at epoch: 17 and batch_num: 153\n",
      "Loss of test set: 0.3105977773666382 at epoch: 17 and batch_num: 154\n",
      "Loss of test set: 0.24833446741104126 at epoch: 17 and batch_num: 155\n",
      "Loss of test set: 0.2917247414588928 at epoch: 17 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8685\n",
      "Loss of train set: 0.28316810727119446 at epoch: 18 and batch_num: 0\n",
      "Loss of train set: 0.1409662365913391 at epoch: 18 and batch_num: 1\n",
      "Loss of train set: 0.14251750707626343 at epoch: 18 and batch_num: 2\n",
      "Loss of train set: 0.2876286506652832 at epoch: 18 and batch_num: 3\n",
      "Loss of train set: 0.3687499761581421 at epoch: 18 and batch_num: 4\n",
      "Loss of train set: 0.23157909512519836 at epoch: 18 and batch_num: 5\n",
      "Loss of train set: 0.45215994119644165 at epoch: 18 and batch_num: 6\n",
      "Loss of train set: 0.24383053183555603 at epoch: 18 and batch_num: 7\n",
      "Loss of train set: 0.2394617795944214 at epoch: 18 and batch_num: 8\n",
      "Loss of train set: 0.3085591197013855 at epoch: 18 and batch_num: 9\n",
      "Loss of train set: 0.39557164907455444 at epoch: 18 and batch_num: 10\n",
      "Loss of train set: 0.19009193778038025 at epoch: 18 and batch_num: 11\n",
      "Loss of train set: 0.3079468905925751 at epoch: 18 and batch_num: 12\n",
      "Loss of train set: 0.3158177137374878 at epoch: 18 and batch_num: 13\n",
      "Loss of train set: 0.26337915658950806 at epoch: 18 and batch_num: 14\n",
      "Loss of train set: 0.26210200786590576 at epoch: 18 and batch_num: 15\n",
      "Loss of train set: 0.17731799185276031 at epoch: 18 and batch_num: 16\n",
      "Loss of train set: 0.3051411211490631 at epoch: 18 and batch_num: 17\n",
      "Loss of train set: 0.49043378233909607 at epoch: 18 and batch_num: 18\n",
      "Loss of train set: 0.4299744963645935 at epoch: 18 and batch_num: 19\n",
      "Loss of train set: 0.2592681348323822 at epoch: 18 and batch_num: 20\n",
      "Loss of train set: 0.26068419218063354 at epoch: 18 and batch_num: 21\n",
      "Loss of train set: 0.22616387903690338 at epoch: 18 and batch_num: 22\n",
      "Loss of train set: 0.25244027376174927 at epoch: 18 and batch_num: 23\n",
      "Loss of train set: 0.25466054677963257 at epoch: 18 and batch_num: 24\n",
      "Loss of train set: 0.21602585911750793 at epoch: 18 and batch_num: 25\n",
      "Loss of train set: 0.37103763222694397 at epoch: 18 and batch_num: 26\n",
      "Loss of train set: 0.2050444781780243 at epoch: 18 and batch_num: 27\n",
      "Loss of train set: 0.21931225061416626 at epoch: 18 and batch_num: 28\n",
      "Loss of train set: 0.22324185073375702 at epoch: 18 and batch_num: 29\n",
      "Loss of train set: 0.21479791402816772 at epoch: 18 and batch_num: 30\n",
      "Loss of train set: 0.2295714169740677 at epoch: 18 and batch_num: 31\n",
      "Loss of train set: 0.45333898067474365 at epoch: 18 and batch_num: 32\n",
      "Loss of train set: 0.2962207496166229 at epoch: 18 and batch_num: 33\n",
      "Loss of train set: 0.2516498565673828 at epoch: 18 and batch_num: 34\n",
      "Loss of train set: 0.23411321640014648 at epoch: 18 and batch_num: 35\n",
      "Loss of train set: 0.3781536817550659 at epoch: 18 and batch_num: 36\n",
      "Loss of train set: 0.32951584458351135 at epoch: 18 and batch_num: 37\n",
      "Loss of train set: 0.3099021911621094 at epoch: 18 and batch_num: 38\n",
      "Loss of train set: 0.2964547872543335 at epoch: 18 and batch_num: 39\n",
      "Loss of train set: 0.32355189323425293 at epoch: 18 and batch_num: 40\n",
      "Loss of train set: 0.2761697769165039 at epoch: 18 and batch_num: 41\n",
      "Loss of train set: 0.18462708592414856 at epoch: 18 and batch_num: 42\n",
      "Loss of train set: 0.40261027216911316 at epoch: 18 and batch_num: 43\n",
      "Loss of train set: 0.2933422327041626 at epoch: 18 and batch_num: 44\n",
      "Loss of train set: 0.35371366143226624 at epoch: 18 and batch_num: 45\n",
      "Loss of train set: 0.18258380889892578 at epoch: 18 and batch_num: 46\n",
      "Loss of train set: 0.20908211171627045 at epoch: 18 and batch_num: 47\n",
      "Loss of train set: 0.259898841381073 at epoch: 18 and batch_num: 48\n",
      "Loss of train set: 0.21154099702835083 at epoch: 18 and batch_num: 49\n",
      "Loss of train set: 0.22512409090995789 at epoch: 18 and batch_num: 50\n",
      "Loss of train set: 0.21279507875442505 at epoch: 18 and batch_num: 51\n",
      "Loss of train set: 0.11778295040130615 at epoch: 18 and batch_num: 52\n",
      "Loss of train set: 0.3264924883842468 at epoch: 18 and batch_num: 53\n",
      "Loss of train set: 0.39681053161621094 at epoch: 18 and batch_num: 54\n",
      "Loss of train set: 0.2311471700668335 at epoch: 18 and batch_num: 55\n",
      "Loss of train set: 0.16326849162578583 at epoch: 18 and batch_num: 56\n",
      "Loss of train set: 0.2486748993396759 at epoch: 18 and batch_num: 57\n",
      "Loss of train set: 0.2965584993362427 at epoch: 18 and batch_num: 58\n",
      "Loss of train set: 0.22280120849609375 at epoch: 18 and batch_num: 59\n",
      "Loss of train set: 0.19761765003204346 at epoch: 18 and batch_num: 60\n",
      "Loss of train set: 0.29724112153053284 at epoch: 18 and batch_num: 61\n",
      "Loss of train set: 0.1625606119632721 at epoch: 18 and batch_num: 62\n",
      "Loss of train set: 0.2553243339061737 at epoch: 18 and batch_num: 63\n",
      "Loss of train set: 0.2635428011417389 at epoch: 18 and batch_num: 64\n",
      "Loss of train set: 0.38557422161102295 at epoch: 18 and batch_num: 65\n",
      "Loss of train set: 0.515843391418457 at epoch: 18 and batch_num: 66\n",
      "Loss of train set: 0.4805491268634796 at epoch: 18 and batch_num: 67\n",
      "Loss of train set: 0.2595433294773102 at epoch: 18 and batch_num: 68\n",
      "Loss of train set: 0.27060866355895996 at epoch: 18 and batch_num: 69\n",
      "Loss of train set: 0.20168176293373108 at epoch: 18 and batch_num: 70\n",
      "Loss of train set: 0.24735307693481445 at epoch: 18 and batch_num: 71\n",
      "Loss of train set: 0.3360082507133484 at epoch: 18 and batch_num: 72\n",
      "Loss of train set: 0.4768729507923126 at epoch: 18 and batch_num: 73\n",
      "Loss of train set: 0.30894070863723755 at epoch: 18 and batch_num: 74\n",
      "Loss of train set: 0.22057899832725525 at epoch: 18 and batch_num: 75\n",
      "Loss of train set: 0.38645994663238525 at epoch: 18 and batch_num: 76\n",
      "Loss of train set: 0.20274075865745544 at epoch: 18 and batch_num: 77\n",
      "Loss of train set: 0.21221591532230377 at epoch: 18 and batch_num: 78\n",
      "Loss of train set: 0.17642883956432343 at epoch: 18 and batch_num: 79\n",
      "Loss of train set: 0.2857094407081604 at epoch: 18 and batch_num: 80\n",
      "Loss of train set: 0.29599225521087646 at epoch: 18 and batch_num: 81\n",
      "Loss of train set: 0.2695939540863037 at epoch: 18 and batch_num: 82\n",
      "Loss of train set: 0.2775494456291199 at epoch: 18 and batch_num: 83\n",
      "Loss of train set: 0.3284786641597748 at epoch: 18 and batch_num: 84\n",
      "Loss of train set: 0.27334490418434143 at epoch: 18 and batch_num: 85\n",
      "Loss of train set: 0.19731101393699646 at epoch: 18 and batch_num: 86\n",
      "Loss of train set: 0.26295235753059387 at epoch: 18 and batch_num: 87\n",
      "Loss of train set: 0.23447267711162567 at epoch: 18 and batch_num: 88\n",
      "Loss of train set: 0.24237379431724548 at epoch: 18 and batch_num: 89\n",
      "Loss of train set: 0.2136285901069641 at epoch: 18 and batch_num: 90\n",
      "Loss of train set: 0.2065422236919403 at epoch: 18 and batch_num: 91\n",
      "Loss of train set: 0.4006054401397705 at epoch: 18 and batch_num: 92\n",
      "Loss of train set: 0.2067616581916809 at epoch: 18 and batch_num: 93\n",
      "Loss of train set: 0.21583005785942078 at epoch: 18 and batch_num: 94\n",
      "Loss of train set: 0.3481469750404358 at epoch: 18 and batch_num: 95\n",
      "Loss of train set: 0.252646803855896 at epoch: 18 and batch_num: 96\n",
      "Loss of train set: 0.2584626078605652 at epoch: 18 and batch_num: 97\n",
      "Loss of train set: 0.24884100258350372 at epoch: 18 and batch_num: 98\n",
      "Loss of train set: 0.17422577738761902 at epoch: 18 and batch_num: 99\n",
      "Loss of train set: 0.24254117906093597 at epoch: 18 and batch_num: 100\n",
      "Loss of train set: 0.2821314334869385 at epoch: 18 and batch_num: 101\n",
      "Loss of train set: 0.3458941876888275 at epoch: 18 and batch_num: 102\n",
      "Loss of train set: 0.3031529188156128 at epoch: 18 and batch_num: 103\n",
      "Loss of train set: 0.1457175761461258 at epoch: 18 and batch_num: 104\n",
      "Loss of train set: 0.25021013617515564 at epoch: 18 and batch_num: 105\n",
      "Loss of train set: 0.21733035147190094 at epoch: 18 and batch_num: 106\n",
      "Loss of train set: 0.4250504970550537 at epoch: 18 and batch_num: 107\n",
      "Loss of train set: 0.1926855891942978 at epoch: 18 and batch_num: 108\n",
      "Loss of train set: 0.2527218163013458 at epoch: 18 and batch_num: 109\n",
      "Loss of train set: 0.27281755208969116 at epoch: 18 and batch_num: 110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.1676366776227951 at epoch: 18 and batch_num: 111\n",
      "Loss of train set: 0.18968237936496735 at epoch: 18 and batch_num: 112\n",
      "Loss of train set: 0.3525463342666626 at epoch: 18 and batch_num: 113\n",
      "Loss of train set: 0.274674654006958 at epoch: 18 and batch_num: 114\n",
      "Loss of train set: 0.29415586590766907 at epoch: 18 and batch_num: 115\n",
      "Loss of train set: 0.23045524954795837 at epoch: 18 and batch_num: 116\n",
      "Loss of train set: 0.21193408966064453 at epoch: 18 and batch_num: 117\n",
      "Loss of train set: 0.388558566570282 at epoch: 18 and batch_num: 118\n",
      "Loss of train set: 0.14035329222679138 at epoch: 18 and batch_num: 119\n",
      "Loss of train set: 0.27891597151756287 at epoch: 18 and batch_num: 120\n",
      "Loss of train set: 0.20580247044563293 at epoch: 18 and batch_num: 121\n",
      "Loss of train set: 0.38049179315567017 at epoch: 18 and batch_num: 122\n",
      "Loss of train set: 0.1789444088935852 at epoch: 18 and batch_num: 123\n",
      "Loss of train set: 0.31035929918289185 at epoch: 18 and batch_num: 124\n",
      "Loss of train set: 0.22014160454273224 at epoch: 18 and batch_num: 125\n",
      "Loss of train set: 0.22625452280044556 at epoch: 18 and batch_num: 126\n",
      "Loss of train set: 0.3793985843658447 at epoch: 18 and batch_num: 127\n",
      "Loss of train set: 0.330852210521698 at epoch: 18 and batch_num: 128\n",
      "Loss of train set: 0.27115193009376526 at epoch: 18 and batch_num: 129\n",
      "Loss of train set: 0.32883909344673157 at epoch: 18 and batch_num: 130\n",
      "Loss of train set: 0.18491199612617493 at epoch: 18 and batch_num: 131\n",
      "Loss of train set: 0.2435334324836731 at epoch: 18 and batch_num: 132\n",
      "Loss of train set: 0.3350837826728821 at epoch: 18 and batch_num: 133\n",
      "Loss of train set: 0.16453275084495544 at epoch: 18 and batch_num: 134\n",
      "Loss of train set: 0.3930945098400116 at epoch: 18 and batch_num: 135\n",
      "Loss of train set: 0.20073361694812775 at epoch: 18 and batch_num: 136\n",
      "Loss of train set: 0.19454708695411682 at epoch: 18 and batch_num: 137\n",
      "Loss of train set: 0.3586583733558655 at epoch: 18 and batch_num: 138\n",
      "Loss of train set: 0.13966988027095795 at epoch: 18 and batch_num: 139\n",
      "Loss of train set: 0.19658103585243225 at epoch: 18 and batch_num: 140\n",
      "Loss of train set: 0.382133424282074 at epoch: 18 and batch_num: 141\n",
      "Loss of train set: 0.48053985834121704 at epoch: 18 and batch_num: 142\n",
      "Loss of train set: 0.49888819456100464 at epoch: 18 and batch_num: 143\n",
      "Loss of train set: 0.3566223084926605 at epoch: 18 and batch_num: 144\n",
      "Loss of train set: 0.29932090640068054 at epoch: 18 and batch_num: 145\n",
      "Loss of train set: 0.25693321228027344 at epoch: 18 and batch_num: 146\n",
      "Loss of train set: 0.35918062925338745 at epoch: 18 and batch_num: 147\n",
      "Loss of train set: 0.2594541311264038 at epoch: 18 and batch_num: 148\n",
      "Loss of train set: 0.21209248900413513 at epoch: 18 and batch_num: 149\n",
      "Loss of train set: 0.2268679291009903 at epoch: 18 and batch_num: 150\n",
      "Loss of train set: 0.3028014302253723 at epoch: 18 and batch_num: 151\n",
      "Loss of train set: 0.3386390805244446 at epoch: 18 and batch_num: 152\n",
      "Loss of train set: 0.24944818019866943 at epoch: 18 and batch_num: 153\n",
      "Loss of train set: 0.3491097688674927 at epoch: 18 and batch_num: 154\n",
      "Loss of train set: 0.22487950325012207 at epoch: 18 and batch_num: 155\n",
      "Loss of train set: 0.3154587745666504 at epoch: 18 and batch_num: 156\n",
      "Loss of train set: 0.1976165771484375 at epoch: 18 and batch_num: 157\n",
      "Loss of train set: 0.2910342216491699 at epoch: 18 and batch_num: 158\n",
      "Loss of train set: 0.26064974069595337 at epoch: 18 and batch_num: 159\n",
      "Loss of train set: 0.2412681132555008 at epoch: 18 and batch_num: 160\n",
      "Loss of train set: 0.2432190477848053 at epoch: 18 and batch_num: 161\n",
      "Loss of train set: 0.17578408122062683 at epoch: 18 and batch_num: 162\n",
      "Loss of train set: 0.28630852699279785 at epoch: 18 and batch_num: 163\n",
      "Loss of train set: 0.1419251561164856 at epoch: 18 and batch_num: 164\n",
      "Loss of train set: 0.3060433864593506 at epoch: 18 and batch_num: 165\n",
      "Loss of train set: 0.2270684838294983 at epoch: 18 and batch_num: 166\n",
      "Loss of train set: 0.23431098461151123 at epoch: 18 and batch_num: 167\n",
      "Loss of train set: 0.416725218296051 at epoch: 18 and batch_num: 168\n",
      "Loss of train set: 0.38041049242019653 at epoch: 18 and batch_num: 169\n",
      "Loss of train set: 0.22944223880767822 at epoch: 18 and batch_num: 170\n",
      "Loss of train set: 0.3432654142379761 at epoch: 18 and batch_num: 171\n",
      "Loss of train set: 0.336677610874176 at epoch: 18 and batch_num: 172\n",
      "Loss of train set: 0.2764778137207031 at epoch: 18 and batch_num: 173\n",
      "Loss of train set: 0.2200927585363388 at epoch: 18 and batch_num: 174\n",
      "Loss of train set: 0.46066445112228394 at epoch: 18 and batch_num: 175\n",
      "Loss of train set: 0.39227285981178284 at epoch: 18 and batch_num: 176\n",
      "Loss of train set: 0.12342546135187149 at epoch: 18 and batch_num: 177\n",
      "Loss of train set: 0.30853134393692017 at epoch: 18 and batch_num: 178\n",
      "Loss of train set: 0.27802956104278564 at epoch: 18 and batch_num: 179\n",
      "Loss of train set: 0.2804258167743683 at epoch: 18 and batch_num: 180\n",
      "Loss of train set: 0.32599300146102905 at epoch: 18 and batch_num: 181\n",
      "Loss of train set: 0.30931931734085083 at epoch: 18 and batch_num: 182\n",
      "Loss of train set: 0.16559192538261414 at epoch: 18 and batch_num: 183\n",
      "Loss of train set: 0.2185012698173523 at epoch: 18 and batch_num: 184\n",
      "Loss of train set: 0.4676383137702942 at epoch: 18 and batch_num: 185\n",
      "Loss of train set: 0.3314386308193207 at epoch: 18 and batch_num: 186\n",
      "Loss of train set: 0.2530883252620697 at epoch: 18 and batch_num: 187\n",
      "Loss of train set: 0.30055224895477295 at epoch: 18 and batch_num: 188\n",
      "Loss of train set: 0.20380496978759766 at epoch: 18 and batch_num: 189\n",
      "Loss of train set: 0.2524961233139038 at epoch: 18 and batch_num: 190\n",
      "Loss of train set: 0.20821958780288696 at epoch: 18 and batch_num: 191\n",
      "Loss of train set: 0.23941826820373535 at epoch: 18 and batch_num: 192\n",
      "Loss of train set: 0.2652364671230316 at epoch: 18 and batch_num: 193\n",
      "Loss of train set: 0.23004567623138428 at epoch: 18 and batch_num: 194\n",
      "Loss of train set: 0.42812246084213257 at epoch: 18 and batch_num: 195\n",
      "Loss of train set: 0.3645917773246765 at epoch: 18 and batch_num: 196\n",
      "Loss of train set: 0.33176445960998535 at epoch: 18 and batch_num: 197\n",
      "Loss of train set: 0.21750274300575256 at epoch: 18 and batch_num: 198\n",
      "Loss of train set: 0.17361611127853394 at epoch: 18 and batch_num: 199\n",
      "Loss of train set: 0.6293309330940247 at epoch: 18 and batch_num: 200\n",
      "Loss of train set: 0.3112478256225586 at epoch: 18 and batch_num: 201\n",
      "Loss of train set: 0.23869305849075317 at epoch: 18 and batch_num: 202\n",
      "Loss of train set: 0.40167349576950073 at epoch: 18 and batch_num: 203\n",
      "Loss of train set: 0.3102315664291382 at epoch: 18 and batch_num: 204\n",
      "Loss of train set: 0.2802717089653015 at epoch: 18 and batch_num: 205\n",
      "Loss of train set: 0.42633092403411865 at epoch: 18 and batch_num: 206\n",
      "Loss of train set: 0.3703199625015259 at epoch: 18 and batch_num: 207\n",
      "Loss of train set: 0.25991523265838623 at epoch: 18 and batch_num: 208\n",
      "Loss of train set: 0.26411277055740356 at epoch: 18 and batch_num: 209\n",
      "Loss of train set: 0.23121394217014313 at epoch: 18 and batch_num: 210\n",
      "Loss of train set: 0.1655723750591278 at epoch: 18 and batch_num: 211\n",
      "Loss of train set: 0.4994106888771057 at epoch: 18 and batch_num: 212\n",
      "Loss of train set: 0.19853460788726807 at epoch: 18 and batch_num: 213\n",
      "Loss of train set: 0.24834614992141724 at epoch: 18 and batch_num: 214\n",
      "Loss of train set: 0.33216723799705505 at epoch: 18 and batch_num: 215\n",
      "Loss of train set: 0.2775157392024994 at epoch: 18 and batch_num: 216\n",
      "Loss of train set: 0.3283827602863312 at epoch: 18 and batch_num: 217\n",
      "Loss of train set: 0.2354579120874405 at epoch: 18 and batch_num: 218\n",
      "Loss of train set: 0.27459877729415894 at epoch: 18 and batch_num: 219\n",
      "Loss of train set: 0.21279732882976532 at epoch: 18 and batch_num: 220\n",
      "Loss of train set: 0.21274492144584656 at epoch: 18 and batch_num: 221\n",
      "Loss of train set: 0.32649385929107666 at epoch: 18 and batch_num: 222\n",
      "Loss of train set: 0.31266045570373535 at epoch: 18 and batch_num: 223\n",
      "Loss of train set: 0.23202258348464966 at epoch: 18 and batch_num: 224\n",
      "Loss of train set: 0.2559363543987274 at epoch: 18 and batch_num: 225\n",
      "Loss of train set: 0.23796039819717407 at epoch: 18 and batch_num: 226\n",
      "Loss of train set: 0.297979474067688 at epoch: 18 and batch_num: 227\n",
      "Loss of train set: 0.21544910967350006 at epoch: 18 and batch_num: 228\n",
      "Loss of train set: 0.32051336765289307 at epoch: 18 and batch_num: 229\n",
      "Loss of train set: 0.3235655128955841 at epoch: 18 and batch_num: 230\n",
      "Loss of train set: 0.1944405436515808 at epoch: 18 and batch_num: 231\n",
      "Loss of train set: 0.3033776581287384 at epoch: 18 and batch_num: 232\n",
      "Loss of train set: 0.18950438499450684 at epoch: 18 and batch_num: 233\n",
      "Loss of train set: 0.25988876819610596 at epoch: 18 and batch_num: 234\n",
      "Loss of train set: 0.2520640790462494 at epoch: 18 and batch_num: 235\n",
      "Loss of train set: 0.22832395136356354 at epoch: 18 and batch_num: 236\n",
      "Loss of train set: 0.24065598845481873 at epoch: 18 and batch_num: 237\n",
      "Loss of train set: 0.21291664242744446 at epoch: 18 and batch_num: 238\n",
      "Loss of train set: 0.30246520042419434 at epoch: 18 and batch_num: 239\n",
      "Loss of train set: 0.33987927436828613 at epoch: 18 and batch_num: 240\n",
      "Loss of train set: 0.1622695028781891 at epoch: 18 and batch_num: 241\n",
      "Loss of train set: 0.37459009885787964 at epoch: 18 and batch_num: 242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.21580299735069275 at epoch: 18 and batch_num: 243\n",
      "Loss of train set: 0.5469850301742554 at epoch: 18 and batch_num: 244\n",
      "Loss of train set: 0.1103140115737915 at epoch: 18 and batch_num: 245\n",
      "Loss of train set: 0.11288177967071533 at epoch: 18 and batch_num: 246\n",
      "Loss of train set: 0.31321102380752563 at epoch: 18 and batch_num: 247\n",
      "Loss of train set: 0.28577595949172974 at epoch: 18 and batch_num: 248\n",
      "Loss of train set: 0.2407756745815277 at epoch: 18 and batch_num: 249\n",
      "Loss of train set: 0.3961661458015442 at epoch: 18 and batch_num: 250\n",
      "Loss of train set: 0.2431163787841797 at epoch: 18 and batch_num: 251\n",
      "Loss of train set: 0.28006070852279663 at epoch: 18 and batch_num: 252\n",
      "Loss of train set: 0.327908992767334 at epoch: 18 and batch_num: 253\n",
      "Loss of train set: 0.4484860897064209 at epoch: 18 and batch_num: 254\n",
      "Loss of train set: 0.3160443902015686 at epoch: 18 and batch_num: 255\n",
      "Loss of train set: 0.32205629348754883 at epoch: 18 and batch_num: 256\n",
      "Loss of train set: 0.34755778312683105 at epoch: 18 and batch_num: 257\n",
      "Loss of train set: 0.47473758459091187 at epoch: 18 and batch_num: 258\n",
      "Loss of train set: 0.3166992962360382 at epoch: 18 and batch_num: 259\n",
      "Loss of train set: 0.25515782833099365 at epoch: 18 and batch_num: 260\n",
      "Loss of train set: 0.34258320927619934 at epoch: 18 and batch_num: 261\n",
      "Loss of train set: 0.2164195030927658 at epoch: 18 and batch_num: 262\n",
      "Loss of train set: 0.12853534519672394 at epoch: 18 and batch_num: 263\n",
      "Loss of train set: 0.20071206986904144 at epoch: 18 and batch_num: 264\n",
      "Loss of train set: 0.16108208894729614 at epoch: 18 and batch_num: 265\n",
      "Loss of train set: 0.28694260120391846 at epoch: 18 and batch_num: 266\n",
      "Loss of train set: 0.32298046350479126 at epoch: 18 and batch_num: 267\n",
      "Loss of train set: 0.3142578899860382 at epoch: 18 and batch_num: 268\n",
      "Loss of train set: 0.32546311616897583 at epoch: 18 and batch_num: 269\n",
      "Loss of train set: 0.21796169877052307 at epoch: 18 and batch_num: 270\n",
      "Loss of train set: 0.2202434241771698 at epoch: 18 and batch_num: 271\n",
      "Loss of train set: 0.31305360794067383 at epoch: 18 and batch_num: 272\n",
      "Loss of train set: 0.3185960054397583 at epoch: 18 and batch_num: 273\n",
      "Loss of train set: 0.24042528867721558 at epoch: 18 and batch_num: 274\n",
      "Loss of train set: 0.3764917254447937 at epoch: 18 and batch_num: 275\n",
      "Loss of train set: 0.4037683606147766 at epoch: 18 and batch_num: 276\n",
      "Loss of train set: 0.2193666249513626 at epoch: 18 and batch_num: 277\n",
      "Loss of train set: 0.29888492822647095 at epoch: 18 and batch_num: 278\n",
      "Loss of train set: 0.15779709815979004 at epoch: 18 and batch_num: 279\n",
      "Loss of train set: 0.21152235567569733 at epoch: 18 and batch_num: 280\n",
      "Loss of train set: 0.14474815130233765 at epoch: 18 and batch_num: 281\n",
      "Loss of train set: 0.2889288663864136 at epoch: 18 and batch_num: 282\n",
      "Loss of train set: 0.24678483605384827 at epoch: 18 and batch_num: 283\n",
      "Loss of train set: 0.17977774143218994 at epoch: 18 and batch_num: 284\n",
      "Loss of train set: 0.2681518793106079 at epoch: 18 and batch_num: 285\n",
      "Loss of train set: 0.30427640676498413 at epoch: 18 and batch_num: 286\n",
      "Loss of train set: 0.33640921115875244 at epoch: 18 and batch_num: 287\n",
      "Loss of train set: 0.30505430698394775 at epoch: 18 and batch_num: 288\n",
      "Loss of train set: 0.28218942880630493 at epoch: 18 and batch_num: 289\n",
      "Loss of train set: 0.2789258360862732 at epoch: 18 and batch_num: 290\n",
      "Loss of train set: 0.27479398250579834 at epoch: 18 and batch_num: 291\n",
      "Loss of train set: 0.3579120635986328 at epoch: 18 and batch_num: 292\n",
      "Loss of train set: 0.21577417850494385 at epoch: 18 and batch_num: 293\n",
      "Loss of train set: 0.13084788620471954 at epoch: 18 and batch_num: 294\n",
      "Loss of train set: 0.31460970640182495 at epoch: 18 and batch_num: 295\n",
      "Loss of train set: 0.42339181900024414 at epoch: 18 and batch_num: 296\n",
      "Loss of train set: 0.2876027524471283 at epoch: 18 and batch_num: 297\n",
      "Loss of train set: 0.24700702726840973 at epoch: 18 and batch_num: 298\n",
      "Loss of train set: 0.5068359971046448 at epoch: 18 and batch_num: 299\n",
      "Loss of train set: 0.26895102858543396 at epoch: 18 and batch_num: 300\n",
      "Loss of train set: 0.34098100662231445 at epoch: 18 and batch_num: 301\n",
      "Loss of train set: 0.21785590052604675 at epoch: 18 and batch_num: 302\n",
      "Loss of train set: 0.34818506240844727 at epoch: 18 and batch_num: 303\n",
      "Loss of train set: 0.32567575573921204 at epoch: 18 and batch_num: 304\n",
      "Loss of train set: 0.3855515718460083 at epoch: 18 and batch_num: 305\n",
      "Loss of train set: 0.19534754753112793 at epoch: 18 and batch_num: 306\n",
      "Loss of train set: 0.16555972397327423 at epoch: 18 and batch_num: 307\n",
      "Loss of train set: 0.22658255696296692 at epoch: 18 and batch_num: 308\n",
      "Loss of train set: 0.19222640991210938 at epoch: 18 and batch_num: 309\n",
      "Loss of train set: 0.26354673504829407 at epoch: 18 and batch_num: 310\n",
      "Loss of train set: 0.28180718421936035 at epoch: 18 and batch_num: 311\n",
      "Loss of train set: 0.3113900125026703 at epoch: 18 and batch_num: 312\n",
      "Loss of train set: 0.159315288066864 at epoch: 18 and batch_num: 313\n",
      "Loss of train set: 0.24319234490394592 at epoch: 18 and batch_num: 314\n",
      "Loss of train set: 0.3203867971897125 at epoch: 18 and batch_num: 315\n",
      "Loss of train set: 0.2728525996208191 at epoch: 18 and batch_num: 316\n",
      "Loss of train set: 0.2664342522621155 at epoch: 18 and batch_num: 317\n",
      "Loss of train set: 0.5289759635925293 at epoch: 18 and batch_num: 318\n",
      "Loss of train set: 0.19121912121772766 at epoch: 18 and batch_num: 319\n",
      "Loss of train set: 0.16421301662921906 at epoch: 18 and batch_num: 320\n",
      "Loss of train set: 0.4401426911354065 at epoch: 18 and batch_num: 321\n",
      "Loss of train set: 0.3565421998500824 at epoch: 18 and batch_num: 322\n",
      "Loss of train set: 0.26756346225738525 at epoch: 18 and batch_num: 323\n",
      "Loss of train set: 0.24818828701972961 at epoch: 18 and batch_num: 324\n",
      "Loss of train set: 0.23641431331634521 at epoch: 18 and batch_num: 325\n",
      "Loss of train set: 0.38667532801628113 at epoch: 18 and batch_num: 326\n",
      "Loss of train set: 0.31161952018737793 at epoch: 18 and batch_num: 327\n",
      "Loss of train set: 0.32795143127441406 at epoch: 18 and batch_num: 328\n",
      "Loss of train set: 0.27496403455734253 at epoch: 18 and batch_num: 329\n",
      "Loss of train set: 0.16796888411045074 at epoch: 18 and batch_num: 330\n",
      "Loss of train set: 0.3123168349266052 at epoch: 18 and batch_num: 331\n",
      "Loss of train set: 0.33391010761260986 at epoch: 18 and batch_num: 332\n",
      "Loss of train set: 0.32848888635635376 at epoch: 18 and batch_num: 333\n",
      "Loss of train set: 0.18006661534309387 at epoch: 18 and batch_num: 334\n",
      "Loss of train set: 0.2527236044406891 at epoch: 18 and batch_num: 335\n",
      "Loss of train set: 0.27093929052352905 at epoch: 18 and batch_num: 336\n",
      "Loss of train set: 0.2927073836326599 at epoch: 18 and batch_num: 337\n",
      "Loss of train set: 0.2990124821662903 at epoch: 18 and batch_num: 338\n",
      "Loss of train set: 0.28984546661376953 at epoch: 18 and batch_num: 339\n",
      "Loss of train set: 0.41668733954429626 at epoch: 18 and batch_num: 340\n",
      "Loss of train set: 0.245601624250412 at epoch: 18 and batch_num: 341\n",
      "Loss of train set: 0.21227619051933289 at epoch: 18 and batch_num: 342\n",
      "Loss of train set: 0.3550497889518738 at epoch: 18 and batch_num: 343\n",
      "Loss of train set: 0.3717474639415741 at epoch: 18 and batch_num: 344\n",
      "Loss of train set: 0.2575124502182007 at epoch: 18 and batch_num: 345\n",
      "Loss of train set: 0.1693727821111679 at epoch: 18 and batch_num: 346\n",
      "Loss of train set: 0.27931177616119385 at epoch: 18 and batch_num: 347\n",
      "Loss of train set: 0.46019864082336426 at epoch: 18 and batch_num: 348\n",
      "Loss of train set: 0.25121816992759705 at epoch: 18 and batch_num: 349\n",
      "Loss of train set: 0.3910805881023407 at epoch: 18 and batch_num: 350\n",
      "Loss of train set: 0.39130374789237976 at epoch: 18 and batch_num: 351\n",
      "Loss of train set: 0.2545433044433594 at epoch: 18 and batch_num: 352\n",
      "Loss of train set: 0.2832984924316406 at epoch: 18 and batch_num: 353\n",
      "Loss of train set: 0.23268160223960876 at epoch: 18 and batch_num: 354\n",
      "Loss of train set: 0.21687060594558716 at epoch: 18 and batch_num: 355\n",
      "Loss of train set: 0.2741653621196747 at epoch: 18 and batch_num: 356\n",
      "Loss of train set: 0.2400982677936554 at epoch: 18 and batch_num: 357\n",
      "Loss of train set: 0.27608364820480347 at epoch: 18 and batch_num: 358\n",
      "Loss of train set: 0.21833905577659607 at epoch: 18 and batch_num: 359\n",
      "Loss of train set: 0.3219168186187744 at epoch: 18 and batch_num: 360\n",
      "Loss of train set: 0.36691200733184814 at epoch: 18 and batch_num: 361\n",
      "Loss of train set: 0.20579969882965088 at epoch: 18 and batch_num: 362\n",
      "Loss of train set: 0.14881792664527893 at epoch: 18 and batch_num: 363\n",
      "Loss of train set: 0.26650702953338623 at epoch: 18 and batch_num: 364\n",
      "Loss of train set: 0.2524314522743225 at epoch: 18 and batch_num: 365\n",
      "Loss of train set: 0.1973600685596466 at epoch: 18 and batch_num: 366\n",
      "Loss of train set: 0.20377972722053528 at epoch: 18 and batch_num: 367\n",
      "Loss of train set: 0.4257549047470093 at epoch: 18 and batch_num: 368\n",
      "Loss of train set: 0.24023717641830444 at epoch: 18 and batch_num: 369\n",
      "Loss of train set: 0.3507400155067444 at epoch: 18 and batch_num: 370\n",
      "Loss of train set: 0.6145639419555664 at epoch: 18 and batch_num: 371\n",
      "Loss of train set: 0.49234408140182495 at epoch: 18 and batch_num: 372\n",
      "Loss of train set: 0.2518707513809204 at epoch: 18 and batch_num: 373\n",
      "Loss of train set: 0.48182815313339233 at epoch: 18 and batch_num: 374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2186073362827301 at epoch: 18 and batch_num: 375\n",
      "Loss of train set: 0.24923276901245117 at epoch: 18 and batch_num: 376\n",
      "Loss of train set: 0.45809003710746765 at epoch: 18 and batch_num: 377\n",
      "Loss of train set: 0.2808226943016052 at epoch: 18 and batch_num: 378\n",
      "Loss of train set: 0.26845213770866394 at epoch: 18 and batch_num: 379\n",
      "Loss of train set: 0.1946842521429062 at epoch: 18 and batch_num: 380\n",
      "Loss of train set: 0.32877594232559204 at epoch: 18 and batch_num: 381\n",
      "Loss of train set: 0.33039289712905884 at epoch: 18 and batch_num: 382\n",
      "Loss of train set: 0.24121393263339996 at epoch: 18 and batch_num: 383\n",
      "Loss of train set: 0.22596795856952667 at epoch: 18 and batch_num: 384\n",
      "Loss of train set: 0.28906580805778503 at epoch: 18 and batch_num: 385\n",
      "Loss of train set: 0.24037814140319824 at epoch: 18 and batch_num: 386\n",
      "Loss of train set: 0.2193068265914917 at epoch: 18 and batch_num: 387\n",
      "Loss of train set: 0.3808652460575104 at epoch: 18 and batch_num: 388\n",
      "Loss of train set: 0.43585914373397827 at epoch: 18 and batch_num: 389\n",
      "Loss of train set: 0.30228763818740845 at epoch: 18 and batch_num: 390\n",
      "Loss of train set: 0.29068639874458313 at epoch: 18 and batch_num: 391\n",
      "Loss of train set: 0.24100345373153687 at epoch: 18 and batch_num: 392\n",
      "Loss of train set: 0.34688544273376465 at epoch: 18 and batch_num: 393\n",
      "Loss of train set: 0.33281394839286804 at epoch: 18 and batch_num: 394\n",
      "Loss of train set: 0.34993481636047363 at epoch: 18 and batch_num: 395\n",
      "Loss of train set: 0.2659274935722351 at epoch: 18 and batch_num: 396\n",
      "Loss of train set: 0.328910231590271 at epoch: 18 and batch_num: 397\n",
      "Loss of train set: 0.46889230608940125 at epoch: 18 and batch_num: 398\n",
      "Loss of train set: 0.1191486269235611 at epoch: 18 and batch_num: 399\n",
      "Loss of train set: 0.33288758993148804 at epoch: 18 and batch_num: 400\n",
      "Loss of train set: 0.3059544265270233 at epoch: 18 and batch_num: 401\n",
      "Loss of train set: 0.35084688663482666 at epoch: 18 and batch_num: 402\n",
      "Loss of train set: 0.2802237272262573 at epoch: 18 and batch_num: 403\n",
      "Loss of train set: 0.2501310110092163 at epoch: 18 and batch_num: 404\n",
      "Loss of train set: 0.3038933277130127 at epoch: 18 and batch_num: 405\n",
      "Loss of train set: 0.3520837426185608 at epoch: 18 and batch_num: 406\n",
      "Loss of train set: 0.27784085273742676 at epoch: 18 and batch_num: 407\n",
      "Loss of train set: 0.21288403868675232 at epoch: 18 and batch_num: 408\n",
      "Loss of train set: 0.1697627156972885 at epoch: 18 and batch_num: 409\n",
      "Loss of train set: 0.1910497099161148 at epoch: 18 and batch_num: 410\n",
      "Loss of train set: 0.22693447768688202 at epoch: 18 and batch_num: 411\n",
      "Loss of train set: 0.4023193120956421 at epoch: 18 and batch_num: 412\n",
      "Loss of train set: 0.2877817153930664 at epoch: 18 and batch_num: 413\n",
      "Loss of train set: 0.40598785877227783 at epoch: 18 and batch_num: 414\n",
      "Loss of train set: 0.3697635531425476 at epoch: 18 and batch_num: 415\n",
      "Loss of train set: 0.43621546030044556 at epoch: 18 and batch_num: 416\n",
      "Loss of train set: 0.27378785610198975 at epoch: 18 and batch_num: 417\n",
      "Loss of train set: 0.3648645281791687 at epoch: 18 and batch_num: 418\n",
      "Loss of train set: 0.44324952363967896 at epoch: 18 and batch_num: 419\n",
      "Loss of train set: 0.2815988063812256 at epoch: 18 and batch_num: 420\n",
      "Loss of train set: 0.1940365433692932 at epoch: 18 and batch_num: 421\n",
      "Loss of train set: 0.40260177850723267 at epoch: 18 and batch_num: 422\n",
      "Loss of train set: 0.19682061672210693 at epoch: 18 and batch_num: 423\n",
      "Loss of train set: 0.18496251106262207 at epoch: 18 and batch_num: 424\n",
      "Loss of train set: 0.3184117078781128 at epoch: 18 and batch_num: 425\n",
      "Loss of train set: 0.4072019159793854 at epoch: 18 and batch_num: 426\n",
      "Loss of train set: 0.1975899040699005 at epoch: 18 and batch_num: 427\n",
      "Loss of train set: 0.16572055220603943 at epoch: 18 and batch_num: 428\n",
      "Loss of train set: 0.3134651184082031 at epoch: 18 and batch_num: 429\n",
      "Loss of train set: 0.3054785132408142 at epoch: 18 and batch_num: 430\n",
      "Loss of train set: 0.41733020544052124 at epoch: 18 and batch_num: 431\n",
      "Loss of train set: 0.27666929364204407 at epoch: 18 and batch_num: 432\n",
      "Loss of train set: 0.2721080183982849 at epoch: 18 and batch_num: 433\n",
      "Loss of train set: 0.36040908098220825 at epoch: 18 and batch_num: 434\n",
      "Loss of train set: 0.37763017416000366 at epoch: 18 and batch_num: 435\n",
      "Loss of train set: 0.48118507862091064 at epoch: 18 and batch_num: 436\n",
      "Loss of train set: 0.3557802736759186 at epoch: 18 and batch_num: 437\n",
      "Loss of train set: 0.2167927473783493 at epoch: 18 and batch_num: 438\n",
      "Loss of train set: 0.3426869809627533 at epoch: 18 and batch_num: 439\n",
      "Loss of train set: 0.19210784137248993 at epoch: 18 and batch_num: 440\n",
      "Loss of train set: 0.38851284980773926 at epoch: 18 and batch_num: 441\n",
      "Loss of train set: 0.4799344837665558 at epoch: 18 and batch_num: 442\n",
      "Loss of train set: 0.31458431482315063 at epoch: 18 and batch_num: 443\n",
      "Loss of train set: 0.18710407614707947 at epoch: 18 and batch_num: 444\n",
      "Loss of train set: 0.1787877082824707 at epoch: 18 and batch_num: 445\n",
      "Loss of train set: 0.2073354721069336 at epoch: 18 and batch_num: 446\n",
      "Loss of train set: 0.3531797230243683 at epoch: 18 and batch_num: 447\n",
      "Loss of train set: 0.3040555715560913 at epoch: 18 and batch_num: 448\n",
      "Loss of train set: 0.5129759907722473 at epoch: 18 and batch_num: 449\n",
      "Loss of train set: 0.4615056812763214 at epoch: 18 and batch_num: 450\n",
      "Loss of train set: 0.23328015208244324 at epoch: 18 and batch_num: 451\n",
      "Loss of train set: 0.29760974645614624 at epoch: 18 and batch_num: 452\n",
      "Loss of train set: 0.3205568790435791 at epoch: 18 and batch_num: 453\n",
      "Loss of train set: 0.33882570266723633 at epoch: 18 and batch_num: 454\n",
      "Loss of train set: 0.3243459165096283 at epoch: 18 and batch_num: 455\n",
      "Loss of train set: 0.471599817276001 at epoch: 18 and batch_num: 456\n",
      "Loss of train set: 0.3077239394187927 at epoch: 18 and batch_num: 457\n",
      "Loss of train set: 0.48260754346847534 at epoch: 18 and batch_num: 458\n",
      "Loss of train set: 0.29058489203453064 at epoch: 18 and batch_num: 459\n",
      "Loss of train set: 0.34798669815063477 at epoch: 18 and batch_num: 460\n",
      "Loss of train set: 0.342357337474823 at epoch: 18 and batch_num: 461\n",
      "Loss of train set: 0.4085671305656433 at epoch: 18 and batch_num: 462\n",
      "Loss of train set: 0.1509694755077362 at epoch: 18 and batch_num: 463\n",
      "Loss of train set: 0.25721657276153564 at epoch: 18 and batch_num: 464\n",
      "Loss of train set: 0.2569662034511566 at epoch: 18 and batch_num: 465\n",
      "Loss of train set: 0.4181869626045227 at epoch: 18 and batch_num: 466\n",
      "Loss of train set: 0.4381491541862488 at epoch: 18 and batch_num: 467\n",
      "Loss of train set: 0.19615718722343445 at epoch: 18 and batch_num: 468\n",
      "Loss of train set: 0.10403060168027878 at epoch: 18 and batch_num: 469\n",
      "Loss of train set: 0.2808145582675934 at epoch: 18 and batch_num: 470\n",
      "Loss of train set: 0.2826104462146759 at epoch: 18 and batch_num: 471\n",
      "Loss of train set: 0.27612778544425964 at epoch: 18 and batch_num: 472\n",
      "Loss of train set: 0.39135634899139404 at epoch: 18 and batch_num: 473\n",
      "Loss of train set: 0.3249185085296631 at epoch: 18 and batch_num: 474\n",
      "Loss of train set: 0.3510175347328186 at epoch: 18 and batch_num: 475\n",
      "Loss of train set: 0.29390451312065125 at epoch: 18 and batch_num: 476\n",
      "Loss of train set: 0.19572065770626068 at epoch: 18 and batch_num: 477\n",
      "Loss of train set: 0.17453426122665405 at epoch: 18 and batch_num: 478\n",
      "Loss of train set: 0.23763173818588257 at epoch: 18 and batch_num: 479\n",
      "Loss of train set: 0.29770439863204956 at epoch: 18 and batch_num: 480\n",
      "Loss of train set: 0.2270987629890442 at epoch: 18 and batch_num: 481\n",
      "Loss of train set: 0.4298078715801239 at epoch: 18 and batch_num: 482\n",
      "Loss of train set: 0.4255640506744385 at epoch: 18 and batch_num: 483\n",
      "Loss of train set: 0.2784072160720825 at epoch: 18 and batch_num: 484\n",
      "Loss of train set: 0.30680328607559204 at epoch: 18 and batch_num: 485\n",
      "Loss of train set: 0.3010636568069458 at epoch: 18 and batch_num: 486\n",
      "Loss of train set: 0.31778645515441895 at epoch: 18 and batch_num: 487\n",
      "Loss of train set: 0.22601911425590515 at epoch: 18 and batch_num: 488\n",
      "Loss of train set: 0.3317263126373291 at epoch: 18 and batch_num: 489\n",
      "Loss of train set: 0.1960986703634262 at epoch: 18 and batch_num: 490\n",
      "Loss of train set: 0.23892220854759216 at epoch: 18 and batch_num: 491\n",
      "Loss of train set: 0.32283979654312134 at epoch: 18 and batch_num: 492\n",
      "Loss of train set: 0.22860375046730042 at epoch: 18 and batch_num: 493\n",
      "Loss of train set: 0.32768797874450684 at epoch: 18 and batch_num: 494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3556945323944092 at epoch: 18 and batch_num: 495\n",
      "Loss of train set: 0.38176432251930237 at epoch: 18 and batch_num: 496\n",
      "Loss of train set: 0.3042279779911041 at epoch: 18 and batch_num: 497\n",
      "Loss of train set: 0.392007976770401 at epoch: 18 and batch_num: 498\n",
      "Loss of train set: 0.26364216208457947 at epoch: 18 and batch_num: 499\n",
      "Loss of train set: 0.20664550364017487 at epoch: 18 and batch_num: 500\n",
      "Loss of train set: 0.27911338210105896 at epoch: 18 and batch_num: 501\n",
      "Loss of train set: 0.25600898265838623 at epoch: 18 and batch_num: 502\n",
      "Loss of train set: 0.41745656728744507 at epoch: 18 and batch_num: 503\n",
      "Loss of train set: 0.24720358848571777 at epoch: 18 and batch_num: 504\n",
      "Loss of train set: 0.2574307322502136 at epoch: 18 and batch_num: 505\n",
      "Loss of train set: 0.23617538809776306 at epoch: 18 and batch_num: 506\n",
      "Loss of train set: 0.2558612823486328 at epoch: 18 and batch_num: 507\n",
      "Loss of train set: 0.2872694134712219 at epoch: 18 and batch_num: 508\n",
      "Loss of train set: 0.2866360545158386 at epoch: 18 and batch_num: 509\n",
      "Loss of train set: 0.34410232305526733 at epoch: 18 and batch_num: 510\n",
      "Loss of train set: 0.2679488956928253 at epoch: 18 and batch_num: 511\n",
      "Loss of train set: 0.3625262975692749 at epoch: 18 and batch_num: 512\n",
      "Loss of train set: 0.2049161195755005 at epoch: 18 and batch_num: 513\n",
      "Loss of train set: 0.16681483387947083 at epoch: 18 and batch_num: 514\n",
      "Loss of train set: 0.35398611426353455 at epoch: 18 and batch_num: 515\n",
      "Loss of train set: 0.31192344427108765 at epoch: 18 and batch_num: 516\n",
      "Loss of train set: 0.3604758679866791 at epoch: 18 and batch_num: 517\n",
      "Loss of train set: 0.26771771907806396 at epoch: 18 and batch_num: 518\n",
      "Loss of train set: 0.2183205634355545 at epoch: 18 and batch_num: 519\n",
      "Loss of train set: 0.44891974329948425 at epoch: 18 and batch_num: 520\n",
      "Loss of train set: 0.2315729558467865 at epoch: 18 and batch_num: 521\n",
      "Loss of train set: 0.20795023441314697 at epoch: 18 and batch_num: 522\n",
      "Loss of train set: 0.32350242137908936 at epoch: 18 and batch_num: 523\n",
      "Loss of train set: 0.441246896982193 at epoch: 18 and batch_num: 524\n",
      "Loss of train set: 0.28021472692489624 at epoch: 18 and batch_num: 525\n",
      "Loss of train set: 0.295627236366272 at epoch: 18 and batch_num: 526\n",
      "Loss of train set: 0.37751466035842896 at epoch: 18 and batch_num: 527\n",
      "Loss of train set: 0.345331072807312 at epoch: 18 and batch_num: 528\n",
      "Loss of train set: 0.2529711127281189 at epoch: 18 and batch_num: 529\n",
      "Loss of train set: 0.3432066738605499 at epoch: 18 and batch_num: 530\n",
      "Loss of train set: 0.3114791512489319 at epoch: 18 and batch_num: 531\n",
      "Loss of train set: 0.1699865758419037 at epoch: 18 and batch_num: 532\n",
      "Loss of train set: 0.2822084128856659 at epoch: 18 and batch_num: 533\n",
      "Loss of train set: 0.40599092841148376 at epoch: 18 and batch_num: 534\n",
      "Loss of train set: 0.267273485660553 at epoch: 18 and batch_num: 535\n",
      "Loss of train set: 0.27003079652786255 at epoch: 18 and batch_num: 536\n",
      "Loss of train set: 0.19412221014499664 at epoch: 18 and batch_num: 537\n",
      "Loss of train set: 0.39340707659721375 at epoch: 18 and batch_num: 538\n",
      "Loss of train set: 0.27168959379196167 at epoch: 18 and batch_num: 539\n",
      "Loss of train set: 0.36025121808052063 at epoch: 18 and batch_num: 540\n",
      "Loss of train set: 0.12255042791366577 at epoch: 18 and batch_num: 541\n",
      "Loss of train set: 0.2645043730735779 at epoch: 18 and batch_num: 542\n",
      "Loss of train set: 0.25157007575035095 at epoch: 18 and batch_num: 543\n",
      "Loss of train set: 0.32716214656829834 at epoch: 18 and batch_num: 544\n",
      "Loss of train set: 0.29953449964523315 at epoch: 18 and batch_num: 545\n",
      "Loss of train set: 0.33317744731903076 at epoch: 18 and batch_num: 546\n",
      "Loss of train set: 0.2512882649898529 at epoch: 18 and batch_num: 547\n",
      "Loss of train set: 0.2447836995124817 at epoch: 18 and batch_num: 548\n",
      "Loss of train set: 0.4644119143486023 at epoch: 18 and batch_num: 549\n",
      "Loss of train set: 0.29229024052619934 at epoch: 18 and batch_num: 550\n",
      "Loss of train set: 0.18863868713378906 at epoch: 18 and batch_num: 551\n",
      "Loss of train set: 0.23361702263355255 at epoch: 18 and batch_num: 552\n",
      "Loss of train set: 0.23302176594734192 at epoch: 18 and batch_num: 553\n",
      "Loss of train set: 0.29881346225738525 at epoch: 18 and batch_num: 554\n",
      "Loss of train set: 0.21825022995471954 at epoch: 18 and batch_num: 555\n",
      "Loss of train set: 0.4629918336868286 at epoch: 18 and batch_num: 556\n",
      "Loss of train set: 0.35204780101776123 at epoch: 18 and batch_num: 557\n",
      "Loss of train set: 0.24234366416931152 at epoch: 18 and batch_num: 558\n",
      "Loss of train set: 0.22727739810943604 at epoch: 18 and batch_num: 559\n",
      "Loss of train set: 0.21554791927337646 at epoch: 18 and batch_num: 560\n",
      "Loss of train set: 0.24506926536560059 at epoch: 18 and batch_num: 561\n",
      "Loss of train set: 0.2452513426542282 at epoch: 18 and batch_num: 562\n",
      "Loss of train set: 0.26657822728157043 at epoch: 18 and batch_num: 563\n",
      "Loss of train set: 0.2830345034599304 at epoch: 18 and batch_num: 564\n",
      "Loss of train set: 0.35110190510749817 at epoch: 18 and batch_num: 565\n",
      "Loss of train set: 0.2643239498138428 at epoch: 18 and batch_num: 566\n",
      "Loss of train set: 0.27363866567611694 at epoch: 18 and batch_num: 567\n",
      "Loss of train set: 0.29783955216407776 at epoch: 18 and batch_num: 568\n",
      "Loss of train set: 0.4706854820251465 at epoch: 18 and batch_num: 569\n",
      "Loss of train set: 0.34820589423179626 at epoch: 18 and batch_num: 570\n",
      "Loss of train set: 0.24054908752441406 at epoch: 18 and batch_num: 571\n",
      "Loss of train set: 0.3147239089012146 at epoch: 18 and batch_num: 572\n",
      "Loss of train set: 0.1087251529097557 at epoch: 18 and batch_num: 573\n",
      "Loss of train set: 0.3655441403388977 at epoch: 18 and batch_num: 574\n",
      "Loss of train set: 0.25018632411956787 at epoch: 18 and batch_num: 575\n",
      "Loss of train set: 0.2621486186981201 at epoch: 18 and batch_num: 576\n",
      "Loss of train set: 0.25507068634033203 at epoch: 18 and batch_num: 577\n",
      "Loss of train set: 0.192903071641922 at epoch: 18 and batch_num: 578\n",
      "Loss of train set: 0.2563779354095459 at epoch: 18 and batch_num: 579\n",
      "Loss of train set: 0.3585669994354248 at epoch: 18 and batch_num: 580\n",
      "Loss of train set: 0.38257262110710144 at epoch: 18 and batch_num: 581\n",
      "Loss of train set: 0.2052832543849945 at epoch: 18 and batch_num: 582\n",
      "Loss of train set: 0.284973680973053 at epoch: 18 and batch_num: 583\n",
      "Loss of train set: 0.40937215089797974 at epoch: 18 and batch_num: 584\n",
      "Loss of train set: 0.1518831104040146 at epoch: 18 and batch_num: 585\n",
      "Loss of train set: 0.21262186765670776 at epoch: 18 and batch_num: 586\n",
      "Loss of train set: 0.19621390104293823 at epoch: 18 and batch_num: 587\n",
      "Loss of train set: 0.3410722315311432 at epoch: 18 and batch_num: 588\n",
      "Loss of train set: 0.2629099190235138 at epoch: 18 and batch_num: 589\n",
      "Loss of train set: 0.17921659350395203 at epoch: 18 and batch_num: 590\n",
      "Loss of train set: 0.2299564778804779 at epoch: 18 and batch_num: 591\n",
      "Loss of train set: 0.10859767347574234 at epoch: 18 and batch_num: 592\n",
      "Loss of train set: 0.3955649733543396 at epoch: 18 and batch_num: 593\n",
      "Loss of train set: 0.3411043882369995 at epoch: 18 and batch_num: 594\n",
      "Loss of train set: 0.33014652132987976 at epoch: 18 and batch_num: 595\n",
      "Loss of train set: 0.2443716824054718 at epoch: 18 and batch_num: 596\n",
      "Loss of train set: 0.19094209372997284 at epoch: 18 and batch_num: 597\n",
      "Loss of train set: 0.3140116333961487 at epoch: 18 and batch_num: 598\n",
      "Loss of train set: 0.21939051151275635 at epoch: 18 and batch_num: 599\n",
      "Loss of train set: 0.3481375277042389 at epoch: 18 and batch_num: 600\n",
      "Loss of train set: 0.2342732846736908 at epoch: 18 and batch_num: 601\n",
      "Loss of train set: 0.5396747589111328 at epoch: 18 and batch_num: 602\n",
      "Loss of train set: 0.2368086874485016 at epoch: 18 and batch_num: 603\n",
      "Loss of train set: 0.15984714031219482 at epoch: 18 and batch_num: 604\n",
      "Loss of train set: 0.4040749967098236 at epoch: 18 and batch_num: 605\n",
      "Loss of train set: 0.39803898334503174 at epoch: 18 and batch_num: 606\n",
      "Loss of train set: 0.4600905776023865 at epoch: 18 and batch_num: 607\n",
      "Loss of train set: 0.4490810036659241 at epoch: 18 and batch_num: 608\n",
      "Loss of train set: 0.20826244354248047 at epoch: 18 and batch_num: 609\n",
      "Loss of train set: 0.24506765604019165 at epoch: 18 and batch_num: 610\n",
      "Loss of train set: 0.3962816596031189 at epoch: 18 and batch_num: 611\n",
      "Loss of train set: 0.31333214044570923 at epoch: 18 and batch_num: 612\n",
      "Loss of train set: 0.27630582451820374 at epoch: 18 and batch_num: 613\n",
      "Loss of train set: 0.23997431993484497 at epoch: 18 and batch_num: 614\n",
      "Loss of train set: 0.3798357844352722 at epoch: 18 and batch_num: 615\n",
      "Loss of train set: 0.3319680988788605 at epoch: 18 and batch_num: 616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.4100798964500427 at epoch: 18 and batch_num: 617\n",
      "Loss of train set: 0.27326902747154236 at epoch: 18 and batch_num: 618\n",
      "Loss of train set: 0.2062755823135376 at epoch: 18 and batch_num: 619\n",
      "Loss of train set: 0.1907416582107544 at epoch: 18 and batch_num: 620\n",
      "Loss of train set: 0.26086875796318054 at epoch: 18 and batch_num: 621\n",
      "Loss of train set: 0.2942415475845337 at epoch: 18 and batch_num: 622\n",
      "Loss of train set: 0.21882188320159912 at epoch: 18 and batch_num: 623\n",
      "Loss of train set: 0.24558603763580322 at epoch: 18 and batch_num: 624\n",
      "Loss of train set: 0.20421311259269714 at epoch: 18 and batch_num: 625\n",
      "Loss of train set: 0.5853357315063477 at epoch: 18 and batch_num: 626\n",
      "Loss of train set: 0.27833351492881775 at epoch: 18 and batch_num: 627\n",
      "Loss of train set: 0.5264739990234375 at epoch: 18 and batch_num: 628\n",
      "Loss of train set: 0.3631315231323242 at epoch: 18 and batch_num: 629\n",
      "Loss of train set: 0.26276320219039917 at epoch: 18 and batch_num: 630\n",
      "Loss of train set: 0.32459962368011475 at epoch: 18 and batch_num: 631\n",
      "Loss of train set: 0.35567617416381836 at epoch: 18 and batch_num: 632\n",
      "Loss of train set: 0.3939133882522583 at epoch: 18 and batch_num: 633\n",
      "Loss of train set: 0.2738470435142517 at epoch: 18 and batch_num: 634\n",
      "Loss of train set: 0.23273403942584991 at epoch: 18 and batch_num: 635\n",
      "Loss of train set: 0.39009591937065125 at epoch: 18 and batch_num: 636\n",
      "Loss of train set: 0.1919591724872589 at epoch: 18 and batch_num: 637\n",
      "Loss of train set: 0.2825353443622589 at epoch: 18 and batch_num: 638\n",
      "Loss of train set: 0.22188347578048706 at epoch: 18 and batch_num: 639\n",
      "Loss of train set: 0.43351465463638306 at epoch: 18 and batch_num: 640\n",
      "Loss of train set: 0.3744373023509979 at epoch: 18 and batch_num: 641\n",
      "Loss of train set: 0.32913294434547424 at epoch: 18 and batch_num: 642\n",
      "Loss of train set: 0.2086973935365677 at epoch: 18 and batch_num: 643\n",
      "Loss of train set: 0.4225182831287384 at epoch: 18 and batch_num: 644\n",
      "Loss of train set: 0.2444676160812378 at epoch: 18 and batch_num: 645\n",
      "Loss of train set: 0.32965564727783203 at epoch: 18 and batch_num: 646\n",
      "Loss of train set: 0.2276143729686737 at epoch: 18 and batch_num: 647\n",
      "Loss of train set: 0.24757294356822968 at epoch: 18 and batch_num: 648\n",
      "Loss of train set: 0.37382233142852783 at epoch: 18 and batch_num: 649\n",
      "Loss of train set: 0.5314842462539673 at epoch: 18 and batch_num: 650\n",
      "Loss of train set: 0.26430565118789673 at epoch: 18 and batch_num: 651\n",
      "Loss of train set: 0.2687208652496338 at epoch: 18 and batch_num: 652\n",
      "Loss of train set: 0.27538132667541504 at epoch: 18 and batch_num: 653\n",
      "Loss of train set: 0.2969837486743927 at epoch: 18 and batch_num: 654\n",
      "Loss of train set: 0.2618369460105896 at epoch: 18 and batch_num: 655\n",
      "Loss of train set: 0.21025805175304413 at epoch: 18 and batch_num: 656\n",
      "Loss of train set: 0.19618050754070282 at epoch: 18 and batch_num: 657\n",
      "Loss of train set: 0.3250594437122345 at epoch: 18 and batch_num: 658\n",
      "Loss of train set: 0.28921788930892944 at epoch: 18 and batch_num: 659\n",
      "Loss of train set: 0.45262736082077026 at epoch: 18 and batch_num: 660\n",
      "Loss of train set: 0.32525309920310974 at epoch: 18 and batch_num: 661\n",
      "Loss of train set: 0.3719412684440613 at epoch: 18 and batch_num: 662\n",
      "Loss of train set: 0.19450204074382782 at epoch: 18 and batch_num: 663\n",
      "Loss of train set: 0.2856682538986206 at epoch: 18 and batch_num: 664\n",
      "Loss of train set: 0.2432267814874649 at epoch: 18 and batch_num: 665\n",
      "Loss of train set: 0.32866719365119934 at epoch: 18 and batch_num: 666\n",
      "Loss of train set: 0.20754440128803253 at epoch: 18 and batch_num: 667\n",
      "Loss of train set: 0.26826149225234985 at epoch: 18 and batch_num: 668\n",
      "Loss of train set: 0.24741095304489136 at epoch: 18 and batch_num: 669\n",
      "Loss of train set: 0.372041791677475 at epoch: 18 and batch_num: 670\n",
      "Loss of train set: 0.22718152403831482 at epoch: 18 and batch_num: 671\n",
      "Loss of train set: 0.19544202089309692 at epoch: 18 and batch_num: 672\n",
      "Loss of train set: 0.5454857349395752 at epoch: 18 and batch_num: 673\n",
      "Loss of train set: 0.2506886422634125 at epoch: 18 and batch_num: 674\n",
      "Loss of train set: 0.38021475076675415 at epoch: 18 and batch_num: 675\n",
      "Loss of train set: 0.343471884727478 at epoch: 18 and batch_num: 676\n",
      "Loss of train set: 0.2744716703891754 at epoch: 18 and batch_num: 677\n",
      "Loss of train set: 0.28132322430610657 at epoch: 18 and batch_num: 678\n",
      "Loss of train set: 0.27631089091300964 at epoch: 18 and batch_num: 679\n",
      "Loss of train set: 0.3044425845146179 at epoch: 18 and batch_num: 680\n",
      "Loss of train set: 0.3578830659389496 at epoch: 18 and batch_num: 681\n",
      "Loss of train set: 0.48167312145233154 at epoch: 18 and batch_num: 682\n",
      "Loss of train set: 0.46691280603408813 at epoch: 18 and batch_num: 683\n",
      "Loss of train set: 0.1771278977394104 at epoch: 18 and batch_num: 684\n",
      "Loss of train set: 0.17572525143623352 at epoch: 18 and batch_num: 685\n",
      "Loss of train set: 0.3450639843940735 at epoch: 18 and batch_num: 686\n",
      "Loss of train set: 0.3080708086490631 at epoch: 18 and batch_num: 687\n",
      "Loss of train set: 0.29088032245635986 at epoch: 18 and batch_num: 688\n",
      "Loss of train set: 0.23688969016075134 at epoch: 18 and batch_num: 689\n",
      "Loss of train set: 0.3516002893447876 at epoch: 18 and batch_num: 690\n",
      "Loss of train set: 0.26329338550567627 at epoch: 18 and batch_num: 691\n",
      "Loss of train set: 0.46259939670562744 at epoch: 18 and batch_num: 692\n",
      "Loss of train set: 0.22624719142913818 at epoch: 18 and batch_num: 693\n",
      "Loss of train set: 0.3010445237159729 at epoch: 18 and batch_num: 694\n",
      "Loss of train set: 0.29329296946525574 at epoch: 18 and batch_num: 695\n",
      "Loss of train set: 0.13461610674858093 at epoch: 18 and batch_num: 696\n",
      "Loss of train set: 0.3304993510246277 at epoch: 18 and batch_num: 697\n",
      "Loss of train set: 0.18169879913330078 at epoch: 18 and batch_num: 698\n",
      "Loss of train set: 0.43462246656417847 at epoch: 18 and batch_num: 699\n",
      "Loss of train set: 0.24154576659202576 at epoch: 18 and batch_num: 700\n",
      "Loss of train set: 0.281981498003006 at epoch: 18 and batch_num: 701\n",
      "Loss of train set: 0.1704874485731125 at epoch: 18 and batch_num: 702\n",
      "Loss of train set: 0.23624293506145477 at epoch: 18 and batch_num: 703\n",
      "Loss of train set: 0.2463904470205307 at epoch: 18 and batch_num: 704\n",
      "Loss of train set: 0.4474237263202667 at epoch: 18 and batch_num: 705\n",
      "Loss of train set: 0.19065168499946594 at epoch: 18 and batch_num: 706\n",
      "Loss of train set: 0.22213217616081238 at epoch: 18 and batch_num: 707\n",
      "Loss of train set: 0.26387834548950195 at epoch: 18 and batch_num: 708\n",
      "Loss of train set: 0.2876264154911041 at epoch: 18 and batch_num: 709\n",
      "Loss of train set: 0.5378017425537109 at epoch: 18 and batch_num: 710\n",
      "Loss of train set: 0.169367253780365 at epoch: 18 and batch_num: 711\n",
      "Loss of train set: 0.411257266998291 at epoch: 18 and batch_num: 712\n",
      "Loss of train set: 0.207036554813385 at epoch: 18 and batch_num: 713\n",
      "Loss of train set: 0.4125107526779175 at epoch: 18 and batch_num: 714\n",
      "Loss of train set: 0.19176700711250305 at epoch: 18 and batch_num: 715\n",
      "Loss of train set: 0.575465202331543 at epoch: 18 and batch_num: 716\n",
      "Loss of train set: 0.38911134004592896 at epoch: 18 and batch_num: 717\n",
      "Loss of train set: 0.1986558437347412 at epoch: 18 and batch_num: 718\n",
      "Loss of train set: 0.2648038864135742 at epoch: 18 and batch_num: 719\n",
      "Loss of train set: 0.15817412734031677 at epoch: 18 and batch_num: 720\n",
      "Loss of train set: 0.2236359715461731 at epoch: 18 and batch_num: 721\n",
      "Loss of train set: 0.2161296010017395 at epoch: 18 and batch_num: 722\n",
      "Loss of train set: 0.1911102682352066 at epoch: 18 and batch_num: 723\n",
      "Loss of train set: 0.20056265592575073 at epoch: 18 and batch_num: 724\n",
      "Loss of train set: 0.36911851167678833 at epoch: 18 and batch_num: 725\n",
      "Loss of train set: 0.235469788312912 at epoch: 18 and batch_num: 726\n",
      "Loss of train set: 0.34235164523124695 at epoch: 18 and batch_num: 727\n",
      "Loss of train set: 0.5184611082077026 at epoch: 18 and batch_num: 728\n",
      "Loss of train set: 0.2013043761253357 at epoch: 18 and batch_num: 729\n",
      "Loss of train set: 0.29031434655189514 at epoch: 18 and batch_num: 730\n",
      "Loss of train set: 0.30798912048339844 at epoch: 18 and batch_num: 731\n",
      "Loss of train set: 0.38244467973709106 at epoch: 18 and batch_num: 732\n",
      "Loss of train set: 0.39748555421829224 at epoch: 18 and batch_num: 733\n",
      "Loss of train set: 0.26100969314575195 at epoch: 18 and batch_num: 734\n",
      "Loss of train set: 0.3085848093032837 at epoch: 18 and batch_num: 735\n",
      "Loss of train set: 0.3811161518096924 at epoch: 18 and batch_num: 736\n",
      "Loss of train set: 0.19602081179618835 at epoch: 18 and batch_num: 737\n",
      "Loss of train set: 0.3937506377696991 at epoch: 18 and batch_num: 738\n",
      "Loss of train set: 0.33102986216545105 at epoch: 18 and batch_num: 739\n",
      "Loss of train set: 0.2790459096431732 at epoch: 18 and batch_num: 740\n",
      "Loss of train set: 0.20142951607704163 at epoch: 18 and batch_num: 741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.25314679741859436 at epoch: 18 and batch_num: 742\n",
      "Loss of train set: 0.2841842472553253 at epoch: 18 and batch_num: 743\n",
      "Loss of train set: 0.34598663449287415 at epoch: 18 and batch_num: 744\n",
      "Loss of train set: 0.6548163294792175 at epoch: 18 and batch_num: 745\n",
      "Loss of train set: 0.18599557876586914 at epoch: 18 and batch_num: 746\n",
      "Loss of train set: 0.21624602377414703 at epoch: 18 and batch_num: 747\n",
      "Loss of train set: 0.29132992029190063 at epoch: 18 and batch_num: 748\n",
      "Loss of train set: 0.2136206328868866 at epoch: 18 and batch_num: 749\n",
      "Loss of train set: 0.27875447273254395 at epoch: 18 and batch_num: 750\n",
      "Loss of train set: 0.3033941388130188 at epoch: 18 and batch_num: 751\n",
      "Loss of train set: 0.5098076462745667 at epoch: 18 and batch_num: 752\n",
      "Loss of train set: 0.29761675000190735 at epoch: 18 and batch_num: 753\n",
      "Loss of train set: 0.32857197523117065 at epoch: 18 and batch_num: 754\n",
      "Loss of train set: 0.30461955070495605 at epoch: 18 and batch_num: 755\n",
      "Loss of train set: 0.21620811522006989 at epoch: 18 and batch_num: 756\n",
      "Loss of train set: 0.2959531545639038 at epoch: 18 and batch_num: 757\n",
      "Loss of train set: 0.24876776337623596 at epoch: 18 and batch_num: 758\n",
      "Loss of train set: 0.3037238121032715 at epoch: 18 and batch_num: 759\n",
      "Loss of train set: 0.2889561653137207 at epoch: 18 and batch_num: 760\n",
      "Loss of train set: 0.2436353713274002 at epoch: 18 and batch_num: 761\n",
      "Loss of train set: 0.2855914235115051 at epoch: 18 and batch_num: 762\n",
      "Loss of train set: 0.3371231257915497 at epoch: 18 and batch_num: 763\n",
      "Loss of train set: 0.353669136762619 at epoch: 18 and batch_num: 764\n",
      "Loss of train set: 0.30030256509780884 at epoch: 18 and batch_num: 765\n",
      "Loss of train set: 0.13513126969337463 at epoch: 18 and batch_num: 766\n",
      "Loss of train set: 0.2850281000137329 at epoch: 18 and batch_num: 767\n",
      "Loss of train set: 0.5468868613243103 at epoch: 18 and batch_num: 768\n",
      "Loss of train set: 0.3095797002315521 at epoch: 18 and batch_num: 769\n",
      "Loss of train set: 0.26198917627334595 at epoch: 18 and batch_num: 770\n",
      "Loss of train set: 0.26633548736572266 at epoch: 18 and batch_num: 771\n",
      "Loss of train set: 0.1898183822631836 at epoch: 18 and batch_num: 772\n",
      "Loss of train set: 0.40788474678993225 at epoch: 18 and batch_num: 773\n",
      "Loss of train set: 0.3029933273792267 at epoch: 18 and batch_num: 774\n",
      "Loss of train set: 0.39751043915748596 at epoch: 18 and batch_num: 775\n",
      "Loss of train set: 0.2440503090620041 at epoch: 18 and batch_num: 776\n",
      "Loss of train set: 0.21758316457271576 at epoch: 18 and batch_num: 777\n",
      "Loss of train set: 0.4771760404109955 at epoch: 18 and batch_num: 778\n",
      "Loss of train set: 0.24515792727470398 at epoch: 18 and batch_num: 779\n",
      "Loss of train set: 0.19497039914131165 at epoch: 18 and batch_num: 780\n",
      "Loss of train set: 0.39952927827835083 at epoch: 18 and batch_num: 781\n",
      "Loss of train set: 0.28566810488700867 at epoch: 18 and batch_num: 782\n",
      "Loss of train set: 0.28379860520362854 at epoch: 18 and batch_num: 783\n",
      "Loss of train set: 0.223509281873703 at epoch: 18 and batch_num: 784\n",
      "Loss of train set: 0.2883802056312561 at epoch: 18 and batch_num: 785\n",
      "Loss of train set: 0.18347235023975372 at epoch: 18 and batch_num: 786\n",
      "Loss of train set: 0.2841795086860657 at epoch: 18 and batch_num: 787\n",
      "Loss of train set: 0.19833031296730042 at epoch: 18 and batch_num: 788\n",
      "Loss of train set: 0.3497828245162964 at epoch: 18 and batch_num: 789\n",
      "Loss of train set: 0.3053141236305237 at epoch: 18 and batch_num: 790\n",
      "Loss of train set: 0.1586090326309204 at epoch: 18 and batch_num: 791\n",
      "Loss of train set: 0.2256994992494583 at epoch: 18 and batch_num: 792\n",
      "Loss of train set: 0.19242136180400848 at epoch: 18 and batch_num: 793\n",
      "Loss of train set: 0.20450067520141602 at epoch: 18 and batch_num: 794\n",
      "Loss of train set: 0.265766978263855 at epoch: 18 and batch_num: 795\n",
      "Loss of train set: 0.4271945059299469 at epoch: 18 and batch_num: 796\n",
      "Loss of train set: 0.4114091992378235 at epoch: 18 and batch_num: 797\n",
      "Loss of train set: 0.2506830096244812 at epoch: 18 and batch_num: 798\n",
      "Loss of train set: 0.35713624954223633 at epoch: 18 and batch_num: 799\n",
      "Loss of train set: 0.38952985405921936 at epoch: 18 and batch_num: 800\n",
      "Loss of train set: 0.3063555657863617 at epoch: 18 and batch_num: 801\n",
      "Loss of train set: 0.167158305644989 at epoch: 18 and batch_num: 802\n",
      "Loss of train set: 0.3352200388908386 at epoch: 18 and batch_num: 803\n",
      "Loss of train set: 0.5330144166946411 at epoch: 18 and batch_num: 804\n",
      "Loss of train set: 0.12201154232025146 at epoch: 18 and batch_num: 805\n",
      "Loss of train set: 0.2617066502571106 at epoch: 18 and batch_num: 806\n",
      "Loss of train set: 0.06661874055862427 at epoch: 18 and batch_num: 807\n",
      "Loss of train set: 0.19007757306098938 at epoch: 18 and batch_num: 808\n",
      "Loss of train set: 0.3268539607524872 at epoch: 18 and batch_num: 809\n",
      "Loss of train set: 0.22462567687034607 at epoch: 18 and batch_num: 810\n",
      "Loss of train set: 0.17089492082595825 at epoch: 18 and batch_num: 811\n",
      "Loss of train set: 0.31316107511520386 at epoch: 18 and batch_num: 812\n",
      "Loss of train set: 0.29327210783958435 at epoch: 18 and batch_num: 813\n",
      "Loss of train set: 0.2559763789176941 at epoch: 18 and batch_num: 814\n",
      "Loss of train set: 0.2867327928543091 at epoch: 18 and batch_num: 815\n",
      "Loss of train set: 0.3256884813308716 at epoch: 18 and batch_num: 816\n",
      "Loss of train set: 0.24497734010219574 at epoch: 18 and batch_num: 817\n",
      "Loss of train set: 0.3444722890853882 at epoch: 18 and batch_num: 818\n",
      "Loss of train set: 0.39889073371887207 at epoch: 18 and batch_num: 819\n",
      "Loss of train set: 0.21981385350227356 at epoch: 18 and batch_num: 820\n",
      "Loss of train set: 0.23874199390411377 at epoch: 18 and batch_num: 821\n",
      "Loss of train set: 0.30738526582717896 at epoch: 18 and batch_num: 822\n",
      "Loss of train set: 0.277817040681839 at epoch: 18 and batch_num: 823\n",
      "Loss of train set: 0.2780439257621765 at epoch: 18 and batch_num: 824\n",
      "Loss of train set: 0.3510166108608246 at epoch: 18 and batch_num: 825\n",
      "Loss of train set: 0.3099217712879181 at epoch: 18 and batch_num: 826\n",
      "Loss of train set: 0.39783209562301636 at epoch: 18 and batch_num: 827\n",
      "Loss of train set: 0.22513869404792786 at epoch: 18 and batch_num: 828\n",
      "Loss of train set: 0.4372953176498413 at epoch: 18 and batch_num: 829\n",
      "Loss of train set: 0.11240724474191666 at epoch: 18 and batch_num: 830\n",
      "Loss of train set: 0.24814894795417786 at epoch: 18 and batch_num: 831\n",
      "Loss of train set: 0.3673178553581238 at epoch: 18 and batch_num: 832\n",
      "Loss of train set: 0.1937538981437683 at epoch: 18 and batch_num: 833\n",
      "Loss of train set: 0.25906774401664734 at epoch: 18 and batch_num: 834\n",
      "Loss of train set: 0.2650921940803528 at epoch: 18 and batch_num: 835\n",
      "Loss of train set: 0.3073928952217102 at epoch: 18 and batch_num: 836\n",
      "Loss of train set: 0.3613932430744171 at epoch: 18 and batch_num: 837\n",
      "Loss of train set: 0.22761930525302887 at epoch: 18 and batch_num: 838\n",
      "Loss of train set: 0.289353609085083 at epoch: 18 and batch_num: 839\n",
      "Loss of train set: 0.23464784026145935 at epoch: 18 and batch_num: 840\n",
      "Loss of train set: 0.2985318899154663 at epoch: 18 and batch_num: 841\n",
      "Loss of train set: 0.17001338303089142 at epoch: 18 and batch_num: 842\n",
      "Loss of train set: 0.29810047149658203 at epoch: 18 and batch_num: 843\n",
      "Loss of train set: 0.3417161703109741 at epoch: 18 and batch_num: 844\n",
      "Loss of train set: 0.2423182874917984 at epoch: 18 and batch_num: 845\n",
      "Loss of train set: 0.2967805862426758 at epoch: 18 and batch_num: 846\n",
      "Loss of train set: 0.22395706176757812 at epoch: 18 and batch_num: 847\n",
      "Loss of train set: 0.2937937378883362 at epoch: 18 and batch_num: 848\n",
      "Loss of train set: 0.5476076602935791 at epoch: 18 and batch_num: 849\n",
      "Loss of train set: 0.21397636830806732 at epoch: 18 and batch_num: 850\n",
      "Loss of train set: 0.2596437633037567 at epoch: 18 and batch_num: 851\n",
      "Loss of train set: 0.29748499393463135 at epoch: 18 and batch_num: 852\n",
      "Loss of train set: 0.26515305042266846 at epoch: 18 and batch_num: 853\n",
      "Loss of train set: 0.358982652425766 at epoch: 18 and batch_num: 854\n",
      "Loss of train set: 0.25466009974479675 at epoch: 18 and batch_num: 855\n",
      "Loss of train set: 0.17235518991947174 at epoch: 18 and batch_num: 856\n",
      "Loss of train set: 0.2570381760597229 at epoch: 18 and batch_num: 857\n",
      "Loss of train set: 0.1807684749364853 at epoch: 18 and batch_num: 858\n",
      "Loss of train set: 0.2460833340883255 at epoch: 18 and batch_num: 859\n",
      "Loss of train set: 0.2499333769083023 at epoch: 18 and batch_num: 860\n",
      "Loss of train set: 0.28504544496536255 at epoch: 18 and batch_num: 861\n",
      "Loss of train set: 0.27588605880737305 at epoch: 18 and batch_num: 862\n",
      "Loss of train set: 0.4008888900279999 at epoch: 18 and batch_num: 863\n",
      "Loss of train set: 0.2905154824256897 at epoch: 18 and batch_num: 864\n",
      "Loss of train set: 0.3551415801048279 at epoch: 18 and batch_num: 865\n",
      "Loss of train set: 0.41688209772109985 at epoch: 18 and batch_num: 866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.169485941529274 at epoch: 18 and batch_num: 867\n",
      "Loss of train set: 0.23608599603176117 at epoch: 18 and batch_num: 868\n",
      "Loss of train set: 0.4154438078403473 at epoch: 18 and batch_num: 869\n",
      "Loss of train set: 0.23818039894104004 at epoch: 18 and batch_num: 870\n",
      "Loss of train set: 0.22381386160850525 at epoch: 18 and batch_num: 871\n",
      "Loss of train set: 0.19271957874298096 at epoch: 18 and batch_num: 872\n",
      "Loss of train set: 0.3560108244419098 at epoch: 18 and batch_num: 873\n",
      "Loss of train set: 0.3980165123939514 at epoch: 18 and batch_num: 874\n",
      "Loss of train set: 0.1813901960849762 at epoch: 18 and batch_num: 875\n",
      "Loss of train set: 0.23561991751194 at epoch: 18 and batch_num: 876\n",
      "Loss of train set: 0.21773001551628113 at epoch: 18 and batch_num: 877\n",
      "Loss of train set: 0.1952192187309265 at epoch: 18 and batch_num: 878\n",
      "Loss of train set: 0.36053967475891113 at epoch: 18 and batch_num: 879\n",
      "Loss of train set: 0.2583248019218445 at epoch: 18 and batch_num: 880\n",
      "Loss of train set: 0.36100029945373535 at epoch: 18 and batch_num: 881\n",
      "Loss of train set: 0.3375067114830017 at epoch: 18 and batch_num: 882\n",
      "Loss of train set: 0.22465556859970093 at epoch: 18 and batch_num: 883\n",
      "Loss of train set: 0.29842936992645264 at epoch: 18 and batch_num: 884\n",
      "Loss of train set: 0.27537795901298523 at epoch: 18 and batch_num: 885\n",
      "Loss of train set: 0.27709826827049255 at epoch: 18 and batch_num: 886\n",
      "Loss of train set: 0.40581250190734863 at epoch: 18 and batch_num: 887\n",
      "Loss of train set: 0.3677356541156769 at epoch: 18 and batch_num: 888\n",
      "Loss of train set: 0.20336413383483887 at epoch: 18 and batch_num: 889\n",
      "Loss of train set: 0.17766451835632324 at epoch: 18 and batch_num: 890\n",
      "Loss of train set: 0.2601301372051239 at epoch: 18 and batch_num: 891\n",
      "Loss of train set: 0.4281616508960724 at epoch: 18 and batch_num: 892\n",
      "Loss of train set: 0.25264957547187805 at epoch: 18 and batch_num: 893\n",
      "Loss of train set: 0.31001710891723633 at epoch: 18 and batch_num: 894\n",
      "Loss of train set: 0.3761105537414551 at epoch: 18 and batch_num: 895\n",
      "Loss of train set: 0.26092392206192017 at epoch: 18 and batch_num: 896\n",
      "Loss of train set: 0.24368730187416077 at epoch: 18 and batch_num: 897\n",
      "Loss of train set: 0.24516963958740234 at epoch: 18 and batch_num: 898\n",
      "Loss of train set: 0.29011696577072144 at epoch: 18 and batch_num: 899\n",
      "Loss of train set: 0.4280635714530945 at epoch: 18 and batch_num: 900\n",
      "Loss of train set: 0.2558964788913727 at epoch: 18 and batch_num: 901\n",
      "Loss of train set: 0.16576896607875824 at epoch: 18 and batch_num: 902\n",
      "Loss of train set: 0.28408169746398926 at epoch: 18 and batch_num: 903\n",
      "Loss of train set: 0.2799888253211975 at epoch: 18 and batch_num: 904\n",
      "Loss of train set: 0.1630588322877884 at epoch: 18 and batch_num: 905\n",
      "Loss of train set: 0.2107020616531372 at epoch: 18 and batch_num: 906\n",
      "Loss of train set: 0.42849594354629517 at epoch: 18 and batch_num: 907\n",
      "Loss of train set: 0.45495906472206116 at epoch: 18 and batch_num: 908\n",
      "Loss of train set: 0.30060267448425293 at epoch: 18 and batch_num: 909\n",
      "Loss of train set: 0.31214338541030884 at epoch: 18 and batch_num: 910\n",
      "Loss of train set: 0.39245742559432983 at epoch: 18 and batch_num: 911\n",
      "Loss of train set: 0.19016854465007782 at epoch: 18 and batch_num: 912\n",
      "Loss of train set: 0.30169564485549927 at epoch: 18 and batch_num: 913\n",
      "Loss of train set: 0.30797913670539856 at epoch: 18 and batch_num: 914\n",
      "Loss of train set: 0.3194401264190674 at epoch: 18 and batch_num: 915\n",
      "Loss of train set: 0.2758324444293976 at epoch: 18 and batch_num: 916\n",
      "Loss of train set: 0.3572205603122711 at epoch: 18 and batch_num: 917\n",
      "Loss of train set: 0.2673085033893585 at epoch: 18 and batch_num: 918\n",
      "Loss of train set: 0.41813525557518005 at epoch: 18 and batch_num: 919\n",
      "Loss of train set: 0.38930419087409973 at epoch: 18 and batch_num: 920\n",
      "Loss of train set: 0.23364408314228058 at epoch: 18 and batch_num: 921\n",
      "Loss of train set: 0.28692421317100525 at epoch: 18 and batch_num: 922\n",
      "Loss of train set: 0.2817477881908417 at epoch: 18 and batch_num: 923\n",
      "Loss of train set: 0.22482667863368988 at epoch: 18 and batch_num: 924\n",
      "Loss of train set: 0.40951794385910034 at epoch: 18 and batch_num: 925\n",
      "Loss of train set: 0.3019447326660156 at epoch: 18 and batch_num: 926\n",
      "Loss of train set: 0.24077777564525604 at epoch: 18 and batch_num: 927\n",
      "Loss of train set: 0.25220727920532227 at epoch: 18 and batch_num: 928\n",
      "Loss of train set: 0.3494938313961029 at epoch: 18 and batch_num: 929\n",
      "Loss of train set: 0.36582469940185547 at epoch: 18 and batch_num: 930\n",
      "Loss of train set: 0.3151492476463318 at epoch: 18 and batch_num: 931\n",
      "Loss of train set: 0.3499259352684021 at epoch: 18 and batch_num: 932\n",
      "Loss of train set: 0.1594208925962448 at epoch: 18 and batch_num: 933\n",
      "Loss of train set: 0.25158828496932983 at epoch: 18 and batch_num: 934\n",
      "Loss of train set: 0.33855223655700684 at epoch: 18 and batch_num: 935\n",
      "Loss of train set: 0.2624592185020447 at epoch: 18 and batch_num: 936\n",
      "Loss of train set: 0.32580846548080444 at epoch: 18 and batch_num: 937\n",
      "Accuracy of train set: 0.8953833333333333\n",
      "Loss of test set: 0.22294646501541138 at epoch: 18 and batch_num: 0\n",
      "Loss of test set: 0.3152770400047302 at epoch: 18 and batch_num: 1\n",
      "Loss of test set: 0.3045860528945923 at epoch: 18 and batch_num: 2\n",
      "Loss of test set: 0.30752962827682495 at epoch: 18 and batch_num: 3\n",
      "Loss of test set: 0.3038145899772644 at epoch: 18 and batch_num: 4\n",
      "Loss of test set: 0.4660588502883911 at epoch: 18 and batch_num: 5\n",
      "Loss of test set: 0.28170275688171387 at epoch: 18 and batch_num: 6\n",
      "Loss of test set: 0.4326961040496826 at epoch: 18 and batch_num: 7\n",
      "Loss of test set: 0.48130470514297485 at epoch: 18 and batch_num: 8\n",
      "Loss of test set: 0.5452395677566528 at epoch: 18 and batch_num: 9\n",
      "Loss of test set: 0.28282877802848816 at epoch: 18 and batch_num: 10\n",
      "Loss of test set: 0.33521199226379395 at epoch: 18 and batch_num: 11\n",
      "Loss of test set: 0.1908537596464157 at epoch: 18 and batch_num: 12\n",
      "Loss of test set: 0.2717054486274719 at epoch: 18 and batch_num: 13\n",
      "Loss of test set: 0.28540998697280884 at epoch: 18 and batch_num: 14\n",
      "Loss of test set: 0.529295802116394 at epoch: 18 and batch_num: 15\n",
      "Loss of test set: 0.33182382583618164 at epoch: 18 and batch_num: 16\n",
      "Loss of test set: 0.4471593499183655 at epoch: 18 and batch_num: 17\n",
      "Loss of test set: 0.2693249583244324 at epoch: 18 and batch_num: 18\n",
      "Loss of test set: 0.2527567148208618 at epoch: 18 and batch_num: 19\n",
      "Loss of test set: 0.38887354731559753 at epoch: 18 and batch_num: 20\n",
      "Loss of test set: 0.5068618655204773 at epoch: 18 and batch_num: 21\n",
      "Loss of test set: 0.2416008710861206 at epoch: 18 and batch_num: 22\n",
      "Loss of test set: 0.36902323365211487 at epoch: 18 and batch_num: 23\n",
      "Loss of test set: 0.1482166349887848 at epoch: 18 and batch_num: 24\n",
      "Loss of test set: 0.26499485969543457 at epoch: 18 and batch_num: 25\n",
      "Loss of test set: 0.2575492560863495 at epoch: 18 and batch_num: 26\n",
      "Loss of test set: 0.5631693601608276 at epoch: 18 and batch_num: 27\n",
      "Loss of test set: 0.14260953664779663 at epoch: 18 and batch_num: 28\n",
      "Loss of test set: 0.33904504776000977 at epoch: 18 and batch_num: 29\n",
      "Loss of test set: 0.20220772922039032 at epoch: 18 and batch_num: 30\n",
      "Loss of test set: 0.3313579261302948 at epoch: 18 and batch_num: 31\n",
      "Loss of test set: 0.3426792025566101 at epoch: 18 and batch_num: 32\n",
      "Loss of test set: 0.3969496488571167 at epoch: 18 and batch_num: 33\n",
      "Loss of test set: 0.23465777933597565 at epoch: 18 and batch_num: 34\n",
      "Loss of test set: 0.22893773019313812 at epoch: 18 and batch_num: 35\n",
      "Loss of test set: 0.3549492061138153 at epoch: 18 and batch_num: 36\n",
      "Loss of test set: 0.39143675565719604 at epoch: 18 and batch_num: 37\n",
      "Loss of test set: 0.6290087699890137 at epoch: 18 and batch_num: 38\n",
      "Loss of test set: 0.6447330713272095 at epoch: 18 and batch_num: 39\n",
      "Loss of test set: 0.4037921726703644 at epoch: 18 and batch_num: 40\n",
      "Loss of test set: 0.2767484784126282 at epoch: 18 and batch_num: 41\n",
      "Loss of test set: 0.4362267851829529 at epoch: 18 and batch_num: 42\n",
      "Loss of test set: 0.4589962661266327 at epoch: 18 and batch_num: 43\n",
      "Loss of test set: 0.374961256980896 at epoch: 18 and batch_num: 44\n",
      "Loss of test set: 0.4264030456542969 at epoch: 18 and batch_num: 45\n",
      "Loss of test set: 0.5128082036972046 at epoch: 18 and batch_num: 46\n",
      "Loss of test set: 0.4829634428024292 at epoch: 18 and batch_num: 47\n",
      "Loss of test set: 0.48454493284225464 at epoch: 18 and batch_num: 48\n",
      "Loss of test set: 0.39099419116973877 at epoch: 18 and batch_num: 49\n",
      "Loss of test set: 0.30761033296585083 at epoch: 18 and batch_num: 50\n",
      "Loss of test set: 0.38664114475250244 at epoch: 18 and batch_num: 51\n",
      "Loss of test set: 0.24414247274398804 at epoch: 18 and batch_num: 52\n",
      "Loss of test set: 0.20535407960414886 at epoch: 18 and batch_num: 53\n",
      "Loss of test set: 0.4154890775680542 at epoch: 18 and batch_num: 54\n",
      "Loss of test set: 0.3343575596809387 at epoch: 18 and batch_num: 55\n",
      "Loss of test set: 0.37545642256736755 at epoch: 18 and batch_num: 56\n",
      "Loss of test set: 0.43865787982940674 at epoch: 18 and batch_num: 57\n",
      "Loss of test set: 0.5218329429626465 at epoch: 18 and batch_num: 58\n",
      "Loss of test set: 0.3025684356689453 at epoch: 18 and batch_num: 59\n",
      "Loss of test set: 0.379113107919693 at epoch: 18 and batch_num: 60\n",
      "Loss of test set: 0.3840380311012268 at epoch: 18 and batch_num: 61\n",
      "Loss of test set: 0.49476510286331177 at epoch: 18 and batch_num: 62\n",
      "Loss of test set: 0.20841924846172333 at epoch: 18 and batch_num: 63\n",
      "Loss of test set: 0.5386123657226562 at epoch: 18 and batch_num: 64\n",
      "Loss of test set: 0.36744919419288635 at epoch: 18 and batch_num: 65\n",
      "Loss of test set: 0.20434920489788055 at epoch: 18 and batch_num: 66\n",
      "Loss of test set: 0.437074214220047 at epoch: 18 and batch_num: 67\n",
      "Loss of test set: 0.451595664024353 at epoch: 18 and batch_num: 68\n",
      "Loss of test set: 0.38694363832473755 at epoch: 18 and batch_num: 69\n",
      "Loss of test set: 0.3302445113658905 at epoch: 18 and batch_num: 70\n",
      "Loss of test set: 0.35847634077072144 at epoch: 18 and batch_num: 71\n",
      "Loss of test set: 0.5420336723327637 at epoch: 18 and batch_num: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.35724353790283203 at epoch: 18 and batch_num: 73\n",
      "Loss of test set: 0.4569637179374695 at epoch: 18 and batch_num: 74\n",
      "Loss of test set: 0.4617939591407776 at epoch: 18 and batch_num: 75\n",
      "Loss of test set: 0.6318283081054688 at epoch: 18 and batch_num: 76\n",
      "Loss of test set: 0.33427000045776367 at epoch: 18 and batch_num: 77\n",
      "Loss of test set: 0.41412919759750366 at epoch: 18 and batch_num: 78\n",
      "Loss of test set: 0.31454768776893616 at epoch: 18 and batch_num: 79\n",
      "Loss of test set: 0.36271995306015015 at epoch: 18 and batch_num: 80\n",
      "Loss of test set: 0.24264542758464813 at epoch: 18 and batch_num: 81\n",
      "Loss of test set: 0.383932888507843 at epoch: 18 and batch_num: 82\n",
      "Loss of test set: 0.34821057319641113 at epoch: 18 and batch_num: 83\n",
      "Loss of test set: 0.4143703579902649 at epoch: 18 and batch_num: 84\n",
      "Loss of test set: 0.4224337339401245 at epoch: 18 and batch_num: 85\n",
      "Loss of test set: 0.48296111822128296 at epoch: 18 and batch_num: 86\n",
      "Loss of test set: 0.39594417810440063 at epoch: 18 and batch_num: 87\n",
      "Loss of test set: 0.5483152866363525 at epoch: 18 and batch_num: 88\n",
      "Loss of test set: 0.5490221977233887 at epoch: 18 and batch_num: 89\n",
      "Loss of test set: 0.4660208821296692 at epoch: 18 and batch_num: 90\n",
      "Loss of test set: 0.3776710331439972 at epoch: 18 and batch_num: 91\n",
      "Loss of test set: 0.27340829372406006 at epoch: 18 and batch_num: 92\n",
      "Loss of test set: 0.4291580021381378 at epoch: 18 and batch_num: 93\n",
      "Loss of test set: 0.46262314915657043 at epoch: 18 and batch_num: 94\n",
      "Loss of test set: 0.31803447008132935 at epoch: 18 and batch_num: 95\n",
      "Loss of test set: 0.2923690974712372 at epoch: 18 and batch_num: 96\n",
      "Loss of test set: 0.4073124825954437 at epoch: 18 and batch_num: 97\n",
      "Loss of test set: 0.39453887939453125 at epoch: 18 and batch_num: 98\n",
      "Loss of test set: 0.4091489315032959 at epoch: 18 and batch_num: 99\n",
      "Loss of test set: 0.3873997628688812 at epoch: 18 and batch_num: 100\n",
      "Loss of test set: 0.32372188568115234 at epoch: 18 and batch_num: 101\n",
      "Loss of test set: 0.2159399688243866 at epoch: 18 and batch_num: 102\n",
      "Loss of test set: 0.46839195489883423 at epoch: 18 and batch_num: 103\n",
      "Loss of test set: 0.2840583920478821 at epoch: 18 and batch_num: 104\n",
      "Loss of test set: 0.4604244828224182 at epoch: 18 and batch_num: 105\n",
      "Loss of test set: 0.34329140186309814 at epoch: 18 and batch_num: 106\n",
      "Loss of test set: 0.3552590608596802 at epoch: 18 and batch_num: 107\n",
      "Loss of test set: 0.4826993942260742 at epoch: 18 and batch_num: 108\n",
      "Loss of test set: 0.31495168805122375 at epoch: 18 and batch_num: 109\n",
      "Loss of test set: 0.3339505195617676 at epoch: 18 and batch_num: 110\n",
      "Loss of test set: 0.49754515290260315 at epoch: 18 and batch_num: 111\n",
      "Loss of test set: 0.3316504955291748 at epoch: 18 and batch_num: 112\n",
      "Loss of test set: 0.32110217213630676 at epoch: 18 and batch_num: 113\n",
      "Loss of test set: 0.1881178617477417 at epoch: 18 and batch_num: 114\n",
      "Loss of test set: 0.4528126120567322 at epoch: 18 and batch_num: 115\n",
      "Loss of test set: 0.3126782178878784 at epoch: 18 and batch_num: 116\n",
      "Loss of test set: 0.4037630259990692 at epoch: 18 and batch_num: 117\n",
      "Loss of test set: 0.4952753186225891 at epoch: 18 and batch_num: 118\n",
      "Loss of test set: 0.48386654257774353 at epoch: 18 and batch_num: 119\n",
      "Loss of test set: 0.42861151695251465 at epoch: 18 and batch_num: 120\n",
      "Loss of test set: 0.3949769139289856 at epoch: 18 and batch_num: 121\n",
      "Loss of test set: 0.3067464232444763 at epoch: 18 and batch_num: 122\n",
      "Loss of test set: 0.3617887496948242 at epoch: 18 and batch_num: 123\n",
      "Loss of test set: 0.5199272632598877 at epoch: 18 and batch_num: 124\n",
      "Loss of test set: 0.34026557207107544 at epoch: 18 and batch_num: 125\n",
      "Loss of test set: 0.15182897448539734 at epoch: 18 and batch_num: 126\n",
      "Loss of test set: 0.3583616316318512 at epoch: 18 and batch_num: 127\n",
      "Loss of test set: 0.45442044734954834 at epoch: 18 and batch_num: 128\n",
      "Loss of test set: 0.38006049394607544 at epoch: 18 and batch_num: 129\n",
      "Loss of test set: 0.31307747960090637 at epoch: 18 and batch_num: 130\n",
      "Loss of test set: 0.23135121166706085 at epoch: 18 and batch_num: 131\n",
      "Loss of test set: 0.2736385762691498 at epoch: 18 and batch_num: 132\n",
      "Loss of test set: 0.5307402610778809 at epoch: 18 and batch_num: 133\n",
      "Loss of test set: 0.6035935282707214 at epoch: 18 and batch_num: 134\n",
      "Loss of test set: 0.4659048914909363 at epoch: 18 and batch_num: 135\n",
      "Loss of test set: 0.3583240509033203 at epoch: 18 and batch_num: 136\n",
      "Loss of test set: 0.45872625708580017 at epoch: 18 and batch_num: 137\n",
      "Loss of test set: 0.36779218912124634 at epoch: 18 and batch_num: 138\n",
      "Loss of test set: 0.3086165189743042 at epoch: 18 and batch_num: 139\n",
      "Loss of test set: 0.4087531864643097 at epoch: 18 and batch_num: 140\n",
      "Loss of test set: 0.37004172801971436 at epoch: 18 and batch_num: 141\n",
      "Loss of test set: 0.4513152241706848 at epoch: 18 and batch_num: 142\n",
      "Loss of test set: 0.5397262573242188 at epoch: 18 and batch_num: 143\n",
      "Loss of test set: 0.33189868927001953 at epoch: 18 and batch_num: 144\n",
      "Loss of test set: 0.2322329431772232 at epoch: 18 and batch_num: 145\n",
      "Loss of test set: 0.290750652551651 at epoch: 18 and batch_num: 146\n",
      "Loss of test set: 0.365733802318573 at epoch: 18 and batch_num: 147\n",
      "Loss of test set: 0.43985456228256226 at epoch: 18 and batch_num: 148\n",
      "Loss of test set: 0.2421032339334488 at epoch: 18 and batch_num: 149\n",
      "Loss of test set: 0.3201943039894104 at epoch: 18 and batch_num: 150\n",
      "Loss of test set: 0.4622983932495117 at epoch: 18 and batch_num: 151\n",
      "Loss of test set: 0.280891090631485 at epoch: 18 and batch_num: 152\n",
      "Loss of test set: 0.37699100375175476 at epoch: 18 and batch_num: 153\n",
      "Loss of test set: 0.18037743866443634 at epoch: 18 and batch_num: 154\n",
      "Loss of test set: 0.13392901420593262 at epoch: 18 and batch_num: 155\n",
      "Loss of test set: 0.5640042424201965 at epoch: 18 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8664\n",
      "Loss of train set: 0.4161243736743927 at epoch: 19 and batch_num: 0\n",
      "Loss of train set: 0.1426488310098648 at epoch: 19 and batch_num: 1\n",
      "Loss of train set: 0.38903820514678955 at epoch: 19 and batch_num: 2\n",
      "Loss of train set: 0.34338074922561646 at epoch: 19 and batch_num: 3\n",
      "Loss of train set: 0.236190527677536 at epoch: 19 and batch_num: 4\n",
      "Loss of train set: 0.22602015733718872 at epoch: 19 and batch_num: 5\n",
      "Loss of train set: 0.3490891456604004 at epoch: 19 and batch_num: 6\n",
      "Loss of train set: 0.3204158544540405 at epoch: 19 and batch_num: 7\n",
      "Loss of train set: 0.38027670979499817 at epoch: 19 and batch_num: 8\n",
      "Loss of train set: 0.38369327783584595 at epoch: 19 and batch_num: 9\n",
      "Loss of train set: 0.132745623588562 at epoch: 19 and batch_num: 10\n",
      "Loss of train set: 0.3797982931137085 at epoch: 19 and batch_num: 11\n",
      "Loss of train set: 0.2166520059108734 at epoch: 19 and batch_num: 12\n",
      "Loss of train set: 0.2006957083940506 at epoch: 19 and batch_num: 13\n",
      "Loss of train set: 0.13262894749641418 at epoch: 19 and batch_num: 14\n",
      "Loss of train set: 0.5115423202514648 at epoch: 19 and batch_num: 15\n",
      "Loss of train set: 0.5230202674865723 at epoch: 19 and batch_num: 16\n",
      "Loss of train set: 0.3561541438102722 at epoch: 19 and batch_num: 17\n",
      "Loss of train set: 0.23488366603851318 at epoch: 19 and batch_num: 18\n",
      "Loss of train set: 0.4442347586154938 at epoch: 19 and batch_num: 19\n",
      "Loss of train set: 0.38420432806015015 at epoch: 19 and batch_num: 20\n",
      "Loss of train set: 0.3560701012611389 at epoch: 19 and batch_num: 21\n",
      "Loss of train set: 0.20221194624900818 at epoch: 19 and batch_num: 22\n",
      "Loss of train set: 0.37493401765823364 at epoch: 19 and batch_num: 23\n",
      "Loss of train set: 0.14852333068847656 at epoch: 19 and batch_num: 24\n",
      "Loss of train set: 0.510141909122467 at epoch: 19 and batch_num: 25\n",
      "Loss of train set: 0.21819573640823364 at epoch: 19 and batch_num: 26\n",
      "Loss of train set: 0.3052646219730377 at epoch: 19 and batch_num: 27\n",
      "Loss of train set: 0.23069147765636444 at epoch: 19 and batch_num: 28\n",
      "Loss of train set: 0.24351716041564941 at epoch: 19 and batch_num: 29\n",
      "Loss of train set: 0.22822389006614685 at epoch: 19 and batch_num: 30\n",
      "Loss of train set: 0.3173261880874634 at epoch: 19 and batch_num: 31\n",
      "Loss of train set: 0.266985148191452 at epoch: 19 and batch_num: 32\n",
      "Loss of train set: 0.31570351123809814 at epoch: 19 and batch_num: 33\n",
      "Loss of train set: 0.23969948291778564 at epoch: 19 and batch_num: 34\n",
      "Loss of train set: 0.1739761382341385 at epoch: 19 and batch_num: 35\n",
      "Loss of train set: 0.3543050289154053 at epoch: 19 and batch_num: 36\n",
      "Loss of train set: 0.3094763159751892 at epoch: 19 and batch_num: 37\n",
      "Loss of train set: 0.2601563036441803 at epoch: 19 and batch_num: 38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.15976011753082275 at epoch: 19 and batch_num: 39\n",
      "Loss of train set: 0.33772116899490356 at epoch: 19 and batch_num: 40\n",
      "Loss of train set: 0.27312278747558594 at epoch: 19 and batch_num: 41\n",
      "Loss of train set: 0.33155471086502075 at epoch: 19 and batch_num: 42\n",
      "Loss of train set: 0.4182119071483612 at epoch: 19 and batch_num: 43\n",
      "Loss of train set: 0.10149868577718735 at epoch: 19 and batch_num: 44\n",
      "Loss of train set: 0.17215348780155182 at epoch: 19 and batch_num: 45\n",
      "Loss of train set: 0.2731400728225708 at epoch: 19 and batch_num: 46\n",
      "Loss of train set: 0.3327100872993469 at epoch: 19 and batch_num: 47\n",
      "Loss of train set: 0.3143811821937561 at epoch: 19 and batch_num: 48\n",
      "Loss of train set: 0.28910940885543823 at epoch: 19 and batch_num: 49\n",
      "Loss of train set: 0.5025680661201477 at epoch: 19 and batch_num: 50\n",
      "Loss of train set: 0.32145828008651733 at epoch: 19 and batch_num: 51\n",
      "Loss of train set: 0.2707812786102295 at epoch: 19 and batch_num: 52\n",
      "Loss of train set: 0.21446317434310913 at epoch: 19 and batch_num: 53\n",
      "Loss of train set: 0.2194870561361313 at epoch: 19 and batch_num: 54\n",
      "Loss of train set: 0.27667874097824097 at epoch: 19 and batch_num: 55\n",
      "Loss of train set: 0.20590832829475403 at epoch: 19 and batch_num: 56\n",
      "Loss of train set: 0.19614514708518982 at epoch: 19 and batch_num: 57\n",
      "Loss of train set: 0.25123655796051025 at epoch: 19 and batch_num: 58\n",
      "Loss of train set: 0.2514815628528595 at epoch: 19 and batch_num: 59\n",
      "Loss of train set: 0.2761767506599426 at epoch: 19 and batch_num: 60\n",
      "Loss of train set: 0.27627629041671753 at epoch: 19 and batch_num: 61\n",
      "Loss of train set: 0.16629105806350708 at epoch: 19 and batch_num: 62\n",
      "Loss of train set: 0.2584226429462433 at epoch: 19 and batch_num: 63\n",
      "Loss of train set: 0.36336976289749146 at epoch: 19 and batch_num: 64\n",
      "Loss of train set: 0.1739429086446762 at epoch: 19 and batch_num: 65\n",
      "Loss of train set: 0.2953241169452667 at epoch: 19 and batch_num: 66\n",
      "Loss of train set: 0.2371448278427124 at epoch: 19 and batch_num: 67\n",
      "Loss of train set: 0.20374000072479248 at epoch: 19 and batch_num: 68\n",
      "Loss of train set: 0.22503523528575897 at epoch: 19 and batch_num: 69\n",
      "Loss of train set: 0.29307055473327637 at epoch: 19 and batch_num: 70\n",
      "Loss of train set: 0.30891287326812744 at epoch: 19 and batch_num: 71\n",
      "Loss of train set: 0.1939484179019928 at epoch: 19 and batch_num: 72\n",
      "Loss of train set: 0.20165672898292542 at epoch: 19 and batch_num: 73\n",
      "Loss of train set: 0.24841970205307007 at epoch: 19 and batch_num: 74\n",
      "Loss of train set: 0.25520920753479004 at epoch: 19 and batch_num: 75\n",
      "Loss of train set: 0.4332439601421356 at epoch: 19 and batch_num: 76\n",
      "Loss of train set: 0.398336797952652 at epoch: 19 and batch_num: 77\n",
      "Loss of train set: 0.3147323429584503 at epoch: 19 and batch_num: 78\n",
      "Loss of train set: 0.28161656856536865 at epoch: 19 and batch_num: 79\n",
      "Loss of train set: 0.32797980308532715 at epoch: 19 and batch_num: 80\n",
      "Loss of train set: 0.4684782326221466 at epoch: 19 and batch_num: 81\n",
      "Loss of train set: 0.25404417514801025 at epoch: 19 and batch_num: 82\n",
      "Loss of train set: 0.18550491333007812 at epoch: 19 and batch_num: 83\n",
      "Loss of train set: 0.1825437992811203 at epoch: 19 and batch_num: 84\n",
      "Loss of train set: 0.2433026283979416 at epoch: 19 and batch_num: 85\n",
      "Loss of train set: 0.3010680079460144 at epoch: 19 and batch_num: 86\n",
      "Loss of train set: 0.2671279311180115 at epoch: 19 and batch_num: 87\n",
      "Loss of train set: 0.45156216621398926 at epoch: 19 and batch_num: 88\n",
      "Loss of train set: 0.3080023229122162 at epoch: 19 and batch_num: 89\n",
      "Loss of train set: 0.1958833783864975 at epoch: 19 and batch_num: 90\n",
      "Loss of train set: 0.362040638923645 at epoch: 19 and batch_num: 91\n",
      "Loss of train set: 0.2503757178783417 at epoch: 19 and batch_num: 92\n",
      "Loss of train set: 0.1982923150062561 at epoch: 19 and batch_num: 93\n",
      "Loss of train set: 0.25575488805770874 at epoch: 19 and batch_num: 94\n",
      "Loss of train set: 0.3860265016555786 at epoch: 19 and batch_num: 95\n",
      "Loss of train set: 0.19546929001808167 at epoch: 19 and batch_num: 96\n",
      "Loss of train set: 0.3552044928073883 at epoch: 19 and batch_num: 97\n",
      "Loss of train set: 0.3434421420097351 at epoch: 19 and batch_num: 98\n",
      "Loss of train set: 0.26950496435165405 at epoch: 19 and batch_num: 99\n",
      "Loss of train set: 0.3704869747161865 at epoch: 19 and batch_num: 100\n",
      "Loss of train set: 0.23928692936897278 at epoch: 19 and batch_num: 101\n",
      "Loss of train set: 0.25987499952316284 at epoch: 19 and batch_num: 102\n",
      "Loss of train set: 0.2662566304206848 at epoch: 19 and batch_num: 103\n",
      "Loss of train set: 0.2780376076698303 at epoch: 19 and batch_num: 104\n",
      "Loss of train set: 0.19861024618148804 at epoch: 19 and batch_num: 105\n",
      "Loss of train set: 0.29859459400177 at epoch: 19 and batch_num: 106\n",
      "Loss of train set: 0.17481765151023865 at epoch: 19 and batch_num: 107\n",
      "Loss of train set: 0.31814295053482056 at epoch: 19 and batch_num: 108\n",
      "Loss of train set: 0.2759527564048767 at epoch: 19 and batch_num: 109\n",
      "Loss of train set: 0.3105013370513916 at epoch: 19 and batch_num: 110\n",
      "Loss of train set: 0.47609949111938477 at epoch: 19 and batch_num: 111\n",
      "Loss of train set: 0.4951019287109375 at epoch: 19 and batch_num: 112\n",
      "Loss of train set: 0.3771092891693115 at epoch: 19 and batch_num: 113\n",
      "Loss of train set: 0.17380070686340332 at epoch: 19 and batch_num: 114\n",
      "Loss of train set: 0.2013855129480362 at epoch: 19 and batch_num: 115\n",
      "Loss of train set: 0.16530375182628632 at epoch: 19 and batch_num: 116\n",
      "Loss of train set: 0.25690460205078125 at epoch: 19 and batch_num: 117\n",
      "Loss of train set: 0.40255409479141235 at epoch: 19 and batch_num: 118\n",
      "Loss of train set: 0.20473001897335052 at epoch: 19 and batch_num: 119\n",
      "Loss of train set: 0.3950275778770447 at epoch: 19 and batch_num: 120\n",
      "Loss of train set: 0.14081105589866638 at epoch: 19 and batch_num: 121\n",
      "Loss of train set: 0.18410027027130127 at epoch: 19 and batch_num: 122\n",
      "Loss of train set: 0.24849148094654083 at epoch: 19 and batch_num: 123\n",
      "Loss of train set: 0.3528571128845215 at epoch: 19 and batch_num: 124\n",
      "Loss of train set: 0.2877015471458435 at epoch: 19 and batch_num: 125\n",
      "Loss of train set: 0.18158867955207825 at epoch: 19 and batch_num: 126\n",
      "Loss of train set: 0.21417994797229767 at epoch: 19 and batch_num: 127\n",
      "Loss of train set: 0.27573102712631226 at epoch: 19 and batch_num: 128\n",
      "Loss of train set: 0.27830180525779724 at epoch: 19 and batch_num: 129\n",
      "Loss of train set: 0.22846099734306335 at epoch: 19 and batch_num: 130\n",
      "Loss of train set: 0.29351723194122314 at epoch: 19 and batch_num: 131\n",
      "Loss of train set: 0.3796568512916565 at epoch: 19 and batch_num: 132\n",
      "Loss of train set: 0.3163244128227234 at epoch: 19 and batch_num: 133\n",
      "Loss of train set: 0.3410762548446655 at epoch: 19 and batch_num: 134\n",
      "Loss of train set: 0.4090275764465332 at epoch: 19 and batch_num: 135\n",
      "Loss of train set: 0.255251944065094 at epoch: 19 and batch_num: 136\n",
      "Loss of train set: 0.20145317912101746 at epoch: 19 and batch_num: 137\n",
      "Loss of train set: 0.42544764280319214 at epoch: 19 and batch_num: 138\n",
      "Loss of train set: 0.2212122082710266 at epoch: 19 and batch_num: 139\n",
      "Loss of train set: 0.3893284499645233 at epoch: 19 and batch_num: 140\n",
      "Loss of train set: 0.26441943645477295 at epoch: 19 and batch_num: 141\n",
      "Loss of train set: 0.43341049551963806 at epoch: 19 and batch_num: 142\n",
      "Loss of train set: 0.3934982717037201 at epoch: 19 and batch_num: 143\n",
      "Loss of train set: 0.23149627447128296 at epoch: 19 and batch_num: 144\n",
      "Loss of train set: 0.41115278005599976 at epoch: 19 and batch_num: 145\n",
      "Loss of train set: 0.32265686988830566 at epoch: 19 and batch_num: 146\n",
      "Loss of train set: 0.35576632618904114 at epoch: 19 and batch_num: 147\n",
      "Loss of train set: 0.15136834979057312 at epoch: 19 and batch_num: 148\n",
      "Loss of train set: 0.287089467048645 at epoch: 19 and batch_num: 149\n",
      "Loss of train set: 0.4260079860687256 at epoch: 19 and batch_num: 150\n",
      "Loss of train set: 0.3166085183620453 at epoch: 19 and batch_num: 151\n",
      "Loss of train set: 0.162409245967865 at epoch: 19 and batch_num: 152\n",
      "Loss of train set: 0.4667012095451355 at epoch: 19 and batch_num: 153\n",
      "Loss of train set: 0.16088497638702393 at epoch: 19 and batch_num: 154\n",
      "Loss of train set: 0.2633584141731262 at epoch: 19 and batch_num: 155\n",
      "Loss of train set: 0.26234379410743713 at epoch: 19 and batch_num: 156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.23378555476665497 at epoch: 19 and batch_num: 157\n",
      "Loss of train set: 0.18905198574066162 at epoch: 19 and batch_num: 158\n",
      "Loss of train set: 0.483593612909317 at epoch: 19 and batch_num: 159\n",
      "Loss of train set: 0.33440184593200684 at epoch: 19 and batch_num: 160\n",
      "Loss of train set: 0.44039517641067505 at epoch: 19 and batch_num: 161\n",
      "Loss of train set: 0.4491845369338989 at epoch: 19 and batch_num: 162\n",
      "Loss of train set: 0.3113986849784851 at epoch: 19 and batch_num: 163\n",
      "Loss of train set: 0.35920238494873047 at epoch: 19 and batch_num: 164\n",
      "Loss of train set: 0.27479180693626404 at epoch: 19 and batch_num: 165\n",
      "Loss of train set: 0.29194480180740356 at epoch: 19 and batch_num: 166\n",
      "Loss of train set: 0.24608571827411652 at epoch: 19 and batch_num: 167\n",
      "Loss of train set: 0.26816850900650024 at epoch: 19 and batch_num: 168\n",
      "Loss of train set: 0.48867136240005493 at epoch: 19 and batch_num: 169\n",
      "Loss of train set: 0.18473218381404877 at epoch: 19 and batch_num: 170\n",
      "Loss of train set: 0.18824505805969238 at epoch: 19 and batch_num: 171\n",
      "Loss of train set: 0.32689860463142395 at epoch: 19 and batch_num: 172\n",
      "Loss of train set: 0.3311958611011505 at epoch: 19 and batch_num: 173\n",
      "Loss of train set: 0.2315724492073059 at epoch: 19 and batch_num: 174\n",
      "Loss of train set: 0.2822364568710327 at epoch: 19 and batch_num: 175\n",
      "Loss of train set: 0.28701189160346985 at epoch: 19 and batch_num: 176\n",
      "Loss of train set: 0.2733943462371826 at epoch: 19 and batch_num: 177\n",
      "Loss of train set: 0.13877172768115997 at epoch: 19 and batch_num: 178\n",
      "Loss of train set: 0.2066599726676941 at epoch: 19 and batch_num: 179\n",
      "Loss of train set: 0.43684300780296326 at epoch: 19 and batch_num: 180\n",
      "Loss of train set: 0.19673976302146912 at epoch: 19 and batch_num: 181\n",
      "Loss of train set: 0.24671080708503723 at epoch: 19 and batch_num: 182\n",
      "Loss of train set: 0.3296043276786804 at epoch: 19 and batch_num: 183\n",
      "Loss of train set: 0.25269532203674316 at epoch: 19 and batch_num: 184\n",
      "Loss of train set: 0.23072096705436707 at epoch: 19 and batch_num: 185\n",
      "Loss of train set: 0.23848161101341248 at epoch: 19 and batch_num: 186\n",
      "Loss of train set: 0.5762325525283813 at epoch: 19 and batch_num: 187\n",
      "Loss of train set: 0.34775662422180176 at epoch: 19 and batch_num: 188\n",
      "Loss of train set: 0.4508766829967499 at epoch: 19 and batch_num: 189\n",
      "Loss of train set: 0.27111780643463135 at epoch: 19 and batch_num: 190\n",
      "Loss of train set: 0.28569045662879944 at epoch: 19 and batch_num: 191\n",
      "Loss of train set: 0.3038909137248993 at epoch: 19 and batch_num: 192\n",
      "Loss of train set: 0.7027767896652222 at epoch: 19 and batch_num: 193\n",
      "Loss of train set: 0.20063310861587524 at epoch: 19 and batch_num: 194\n",
      "Loss of train set: 0.19694098830223083 at epoch: 19 and batch_num: 195\n",
      "Loss of train set: 0.388588011264801 at epoch: 19 and batch_num: 196\n",
      "Loss of train set: 0.3673371970653534 at epoch: 19 and batch_num: 197\n",
      "Loss of train set: 0.38427385687828064 at epoch: 19 and batch_num: 198\n",
      "Loss of train set: 0.16004735231399536 at epoch: 19 and batch_num: 199\n",
      "Loss of train set: 0.3774909973144531 at epoch: 19 and batch_num: 200\n",
      "Loss of train set: 0.3582403063774109 at epoch: 19 and batch_num: 201\n",
      "Loss of train set: 0.21038243174552917 at epoch: 19 and batch_num: 202\n",
      "Loss of train set: 0.2682768702507019 at epoch: 19 and batch_num: 203\n",
      "Loss of train set: 0.32337766885757446 at epoch: 19 and batch_num: 204\n",
      "Loss of train set: 0.2643364369869232 at epoch: 19 and batch_num: 205\n",
      "Loss of train set: 0.1560373306274414 at epoch: 19 and batch_num: 206\n",
      "Loss of train set: 0.3861914873123169 at epoch: 19 and batch_num: 207\n",
      "Loss of train set: 0.2586842179298401 at epoch: 19 and batch_num: 208\n",
      "Loss of train set: 0.2528891861438751 at epoch: 19 and batch_num: 209\n",
      "Loss of train set: 0.3231252431869507 at epoch: 19 and batch_num: 210\n",
      "Loss of train set: 0.1936657875776291 at epoch: 19 and batch_num: 211\n",
      "Loss of train set: 0.23277026414871216 at epoch: 19 and batch_num: 212\n",
      "Loss of train set: 0.38742944598197937 at epoch: 19 and batch_num: 213\n",
      "Loss of train set: 0.2670556306838989 at epoch: 19 and batch_num: 214\n",
      "Loss of train set: 0.23445983231067657 at epoch: 19 and batch_num: 215\n",
      "Loss of train set: 0.10894855856895447 at epoch: 19 and batch_num: 216\n",
      "Loss of train set: 0.3483823239803314 at epoch: 19 and batch_num: 217\n",
      "Loss of train set: 0.2888953685760498 at epoch: 19 and batch_num: 218\n",
      "Loss of train set: 0.23574432730674744 at epoch: 19 and batch_num: 219\n",
      "Loss of train set: 0.3171747624874115 at epoch: 19 and batch_num: 220\n",
      "Loss of train set: 0.28745555877685547 at epoch: 19 and batch_num: 221\n",
      "Loss of train set: 0.36993178725242615 at epoch: 19 and batch_num: 222\n",
      "Loss of train set: 0.2819986343383789 at epoch: 19 and batch_num: 223\n",
      "Loss of train set: 0.3989848494529724 at epoch: 19 and batch_num: 224\n",
      "Loss of train set: 0.4307752549648285 at epoch: 19 and batch_num: 225\n",
      "Loss of train set: 0.20946823060512543 at epoch: 19 and batch_num: 226\n",
      "Loss of train set: 0.12308001518249512 at epoch: 19 and batch_num: 227\n",
      "Loss of train set: 0.24554118514060974 at epoch: 19 and batch_num: 228\n",
      "Loss of train set: 0.2904861271381378 at epoch: 19 and batch_num: 229\n",
      "Loss of train set: 0.3391109108924866 at epoch: 19 and batch_num: 230\n",
      "Loss of train set: 0.3166343569755554 at epoch: 19 and batch_num: 231\n",
      "Loss of train set: 0.24567696452140808 at epoch: 19 and batch_num: 232\n",
      "Loss of train set: 0.19701392948627472 at epoch: 19 and batch_num: 233\n",
      "Loss of train set: 0.16230346262454987 at epoch: 19 and batch_num: 234\n",
      "Loss of train set: 0.2434166818857193 at epoch: 19 and batch_num: 235\n",
      "Loss of train set: 0.1729387640953064 at epoch: 19 and batch_num: 236\n",
      "Loss of train set: 0.14984263479709625 at epoch: 19 and batch_num: 237\n",
      "Loss of train set: 0.21278153359889984 at epoch: 19 and batch_num: 238\n",
      "Loss of train set: 0.21783429384231567 at epoch: 19 and batch_num: 239\n",
      "Loss of train set: 0.4471152722835541 at epoch: 19 and batch_num: 240\n",
      "Loss of train set: 0.4014210104942322 at epoch: 19 and batch_num: 241\n",
      "Loss of train set: 0.17641496658325195 at epoch: 19 and batch_num: 242\n",
      "Loss of train set: 0.21535521745681763 at epoch: 19 and batch_num: 243\n",
      "Loss of train set: 0.3463437855243683 at epoch: 19 and batch_num: 244\n",
      "Loss of train set: 0.26959165930747986 at epoch: 19 and batch_num: 245\n",
      "Loss of train set: 0.4113822281360626 at epoch: 19 and batch_num: 246\n",
      "Loss of train set: 0.24886657297611237 at epoch: 19 and batch_num: 247\n",
      "Loss of train set: 0.2450886070728302 at epoch: 19 and batch_num: 248\n",
      "Loss of train set: 0.12426295131444931 at epoch: 19 and batch_num: 249\n",
      "Loss of train set: 0.1605522334575653 at epoch: 19 and batch_num: 250\n",
      "Loss of train set: 0.2559182643890381 at epoch: 19 and batch_num: 251\n",
      "Loss of train set: 0.2732498049736023 at epoch: 19 and batch_num: 252\n",
      "Loss of train set: 0.2077002376317978 at epoch: 19 and batch_num: 253\n",
      "Loss of train set: 0.3230692446231842 at epoch: 19 and batch_num: 254\n",
      "Loss of train set: 0.35795092582702637 at epoch: 19 and batch_num: 255\n",
      "Loss of train set: 0.2815410792827606 at epoch: 19 and batch_num: 256\n",
      "Loss of train set: 0.3373039662837982 at epoch: 19 and batch_num: 257\n",
      "Loss of train set: 0.23443298041820526 at epoch: 19 and batch_num: 258\n",
      "Loss of train set: 0.251964807510376 at epoch: 19 and batch_num: 259\n",
      "Loss of train set: 0.26924702525138855 at epoch: 19 and batch_num: 260\n",
      "Loss of train set: 0.27137014269828796 at epoch: 19 and batch_num: 261\n",
      "Loss of train set: 0.3203141987323761 at epoch: 19 and batch_num: 262\n",
      "Loss of train set: 0.3781968355178833 at epoch: 19 and batch_num: 263\n",
      "Loss of train set: 0.2914501428604126 at epoch: 19 and batch_num: 264\n",
      "Loss of train set: 0.23033973574638367 at epoch: 19 and batch_num: 265\n",
      "Loss of train set: 0.316270112991333 at epoch: 19 and batch_num: 266\n",
      "Loss of train set: 0.2938676178455353 at epoch: 19 and batch_num: 267\n",
      "Loss of train set: 0.3377680778503418 at epoch: 19 and batch_num: 268\n",
      "Loss of train set: 0.17596934735774994 at epoch: 19 and batch_num: 269\n",
      "Loss of train set: 0.2567146420478821 at epoch: 19 and batch_num: 270\n",
      "Loss of train set: 0.422993004322052 at epoch: 19 and batch_num: 271\n",
      "Loss of train set: 0.3556101322174072 at epoch: 19 and batch_num: 272\n",
      "Loss of train set: 0.44194233417510986 at epoch: 19 and batch_num: 273\n",
      "Loss of train set: 0.3686505854129791 at epoch: 19 and batch_num: 274\n",
      "Loss of train set: 0.2570890188217163 at epoch: 19 and batch_num: 275\n",
      "Loss of train set: 0.3707953691482544 at epoch: 19 and batch_num: 276\n",
      "Loss of train set: 0.26249706745147705 at epoch: 19 and batch_num: 277\n",
      "Loss of train set: 0.2920771539211273 at epoch: 19 and batch_num: 278\n",
      "Loss of train set: 0.38257092237472534 at epoch: 19 and batch_num: 279\n",
      "Loss of train set: 0.2559679448604584 at epoch: 19 and batch_num: 280\n",
      "Loss of train set: 0.38065654039382935 at epoch: 19 and batch_num: 281\n",
      "Loss of train set: 0.4175737500190735 at epoch: 19 and batch_num: 282\n",
      "Loss of train set: 0.3388863801956177 at epoch: 19 and batch_num: 283\n",
      "Loss of train set: 0.21540464460849762 at epoch: 19 and batch_num: 284\n",
      "Loss of train set: 0.3475337326526642 at epoch: 19 and batch_num: 285\n",
      "Loss of train set: 0.18747355043888092 at epoch: 19 and batch_num: 286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3463725447654724 at epoch: 19 and batch_num: 287\n",
      "Loss of train set: 0.25999245047569275 at epoch: 19 and batch_num: 288\n",
      "Loss of train set: 0.2184315025806427 at epoch: 19 and batch_num: 289\n",
      "Loss of train set: 0.29648852348327637 at epoch: 19 and batch_num: 290\n",
      "Loss of train set: 0.21360942721366882 at epoch: 19 and batch_num: 291\n",
      "Loss of train set: 0.4156636893749237 at epoch: 19 and batch_num: 292\n",
      "Loss of train set: 0.19669479131698608 at epoch: 19 and batch_num: 293\n",
      "Loss of train set: 0.20345914363861084 at epoch: 19 and batch_num: 294\n",
      "Loss of train set: 0.27418094873428345 at epoch: 19 and batch_num: 295\n",
      "Loss of train set: 0.3428466320037842 at epoch: 19 and batch_num: 296\n",
      "Loss of train set: 0.21907830238342285 at epoch: 19 and batch_num: 297\n",
      "Loss of train set: 0.2810795307159424 at epoch: 19 and batch_num: 298\n",
      "Loss of train set: 0.28975749015808105 at epoch: 19 and batch_num: 299\n",
      "Loss of train set: 0.345725417137146 at epoch: 19 and batch_num: 300\n",
      "Loss of train set: 0.46611136198043823 at epoch: 19 and batch_num: 301\n",
      "Loss of train set: 0.28370994329452515 at epoch: 19 and batch_num: 302\n",
      "Loss of train set: 0.2291494607925415 at epoch: 19 and batch_num: 303\n",
      "Loss of train set: 0.23377791047096252 at epoch: 19 and batch_num: 304\n",
      "Loss of train set: 0.2961910367012024 at epoch: 19 and batch_num: 305\n",
      "Loss of train set: 0.2584465742111206 at epoch: 19 and batch_num: 306\n",
      "Loss of train set: 0.3005329668521881 at epoch: 19 and batch_num: 307\n",
      "Loss of train set: 0.2071351408958435 at epoch: 19 and batch_num: 308\n",
      "Loss of train set: 0.30970197916030884 at epoch: 19 and batch_num: 309\n",
      "Loss of train set: 0.3114090859889984 at epoch: 19 and batch_num: 310\n",
      "Loss of train set: 0.2341035157442093 at epoch: 19 and batch_num: 311\n",
      "Loss of train set: 0.2675796151161194 at epoch: 19 and batch_num: 312\n",
      "Loss of train set: 0.3476010262966156 at epoch: 19 and batch_num: 313\n",
      "Loss of train set: 0.2262830287218094 at epoch: 19 and batch_num: 314\n",
      "Loss of train set: 0.42491066455841064 at epoch: 19 and batch_num: 315\n",
      "Loss of train set: 0.2339174896478653 at epoch: 19 and batch_num: 316\n",
      "Loss of train set: 0.3071446120738983 at epoch: 19 and batch_num: 317\n",
      "Loss of train set: 0.23440667986869812 at epoch: 19 and batch_num: 318\n",
      "Loss of train set: 0.1761176735162735 at epoch: 19 and batch_num: 319\n",
      "Loss of train set: 0.44953179359436035 at epoch: 19 and batch_num: 320\n",
      "Loss of train set: 0.49225935339927673 at epoch: 19 and batch_num: 321\n",
      "Loss of train set: 0.2529776990413666 at epoch: 19 and batch_num: 322\n",
      "Loss of train set: 0.328889399766922 at epoch: 19 and batch_num: 323\n",
      "Loss of train set: 0.4112014174461365 at epoch: 19 and batch_num: 324\n",
      "Loss of train set: 0.28222769498825073 at epoch: 19 and batch_num: 325\n",
      "Loss of train set: 0.3994118571281433 at epoch: 19 and batch_num: 326\n",
      "Loss of train set: 0.24854058027267456 at epoch: 19 and batch_num: 327\n",
      "Loss of train set: 0.26873600482940674 at epoch: 19 and batch_num: 328\n",
      "Loss of train set: 0.21833859384059906 at epoch: 19 and batch_num: 329\n",
      "Loss of train set: 0.2775036096572876 at epoch: 19 and batch_num: 330\n",
      "Loss of train set: 0.34084898233413696 at epoch: 19 and batch_num: 331\n",
      "Loss of train set: 0.31177353858947754 at epoch: 19 and batch_num: 332\n",
      "Loss of train set: 0.3988926112651825 at epoch: 19 and batch_num: 333\n",
      "Loss of train set: 0.3084002137184143 at epoch: 19 and batch_num: 334\n",
      "Loss of train set: 0.13703085482120514 at epoch: 19 and batch_num: 335\n",
      "Loss of train set: 0.3477264642715454 at epoch: 19 and batch_num: 336\n",
      "Loss of train set: 0.23830296099185944 at epoch: 19 and batch_num: 337\n",
      "Loss of train set: 0.4171487092971802 at epoch: 19 and batch_num: 338\n",
      "Loss of train set: 0.31445634365081787 at epoch: 19 and batch_num: 339\n",
      "Loss of train set: 0.31982025504112244 at epoch: 19 and batch_num: 340\n",
      "Loss of train set: 0.29262542724609375 at epoch: 19 and batch_num: 341\n",
      "Loss of train set: 0.22682785987854004 at epoch: 19 and batch_num: 342\n",
      "Loss of train set: 0.152370885014534 at epoch: 19 and batch_num: 343\n",
      "Loss of train set: 0.35814547538757324 at epoch: 19 and batch_num: 344\n",
      "Loss of train set: 0.23637279868125916 at epoch: 19 and batch_num: 345\n",
      "Loss of train set: 0.3458693027496338 at epoch: 19 and batch_num: 346\n",
      "Loss of train set: 0.26723259687423706 at epoch: 19 and batch_num: 347\n",
      "Loss of train set: 0.4072261154651642 at epoch: 19 and batch_num: 348\n",
      "Loss of train set: 0.2340010702610016 at epoch: 19 and batch_num: 349\n",
      "Loss of train set: 0.18978112936019897 at epoch: 19 and batch_num: 350\n",
      "Loss of train set: 0.1770162284374237 at epoch: 19 and batch_num: 351\n",
      "Loss of train set: 0.13212041556835175 at epoch: 19 and batch_num: 352\n",
      "Loss of train set: 0.19888627529144287 at epoch: 19 and batch_num: 353\n",
      "Loss of train set: 0.2628359794616699 at epoch: 19 and batch_num: 354\n",
      "Loss of train set: 0.24233809113502502 at epoch: 19 and batch_num: 355\n",
      "Loss of train set: 0.36317193508148193 at epoch: 19 and batch_num: 356\n",
      "Loss of train set: 0.4382392168045044 at epoch: 19 and batch_num: 357\n",
      "Loss of train set: 0.3435676693916321 at epoch: 19 and batch_num: 358\n",
      "Loss of train set: 0.4047071039676666 at epoch: 19 and batch_num: 359\n",
      "Loss of train set: 0.17315781116485596 at epoch: 19 and batch_num: 360\n",
      "Loss of train set: 0.2691720724105835 at epoch: 19 and batch_num: 361\n",
      "Loss of train set: 0.45615577697753906 at epoch: 19 and batch_num: 362\n",
      "Loss of train set: 0.1354348361492157 at epoch: 19 and batch_num: 363\n",
      "Loss of train set: 0.22760270535945892 at epoch: 19 and batch_num: 364\n",
      "Loss of train set: 0.27557486295700073 at epoch: 19 and batch_num: 365\n",
      "Loss of train set: 0.20854389667510986 at epoch: 19 and batch_num: 366\n",
      "Loss of train set: 0.3368076682090759 at epoch: 19 and batch_num: 367\n",
      "Loss of train set: 0.21448040008544922 at epoch: 19 and batch_num: 368\n",
      "Loss of train set: 0.19379577040672302 at epoch: 19 and batch_num: 369\n",
      "Loss of train set: 0.30525147914886475 at epoch: 19 and batch_num: 370\n",
      "Loss of train set: 0.16903333365917206 at epoch: 19 and batch_num: 371\n",
      "Loss of train set: 0.4609215557575226 at epoch: 19 and batch_num: 372\n",
      "Loss of train set: 0.2956467270851135 at epoch: 19 and batch_num: 373\n",
      "Loss of train set: 0.23033252358436584 at epoch: 19 and batch_num: 374\n",
      "Loss of train set: 0.37355369329452515 at epoch: 19 and batch_num: 375\n",
      "Loss of train set: 0.21017953753471375 at epoch: 19 and batch_num: 376\n",
      "Loss of train set: 0.2874608039855957 at epoch: 19 and batch_num: 377\n",
      "Loss of train set: 0.39719733595848083 at epoch: 19 and batch_num: 378\n",
      "Loss of train set: 0.2746223211288452 at epoch: 19 and batch_num: 379\n",
      "Loss of train set: 0.26913487911224365 at epoch: 19 and batch_num: 380\n",
      "Loss of train set: 0.25758254528045654 at epoch: 19 and batch_num: 381\n",
      "Loss of train set: 0.27566200494766235 at epoch: 19 and batch_num: 382\n",
      "Loss of train set: 0.2697915732860565 at epoch: 19 and batch_num: 383\n",
      "Loss of train set: 0.2534400522708893 at epoch: 19 and batch_num: 384\n",
      "Loss of train set: 0.5861347913742065 at epoch: 19 and batch_num: 385\n",
      "Loss of train set: 0.24316908419132233 at epoch: 19 and batch_num: 386\n",
      "Loss of train set: 0.3252805769443512 at epoch: 19 and batch_num: 387\n",
      "Loss of train set: 0.3105695843696594 at epoch: 19 and batch_num: 388\n",
      "Loss of train set: 0.3322898745536804 at epoch: 19 and batch_num: 389\n",
      "Loss of train set: 0.25365957617759705 at epoch: 19 and batch_num: 390\n",
      "Loss of train set: 0.22598642110824585 at epoch: 19 and batch_num: 391\n",
      "Loss of train set: 0.19321471452713013 at epoch: 19 and batch_num: 392\n",
      "Loss of train set: 0.25676798820495605 at epoch: 19 and batch_num: 393\n",
      "Loss of train set: 0.2976575195789337 at epoch: 19 and batch_num: 394\n",
      "Loss of train set: 0.3521769046783447 at epoch: 19 and batch_num: 395\n",
      "Loss of train set: 0.28921639919281006 at epoch: 19 and batch_num: 396\n",
      "Loss of train set: 0.39427649974823 at epoch: 19 and batch_num: 397\n",
      "Loss of train set: 0.23537394404411316 at epoch: 19 and batch_num: 398\n",
      "Loss of train set: 0.3418195843696594 at epoch: 19 and batch_num: 399\n",
      "Loss of train set: 0.2527712285518646 at epoch: 19 and batch_num: 400\n",
      "Loss of train set: 0.20209136605262756 at epoch: 19 and batch_num: 401\n",
      "Loss of train set: 0.2990148067474365 at epoch: 19 and batch_num: 402\n",
      "Loss of train set: 0.30096280574798584 at epoch: 19 and batch_num: 403\n",
      "Loss of train set: 0.2963730990886688 at epoch: 19 and batch_num: 404\n",
      "Loss of train set: 0.25153353810310364 at epoch: 19 and batch_num: 405\n",
      "Loss of train set: 0.30706435441970825 at epoch: 19 and batch_num: 406\n",
      "Loss of train set: 0.2100825309753418 at epoch: 19 and batch_num: 407\n",
      "Loss of train set: 0.2780216336250305 at epoch: 19 and batch_num: 408\n",
      "Loss of train set: 0.27347123622894287 at epoch: 19 and batch_num: 409\n",
      "Loss of train set: 0.3925962746143341 at epoch: 19 and batch_num: 410\n",
      "Loss of train set: 0.33660173416137695 at epoch: 19 and batch_num: 411\n",
      "Loss of train set: 0.35711047053337097 at epoch: 19 and batch_num: 412\n",
      "Loss of train set: 0.3073485493659973 at epoch: 19 and batch_num: 413\n",
      "Loss of train set: 0.1897243857383728 at epoch: 19 and batch_num: 414\n",
      "Loss of train set: 0.12873703241348267 at epoch: 19 and batch_num: 415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.27208369970321655 at epoch: 19 and batch_num: 416\n",
      "Loss of train set: 0.2783541679382324 at epoch: 19 and batch_num: 417\n",
      "Loss of train set: 0.24717502295970917 at epoch: 19 and batch_num: 418\n",
      "Loss of train set: 0.2899647653102875 at epoch: 19 and batch_num: 419\n",
      "Loss of train set: 0.26976585388183594 at epoch: 19 and batch_num: 420\n",
      "Loss of train set: 0.32226356863975525 at epoch: 19 and batch_num: 421\n",
      "Loss of train set: 0.22597169876098633 at epoch: 19 and batch_num: 422\n",
      "Loss of train set: 0.2782785892486572 at epoch: 19 and batch_num: 423\n",
      "Loss of train set: 0.29777097702026367 at epoch: 19 and batch_num: 424\n",
      "Loss of train set: 0.28907108306884766 at epoch: 19 and batch_num: 425\n",
      "Loss of train set: 0.24538008868694305 at epoch: 19 and batch_num: 426\n",
      "Loss of train set: 0.30580341815948486 at epoch: 19 and batch_num: 427\n",
      "Loss of train set: 0.08010561019182205 at epoch: 19 and batch_num: 428\n",
      "Loss of train set: 0.36389732360839844 at epoch: 19 and batch_num: 429\n",
      "Loss of train set: 0.26272067427635193 at epoch: 19 and batch_num: 430\n",
      "Loss of train set: 0.18198777735233307 at epoch: 19 and batch_num: 431\n",
      "Loss of train set: 0.3269364535808563 at epoch: 19 and batch_num: 432\n",
      "Loss of train set: 0.26593852043151855 at epoch: 19 and batch_num: 433\n",
      "Loss of train set: 0.21730728447437286 at epoch: 19 and batch_num: 434\n",
      "Loss of train set: 0.1873553842306137 at epoch: 19 and batch_num: 435\n",
      "Loss of train set: 0.24492564797401428 at epoch: 19 and batch_num: 436\n",
      "Loss of train set: 0.27304717898368835 at epoch: 19 and batch_num: 437\n",
      "Loss of train set: 0.27821671962738037 at epoch: 19 and batch_num: 438\n",
      "Loss of train set: 0.2739599943161011 at epoch: 19 and batch_num: 439\n",
      "Loss of train set: 0.16009116172790527 at epoch: 19 and batch_num: 440\n",
      "Loss of train set: 0.2240360677242279 at epoch: 19 and batch_num: 441\n",
      "Loss of train set: 0.17524144053459167 at epoch: 19 and batch_num: 442\n",
      "Loss of train set: 0.25043007731437683 at epoch: 19 and batch_num: 443\n",
      "Loss of train set: 0.32628753781318665 at epoch: 19 and batch_num: 444\n",
      "Loss of train set: 0.1492151916027069 at epoch: 19 and batch_num: 445\n",
      "Loss of train set: 0.3473469913005829 at epoch: 19 and batch_num: 446\n",
      "Loss of train set: 0.2017795592546463 at epoch: 19 and batch_num: 447\n",
      "Loss of train set: 0.4045867919921875 at epoch: 19 and batch_num: 448\n",
      "Loss of train set: 0.21969160437583923 at epoch: 19 and batch_num: 449\n",
      "Loss of train set: 0.33887696266174316 at epoch: 19 and batch_num: 450\n",
      "Loss of train set: 0.3261551856994629 at epoch: 19 and batch_num: 451\n",
      "Loss of train set: 0.16547831892967224 at epoch: 19 and batch_num: 452\n",
      "Loss of train set: 0.24800245463848114 at epoch: 19 and batch_num: 453\n",
      "Loss of train set: 0.2867448627948761 at epoch: 19 and batch_num: 454\n",
      "Loss of train set: 0.29621896147727966 at epoch: 19 and batch_num: 455\n",
      "Loss of train set: 0.43864160776138306 at epoch: 19 and batch_num: 456\n",
      "Loss of train set: 0.2667703926563263 at epoch: 19 and batch_num: 457\n",
      "Loss of train set: 0.321653813123703 at epoch: 19 and batch_num: 458\n",
      "Loss of train set: 0.12868984043598175 at epoch: 19 and batch_num: 459\n",
      "Loss of train set: 0.3637382984161377 at epoch: 19 and batch_num: 460\n",
      "Loss of train set: 0.26591888070106506 at epoch: 19 and batch_num: 461\n",
      "Loss of train set: 0.2651686668395996 at epoch: 19 and batch_num: 462\n",
      "Loss of train set: 0.23416638374328613 at epoch: 19 and batch_num: 463\n",
      "Loss of train set: 0.3087920844554901 at epoch: 19 and batch_num: 464\n",
      "Loss of train set: 0.13710489869117737 at epoch: 19 and batch_num: 465\n",
      "Loss of train set: 0.1590741127729416 at epoch: 19 and batch_num: 466\n",
      "Loss of train set: 0.14024625718593597 at epoch: 19 and batch_num: 467\n",
      "Loss of train set: 0.13590899109840393 at epoch: 19 and batch_num: 468\n",
      "Loss of train set: 0.30915385484695435 at epoch: 19 and batch_num: 469\n",
      "Loss of train set: 0.26167115569114685 at epoch: 19 and batch_num: 470\n",
      "Loss of train set: 0.332090824842453 at epoch: 19 and batch_num: 471\n",
      "Loss of train set: 0.26744601130485535 at epoch: 19 and batch_num: 472\n",
      "Loss of train set: 0.3069131672382355 at epoch: 19 and batch_num: 473\n",
      "Loss of train set: 0.17396509647369385 at epoch: 19 and batch_num: 474\n",
      "Loss of train set: 0.4090093970298767 at epoch: 19 and batch_num: 475\n",
      "Loss of train set: 0.21326175332069397 at epoch: 19 and batch_num: 476\n",
      "Loss of train set: 0.3019084930419922 at epoch: 19 and batch_num: 477\n",
      "Loss of train set: 0.2242334485054016 at epoch: 19 and batch_num: 478\n",
      "Loss of train set: 0.23745644092559814 at epoch: 19 and batch_num: 479\n",
      "Loss of train set: 0.2825409173965454 at epoch: 19 and batch_num: 480\n",
      "Loss of train set: 0.26476988196372986 at epoch: 19 and batch_num: 481\n",
      "Loss of train set: 0.15514203906059265 at epoch: 19 and batch_num: 482\n",
      "Loss of train set: 0.2947354316711426 at epoch: 19 and batch_num: 483\n",
      "Loss of train set: 0.36194175481796265 at epoch: 19 and batch_num: 484\n",
      "Loss of train set: 0.2068856954574585 at epoch: 19 and batch_num: 485\n",
      "Loss of train set: 0.2432316094636917 at epoch: 19 and batch_num: 486\n",
      "Loss of train set: 0.14336061477661133 at epoch: 19 and batch_num: 487\n",
      "Loss of train set: 0.31629297137260437 at epoch: 19 and batch_num: 488\n",
      "Loss of train set: 0.23945415019989014 at epoch: 19 and batch_num: 489\n",
      "Loss of train set: 0.26542508602142334 at epoch: 19 and batch_num: 490\n",
      "Loss of train set: 0.35033881664276123 at epoch: 19 and batch_num: 491\n",
      "Loss of train set: 0.18985125422477722 at epoch: 19 and batch_num: 492\n",
      "Loss of train set: 0.23215416073799133 at epoch: 19 and batch_num: 493\n",
      "Loss of train set: 0.21458745002746582 at epoch: 19 and batch_num: 494\n",
      "Loss of train set: 0.24472767114639282 at epoch: 19 and batch_num: 495\n",
      "Loss of train set: 0.33868467807769775 at epoch: 19 and batch_num: 496\n",
      "Loss of train set: 0.31063657999038696 at epoch: 19 and batch_num: 497\n",
      "Loss of train set: 0.2413732409477234 at epoch: 19 and batch_num: 498\n",
      "Loss of train set: 0.4105837047100067 at epoch: 19 and batch_num: 499\n",
      "Loss of train set: 0.27333399653434753 at epoch: 19 and batch_num: 500\n",
      "Loss of train set: 0.2242046296596527 at epoch: 19 and batch_num: 501\n",
      "Loss of train set: 0.3285427689552307 at epoch: 19 and batch_num: 502\n",
      "Loss of train set: 0.28528594970703125 at epoch: 19 and batch_num: 503\n",
      "Loss of train set: 0.31762897968292236 at epoch: 19 and batch_num: 504\n",
      "Loss of train set: 0.2914881706237793 at epoch: 19 and batch_num: 505\n",
      "Loss of train set: 0.4739157259464264 at epoch: 19 and batch_num: 506\n",
      "Loss of train set: 0.22491319477558136 at epoch: 19 and batch_num: 507\n",
      "Loss of train set: 0.2176135778427124 at epoch: 19 and batch_num: 508\n",
      "Loss of train set: 0.19302481412887573 at epoch: 19 and batch_num: 509\n",
      "Loss of train set: 0.2690976858139038 at epoch: 19 and batch_num: 510\n",
      "Loss of train set: 0.3608993589878082 at epoch: 19 and batch_num: 511\n",
      "Loss of train set: 0.4247155785560608 at epoch: 19 and batch_num: 512\n",
      "Loss of train set: 0.26406165957450867 at epoch: 19 and batch_num: 513\n",
      "Loss of train set: 0.4098438024520874 at epoch: 19 and batch_num: 514\n",
      "Loss of train set: 0.4049628973007202 at epoch: 19 and batch_num: 515\n",
      "Loss of train set: 0.2712485194206238 at epoch: 19 and batch_num: 516\n",
      "Loss of train set: 0.25284647941589355 at epoch: 19 and batch_num: 517\n",
      "Loss of train set: 0.41297829151153564 at epoch: 19 and batch_num: 518\n",
      "Loss of train set: 0.19358310103416443 at epoch: 19 and batch_num: 519\n",
      "Loss of train set: 0.23739884793758392 at epoch: 19 and batch_num: 520\n",
      "Loss of train set: 0.19871395826339722 at epoch: 19 and batch_num: 521\n",
      "Loss of train set: 0.2145080417394638 at epoch: 19 and batch_num: 522\n",
      "Loss of train set: 0.2575450837612152 at epoch: 19 and batch_num: 523\n",
      "Loss of train set: 0.3579598069190979 at epoch: 19 and batch_num: 524\n",
      "Loss of train set: 0.46016281843185425 at epoch: 19 and batch_num: 525\n",
      "Loss of train set: 0.2387174367904663 at epoch: 19 and batch_num: 526\n",
      "Loss of train set: 0.29011332988739014 at epoch: 19 and batch_num: 527\n",
      "Loss of train set: 0.4512084722518921 at epoch: 19 and batch_num: 528\n",
      "Loss of train set: 0.31730917096138 at epoch: 19 and batch_num: 529\n",
      "Loss of train set: 0.23085609078407288 at epoch: 19 and batch_num: 530\n",
      "Loss of train set: 0.1400182545185089 at epoch: 19 and batch_num: 531\n",
      "Loss of train set: 0.25804466009140015 at epoch: 19 and batch_num: 532\n",
      "Loss of train set: 0.18116417527198792 at epoch: 19 and batch_num: 533\n",
      "Loss of train set: 0.3007246255874634 at epoch: 19 and batch_num: 534\n",
      "Loss of train set: 0.29082146286964417 at epoch: 19 and batch_num: 535\n",
      "Loss of train set: 0.22780099511146545 at epoch: 19 and batch_num: 536\n",
      "Loss of train set: 0.3468436598777771 at epoch: 19 and batch_num: 537\n",
      "Loss of train set: 0.11457501351833344 at epoch: 19 and batch_num: 538\n",
      "Loss of train set: 0.2080652415752411 at epoch: 19 and batch_num: 539\n",
      "Loss of train set: 0.3923378884792328 at epoch: 19 and batch_num: 540\n",
      "Loss of train set: 0.3631315231323242 at epoch: 19 and batch_num: 541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.38643068075180054 at epoch: 19 and batch_num: 542\n",
      "Loss of train set: 0.20236293971538544 at epoch: 19 and batch_num: 543\n",
      "Loss of train set: 0.231503427028656 at epoch: 19 and batch_num: 544\n",
      "Loss of train set: 0.3181065320968628 at epoch: 19 and batch_num: 545\n",
      "Loss of train set: 0.3778427243232727 at epoch: 19 and batch_num: 546\n",
      "Loss of train set: 0.3191816806793213 at epoch: 19 and batch_num: 547\n",
      "Loss of train set: 0.2962803244590759 at epoch: 19 and batch_num: 548\n",
      "Loss of train set: 0.341113805770874 at epoch: 19 and batch_num: 549\n",
      "Loss of train set: 0.2789962887763977 at epoch: 19 and batch_num: 550\n",
      "Loss of train set: 0.18498390913009644 at epoch: 19 and batch_num: 551\n",
      "Loss of train set: 0.3572939932346344 at epoch: 19 and batch_num: 552\n",
      "Loss of train set: 0.22623664140701294 at epoch: 19 and batch_num: 553\n",
      "Loss of train set: 0.3625332713127136 at epoch: 19 and batch_num: 554\n",
      "Loss of train set: 0.2943776249885559 at epoch: 19 and batch_num: 555\n",
      "Loss of train set: 0.262967050075531 at epoch: 19 and batch_num: 556\n",
      "Loss of train set: 0.26121261715888977 at epoch: 19 and batch_num: 557\n",
      "Loss of train set: 0.1661488115787506 at epoch: 19 and batch_num: 558\n",
      "Loss of train set: 0.129608154296875 at epoch: 19 and batch_num: 559\n",
      "Loss of train set: 0.3401005268096924 at epoch: 19 and batch_num: 560\n",
      "Loss of train set: 0.2491762489080429 at epoch: 19 and batch_num: 561\n",
      "Loss of train set: 0.35180404782295227 at epoch: 19 and batch_num: 562\n",
      "Loss of train set: 0.36284226179122925 at epoch: 19 and batch_num: 563\n",
      "Loss of train set: 0.3076843023300171 at epoch: 19 and batch_num: 564\n",
      "Loss of train set: 0.2809128165245056 at epoch: 19 and batch_num: 565\n",
      "Loss of train set: 0.2317502796649933 at epoch: 19 and batch_num: 566\n",
      "Loss of train set: 0.1808847337961197 at epoch: 19 and batch_num: 567\n",
      "Loss of train set: 0.31400471925735474 at epoch: 19 and batch_num: 568\n",
      "Loss of train set: 0.35384058952331543 at epoch: 19 and batch_num: 569\n",
      "Loss of train set: 0.33528247475624084 at epoch: 19 and batch_num: 570\n",
      "Loss of train set: 0.17231062054634094 at epoch: 19 and batch_num: 571\n",
      "Loss of train set: 0.4840487837791443 at epoch: 19 and batch_num: 572\n",
      "Loss of train set: 0.47409307956695557 at epoch: 19 and batch_num: 573\n",
      "Loss of train set: 0.20904332399368286 at epoch: 19 and batch_num: 574\n",
      "Loss of train set: 0.40022385120391846 at epoch: 19 and batch_num: 575\n",
      "Loss of train set: 0.3135074973106384 at epoch: 19 and batch_num: 576\n",
      "Loss of train set: 0.23359215259552002 at epoch: 19 and batch_num: 577\n",
      "Loss of train set: 0.1829616278409958 at epoch: 19 and batch_num: 578\n",
      "Loss of train set: 0.35013625025749207 at epoch: 19 and batch_num: 579\n",
      "Loss of train set: 0.2172866314649582 at epoch: 19 and batch_num: 580\n",
      "Loss of train set: 0.4299730956554413 at epoch: 19 and batch_num: 581\n",
      "Loss of train set: 0.2441425919532776 at epoch: 19 and batch_num: 582\n",
      "Loss of train set: 0.1727759838104248 at epoch: 19 and batch_num: 583\n",
      "Loss of train set: 0.3390781283378601 at epoch: 19 and batch_num: 584\n",
      "Loss of train set: 0.206019788980484 at epoch: 19 and batch_num: 585\n",
      "Loss of train set: 0.21472123265266418 at epoch: 19 and batch_num: 586\n",
      "Loss of train set: 0.13457311689853668 at epoch: 19 and batch_num: 587\n",
      "Loss of train set: 0.29313188791275024 at epoch: 19 and batch_num: 588\n",
      "Loss of train set: 0.3561559319496155 at epoch: 19 and batch_num: 589\n",
      "Loss of train set: 0.20111170411109924 at epoch: 19 and batch_num: 590\n",
      "Loss of train set: 0.3329024612903595 at epoch: 19 and batch_num: 591\n",
      "Loss of train set: 0.22735673189163208 at epoch: 19 and batch_num: 592\n",
      "Loss of train set: 0.35511133074760437 at epoch: 19 and batch_num: 593\n",
      "Loss of train set: 0.29194915294647217 at epoch: 19 and batch_num: 594\n",
      "Loss of train set: 0.3223266005516052 at epoch: 19 and batch_num: 595\n",
      "Loss of train set: 0.2303331047296524 at epoch: 19 and batch_num: 596\n",
      "Loss of train set: 0.35574567317962646 at epoch: 19 and batch_num: 597\n",
      "Loss of train set: 0.41958072781562805 at epoch: 19 and batch_num: 598\n",
      "Loss of train set: 0.22818022966384888 at epoch: 19 and batch_num: 599\n",
      "Loss of train set: 0.3436850905418396 at epoch: 19 and batch_num: 600\n",
      "Loss of train set: 0.2960459589958191 at epoch: 19 and batch_num: 601\n",
      "Loss of train set: 0.26501691341400146 at epoch: 19 and batch_num: 602\n",
      "Loss of train set: 0.3067312240600586 at epoch: 19 and batch_num: 603\n",
      "Loss of train set: 0.19898000359535217 at epoch: 19 and batch_num: 604\n",
      "Loss of train set: 0.30468064546585083 at epoch: 19 and batch_num: 605\n",
      "Loss of train set: 0.4872003197669983 at epoch: 19 and batch_num: 606\n",
      "Loss of train set: 0.30443716049194336 at epoch: 19 and batch_num: 607\n",
      "Loss of train set: 0.31747978925704956 at epoch: 19 and batch_num: 608\n",
      "Loss of train set: 0.18697956204414368 at epoch: 19 and batch_num: 609\n",
      "Loss of train set: 0.17458292841911316 at epoch: 19 and batch_num: 610\n",
      "Loss of train set: 0.252003937959671 at epoch: 19 and batch_num: 611\n",
      "Loss of train set: 0.2335253655910492 at epoch: 19 and batch_num: 612\n",
      "Loss of train set: 0.24456845223903656 at epoch: 19 and batch_num: 613\n",
      "Loss of train set: 0.25473737716674805 at epoch: 19 and batch_num: 614\n",
      "Loss of train set: 0.28031474351882935 at epoch: 19 and batch_num: 615\n",
      "Loss of train set: 0.25591903924942017 at epoch: 19 and batch_num: 616\n",
      "Loss of train set: 0.3305310904979706 at epoch: 19 and batch_num: 617\n",
      "Loss of train set: 0.37071776390075684 at epoch: 19 and batch_num: 618\n",
      "Loss of train set: 0.22728486359119415 at epoch: 19 and batch_num: 619\n",
      "Loss of train set: 0.18531259894371033 at epoch: 19 and batch_num: 620\n",
      "Loss of train set: 0.5025656223297119 at epoch: 19 and batch_num: 621\n",
      "Loss of train set: 0.42411115765571594 at epoch: 19 and batch_num: 622\n",
      "Loss of train set: 0.23684850335121155 at epoch: 19 and batch_num: 623\n",
      "Loss of train set: 0.3466152548789978 at epoch: 19 and batch_num: 624\n",
      "Loss of train set: 0.350940465927124 at epoch: 19 and batch_num: 625\n",
      "Loss of train set: 0.3959510922431946 at epoch: 19 and batch_num: 626\n",
      "Loss of train set: 0.4509384334087372 at epoch: 19 and batch_num: 627\n",
      "Loss of train set: 0.3104103207588196 at epoch: 19 and batch_num: 628\n",
      "Loss of train set: 0.32612887024879456 at epoch: 19 and batch_num: 629\n",
      "Loss of train set: 0.23953402042388916 at epoch: 19 and batch_num: 630\n",
      "Loss of train set: 0.19689157605171204 at epoch: 19 and batch_num: 631\n",
      "Loss of train set: 0.17289654910564423 at epoch: 19 and batch_num: 632\n",
      "Loss of train set: 0.27139705419540405 at epoch: 19 and batch_num: 633\n",
      "Loss of train set: 0.19562119245529175 at epoch: 19 and batch_num: 634\n",
      "Loss of train set: 0.21597863733768463 at epoch: 19 and batch_num: 635\n",
      "Loss of train set: 0.20788833498954773 at epoch: 19 and batch_num: 636\n",
      "Loss of train set: 0.2314780056476593 at epoch: 19 and batch_num: 637\n",
      "Loss of train set: 0.30422621965408325 at epoch: 19 and batch_num: 638\n",
      "Loss of train set: 0.2446070909500122 at epoch: 19 and batch_num: 639\n",
      "Loss of train set: 0.29185950756073 at epoch: 19 and batch_num: 640\n",
      "Loss of train set: 0.1981850117444992 at epoch: 19 and batch_num: 641\n",
      "Loss of train set: 0.1679929792881012 at epoch: 19 and batch_num: 642\n",
      "Loss of train set: 0.19497311115264893 at epoch: 19 and batch_num: 643\n",
      "Loss of train set: 0.3439174294471741 at epoch: 19 and batch_num: 644\n",
      "Loss of train set: 0.28584223985671997 at epoch: 19 and batch_num: 645\n",
      "Loss of train set: 0.2524354159832001 at epoch: 19 and batch_num: 646\n",
      "Loss of train set: 0.2420121133327484 at epoch: 19 and batch_num: 647\n",
      "Loss of train set: 0.38679760694503784 at epoch: 19 and batch_num: 648\n",
      "Loss of train set: 0.2817264199256897 at epoch: 19 and batch_num: 649\n",
      "Loss of train set: 0.31165599822998047 at epoch: 19 and batch_num: 650\n",
      "Loss of train set: 0.23291492462158203 at epoch: 19 and batch_num: 651\n",
      "Loss of train set: 0.25126785039901733 at epoch: 19 and batch_num: 652\n",
      "Loss of train set: 0.13679781556129456 at epoch: 19 and batch_num: 653\n",
      "Loss of train set: 0.2969942092895508 at epoch: 19 and batch_num: 654\n",
      "Loss of train set: 0.3125200867652893 at epoch: 19 and batch_num: 655\n",
      "Loss of train set: 0.24107550084590912 at epoch: 19 and batch_num: 656\n",
      "Loss of train set: 0.2871565818786621 at epoch: 19 and batch_num: 657\n",
      "Loss of train set: 0.3033207356929779 at epoch: 19 and batch_num: 658\n",
      "Loss of train set: 0.3228861093521118 at epoch: 19 and batch_num: 659\n",
      "Loss of train set: 0.2606569230556488 at epoch: 19 and batch_num: 660\n",
      "Loss of train set: 0.27209484577178955 at epoch: 19 and batch_num: 661\n",
      "Loss of train set: 0.6514597535133362 at epoch: 19 and batch_num: 662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2611169219017029 at epoch: 19 and batch_num: 663\n",
      "Loss of train set: 0.18925201892852783 at epoch: 19 and batch_num: 664\n",
      "Loss of train set: 0.3929784297943115 at epoch: 19 and batch_num: 665\n",
      "Loss of train set: 0.2455618530511856 at epoch: 19 and batch_num: 666\n",
      "Loss of train set: 0.319769024848938 at epoch: 19 and batch_num: 667\n",
      "Loss of train set: 0.29771482944488525 at epoch: 19 and batch_num: 668\n",
      "Loss of train set: 0.28617313504219055 at epoch: 19 and batch_num: 669\n",
      "Loss of train set: 0.34296897053718567 at epoch: 19 and batch_num: 670\n",
      "Loss of train set: 0.34457194805145264 at epoch: 19 and batch_num: 671\n",
      "Loss of train set: 0.48234832286834717 at epoch: 19 and batch_num: 672\n",
      "Loss of train set: 0.2743482291698456 at epoch: 19 and batch_num: 673\n",
      "Loss of train set: 0.377128541469574 at epoch: 19 and batch_num: 674\n",
      "Loss of train set: 0.3551596403121948 at epoch: 19 and batch_num: 675\n",
      "Loss of train set: 0.234259694814682 at epoch: 19 and batch_num: 676\n",
      "Loss of train set: 0.2328222393989563 at epoch: 19 and batch_num: 677\n",
      "Loss of train set: 0.4482622742652893 at epoch: 19 and batch_num: 678\n",
      "Loss of train set: 0.389761358499527 at epoch: 19 and batch_num: 679\n",
      "Loss of train set: 0.2861098647117615 at epoch: 19 and batch_num: 680\n",
      "Loss of train set: 0.2600717842578888 at epoch: 19 and batch_num: 681\n",
      "Loss of train set: 0.22596804797649384 at epoch: 19 and batch_num: 682\n",
      "Loss of train set: 0.32019877433776855 at epoch: 19 and batch_num: 683\n",
      "Loss of train set: 0.13090477883815765 at epoch: 19 and batch_num: 684\n",
      "Loss of train set: 0.30159223079681396 at epoch: 19 and batch_num: 685\n",
      "Loss of train set: 0.12608297169208527 at epoch: 19 and batch_num: 686\n",
      "Loss of train set: 0.24265636503696442 at epoch: 19 and batch_num: 687\n",
      "Loss of train set: 0.24522313475608826 at epoch: 19 and batch_num: 688\n",
      "Loss of train set: 0.3316100835800171 at epoch: 19 and batch_num: 689\n",
      "Loss of train set: 0.2333764135837555 at epoch: 19 and batch_num: 690\n",
      "Loss of train set: 0.3671379089355469 at epoch: 19 and batch_num: 691\n",
      "Loss of train set: 0.21602863073349 at epoch: 19 and batch_num: 692\n",
      "Loss of train set: 0.2759004235267639 at epoch: 19 and batch_num: 693\n",
      "Loss of train set: 0.32806628942489624 at epoch: 19 and batch_num: 694\n",
      "Loss of train set: 0.2574325501918793 at epoch: 19 and batch_num: 695\n",
      "Loss of train set: 0.23791062831878662 at epoch: 19 and batch_num: 696\n",
      "Loss of train set: 0.2415926605463028 at epoch: 19 and batch_num: 697\n",
      "Loss of train set: 0.36815449595451355 at epoch: 19 and batch_num: 698\n",
      "Loss of train set: 0.3040090501308441 at epoch: 19 and batch_num: 699\n",
      "Loss of train set: 0.2510320842266083 at epoch: 19 and batch_num: 700\n",
      "Loss of train set: 0.21403798460960388 at epoch: 19 and batch_num: 701\n",
      "Loss of train set: 0.36316168308258057 at epoch: 19 and batch_num: 702\n",
      "Loss of train set: 0.23793616890907288 at epoch: 19 and batch_num: 703\n",
      "Loss of train set: 0.4066457748413086 at epoch: 19 and batch_num: 704\n",
      "Loss of train set: 0.23836737871170044 at epoch: 19 and batch_num: 705\n",
      "Loss of train set: 0.34958526492118835 at epoch: 19 and batch_num: 706\n",
      "Loss of train set: 0.2869601249694824 at epoch: 19 and batch_num: 707\n",
      "Loss of train set: 0.2653070092201233 at epoch: 19 and batch_num: 708\n",
      "Loss of train set: 0.2676714360713959 at epoch: 19 and batch_num: 709\n",
      "Loss of train set: 0.22577588260173798 at epoch: 19 and batch_num: 710\n",
      "Loss of train set: 0.21001628041267395 at epoch: 19 and batch_num: 711\n",
      "Loss of train set: 0.3390503525733948 at epoch: 19 and batch_num: 712\n",
      "Loss of train set: 0.1300828903913498 at epoch: 19 and batch_num: 713\n",
      "Loss of train set: 0.24294033646583557 at epoch: 19 and batch_num: 714\n",
      "Loss of train set: 0.26123398542404175 at epoch: 19 and batch_num: 715\n",
      "Loss of train set: 0.4101349115371704 at epoch: 19 and batch_num: 716\n",
      "Loss of train set: 0.25808238983154297 at epoch: 19 and batch_num: 717\n",
      "Loss of train set: 0.26951274275779724 at epoch: 19 and batch_num: 718\n",
      "Loss of train set: 0.3735535740852356 at epoch: 19 and batch_num: 719\n",
      "Loss of train set: 0.30645692348480225 at epoch: 19 and batch_num: 720\n",
      "Loss of train set: 0.3865179717540741 at epoch: 19 and batch_num: 721\n",
      "Loss of train set: 0.32016056776046753 at epoch: 19 and batch_num: 722\n",
      "Loss of train set: 0.2738325595855713 at epoch: 19 and batch_num: 723\n",
      "Loss of train set: 0.2015097439289093 at epoch: 19 and batch_num: 724\n",
      "Loss of train set: 0.23750339448451996 at epoch: 19 and batch_num: 725\n",
      "Loss of train set: 0.3472357392311096 at epoch: 19 and batch_num: 726\n",
      "Loss of train set: 0.3582470118999481 at epoch: 19 and batch_num: 727\n",
      "Loss of train set: 0.2647976577281952 at epoch: 19 and batch_num: 728\n",
      "Loss of train set: 0.25281643867492676 at epoch: 19 and batch_num: 729\n",
      "Loss of train set: 0.2868179678916931 at epoch: 19 and batch_num: 730\n",
      "Loss of train set: 0.3143394887447357 at epoch: 19 and batch_num: 731\n",
      "Loss of train set: 0.21689847111701965 at epoch: 19 and batch_num: 732\n",
      "Loss of train set: 0.3051651120185852 at epoch: 19 and batch_num: 733\n",
      "Loss of train set: 0.3917269706726074 at epoch: 19 and batch_num: 734\n",
      "Loss of train set: 0.47061586380004883 at epoch: 19 and batch_num: 735\n",
      "Loss of train set: 0.30865514278411865 at epoch: 19 and batch_num: 736\n",
      "Loss of train set: 0.24457839131355286 at epoch: 19 and batch_num: 737\n",
      "Loss of train set: 0.3294121325016022 at epoch: 19 and batch_num: 738\n",
      "Loss of train set: 0.23550936579704285 at epoch: 19 and batch_num: 739\n",
      "Loss of train set: 0.499220073223114 at epoch: 19 and batch_num: 740\n",
      "Loss of train set: 0.23895598948001862 at epoch: 19 and batch_num: 741\n",
      "Loss of train set: 0.2735294997692108 at epoch: 19 and batch_num: 742\n",
      "Loss of train set: 0.3415382504463196 at epoch: 19 and batch_num: 743\n",
      "Loss of train set: 0.2413894534111023 at epoch: 19 and batch_num: 744\n",
      "Loss of train set: 0.43419837951660156 at epoch: 19 and batch_num: 745\n",
      "Loss of train set: 0.23545952141284943 at epoch: 19 and batch_num: 746\n",
      "Loss of train set: 0.19138163328170776 at epoch: 19 and batch_num: 747\n",
      "Loss of train set: 0.39774027466773987 at epoch: 19 and batch_num: 748\n",
      "Loss of train set: 0.3315615653991699 at epoch: 19 and batch_num: 749\n",
      "Loss of train set: 0.343889981508255 at epoch: 19 and batch_num: 750\n",
      "Loss of train set: 0.24981310963630676 at epoch: 19 and batch_num: 751\n",
      "Loss of train set: 0.276350736618042 at epoch: 19 and batch_num: 752\n",
      "Loss of train set: 0.28952836990356445 at epoch: 19 and batch_num: 753\n",
      "Loss of train set: 0.15418708324432373 at epoch: 19 and batch_num: 754\n",
      "Loss of train set: 0.27812567353248596 at epoch: 19 and batch_num: 755\n",
      "Loss of train set: 0.20969778299331665 at epoch: 19 and batch_num: 756\n",
      "Loss of train set: 0.23841840028762817 at epoch: 19 and batch_num: 757\n",
      "Loss of train set: 0.40580472350120544 at epoch: 19 and batch_num: 758\n",
      "Loss of train set: 0.2141902595758438 at epoch: 19 and batch_num: 759\n",
      "Loss of train set: 0.2758287191390991 at epoch: 19 and batch_num: 760\n",
      "Loss of train set: 0.23670905828475952 at epoch: 19 and batch_num: 761\n",
      "Loss of train set: 0.3151013255119324 at epoch: 19 and batch_num: 762\n",
      "Loss of train set: 0.37775546312332153 at epoch: 19 and batch_num: 763\n",
      "Loss of train set: 0.5829387903213501 at epoch: 19 and batch_num: 764\n",
      "Loss of train set: 0.34397009015083313 at epoch: 19 and batch_num: 765\n",
      "Loss of train set: 0.2630472183227539 at epoch: 19 and batch_num: 766\n",
      "Loss of train set: 0.3601762652397156 at epoch: 19 and batch_num: 767\n",
      "Loss of train set: 0.2713857889175415 at epoch: 19 and batch_num: 768\n",
      "Loss of train set: 0.38010552525520325 at epoch: 19 and batch_num: 769\n",
      "Loss of train set: 0.23189066350460052 at epoch: 19 and batch_num: 770\n",
      "Loss of train set: 0.24965164065361023 at epoch: 19 and batch_num: 771\n",
      "Loss of train set: 0.4491729140281677 at epoch: 19 and batch_num: 772\n",
      "Loss of train set: 0.26235300302505493 at epoch: 19 and batch_num: 773\n",
      "Loss of train set: 0.22503820061683655 at epoch: 19 and batch_num: 774\n",
      "Loss of train set: 0.22649087011814117 at epoch: 19 and batch_num: 775\n",
      "Loss of train set: 0.24222563207149506 at epoch: 19 and batch_num: 776\n",
      "Loss of train set: 0.31026601791381836 at epoch: 19 and batch_num: 777\n",
      "Loss of train set: 0.39968690276145935 at epoch: 19 and batch_num: 778\n",
      "Loss of train set: 0.18696331977844238 at epoch: 19 and batch_num: 779\n",
      "Loss of train set: 0.24875226616859436 at epoch: 19 and batch_num: 780\n",
      "Loss of train set: 0.19462168216705322 at epoch: 19 and batch_num: 781\n",
      "Loss of train set: 0.2187468260526657 at epoch: 19 and batch_num: 782\n",
      "Loss of train set: 0.168552964925766 at epoch: 19 and batch_num: 783\n",
      "Loss of train set: 0.24139925837516785 at epoch: 19 and batch_num: 784\n",
      "Loss of train set: 0.3199203908443451 at epoch: 19 and batch_num: 785\n",
      "Loss of train set: 0.2514602243900299 at epoch: 19 and batch_num: 786\n",
      "Loss of train set: 0.4199630618095398 at epoch: 19 and batch_num: 787\n",
      "Loss of train set: 0.43187129497528076 at epoch: 19 and batch_num: 788\n",
      "Loss of train set: 0.30845314264297485 at epoch: 19 and batch_num: 789\n",
      "Loss of train set: 0.29462867975234985 at epoch: 19 and batch_num: 790\n",
      "Loss of train set: 0.2567298412322998 at epoch: 19 and batch_num: 791\n",
      "Loss of train set: 0.18024691939353943 at epoch: 19 and batch_num: 792\n",
      "Loss of train set: 0.28744977712631226 at epoch: 19 and batch_num: 793\n",
      "Loss of train set: 0.268401175737381 at epoch: 19 and batch_num: 794\n",
      "Loss of train set: 0.5149421095848083 at epoch: 19 and batch_num: 795\n",
      "Loss of train set: 0.21620900928974152 at epoch: 19 and batch_num: 796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.39062872529029846 at epoch: 19 and batch_num: 797\n",
      "Loss of train set: 0.3865680694580078 at epoch: 19 and batch_num: 798\n",
      "Loss of train set: 0.16030505299568176 at epoch: 19 and batch_num: 799\n",
      "Loss of train set: 0.3669893741607666 at epoch: 19 and batch_num: 800\n",
      "Loss of train set: 0.280782550573349 at epoch: 19 and batch_num: 801\n",
      "Loss of train set: 0.22999447584152222 at epoch: 19 and batch_num: 802\n",
      "Loss of train set: 0.27820298075675964 at epoch: 19 and batch_num: 803\n",
      "Loss of train set: 0.34491661190986633 at epoch: 19 and batch_num: 804\n",
      "Loss of train set: 0.38764488697052 at epoch: 19 and batch_num: 805\n",
      "Loss of train set: 0.20554518699645996 at epoch: 19 and batch_num: 806\n",
      "Loss of train set: 0.31598395109176636 at epoch: 19 and batch_num: 807\n",
      "Loss of train set: 0.27086350321769714 at epoch: 19 and batch_num: 808\n",
      "Loss of train set: 0.35005587339401245 at epoch: 19 and batch_num: 809\n",
      "Loss of train set: 0.1961139440536499 at epoch: 19 and batch_num: 810\n",
      "Loss of train set: 0.22321370244026184 at epoch: 19 and batch_num: 811\n",
      "Loss of train set: 0.215311199426651 at epoch: 19 and batch_num: 812\n",
      "Loss of train set: 0.3463270962238312 at epoch: 19 and batch_num: 813\n",
      "Loss of train set: 0.3191452622413635 at epoch: 19 and batch_num: 814\n",
      "Loss of train set: 0.27179357409477234 at epoch: 19 and batch_num: 815\n",
      "Loss of train set: 0.5496661067008972 at epoch: 19 and batch_num: 816\n",
      "Loss of train set: 0.17664305865764618 at epoch: 19 and batch_num: 817\n",
      "Loss of train set: 0.33880817890167236 at epoch: 19 and batch_num: 818\n",
      "Loss of train set: 0.4733833074569702 at epoch: 19 and batch_num: 819\n",
      "Loss of train set: 0.4005240797996521 at epoch: 19 and batch_num: 820\n",
      "Loss of train set: 0.38529402017593384 at epoch: 19 and batch_num: 821\n",
      "Loss of train set: 0.14829203486442566 at epoch: 19 and batch_num: 822\n",
      "Loss of train set: 0.2971954345703125 at epoch: 19 and batch_num: 823\n",
      "Loss of train set: 0.26488417387008667 at epoch: 19 and batch_num: 824\n",
      "Loss of train set: 0.2007330358028412 at epoch: 19 and batch_num: 825\n",
      "Loss of train set: 0.3540775775909424 at epoch: 19 and batch_num: 826\n",
      "Loss of train set: 0.27416718006134033 at epoch: 19 and batch_num: 827\n",
      "Loss of train set: 0.2802713215351105 at epoch: 19 and batch_num: 828\n",
      "Loss of train set: 0.3068894147872925 at epoch: 19 and batch_num: 829\n",
      "Loss of train set: 0.23686744272708893 at epoch: 19 and batch_num: 830\n",
      "Loss of train set: 0.26915958523750305 at epoch: 19 and batch_num: 831\n",
      "Loss of train set: 0.12764525413513184 at epoch: 19 and batch_num: 832\n",
      "Loss of train set: 0.26031312346458435 at epoch: 19 and batch_num: 833\n",
      "Loss of train set: 0.28615549206733704 at epoch: 19 and batch_num: 834\n",
      "Loss of train set: 0.22920072078704834 at epoch: 19 and batch_num: 835\n",
      "Loss of train set: 0.3023185431957245 at epoch: 19 and batch_num: 836\n",
      "Loss of train set: 0.4621986746788025 at epoch: 19 and batch_num: 837\n",
      "Loss of train set: 0.39101290702819824 at epoch: 19 and batch_num: 838\n",
      "Loss of train set: 0.2024332582950592 at epoch: 19 and batch_num: 839\n",
      "Loss of train set: 0.36057713627815247 at epoch: 19 and batch_num: 840\n",
      "Loss of train set: 0.32054150104522705 at epoch: 19 and batch_num: 841\n",
      "Loss of train set: 0.4152478873729706 at epoch: 19 and batch_num: 842\n",
      "Loss of train set: 0.2589679956436157 at epoch: 19 and batch_num: 843\n",
      "Loss of train set: 0.30089429020881653 at epoch: 19 and batch_num: 844\n",
      "Loss of train set: 0.31069138646125793 at epoch: 19 and batch_num: 845\n",
      "Loss of train set: 0.1832514852285385 at epoch: 19 and batch_num: 846\n",
      "Loss of train set: 0.34279805421829224 at epoch: 19 and batch_num: 847\n",
      "Loss of train set: 0.4025549292564392 at epoch: 19 and batch_num: 848\n",
      "Loss of train set: 0.2662668824195862 at epoch: 19 and batch_num: 849\n",
      "Loss of train set: 0.22674575448036194 at epoch: 19 and batch_num: 850\n",
      "Loss of train set: 0.2063652127981186 at epoch: 19 and batch_num: 851\n",
      "Loss of train set: 0.2657504379749298 at epoch: 19 and batch_num: 852\n",
      "Loss of train set: 0.18213215470314026 at epoch: 19 and batch_num: 853\n",
      "Loss of train set: 0.2043454945087433 at epoch: 19 and batch_num: 854\n",
      "Loss of train set: 0.39718276262283325 at epoch: 19 and batch_num: 855\n",
      "Loss of train set: 0.27021709084510803 at epoch: 19 and batch_num: 856\n",
      "Loss of train set: 0.48034536838531494 at epoch: 19 and batch_num: 857\n",
      "Loss of train set: 0.2692527770996094 at epoch: 19 and batch_num: 858\n",
      "Loss of train set: 0.4228252172470093 at epoch: 19 and batch_num: 859\n",
      "Loss of train set: 0.21930640935897827 at epoch: 19 and batch_num: 860\n",
      "Loss of train set: 0.24351903796195984 at epoch: 19 and batch_num: 861\n",
      "Loss of train set: 0.24367614090442657 at epoch: 19 and batch_num: 862\n",
      "Loss of train set: 0.2206423282623291 at epoch: 19 and batch_num: 863\n",
      "Loss of train set: 0.2673531770706177 at epoch: 19 and batch_num: 864\n",
      "Loss of train set: 0.33358705043792725 at epoch: 19 and batch_num: 865\n",
      "Loss of train set: 0.3458775281906128 at epoch: 19 and batch_num: 866\n",
      "Loss of train set: 0.35788488388061523 at epoch: 19 and batch_num: 867\n",
      "Loss of train set: 0.47942662239074707 at epoch: 19 and batch_num: 868\n",
      "Loss of train set: 0.2319788634777069 at epoch: 19 and batch_num: 869\n",
      "Loss of train set: 0.33334338665008545 at epoch: 19 and batch_num: 870\n",
      "Loss of train set: 0.32703810930252075 at epoch: 19 and batch_num: 871\n",
      "Loss of train set: 0.2845413088798523 at epoch: 19 and batch_num: 872\n",
      "Loss of train set: 0.29931962490081787 at epoch: 19 and batch_num: 873\n",
      "Loss of train set: 0.34546542167663574 at epoch: 19 and batch_num: 874\n",
      "Loss of train set: 0.2051703929901123 at epoch: 19 and batch_num: 875\n",
      "Loss of train set: 0.19652414321899414 at epoch: 19 and batch_num: 876\n",
      "Loss of train set: 0.21364504098892212 at epoch: 19 and batch_num: 877\n",
      "Loss of train set: 0.2314126193523407 at epoch: 19 and batch_num: 878\n",
      "Loss of train set: 0.2900122404098511 at epoch: 19 and batch_num: 879\n",
      "Loss of train set: 0.22686129808425903 at epoch: 19 and batch_num: 880\n",
      "Loss of train set: 0.25488048791885376 at epoch: 19 and batch_num: 881\n",
      "Loss of train set: 0.22598528861999512 at epoch: 19 and batch_num: 882\n",
      "Loss of train set: 0.3040463626384735 at epoch: 19 and batch_num: 883\n",
      "Loss of train set: 0.2507379353046417 at epoch: 19 and batch_num: 884\n",
      "Loss of train set: 0.21559160947799683 at epoch: 19 and batch_num: 885\n",
      "Loss of train set: 0.341289758682251 at epoch: 19 and batch_num: 886\n",
      "Loss of train set: 0.293805330991745 at epoch: 19 and batch_num: 887\n",
      "Loss of train set: 0.1920960545539856 at epoch: 19 and batch_num: 888\n",
      "Loss of train set: 0.31759440898895264 at epoch: 19 and batch_num: 889\n",
      "Loss of train set: 0.3349834680557251 at epoch: 19 and batch_num: 890\n",
      "Loss of train set: 0.21293896436691284 at epoch: 19 and batch_num: 891\n",
      "Loss of train set: 0.19887027144432068 at epoch: 19 and batch_num: 892\n",
      "Loss of train set: 0.2613319754600525 at epoch: 19 and batch_num: 893\n",
      "Loss of train set: 0.23914463818073273 at epoch: 19 and batch_num: 894\n",
      "Loss of train set: 0.41947251558303833 at epoch: 19 and batch_num: 895\n",
      "Loss of train set: 0.2099645435810089 at epoch: 19 and batch_num: 896\n",
      "Loss of train set: 0.20756852626800537 at epoch: 19 and batch_num: 897\n",
      "Loss of train set: 0.38402920961380005 at epoch: 19 and batch_num: 898\n",
      "Loss of train set: 0.2547317445278168 at epoch: 19 and batch_num: 899\n",
      "Loss of train set: 0.20399366319179535 at epoch: 19 and batch_num: 900\n",
      "Loss of train set: 0.40251195430755615 at epoch: 19 and batch_num: 901\n",
      "Loss of train set: 0.4755830466747284 at epoch: 19 and batch_num: 902\n",
      "Loss of train set: 0.32281553745269775 at epoch: 19 and batch_num: 903\n",
      "Loss of train set: 0.297071248292923 at epoch: 19 and batch_num: 904\n",
      "Loss of train set: 0.3252981901168823 at epoch: 19 and batch_num: 905\n",
      "Loss of train set: 0.36535948514938354 at epoch: 19 and batch_num: 906\n",
      "Loss of train set: 0.2579813003540039 at epoch: 19 and batch_num: 907\n",
      "Loss of train set: 0.2974322438240051 at epoch: 19 and batch_num: 908\n",
      "Loss of train set: 0.28990793228149414 at epoch: 19 and batch_num: 909\n",
      "Loss of train set: 0.20407938957214355 at epoch: 19 and batch_num: 910\n",
      "Loss of train set: 0.5332043170928955 at epoch: 19 and batch_num: 911\n",
      "Loss of train set: 0.22062765061855316 at epoch: 19 and batch_num: 912\n",
      "Loss of train set: 0.2057507485151291 at epoch: 19 and batch_num: 913\n",
      "Loss of train set: 0.2007756531238556 at epoch: 19 and batch_num: 914\n",
      "Loss of train set: 0.23190757632255554 at epoch: 19 and batch_num: 915\n",
      "Loss of train set: 0.22627559304237366 at epoch: 19 and batch_num: 916\n",
      "Loss of train set: 0.307963490486145 at epoch: 19 and batch_num: 917\n",
      "Loss of train set: 0.3443406820297241 at epoch: 19 and batch_num: 918\n",
      "Loss of train set: 0.38012421131134033 at epoch: 19 and batch_num: 919\n",
      "Loss of train set: 0.4123707711696625 at epoch: 19 and batch_num: 920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.286467581987381 at epoch: 19 and batch_num: 921\n",
      "Loss of train set: 0.17495548725128174 at epoch: 19 and batch_num: 922\n",
      "Loss of train set: 0.2485497146844864 at epoch: 19 and batch_num: 923\n",
      "Loss of train set: 0.34727051854133606 at epoch: 19 and batch_num: 924\n",
      "Loss of train set: 0.3073427677154541 at epoch: 19 and batch_num: 925\n",
      "Loss of train set: 0.17465437948703766 at epoch: 19 and batch_num: 926\n",
      "Loss of train set: 0.23173601925373077 at epoch: 19 and batch_num: 927\n",
      "Loss of train set: 0.3622635006904602 at epoch: 19 and batch_num: 928\n",
      "Loss of train set: 0.49192798137664795 at epoch: 19 and batch_num: 929\n",
      "Loss of train set: 0.15381310880184174 at epoch: 19 and batch_num: 930\n",
      "Loss of train set: 0.20907849073410034 at epoch: 19 and batch_num: 931\n",
      "Loss of train set: 0.2062404751777649 at epoch: 19 and batch_num: 932\n",
      "Loss of train set: 0.2426430583000183 at epoch: 19 and batch_num: 933\n",
      "Loss of train set: 0.3118312954902649 at epoch: 19 and batch_num: 934\n",
      "Loss of train set: 0.2732491195201874 at epoch: 19 and batch_num: 935\n",
      "Loss of train set: 0.2131490260362625 at epoch: 19 and batch_num: 936\n",
      "Loss of train set: 0.13326765596866608 at epoch: 19 and batch_num: 937\n",
      "Accuracy of train set: 0.8968\n",
      "Loss of test set: 0.3040294647216797 at epoch: 19 and batch_num: 0\n",
      "Loss of test set: 0.19444191455841064 at epoch: 19 and batch_num: 1\n",
      "Loss of test set: 0.3021945357322693 at epoch: 19 and batch_num: 2\n",
      "Loss of test set: 0.28784042596817017 at epoch: 19 and batch_num: 3\n",
      "Loss of test set: 0.43495091795921326 at epoch: 19 and batch_num: 4\n",
      "Loss of test set: 0.3787904977798462 at epoch: 19 and batch_num: 5\n",
      "Loss of test set: 0.27611005306243896 at epoch: 19 and batch_num: 6\n",
      "Loss of test set: 0.4330912232398987 at epoch: 19 and batch_num: 7\n",
      "Loss of test set: 0.36792007088661194 at epoch: 19 and batch_num: 8\n",
      "Loss of test set: 0.2755240797996521 at epoch: 19 and batch_num: 9\n",
      "Loss of test set: 0.23784099519252777 at epoch: 19 and batch_num: 10\n",
      "Loss of test set: 0.3930743336677551 at epoch: 19 and batch_num: 11\n",
      "Loss of test set: 0.26641666889190674 at epoch: 19 and batch_num: 12\n",
      "Loss of test set: 0.3153436779975891 at epoch: 19 and batch_num: 13\n",
      "Loss of test set: 0.38248211145401 at epoch: 19 and batch_num: 14\n",
      "Loss of test set: 0.2094704806804657 at epoch: 19 and batch_num: 15\n",
      "Loss of test set: 0.20820865035057068 at epoch: 19 and batch_num: 16\n",
      "Loss of test set: 0.5401471853256226 at epoch: 19 and batch_num: 17\n",
      "Loss of test set: 0.4107119143009186 at epoch: 19 and batch_num: 18\n",
      "Loss of test set: 0.25095221400260925 at epoch: 19 and batch_num: 19\n",
      "Loss of test set: 0.43559056520462036 at epoch: 19 and batch_num: 20\n",
      "Loss of test set: 0.2922697067260742 at epoch: 19 and batch_num: 21\n",
      "Loss of test set: 0.3730165362358093 at epoch: 19 and batch_num: 22\n",
      "Loss of test set: 0.33881595730781555 at epoch: 19 and batch_num: 23\n",
      "Loss of test set: 0.24501901865005493 at epoch: 19 and batch_num: 24\n",
      "Loss of test set: 0.2272340953350067 at epoch: 19 and batch_num: 25\n",
      "Loss of test set: 0.3035105764865875 at epoch: 19 and batch_num: 26\n",
      "Loss of test set: 0.3886033296585083 at epoch: 19 and batch_num: 27\n",
      "Loss of test set: 0.29842281341552734 at epoch: 19 and batch_num: 28\n",
      "Loss of test set: 0.543942391872406 at epoch: 19 and batch_num: 29\n",
      "Loss of test set: 0.5073816180229187 at epoch: 19 and batch_num: 30\n",
      "Loss of test set: 0.2100166380405426 at epoch: 19 and batch_num: 31\n",
      "Loss of test set: 0.2381151169538498 at epoch: 19 and batch_num: 32\n",
      "Loss of test set: 0.3385846018791199 at epoch: 19 and batch_num: 33\n",
      "Loss of test set: 0.22209647297859192 at epoch: 19 and batch_num: 34\n",
      "Loss of test set: 0.3782297372817993 at epoch: 19 and batch_num: 35\n",
      "Loss of test set: 0.36278679966926575 at epoch: 19 and batch_num: 36\n",
      "Loss of test set: 0.463628888130188 at epoch: 19 and batch_num: 37\n",
      "Loss of test set: 0.3357028365135193 at epoch: 19 and batch_num: 38\n",
      "Loss of test set: 0.48367664217948914 at epoch: 19 and batch_num: 39\n",
      "Loss of test set: 0.13360559940338135 at epoch: 19 and batch_num: 40\n",
      "Loss of test set: 0.3000226318836212 at epoch: 19 and batch_num: 41\n",
      "Loss of test set: 0.3354911208152771 at epoch: 19 and batch_num: 42\n",
      "Loss of test set: 0.2920987606048584 at epoch: 19 and batch_num: 43\n",
      "Loss of test set: 0.27780288457870483 at epoch: 19 and batch_num: 44\n",
      "Loss of test set: 0.48401159048080444 at epoch: 19 and batch_num: 45\n",
      "Loss of test set: 0.4073042869567871 at epoch: 19 and batch_num: 46\n",
      "Loss of test set: 0.4651261270046234 at epoch: 19 and batch_num: 47\n",
      "Loss of test set: 0.3455210030078888 at epoch: 19 and batch_num: 48\n",
      "Loss of test set: 0.3171323835849762 at epoch: 19 and batch_num: 49\n",
      "Loss of test set: 0.2714061141014099 at epoch: 19 and batch_num: 50\n",
      "Loss of test set: 0.17537522315979004 at epoch: 19 and batch_num: 51\n",
      "Loss of test set: 0.44290047883987427 at epoch: 19 and batch_num: 52\n",
      "Loss of test set: 0.646878182888031 at epoch: 19 and batch_num: 53\n",
      "Loss of test set: 0.32896554470062256 at epoch: 19 and batch_num: 54\n",
      "Loss of test set: 0.28659093379974365 at epoch: 19 and batch_num: 55\n",
      "Loss of test set: 0.46927574276924133 at epoch: 19 and batch_num: 56\n",
      "Loss of test set: 0.19163799285888672 at epoch: 19 and batch_num: 57\n",
      "Loss of test set: 0.37578970193862915 at epoch: 19 and batch_num: 58\n",
      "Loss of test set: 0.3376888036727905 at epoch: 19 and batch_num: 59\n",
      "Loss of test set: 0.3771445155143738 at epoch: 19 and batch_num: 60\n",
      "Loss of test set: 0.4273669719696045 at epoch: 19 and batch_num: 61\n",
      "Loss of test set: 0.16750408709049225 at epoch: 19 and batch_num: 62\n",
      "Loss of test set: 0.5011779069900513 at epoch: 19 and batch_num: 63\n",
      "Loss of test set: 0.3463745713233948 at epoch: 19 and batch_num: 64\n",
      "Loss of test set: 0.34253624081611633 at epoch: 19 and batch_num: 65\n",
      "Loss of test set: 0.3683267831802368 at epoch: 19 and batch_num: 66\n",
      "Loss of test set: 0.42686817049980164 at epoch: 19 and batch_num: 67\n",
      "Loss of test set: 0.3502548933029175 at epoch: 19 and batch_num: 68\n",
      "Loss of test set: 0.3589283525943756 at epoch: 19 and batch_num: 69\n",
      "Loss of test set: 0.4962031841278076 at epoch: 19 and batch_num: 70\n",
      "Loss of test set: 0.25034257769584656 at epoch: 19 and batch_num: 71\n",
      "Loss of test set: 0.3258388042449951 at epoch: 19 and batch_num: 72\n",
      "Loss of test set: 0.3317592144012451 at epoch: 19 and batch_num: 73\n",
      "Loss of test set: 0.214541494846344 at epoch: 19 and batch_num: 74\n",
      "Loss of test set: 0.3932996392250061 at epoch: 19 and batch_num: 75\n",
      "Loss of test set: 0.20913361012935638 at epoch: 19 and batch_num: 76\n",
      "Loss of test set: 0.3026459813117981 at epoch: 19 and batch_num: 77\n",
      "Loss of test set: 0.2653050124645233 at epoch: 19 and batch_num: 78\n",
      "Loss of test set: 0.401151180267334 at epoch: 19 and batch_num: 79\n",
      "Loss of test set: 0.31725895404815674 at epoch: 19 and batch_num: 80\n",
      "Loss of test set: 0.4636840224266052 at epoch: 19 and batch_num: 81\n",
      "Loss of test set: 0.21709510684013367 at epoch: 19 and batch_num: 82\n",
      "Loss of test set: 0.45511478185653687 at epoch: 19 and batch_num: 83\n",
      "Loss of test set: 0.18664728105068207 at epoch: 19 and batch_num: 84\n",
      "Loss of test set: 0.321181982755661 at epoch: 19 and batch_num: 85\n",
      "Loss of test set: 0.45749807357788086 at epoch: 19 and batch_num: 86\n",
      "Loss of test set: 0.533847987651825 at epoch: 19 and batch_num: 87\n",
      "Loss of test set: 0.3397609293460846 at epoch: 19 and batch_num: 88\n",
      "Loss of test set: 0.5049934387207031 at epoch: 19 and batch_num: 89\n",
      "Loss of test set: 0.3569713234901428 at epoch: 19 and batch_num: 90\n",
      "Loss of test set: 0.22053399682044983 at epoch: 19 and batch_num: 91\n",
      "Loss of test set: 0.4283788800239563 at epoch: 19 and batch_num: 92\n",
      "Loss of test set: 0.487134724855423 at epoch: 19 and batch_num: 93\n",
      "Loss of test set: 0.47388339042663574 at epoch: 19 and batch_num: 94\n",
      "Loss of test set: 0.38147151470184326 at epoch: 19 and batch_num: 95\n",
      "Loss of test set: 0.4221194386482239 at epoch: 19 and batch_num: 96\n",
      "Loss of test set: 0.2981634736061096 at epoch: 19 and batch_num: 97\n",
      "Loss of test set: 0.23581260442733765 at epoch: 19 and batch_num: 98\n",
      "Loss of test set: 0.30201655626296997 at epoch: 19 and batch_num: 99\n",
      "Loss of test set: 0.5090331435203552 at epoch: 19 and batch_num: 100\n",
      "Loss of test set: 0.3205721974372864 at epoch: 19 and batch_num: 101\n",
      "Loss of test set: 0.3651309311389923 at epoch: 19 and batch_num: 102\n",
      "Loss of test set: 0.36860448122024536 at epoch: 19 and batch_num: 103\n",
      "Loss of test set: 0.20417875051498413 at epoch: 19 and batch_num: 104\n",
      "Loss of test set: 0.44975391030311584 at epoch: 19 and batch_num: 105\n",
      "Loss of test set: 0.40727710723876953 at epoch: 19 and batch_num: 106\n",
      "Loss of test set: 0.4589827358722687 at epoch: 19 and batch_num: 107\n",
      "Loss of test set: 0.3841876983642578 at epoch: 19 and batch_num: 108\n",
      "Loss of test set: 0.24974989891052246 at epoch: 19 and batch_num: 109\n",
      "Loss of test set: 0.2809903919696808 at epoch: 19 and batch_num: 110\n",
      "Loss of test set: 0.41836732625961304 at epoch: 19 and batch_num: 111\n",
      "Loss of test set: 0.5506987571716309 at epoch: 19 and batch_num: 112\n",
      "Loss of test set: 0.7491916418075562 at epoch: 19 and batch_num: 113\n",
      "Loss of test set: 0.4661942720413208 at epoch: 19 and batch_num: 114\n",
      "Loss of test set: 0.35908931493759155 at epoch: 19 and batch_num: 115\n",
      "Loss of test set: 0.3353481590747833 at epoch: 19 and batch_num: 116\n",
      "Loss of test set: 0.22145475447177887 at epoch: 19 and batch_num: 117\n",
      "Loss of test set: 0.42482900619506836 at epoch: 19 and batch_num: 118\n",
      "Loss of test set: 0.4863602817058563 at epoch: 19 and batch_num: 119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.5276307463645935 at epoch: 19 and batch_num: 120\n",
      "Loss of test set: 0.33185768127441406 at epoch: 19 and batch_num: 121\n",
      "Loss of test set: 0.41602790355682373 at epoch: 19 and batch_num: 122\n",
      "Loss of test set: 0.2965772747993469 at epoch: 19 and batch_num: 123\n",
      "Loss of test set: 0.3381907343864441 at epoch: 19 and batch_num: 124\n",
      "Loss of test set: 0.3818510174751282 at epoch: 19 and batch_num: 125\n",
      "Loss of test set: 0.2620275020599365 at epoch: 19 and batch_num: 126\n",
      "Loss of test set: 0.4449927806854248 at epoch: 19 and batch_num: 127\n",
      "Loss of test set: 0.3797941207885742 at epoch: 19 and batch_num: 128\n",
      "Loss of test set: 0.24281390011310577 at epoch: 19 and batch_num: 129\n",
      "Loss of test set: 0.26380977034568787 at epoch: 19 and batch_num: 130\n",
      "Loss of test set: 0.2933642864227295 at epoch: 19 and batch_num: 131\n",
      "Loss of test set: 0.3664339780807495 at epoch: 19 and batch_num: 132\n",
      "Loss of test set: 0.27491241693496704 at epoch: 19 and batch_num: 133\n",
      "Loss of test set: 0.35533496737480164 at epoch: 19 and batch_num: 134\n",
      "Loss of test set: 0.5404119491577148 at epoch: 19 and batch_num: 135\n",
      "Loss of test set: 0.1862701177597046 at epoch: 19 and batch_num: 136\n",
      "Loss of test set: 0.3217696249485016 at epoch: 19 and batch_num: 137\n",
      "Loss of test set: 0.49325647950172424 at epoch: 19 and batch_num: 138\n",
      "Loss of test set: 0.5446738004684448 at epoch: 19 and batch_num: 139\n",
      "Loss of test set: 0.2835823893547058 at epoch: 19 and batch_num: 140\n",
      "Loss of test set: 0.36456298828125 at epoch: 19 and batch_num: 141\n",
      "Loss of test set: 0.4841070771217346 at epoch: 19 and batch_num: 142\n",
      "Loss of test set: 0.33797311782836914 at epoch: 19 and batch_num: 143\n",
      "Loss of test set: 0.16384384036064148 at epoch: 19 and batch_num: 144\n",
      "Loss of test set: 0.3440333306789398 at epoch: 19 and batch_num: 145\n",
      "Loss of test set: 0.27487635612487793 at epoch: 19 and batch_num: 146\n",
      "Loss of test set: 0.4618954062461853 at epoch: 19 and batch_num: 147\n",
      "Loss of test set: 0.2460935413837433 at epoch: 19 and batch_num: 148\n",
      "Loss of test set: 0.40785062313079834 at epoch: 19 and batch_num: 149\n",
      "Loss of test set: 0.171033576130867 at epoch: 19 and batch_num: 150\n",
      "Loss of test set: 0.39924442768096924 at epoch: 19 and batch_num: 151\n",
      "Loss of test set: 0.4057530164718628 at epoch: 19 and batch_num: 152\n",
      "Loss of test set: 0.23638860881328583 at epoch: 19 and batch_num: 153\n",
      "Loss of test set: 0.41716766357421875 at epoch: 19 and batch_num: 154\n",
      "Loss of test set: 0.2764004170894623 at epoch: 19 and batch_num: 155\n",
      "Loss of test set: 0.5461032390594482 at epoch: 19 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8712\n",
      "Loss of train set: 0.1345944106578827 at epoch: 20 and batch_num: 0\n",
      "Loss of train set: 0.3486992120742798 at epoch: 20 and batch_num: 1\n",
      "Loss of train set: 0.5089566707611084 at epoch: 20 and batch_num: 2\n",
      "Loss of train set: 0.26741039752960205 at epoch: 20 and batch_num: 3\n",
      "Loss of train set: 0.29986441135406494 at epoch: 20 and batch_num: 4\n",
      "Loss of train set: 0.3160075545310974 at epoch: 20 and batch_num: 5\n",
      "Loss of train set: 0.29968926310539246 at epoch: 20 and batch_num: 6\n",
      "Loss of train set: 0.2225273847579956 at epoch: 20 and batch_num: 7\n",
      "Loss of train set: 0.20424878597259521 at epoch: 20 and batch_num: 8\n",
      "Loss of train set: 0.2571154236793518 at epoch: 20 and batch_num: 9\n",
      "Loss of train set: 0.21460573375225067 at epoch: 20 and batch_num: 10\n",
      "Loss of train set: 0.20759078860282898 at epoch: 20 and batch_num: 11\n",
      "Loss of train set: 0.428244411945343 at epoch: 20 and batch_num: 12\n",
      "Loss of train set: 0.34890633821487427 at epoch: 20 and batch_num: 13\n",
      "Loss of train set: 0.33957362174987793 at epoch: 20 and batch_num: 14\n",
      "Loss of train set: 0.2861673831939697 at epoch: 20 and batch_num: 15\n",
      "Loss of train set: 0.29480230808258057 at epoch: 20 and batch_num: 16\n",
      "Loss of train set: 0.28769099712371826 at epoch: 20 and batch_num: 17\n",
      "Loss of train set: 0.22175736725330353 at epoch: 20 and batch_num: 18\n",
      "Loss of train set: 0.26631835103034973 at epoch: 20 and batch_num: 19\n",
      "Loss of train set: 0.23074094951152802 at epoch: 20 and batch_num: 20\n",
      "Loss of train set: 0.3233814537525177 at epoch: 20 and batch_num: 21\n",
      "Loss of train set: 0.33584997057914734 at epoch: 20 and batch_num: 22\n",
      "Loss of train set: 0.2657577097415924 at epoch: 20 and batch_num: 23\n",
      "Loss of train set: 0.1803884506225586 at epoch: 20 and batch_num: 24\n",
      "Loss of train set: 0.4349490702152252 at epoch: 20 and batch_num: 25\n",
      "Loss of train set: 0.4203043580055237 at epoch: 20 and batch_num: 26\n",
      "Loss of train set: 0.337402880191803 at epoch: 20 and batch_num: 27\n",
      "Loss of train set: 0.25798681378364563 at epoch: 20 and batch_num: 28\n",
      "Loss of train set: 0.2939872741699219 at epoch: 20 and batch_num: 29\n",
      "Loss of train set: 0.5157886743545532 at epoch: 20 and batch_num: 30\n",
      "Loss of train set: 0.33492061495780945 at epoch: 20 and batch_num: 31\n",
      "Loss of train set: 0.5302047729492188 at epoch: 20 and batch_num: 32\n",
      "Loss of train set: 0.33772680163383484 at epoch: 20 and batch_num: 33\n",
      "Loss of train set: 0.2909032702445984 at epoch: 20 and batch_num: 34\n",
      "Loss of train set: 0.17928919196128845 at epoch: 20 and batch_num: 35\n",
      "Loss of train set: 0.2204776257276535 at epoch: 20 and batch_num: 36\n",
      "Loss of train set: 0.2586113214492798 at epoch: 20 and batch_num: 37\n",
      "Loss of train set: 0.2764197885990143 at epoch: 20 and batch_num: 38\n",
      "Loss of train set: 0.23439544439315796 at epoch: 20 and batch_num: 39\n",
      "Loss of train set: 0.2662366032600403 at epoch: 20 and batch_num: 40\n",
      "Loss of train set: 0.17628446221351624 at epoch: 20 and batch_num: 41\n",
      "Loss of train set: 0.2465587854385376 at epoch: 20 and batch_num: 42\n",
      "Loss of train set: 0.263868510723114 at epoch: 20 and batch_num: 43\n",
      "Loss of train set: 0.22658509016036987 at epoch: 20 and batch_num: 44\n",
      "Loss of train set: 0.2333463430404663 at epoch: 20 and batch_num: 45\n",
      "Loss of train set: 0.2556491494178772 at epoch: 20 and batch_num: 46\n",
      "Loss of train set: 0.3825302720069885 at epoch: 20 and batch_num: 47\n",
      "Loss of train set: 0.2723524570465088 at epoch: 20 and batch_num: 48\n",
      "Loss of train set: 0.296785831451416 at epoch: 20 and batch_num: 49\n",
      "Loss of train set: 0.25652241706848145 at epoch: 20 and batch_num: 50\n",
      "Loss of train set: 0.25166305899620056 at epoch: 20 and batch_num: 51\n",
      "Loss of train set: 0.2762342691421509 at epoch: 20 and batch_num: 52\n",
      "Loss of train set: 0.2036433219909668 at epoch: 20 and batch_num: 53\n",
      "Loss of train set: 0.29884639382362366 at epoch: 20 and batch_num: 54\n",
      "Loss of train set: 0.213019460439682 at epoch: 20 and batch_num: 55\n",
      "Loss of train set: 0.2915703058242798 at epoch: 20 and batch_num: 56\n",
      "Loss of train set: 0.2532989978790283 at epoch: 20 and batch_num: 57\n",
      "Loss of train set: 0.19802868366241455 at epoch: 20 and batch_num: 58\n",
      "Loss of train set: 0.15871062874794006 at epoch: 20 and batch_num: 59\n",
      "Loss of train set: 0.17017993330955505 at epoch: 20 and batch_num: 60\n",
      "Loss of train set: 0.32809942960739136 at epoch: 20 and batch_num: 61\n",
      "Loss of train set: 0.201850026845932 at epoch: 20 and batch_num: 62\n",
      "Loss of train set: 0.22635620832443237 at epoch: 20 and batch_num: 63\n",
      "Loss of train set: 0.16074924170970917 at epoch: 20 and batch_num: 64\n",
      "Loss of train set: 0.1762687861919403 at epoch: 20 and batch_num: 65\n",
      "Loss of train set: 0.1911691576242447 at epoch: 20 and batch_num: 66\n",
      "Loss of train set: 0.19990548491477966 at epoch: 20 and batch_num: 67\n",
      "Loss of train set: 0.1807456612586975 at epoch: 20 and batch_num: 68\n",
      "Loss of train set: 0.22671794891357422 at epoch: 20 and batch_num: 69\n",
      "Loss of train set: 0.17402291297912598 at epoch: 20 and batch_num: 70\n",
      "Loss of train set: 0.3371564447879791 at epoch: 20 and batch_num: 71\n",
      "Loss of train set: 0.20284049212932587 at epoch: 20 and batch_num: 72\n",
      "Loss of train set: 0.2528237998485565 at epoch: 20 and batch_num: 73\n",
      "Loss of train set: 0.2521085739135742 at epoch: 20 and batch_num: 74\n",
      "Loss of train set: 0.34321317076683044 at epoch: 20 and batch_num: 75\n",
      "Loss of train set: 0.3904680013656616 at epoch: 20 and batch_num: 76\n",
      "Loss of train set: 0.22598391771316528 at epoch: 20 and batch_num: 77\n",
      "Loss of train set: 0.23205706477165222 at epoch: 20 and batch_num: 78\n",
      "Loss of train set: 0.3591737747192383 at epoch: 20 and batch_num: 79\n",
      "Loss of train set: 0.25750190019607544 at epoch: 20 and batch_num: 80\n",
      "Loss of train set: 0.19124361872673035 at epoch: 20 and batch_num: 81\n",
      "Loss of train set: 0.3904270529747009 at epoch: 20 and batch_num: 82\n",
      "Loss of train set: 0.2729262113571167 at epoch: 20 and batch_num: 83\n",
      "Loss of train set: 0.33250659704208374 at epoch: 20 and batch_num: 84\n",
      "Loss of train set: 0.4680213928222656 at epoch: 20 and batch_num: 85\n",
      "Loss of train set: 0.26821884512901306 at epoch: 20 and batch_num: 86\n",
      "Loss of train set: 0.2737042307853699 at epoch: 20 and batch_num: 87\n",
      "Loss of train set: 0.30270928144454956 at epoch: 20 and batch_num: 88\n",
      "Loss of train set: 0.17895270884037018 at epoch: 20 and batch_num: 89\n",
      "Loss of train set: 0.18606092035770416 at epoch: 20 and batch_num: 90\n",
      "Loss of train set: 0.3007621765136719 at epoch: 20 and batch_num: 91\n",
      "Loss of train set: 0.27799007296562195 at epoch: 20 and batch_num: 92\n",
      "Loss of train set: 0.24047473073005676 at epoch: 20 and batch_num: 93\n",
      "Loss of train set: 0.2833813726902008 at epoch: 20 and batch_num: 94\n",
      "Loss of train set: 0.41962745785713196 at epoch: 20 and batch_num: 95\n",
      "Loss of train set: 0.3422943353652954 at epoch: 20 and batch_num: 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.31787946820259094 at epoch: 20 and batch_num: 97\n",
      "Loss of train set: 0.23783659934997559 at epoch: 20 and batch_num: 98\n",
      "Loss of train set: 0.2664623260498047 at epoch: 20 and batch_num: 99\n",
      "Loss of train set: 0.16268165409564972 at epoch: 20 and batch_num: 100\n",
      "Loss of train set: 0.22974634170532227 at epoch: 20 and batch_num: 101\n",
      "Loss of train set: 0.13302768766880035 at epoch: 20 and batch_num: 102\n",
      "Loss of train set: 0.22642195224761963 at epoch: 20 and batch_num: 103\n",
      "Loss of train set: 0.49186694622039795 at epoch: 20 and batch_num: 104\n",
      "Loss of train set: 0.2602503001689911 at epoch: 20 and batch_num: 105\n",
      "Loss of train set: 0.2931278944015503 at epoch: 20 and batch_num: 106\n",
      "Loss of train set: 0.32355403900146484 at epoch: 20 and batch_num: 107\n",
      "Loss of train set: 0.3027508556842804 at epoch: 20 and batch_num: 108\n",
      "Loss of train set: 0.2943684160709381 at epoch: 20 and batch_num: 109\n",
      "Loss of train set: 0.3253023028373718 at epoch: 20 and batch_num: 110\n",
      "Loss of train set: 0.2329225391149521 at epoch: 20 and batch_num: 111\n",
      "Loss of train set: 0.19136694073677063 at epoch: 20 and batch_num: 112\n",
      "Loss of train set: 0.1944904774427414 at epoch: 20 and batch_num: 113\n",
      "Loss of train set: 0.35599783062934875 at epoch: 20 and batch_num: 114\n",
      "Loss of train set: 0.299315482378006 at epoch: 20 and batch_num: 115\n",
      "Loss of train set: 0.27947571873664856 at epoch: 20 and batch_num: 116\n",
      "Loss of train set: 0.28915855288505554 at epoch: 20 and batch_num: 117\n",
      "Loss of train set: 0.42539680004119873 at epoch: 20 and batch_num: 118\n",
      "Loss of train set: 0.2672044038772583 at epoch: 20 and batch_num: 119\n",
      "Loss of train set: 0.17837920784950256 at epoch: 20 and batch_num: 120\n",
      "Loss of train set: 0.2829940617084503 at epoch: 20 and batch_num: 121\n",
      "Loss of train set: 0.2031780183315277 at epoch: 20 and batch_num: 122\n",
      "Loss of train set: 0.21737077832221985 at epoch: 20 and batch_num: 123\n",
      "Loss of train set: 0.26403775811195374 at epoch: 20 and batch_num: 124\n",
      "Loss of train set: 0.335574209690094 at epoch: 20 and batch_num: 125\n",
      "Loss of train set: 0.1937440037727356 at epoch: 20 and batch_num: 126\n",
      "Loss of train set: 0.2514089345932007 at epoch: 20 and batch_num: 127\n",
      "Loss of train set: 0.4473411440849304 at epoch: 20 and batch_num: 128\n",
      "Loss of train set: 0.283995121717453 at epoch: 20 and batch_num: 129\n",
      "Loss of train set: 0.2233918309211731 at epoch: 20 and batch_num: 130\n",
      "Loss of train set: 0.3613407015800476 at epoch: 20 and batch_num: 131\n",
      "Loss of train set: 0.14786462485790253 at epoch: 20 and batch_num: 132\n",
      "Loss of train set: 0.2744448781013489 at epoch: 20 and batch_num: 133\n",
      "Loss of train set: 0.24232697486877441 at epoch: 20 and batch_num: 134\n",
      "Loss of train set: 0.47985726594924927 at epoch: 20 and batch_num: 135\n",
      "Loss of train set: 0.3157365918159485 at epoch: 20 and batch_num: 136\n",
      "Loss of train set: 0.2350696623325348 at epoch: 20 and batch_num: 137\n",
      "Loss of train set: 0.44786325097084045 at epoch: 20 and batch_num: 138\n",
      "Loss of train set: 0.2697864770889282 at epoch: 20 and batch_num: 139\n",
      "Loss of train set: 0.18269623816013336 at epoch: 20 and batch_num: 140\n",
      "Loss of train set: 0.22810810804367065 at epoch: 20 and batch_num: 141\n",
      "Loss of train set: 0.15048718452453613 at epoch: 20 and batch_num: 142\n",
      "Loss of train set: 0.273160457611084 at epoch: 20 and batch_num: 143\n",
      "Loss of train set: 0.3159867525100708 at epoch: 20 and batch_num: 144\n",
      "Loss of train set: 0.1702468991279602 at epoch: 20 and batch_num: 145\n",
      "Loss of train set: 0.2505524456501007 at epoch: 20 and batch_num: 146\n",
      "Loss of train set: 0.14246252179145813 at epoch: 20 and batch_num: 147\n",
      "Loss of train set: 0.3328549265861511 at epoch: 20 and batch_num: 148\n",
      "Loss of train set: 0.4494929611682892 at epoch: 20 and batch_num: 149\n",
      "Loss of train set: 0.3141579031944275 at epoch: 20 and batch_num: 150\n",
      "Loss of train set: 0.3759152889251709 at epoch: 20 and batch_num: 151\n",
      "Loss of train set: 0.2295452356338501 at epoch: 20 and batch_num: 152\n",
      "Loss of train set: 0.2989765703678131 at epoch: 20 and batch_num: 153\n",
      "Loss of train set: 0.20580795407295227 at epoch: 20 and batch_num: 154\n",
      "Loss of train set: 0.29860377311706543 at epoch: 20 and batch_num: 155\n",
      "Loss of train set: 0.2961200475692749 at epoch: 20 and batch_num: 156\n",
      "Loss of train set: 0.22172634303569794 at epoch: 20 and batch_num: 157\n",
      "Loss of train set: 0.352749764919281 at epoch: 20 and batch_num: 158\n",
      "Loss of train set: 0.24500541388988495 at epoch: 20 and batch_num: 159\n",
      "Loss of train set: 0.1323586404323578 at epoch: 20 and batch_num: 160\n",
      "Loss of train set: 0.22518156468868256 at epoch: 20 and batch_num: 161\n",
      "Loss of train set: 0.15163370966911316 at epoch: 20 and batch_num: 162\n",
      "Loss of train set: 0.19393113255500793 at epoch: 20 and batch_num: 163\n",
      "Loss of train set: 0.31592023372650146 at epoch: 20 and batch_num: 164\n",
      "Loss of train set: 0.19466768205165863 at epoch: 20 and batch_num: 165\n",
      "Loss of train set: 0.32589399814605713 at epoch: 20 and batch_num: 166\n",
      "Loss of train set: 0.299982488155365 at epoch: 20 and batch_num: 167\n",
      "Loss of train set: 0.20938079059123993 at epoch: 20 and batch_num: 168\n",
      "Loss of train set: 0.3927452266216278 at epoch: 20 and batch_num: 169\n",
      "Loss of train set: 0.3290133476257324 at epoch: 20 and batch_num: 170\n",
      "Loss of train set: 0.2483912855386734 at epoch: 20 and batch_num: 171\n",
      "Loss of train set: 0.2602633237838745 at epoch: 20 and batch_num: 172\n",
      "Loss of train set: 0.22008967399597168 at epoch: 20 and batch_num: 173\n",
      "Loss of train set: 0.320155531167984 at epoch: 20 and batch_num: 174\n",
      "Loss of train set: 0.5327346324920654 at epoch: 20 and batch_num: 175\n",
      "Loss of train set: 0.34935277700424194 at epoch: 20 and batch_num: 176\n",
      "Loss of train set: 0.2482108622789383 at epoch: 20 and batch_num: 177\n",
      "Loss of train set: 0.23286154866218567 at epoch: 20 and batch_num: 178\n",
      "Loss of train set: 0.25254514813423157 at epoch: 20 and batch_num: 179\n",
      "Loss of train set: 0.34470492601394653 at epoch: 20 and batch_num: 180\n",
      "Loss of train set: 0.31778132915496826 at epoch: 20 and batch_num: 181\n",
      "Loss of train set: 0.4060063362121582 at epoch: 20 and batch_num: 182\n",
      "Loss of train set: 0.13074709475040436 at epoch: 20 and batch_num: 183\n",
      "Loss of train set: 0.26651883125305176 at epoch: 20 and batch_num: 184\n",
      "Loss of train set: 0.27877306938171387 at epoch: 20 and batch_num: 185\n",
      "Loss of train set: 0.4728619456291199 at epoch: 20 and batch_num: 186\n",
      "Loss of train set: 0.20211327075958252 at epoch: 20 and batch_num: 187\n",
      "Loss of train set: 0.2989310324192047 at epoch: 20 and batch_num: 188\n",
      "Loss of train set: 0.14260682463645935 at epoch: 20 and batch_num: 189\n",
      "Loss of train set: 0.3353191614151001 at epoch: 20 and batch_num: 190\n",
      "Loss of train set: 0.29305601119995117 at epoch: 20 and batch_num: 191\n",
      "Loss of train set: 0.32429060339927673 at epoch: 20 and batch_num: 192\n",
      "Loss of train set: 0.33165472745895386 at epoch: 20 and batch_num: 193\n",
      "Loss of train set: 0.3264726996421814 at epoch: 20 and batch_num: 194\n",
      "Loss of train set: 0.31214648485183716 at epoch: 20 and batch_num: 195\n",
      "Loss of train set: 0.26358193159103394 at epoch: 20 and batch_num: 196\n",
      "Loss of train set: 0.2950311601161957 at epoch: 20 and batch_num: 197\n",
      "Loss of train set: 0.3347376585006714 at epoch: 20 and batch_num: 198\n",
      "Loss of train set: 0.15611714124679565 at epoch: 20 and batch_num: 199\n",
      "Loss of train set: 0.15225298702716827 at epoch: 20 and batch_num: 200\n",
      "Loss of train set: 0.3157046437263489 at epoch: 20 and batch_num: 201\n",
      "Loss of train set: 0.1403898447751999 at epoch: 20 and batch_num: 202\n",
      "Loss of train set: 0.31033143401145935 at epoch: 20 and batch_num: 203\n",
      "Loss of train set: 0.23410038650035858 at epoch: 20 and batch_num: 204\n",
      "Loss of train set: 0.27402254939079285 at epoch: 20 and batch_num: 205\n",
      "Loss of train set: 0.301464319229126 at epoch: 20 and batch_num: 206\n",
      "Loss of train set: 0.13120126724243164 at epoch: 20 and batch_num: 207\n",
      "Loss of train set: 0.2549455463886261 at epoch: 20 and batch_num: 208\n",
      "Loss of train set: 0.3042095899581909 at epoch: 20 and batch_num: 209\n",
      "Loss of train set: 0.19734343886375427 at epoch: 20 and batch_num: 210\n",
      "Loss of train set: 0.20105452835559845 at epoch: 20 and batch_num: 211\n",
      "Loss of train set: 0.2978796660900116 at epoch: 20 and batch_num: 212\n",
      "Loss of train set: 0.2956346869468689 at epoch: 20 and batch_num: 213\n",
      "Loss of train set: 0.22349813580513 at epoch: 20 and batch_num: 214\n",
      "Loss of train set: 0.4510909914970398 at epoch: 20 and batch_num: 215\n",
      "Loss of train set: 0.17742609977722168 at epoch: 20 and batch_num: 216\n",
      "Loss of train set: 0.2882305383682251 at epoch: 20 and batch_num: 217\n",
      "Loss of train set: 0.33426517248153687 at epoch: 20 and batch_num: 218\n",
      "Loss of train set: 0.2887685298919678 at epoch: 20 and batch_num: 219\n",
      "Loss of train set: 0.34874773025512695 at epoch: 20 and batch_num: 220\n",
      "Loss of train set: 0.2918941378593445 at epoch: 20 and batch_num: 221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2529609203338623 at epoch: 20 and batch_num: 222\n",
      "Loss of train set: 0.2395818680524826 at epoch: 20 and batch_num: 223\n",
      "Loss of train set: 0.3090066909790039 at epoch: 20 and batch_num: 224\n",
      "Loss of train set: 0.28735804557800293 at epoch: 20 and batch_num: 225\n",
      "Loss of train set: 0.34319230914115906 at epoch: 20 and batch_num: 226\n",
      "Loss of train set: 0.21403560042381287 at epoch: 20 and batch_num: 227\n",
      "Loss of train set: 0.309259831905365 at epoch: 20 and batch_num: 228\n",
      "Loss of train set: 0.6779534816741943 at epoch: 20 and batch_num: 229\n",
      "Loss of train set: 0.302526593208313 at epoch: 20 and batch_num: 230\n",
      "Loss of train set: 0.2695809602737427 at epoch: 20 and batch_num: 231\n",
      "Loss of train set: 0.2865462601184845 at epoch: 20 and batch_num: 232\n",
      "Loss of train set: 0.30655258893966675 at epoch: 20 and batch_num: 233\n",
      "Loss of train set: 0.36676737666130066 at epoch: 20 and batch_num: 234\n",
      "Loss of train set: 0.2758669853210449 at epoch: 20 and batch_num: 235\n",
      "Loss of train set: 0.1940850168466568 at epoch: 20 and batch_num: 236\n",
      "Loss of train set: 0.2739029824733734 at epoch: 20 and batch_num: 237\n",
      "Loss of train set: 0.22696825861930847 at epoch: 20 and batch_num: 238\n",
      "Loss of train set: 0.21258291602134705 at epoch: 20 and batch_num: 239\n",
      "Loss of train set: 0.3800644874572754 at epoch: 20 and batch_num: 240\n",
      "Loss of train set: 0.21642163395881653 at epoch: 20 and batch_num: 241\n",
      "Loss of train set: 0.34837988018989563 at epoch: 20 and batch_num: 242\n",
      "Loss of train set: 0.30943813920021057 at epoch: 20 and batch_num: 243\n",
      "Loss of train set: 0.2576483190059662 at epoch: 20 and batch_num: 244\n",
      "Loss of train set: 0.17676013708114624 at epoch: 20 and batch_num: 245\n",
      "Loss of train set: 0.2678576707839966 at epoch: 20 and batch_num: 246\n",
      "Loss of train set: 0.2823515236377716 at epoch: 20 and batch_num: 247\n",
      "Loss of train set: 0.2840121388435364 at epoch: 20 and batch_num: 248\n",
      "Loss of train set: 0.25804468989372253 at epoch: 20 and batch_num: 249\n",
      "Loss of train set: 0.2301916778087616 at epoch: 20 and batch_num: 250\n",
      "Loss of train set: 0.4329114854335785 at epoch: 20 and batch_num: 251\n",
      "Loss of train set: 0.33002904057502747 at epoch: 20 and batch_num: 252\n",
      "Loss of train set: 0.27151137590408325 at epoch: 20 and batch_num: 253\n",
      "Loss of train set: 0.1266838014125824 at epoch: 20 and batch_num: 254\n",
      "Loss of train set: 0.24871991574764252 at epoch: 20 and batch_num: 255\n",
      "Loss of train set: 0.2416040301322937 at epoch: 20 and batch_num: 256\n",
      "Loss of train set: 0.46829110383987427 at epoch: 20 and batch_num: 257\n",
      "Loss of train set: 0.39154064655303955 at epoch: 20 and batch_num: 258\n",
      "Loss of train set: 0.27539780735969543 at epoch: 20 and batch_num: 259\n",
      "Loss of train set: 0.2799617052078247 at epoch: 20 and batch_num: 260\n",
      "Loss of train set: 0.11905979365110397 at epoch: 20 and batch_num: 261\n",
      "Loss of train set: 0.2632625102996826 at epoch: 20 and batch_num: 262\n",
      "Loss of train set: 0.49364495277404785 at epoch: 20 and batch_num: 263\n",
      "Loss of train set: 0.23258444666862488 at epoch: 20 and batch_num: 264\n",
      "Loss of train set: 0.22729110717773438 at epoch: 20 and batch_num: 265\n",
      "Loss of train set: 0.47210508584976196 at epoch: 20 and batch_num: 266\n",
      "Loss of train set: 0.35449910163879395 at epoch: 20 and batch_num: 267\n",
      "Loss of train set: 0.11893804371356964 at epoch: 20 and batch_num: 268\n",
      "Loss of train set: 0.50295090675354 at epoch: 20 and batch_num: 269\n",
      "Loss of train set: 0.2041025310754776 at epoch: 20 and batch_num: 270\n",
      "Loss of train set: 0.17419934272766113 at epoch: 20 and batch_num: 271\n",
      "Loss of train set: 0.3129325807094574 at epoch: 20 and batch_num: 272\n",
      "Loss of train set: 0.2058924436569214 at epoch: 20 and batch_num: 273\n",
      "Loss of train set: 0.2360418140888214 at epoch: 20 and batch_num: 274\n",
      "Loss of train set: 0.4414365291595459 at epoch: 20 and batch_num: 275\n",
      "Loss of train set: 0.2971910238265991 at epoch: 20 and batch_num: 276\n",
      "Loss of train set: 0.34120237827301025 at epoch: 20 and batch_num: 277\n",
      "Loss of train set: 0.2130041867494583 at epoch: 20 and batch_num: 278\n",
      "Loss of train set: 0.21407881379127502 at epoch: 20 and batch_num: 279\n",
      "Loss of train set: 0.35979458689689636 at epoch: 20 and batch_num: 280\n",
      "Loss of train set: 0.45873790979385376 at epoch: 20 and batch_num: 281\n",
      "Loss of train set: 0.43191051483154297 at epoch: 20 and batch_num: 282\n",
      "Loss of train set: 0.16713079810142517 at epoch: 20 and batch_num: 283\n",
      "Loss of train set: 0.13816124200820923 at epoch: 20 and batch_num: 284\n",
      "Loss of train set: 0.3736087679862976 at epoch: 20 and batch_num: 285\n",
      "Loss of train set: 0.2068476378917694 at epoch: 20 and batch_num: 286\n",
      "Loss of train set: 0.31694915890693665 at epoch: 20 and batch_num: 287\n",
      "Loss of train set: 0.1966468095779419 at epoch: 20 and batch_num: 288\n",
      "Loss of train set: 0.31064844131469727 at epoch: 20 and batch_num: 289\n",
      "Loss of train set: 0.28826501965522766 at epoch: 20 and batch_num: 290\n",
      "Loss of train set: 0.339966744184494 at epoch: 20 and batch_num: 291\n",
      "Loss of train set: 0.23884908854961395 at epoch: 20 and batch_num: 292\n",
      "Loss of train set: 0.32460880279541016 at epoch: 20 and batch_num: 293\n",
      "Loss of train set: 0.32142168283462524 at epoch: 20 and batch_num: 294\n",
      "Loss of train set: 0.16201293468475342 at epoch: 20 and batch_num: 295\n",
      "Loss of train set: 0.34417924284935 at epoch: 20 and batch_num: 296\n",
      "Loss of train set: 0.27695971727371216 at epoch: 20 and batch_num: 297\n",
      "Loss of train set: 0.2714827060699463 at epoch: 20 and batch_num: 298\n",
      "Loss of train set: 0.23463751375675201 at epoch: 20 and batch_num: 299\n",
      "Loss of train set: 0.3611946403980255 at epoch: 20 and batch_num: 300\n",
      "Loss of train set: 0.13365530967712402 at epoch: 20 and batch_num: 301\n",
      "Loss of train set: 0.09520113468170166 at epoch: 20 and batch_num: 302\n",
      "Loss of train set: 0.2402774691581726 at epoch: 20 and batch_num: 303\n",
      "Loss of train set: 0.29107415676116943 at epoch: 20 and batch_num: 304\n",
      "Loss of train set: 0.3262324035167694 at epoch: 20 and batch_num: 305\n",
      "Loss of train set: 0.10867995023727417 at epoch: 20 and batch_num: 306\n",
      "Loss of train set: 0.1434268355369568 at epoch: 20 and batch_num: 307\n",
      "Loss of train set: 0.28653839230537415 at epoch: 20 and batch_num: 308\n",
      "Loss of train set: 0.24698194861412048 at epoch: 20 and batch_num: 309\n",
      "Loss of train set: 0.2270265817642212 at epoch: 20 and batch_num: 310\n",
      "Loss of train set: 0.3907436728477478 at epoch: 20 and batch_num: 311\n",
      "Loss of train set: 0.2858257293701172 at epoch: 20 and batch_num: 312\n",
      "Loss of train set: 0.21528734266757965 at epoch: 20 and batch_num: 313\n",
      "Loss of train set: 0.2550922930240631 at epoch: 20 and batch_num: 314\n",
      "Loss of train set: 0.19750088453292847 at epoch: 20 and batch_num: 315\n",
      "Loss of train set: 0.2980943024158478 at epoch: 20 and batch_num: 316\n",
      "Loss of train set: 0.18694384396076202 at epoch: 20 and batch_num: 317\n",
      "Loss of train set: 0.3203696012496948 at epoch: 20 and batch_num: 318\n",
      "Loss of train set: 0.2954920530319214 at epoch: 20 and batch_num: 319\n",
      "Loss of train set: 0.4541734457015991 at epoch: 20 and batch_num: 320\n",
      "Loss of train set: 0.25669506192207336 at epoch: 20 and batch_num: 321\n",
      "Loss of train set: 0.34407302737236023 at epoch: 20 and batch_num: 322\n",
      "Loss of train set: 0.2619835138320923 at epoch: 20 and batch_num: 323\n",
      "Loss of train set: 0.3247597813606262 at epoch: 20 and batch_num: 324\n",
      "Loss of train set: 0.28012827038764954 at epoch: 20 and batch_num: 325\n",
      "Loss of train set: 0.4085826873779297 at epoch: 20 and batch_num: 326\n",
      "Loss of train set: 0.1859746277332306 at epoch: 20 and batch_num: 327\n",
      "Loss of train set: 0.2688753306865692 at epoch: 20 and batch_num: 328\n",
      "Loss of train set: 0.37620028853416443 at epoch: 20 and batch_num: 329\n",
      "Loss of train set: 0.27309733629226685 at epoch: 20 and batch_num: 330\n",
      "Loss of train set: 0.2776132822036743 at epoch: 20 and batch_num: 331\n",
      "Loss of train set: 0.41035979986190796 at epoch: 20 and batch_num: 332\n",
      "Loss of train set: 0.1722993552684784 at epoch: 20 and batch_num: 333\n",
      "Loss of train set: 0.27056100964546204 at epoch: 20 and batch_num: 334\n",
      "Loss of train set: 0.3217467665672302 at epoch: 20 and batch_num: 335\n",
      "Loss of train set: 0.3100702166557312 at epoch: 20 and batch_num: 336\n",
      "Loss of train set: 0.23594090342521667 at epoch: 20 and batch_num: 337\n",
      "Loss of train set: 0.3461674451828003 at epoch: 20 and batch_num: 338\n",
      "Loss of train set: 0.26589590311050415 at epoch: 20 and batch_num: 339\n",
      "Loss of train set: 0.42448747158050537 at epoch: 20 and batch_num: 340\n",
      "Loss of train set: 0.3142698407173157 at epoch: 20 and batch_num: 341\n",
      "Loss of train set: 0.1789310872554779 at epoch: 20 and batch_num: 342\n",
      "Loss of train set: 0.1761772334575653 at epoch: 20 and batch_num: 343\n",
      "Loss of train set: 0.29081299901008606 at epoch: 20 and batch_num: 344\n",
      "Loss of train set: 0.24030092358589172 at epoch: 20 and batch_num: 345\n",
      "Loss of train set: 0.561322033405304 at epoch: 20 and batch_num: 346\n",
      "Loss of train set: 0.1615816056728363 at epoch: 20 and batch_num: 347\n",
      "Loss of train set: 0.26707860827445984 at epoch: 20 and batch_num: 348\n",
      "Loss of train set: 0.4499114453792572 at epoch: 20 and batch_num: 349\n",
      "Loss of train set: 0.3402637243270874 at epoch: 20 and batch_num: 350\n",
      "Loss of train set: 0.30222606658935547 at epoch: 20 and batch_num: 351\n",
      "Loss of train set: 0.22100727260112762 at epoch: 20 and batch_num: 352\n",
      "Loss of train set: 0.30862390995025635 at epoch: 20 and batch_num: 353\n",
      "Loss of train set: 0.2124112993478775 at epoch: 20 and batch_num: 354\n",
      "Loss of train set: 0.2690981924533844 at epoch: 20 and batch_num: 355\n",
      "Loss of train set: 0.18554168939590454 at epoch: 20 and batch_num: 356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.25450223684310913 at epoch: 20 and batch_num: 357\n",
      "Loss of train set: 0.2373473048210144 at epoch: 20 and batch_num: 358\n",
      "Loss of train set: 0.39908987283706665 at epoch: 20 and batch_num: 359\n",
      "Loss of train set: 0.29086315631866455 at epoch: 20 and batch_num: 360\n",
      "Loss of train set: 0.40677595138549805 at epoch: 20 and batch_num: 361\n",
      "Loss of train set: 0.2962682843208313 at epoch: 20 and batch_num: 362\n",
      "Loss of train set: 0.13484802842140198 at epoch: 20 and batch_num: 363\n",
      "Loss of train set: 0.39609262347221375 at epoch: 20 and batch_num: 364\n",
      "Loss of train set: 0.325824499130249 at epoch: 20 and batch_num: 365\n",
      "Loss of train set: 0.22611764073371887 at epoch: 20 and batch_num: 366\n",
      "Loss of train set: 0.23680047690868378 at epoch: 20 and batch_num: 367\n",
      "Loss of train set: 0.36232030391693115 at epoch: 20 and batch_num: 368\n",
      "Loss of train set: 0.23778016865253448 at epoch: 20 and batch_num: 369\n",
      "Loss of train set: 0.4357544481754303 at epoch: 20 and batch_num: 370\n",
      "Loss of train set: 0.28952622413635254 at epoch: 20 and batch_num: 371\n",
      "Loss of train set: 0.3550049066543579 at epoch: 20 and batch_num: 372\n",
      "Loss of train set: 0.38092994689941406 at epoch: 20 and batch_num: 373\n",
      "Loss of train set: 0.17418186366558075 at epoch: 20 and batch_num: 374\n",
      "Loss of train set: 0.2690708637237549 at epoch: 20 and batch_num: 375\n",
      "Loss of train set: 0.2571870684623718 at epoch: 20 and batch_num: 376\n",
      "Loss of train set: 0.2352045178413391 at epoch: 20 and batch_num: 377\n",
      "Loss of train set: 0.23219585418701172 at epoch: 20 and batch_num: 378\n",
      "Loss of train set: 0.2707598805427551 at epoch: 20 and batch_num: 379\n",
      "Loss of train set: 0.21122220158576965 at epoch: 20 and batch_num: 380\n",
      "Loss of train set: 0.14197328686714172 at epoch: 20 and batch_num: 381\n",
      "Loss of train set: 0.341316819190979 at epoch: 20 and batch_num: 382\n",
      "Loss of train set: 0.38943997025489807 at epoch: 20 and batch_num: 383\n",
      "Loss of train set: 0.37327662110328674 at epoch: 20 and batch_num: 384\n",
      "Loss of train set: 0.22587928175926208 at epoch: 20 and batch_num: 385\n",
      "Loss of train set: 0.29635003209114075 at epoch: 20 and batch_num: 386\n",
      "Loss of train set: 0.3445241451263428 at epoch: 20 and batch_num: 387\n",
      "Loss of train set: 0.266215443611145 at epoch: 20 and batch_num: 388\n",
      "Loss of train set: 0.33676183223724365 at epoch: 20 and batch_num: 389\n",
      "Loss of train set: 0.2878895699977875 at epoch: 20 and batch_num: 390\n",
      "Loss of train set: 0.7102789282798767 at epoch: 20 and batch_num: 391\n",
      "Loss of train set: 0.25524353981018066 at epoch: 20 and batch_num: 392\n",
      "Loss of train set: 0.304756224155426 at epoch: 20 and batch_num: 393\n",
      "Loss of train set: 0.20497757196426392 at epoch: 20 and batch_num: 394\n",
      "Loss of train set: 0.19392769038677216 at epoch: 20 and batch_num: 395\n",
      "Loss of train set: 0.2780382037162781 at epoch: 20 and batch_num: 396\n",
      "Loss of train set: 0.3899513781070709 at epoch: 20 and batch_num: 397\n",
      "Loss of train set: 0.23133227229118347 at epoch: 20 and batch_num: 398\n",
      "Loss of train set: 0.40473300218582153 at epoch: 20 and batch_num: 399\n",
      "Loss of train set: 0.26439571380615234 at epoch: 20 and batch_num: 400\n",
      "Loss of train set: 0.2578754127025604 at epoch: 20 and batch_num: 401\n",
      "Loss of train set: 0.35233286023139954 at epoch: 20 and batch_num: 402\n",
      "Loss of train set: 0.26505863666534424 at epoch: 20 and batch_num: 403\n",
      "Loss of train set: 0.22301791608333588 at epoch: 20 and batch_num: 404\n",
      "Loss of train set: 0.23143306374549866 at epoch: 20 and batch_num: 405\n",
      "Loss of train set: 0.2725193500518799 at epoch: 20 and batch_num: 406\n",
      "Loss of train set: 0.3902803361415863 at epoch: 20 and batch_num: 407\n",
      "Loss of train set: 0.19746147096157074 at epoch: 20 and batch_num: 408\n",
      "Loss of train set: 0.1958974450826645 at epoch: 20 and batch_num: 409\n",
      "Loss of train set: 0.4311951994895935 at epoch: 20 and batch_num: 410\n",
      "Loss of train set: 0.2802080810070038 at epoch: 20 and batch_num: 411\n",
      "Loss of train set: 0.31274014711380005 at epoch: 20 and batch_num: 412\n",
      "Loss of train set: 0.4327293038368225 at epoch: 20 and batch_num: 413\n",
      "Loss of train set: 0.28456589579582214 at epoch: 20 and batch_num: 414\n",
      "Loss of train set: 0.33700206875801086 at epoch: 20 and batch_num: 415\n",
      "Loss of train set: 0.3109557032585144 at epoch: 20 and batch_num: 416\n",
      "Loss of train set: 0.2383444905281067 at epoch: 20 and batch_num: 417\n",
      "Loss of train set: 0.4811924695968628 at epoch: 20 and batch_num: 418\n",
      "Loss of train set: 0.42955896258354187 at epoch: 20 and batch_num: 419\n",
      "Loss of train set: 0.199722558259964 at epoch: 20 and batch_num: 420\n",
      "Loss of train set: 0.3705569803714752 at epoch: 20 and batch_num: 421\n",
      "Loss of train set: 0.24120013415813446 at epoch: 20 and batch_num: 422\n",
      "Loss of train set: 0.5328023433685303 at epoch: 20 and batch_num: 423\n",
      "Loss of train set: 0.309531033039093 at epoch: 20 and batch_num: 424\n",
      "Loss of train set: 0.3095008134841919 at epoch: 20 and batch_num: 425\n",
      "Loss of train set: 0.29092246294021606 at epoch: 20 and batch_num: 426\n",
      "Loss of train set: 0.27389031648635864 at epoch: 20 and batch_num: 427\n",
      "Loss of train set: 0.23667912185192108 at epoch: 20 and batch_num: 428\n",
      "Loss of train set: 0.23664727807044983 at epoch: 20 and batch_num: 429\n",
      "Loss of train set: 0.39280277490615845 at epoch: 20 and batch_num: 430\n",
      "Loss of train set: 0.39685532450675964 at epoch: 20 and batch_num: 431\n",
      "Loss of train set: 0.07583212852478027 at epoch: 20 and batch_num: 432\n",
      "Loss of train set: 0.3357085585594177 at epoch: 20 and batch_num: 433\n",
      "Loss of train set: 0.20749545097351074 at epoch: 20 and batch_num: 434\n",
      "Loss of train set: 0.1621711254119873 at epoch: 20 and batch_num: 435\n",
      "Loss of train set: 0.2402266561985016 at epoch: 20 and batch_num: 436\n",
      "Loss of train set: 0.1579587310552597 at epoch: 20 and batch_num: 437\n",
      "Loss of train set: 0.37602633237838745 at epoch: 20 and batch_num: 438\n",
      "Loss of train set: 0.28908538818359375 at epoch: 20 and batch_num: 439\n",
      "Loss of train set: 0.2201971411705017 at epoch: 20 and batch_num: 440\n",
      "Loss of train set: 0.21330001950263977 at epoch: 20 and batch_num: 441\n",
      "Loss of train set: 0.2849730849266052 at epoch: 20 and batch_num: 442\n",
      "Loss of train set: 0.3372581899166107 at epoch: 20 and batch_num: 443\n",
      "Loss of train set: 0.12375767529010773 at epoch: 20 and batch_num: 444\n",
      "Loss of train set: 0.22214874625205994 at epoch: 20 and batch_num: 445\n",
      "Loss of train set: 0.2920321226119995 at epoch: 20 and batch_num: 446\n",
      "Loss of train set: 0.3846399784088135 at epoch: 20 and batch_num: 447\n",
      "Loss of train set: 0.2470473349094391 at epoch: 20 and batch_num: 448\n",
      "Loss of train set: 0.40854066610336304 at epoch: 20 and batch_num: 449\n",
      "Loss of train set: 0.3194887042045593 at epoch: 20 and batch_num: 450\n",
      "Loss of train set: 0.22138454020023346 at epoch: 20 and batch_num: 451\n",
      "Loss of train set: 0.27945676445961 at epoch: 20 and batch_num: 452\n",
      "Loss of train set: 0.18207329511642456 at epoch: 20 and batch_num: 453\n",
      "Loss of train set: 0.25594934821128845 at epoch: 20 and batch_num: 454\n",
      "Loss of train set: 0.1622435301542282 at epoch: 20 and batch_num: 455\n",
      "Loss of train set: 0.316675066947937 at epoch: 20 and batch_num: 456\n",
      "Loss of train set: 0.13326221704483032 at epoch: 20 and batch_num: 457\n",
      "Loss of train set: 0.21952053904533386 at epoch: 20 and batch_num: 458\n",
      "Loss of train set: 0.2552078664302826 at epoch: 20 and batch_num: 459\n",
      "Loss of train set: 0.16718673706054688 at epoch: 20 and batch_num: 460\n",
      "Loss of train set: 0.22760933637619019 at epoch: 20 and batch_num: 461\n",
      "Loss of train set: 0.22824284434318542 at epoch: 20 and batch_num: 462\n",
      "Loss of train set: 0.3211907148361206 at epoch: 20 and batch_num: 463\n",
      "Loss of train set: 0.3430083394050598 at epoch: 20 and batch_num: 464\n",
      "Loss of train set: 0.4027344584465027 at epoch: 20 and batch_num: 465\n",
      "Loss of train set: 0.24547117948532104 at epoch: 20 and batch_num: 466\n",
      "Loss of train set: 0.24177122116088867 at epoch: 20 and batch_num: 467\n",
      "Loss of train set: 0.29396331310272217 at epoch: 20 and batch_num: 468\n",
      "Loss of train set: 0.12431520223617554 at epoch: 20 and batch_num: 469\n",
      "Loss of train set: 0.3638603389263153 at epoch: 20 and batch_num: 470\n",
      "Loss of train set: 0.2914343476295471 at epoch: 20 and batch_num: 471\n",
      "Loss of train set: 0.38171982765197754 at epoch: 20 and batch_num: 472\n",
      "Loss of train set: 0.4420222043991089 at epoch: 20 and batch_num: 473\n",
      "Loss of train set: 0.1353999823331833 at epoch: 20 and batch_num: 474\n",
      "Loss of train set: 0.12849551439285278 at epoch: 20 and batch_num: 475\n",
      "Loss of train set: 0.27157917618751526 at epoch: 20 and batch_num: 476\n",
      "Loss of train set: 0.24488677084445953 at epoch: 20 and batch_num: 477\n",
      "Loss of train set: 0.3305075764656067 at epoch: 20 and batch_num: 478\n",
      "Loss of train set: 0.31458383798599243 at epoch: 20 and batch_num: 479\n",
      "Loss of train set: 0.2623481750488281 at epoch: 20 and batch_num: 480\n",
      "Loss of train set: 0.35490119457244873 at epoch: 20 and batch_num: 481\n",
      "Loss of train set: 0.29010331630706787 at epoch: 20 and batch_num: 482\n",
      "Loss of train set: 0.28536456823349 at epoch: 20 and batch_num: 483\n",
      "Loss of train set: 0.3893886208534241 at epoch: 20 and batch_num: 484\n",
      "Loss of train set: 0.19412454962730408 at epoch: 20 and batch_num: 485\n",
      "Loss of train set: 0.466227650642395 at epoch: 20 and batch_num: 486\n",
      "Loss of train set: 0.22771969437599182 at epoch: 20 and batch_num: 487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.23170055449008942 at epoch: 20 and batch_num: 488\n",
      "Loss of train set: 0.18105778098106384 at epoch: 20 and batch_num: 489\n",
      "Loss of train set: 0.3025321364402771 at epoch: 20 and batch_num: 490\n",
      "Loss of train set: 0.4683762490749359 at epoch: 20 and batch_num: 491\n",
      "Loss of train set: 0.26803258061408997 at epoch: 20 and batch_num: 492\n",
      "Loss of train set: 0.23425965011119843 at epoch: 20 and batch_num: 493\n",
      "Loss of train set: 0.4645962715148926 at epoch: 20 and batch_num: 494\n",
      "Loss of train set: 0.3688566982746124 at epoch: 20 and batch_num: 495\n",
      "Loss of train set: 0.13461774587631226 at epoch: 20 and batch_num: 496\n",
      "Loss of train set: 0.4607072174549103 at epoch: 20 and batch_num: 497\n",
      "Loss of train set: 0.2556893229484558 at epoch: 20 and batch_num: 498\n",
      "Loss of train set: 0.20356173813343048 at epoch: 20 and batch_num: 499\n",
      "Loss of train set: 0.2443242222070694 at epoch: 20 and batch_num: 500\n",
      "Loss of train set: 0.2606929540634155 at epoch: 20 and batch_num: 501\n",
      "Loss of train set: 0.40447741746902466 at epoch: 20 and batch_num: 502\n",
      "Loss of train set: 0.2484186291694641 at epoch: 20 and batch_num: 503\n",
      "Loss of train set: 0.19007980823516846 at epoch: 20 and batch_num: 504\n",
      "Loss of train set: 0.344062864780426 at epoch: 20 and batch_num: 505\n",
      "Loss of train set: 0.27998271584510803 at epoch: 20 and batch_num: 506\n",
      "Loss of train set: 0.22451776266098022 at epoch: 20 and batch_num: 507\n",
      "Loss of train set: 0.2487724870443344 at epoch: 20 and batch_num: 508\n",
      "Loss of train set: 0.18095147609710693 at epoch: 20 and batch_num: 509\n",
      "Loss of train set: 0.32690268754959106 at epoch: 20 and batch_num: 510\n",
      "Loss of train set: 0.19625696539878845 at epoch: 20 and batch_num: 511\n",
      "Loss of train set: 0.3249957859516144 at epoch: 20 and batch_num: 512\n",
      "Loss of train set: 0.18135347962379456 at epoch: 20 and batch_num: 513\n",
      "Loss of train set: 0.4547337293624878 at epoch: 20 and batch_num: 514\n",
      "Loss of train set: 0.29217833280563354 at epoch: 20 and batch_num: 515\n",
      "Loss of train set: 0.3571615517139435 at epoch: 20 and batch_num: 516\n",
      "Loss of train set: 0.42684146761894226 at epoch: 20 and batch_num: 517\n",
      "Loss of train set: 0.33951303362846375 at epoch: 20 and batch_num: 518\n",
      "Loss of train set: 0.3085889220237732 at epoch: 20 and batch_num: 519\n",
      "Loss of train set: 0.380248486995697 at epoch: 20 and batch_num: 520\n",
      "Loss of train set: 0.23737981915473938 at epoch: 20 and batch_num: 521\n",
      "Loss of train set: 0.24405823647975922 at epoch: 20 and batch_num: 522\n",
      "Loss of train set: 0.34194475412368774 at epoch: 20 and batch_num: 523\n",
      "Loss of train set: 0.1286393702030182 at epoch: 20 and batch_num: 524\n",
      "Loss of train set: 0.2836829125881195 at epoch: 20 and batch_num: 525\n",
      "Loss of train set: 0.24057501554489136 at epoch: 20 and batch_num: 526\n",
      "Loss of train set: 0.21924255788326263 at epoch: 20 and batch_num: 527\n",
      "Loss of train set: 0.23149096965789795 at epoch: 20 and batch_num: 528\n",
      "Loss of train set: 0.26435887813568115 at epoch: 20 and batch_num: 529\n",
      "Loss of train set: 0.44784265756607056 at epoch: 20 and batch_num: 530\n",
      "Loss of train set: 0.2592551112174988 at epoch: 20 and batch_num: 531\n",
      "Loss of train set: 0.3391648530960083 at epoch: 20 and batch_num: 532\n",
      "Loss of train set: 0.29327866435050964 at epoch: 20 and batch_num: 533\n",
      "Loss of train set: 0.2641216218471527 at epoch: 20 and batch_num: 534\n",
      "Loss of train set: 0.2911801040172577 at epoch: 20 and batch_num: 535\n",
      "Loss of train set: 0.44183021783828735 at epoch: 20 and batch_num: 536\n",
      "Loss of train set: 0.15716665983200073 at epoch: 20 and batch_num: 537\n",
      "Loss of train set: 0.34148386120796204 at epoch: 20 and batch_num: 538\n",
      "Loss of train set: 0.09993915259838104 at epoch: 20 and batch_num: 539\n",
      "Loss of train set: 0.32372498512268066 at epoch: 20 and batch_num: 540\n",
      "Loss of train set: 0.3022686839103699 at epoch: 20 and batch_num: 541\n",
      "Loss of train set: 0.2067272961139679 at epoch: 20 and batch_num: 542\n",
      "Loss of train set: 0.19813643395900726 at epoch: 20 and batch_num: 543\n",
      "Loss of train set: 0.2770293951034546 at epoch: 20 and batch_num: 544\n",
      "Loss of train set: 0.42089369893074036 at epoch: 20 and batch_num: 545\n",
      "Loss of train set: 0.4036276936531067 at epoch: 20 and batch_num: 546\n",
      "Loss of train set: 0.3448289632797241 at epoch: 20 and batch_num: 547\n",
      "Loss of train set: 0.17885372042655945 at epoch: 20 and batch_num: 548\n",
      "Loss of train set: 0.2452949583530426 at epoch: 20 and batch_num: 549\n",
      "Loss of train set: 0.2453322857618332 at epoch: 20 and batch_num: 550\n",
      "Loss of train set: 0.29947617650032043 at epoch: 20 and batch_num: 551\n",
      "Loss of train set: 0.2069518119096756 at epoch: 20 and batch_num: 552\n",
      "Loss of train set: 0.3374733328819275 at epoch: 20 and batch_num: 553\n",
      "Loss of train set: 0.2548406720161438 at epoch: 20 and batch_num: 554\n",
      "Loss of train set: 0.22270213067531586 at epoch: 20 and batch_num: 555\n",
      "Loss of train set: 0.12021412700414658 at epoch: 20 and batch_num: 556\n",
      "Loss of train set: 0.2160794585943222 at epoch: 20 and batch_num: 557\n",
      "Loss of train set: 0.18945097923278809 at epoch: 20 and batch_num: 558\n",
      "Loss of train set: 0.35691940784454346 at epoch: 20 and batch_num: 559\n",
      "Loss of train set: 0.25949931144714355 at epoch: 20 and batch_num: 560\n",
      "Loss of train set: 0.45886752009391785 at epoch: 20 and batch_num: 561\n",
      "Loss of train set: 0.12786877155303955 at epoch: 20 and batch_num: 562\n",
      "Loss of train set: 0.2717971205711365 at epoch: 20 and batch_num: 563\n",
      "Loss of train set: 0.36336031556129456 at epoch: 20 and batch_num: 564\n",
      "Loss of train set: 0.29614466428756714 at epoch: 20 and batch_num: 565\n",
      "Loss of train set: 0.3119933009147644 at epoch: 20 and batch_num: 566\n",
      "Loss of train set: 0.35545772314071655 at epoch: 20 and batch_num: 567\n",
      "Loss of train set: 0.22605210542678833 at epoch: 20 and batch_num: 568\n",
      "Loss of train set: 0.18958011269569397 at epoch: 20 and batch_num: 569\n",
      "Loss of train set: 0.30533501505851746 at epoch: 20 and batch_num: 570\n",
      "Loss of train set: 0.19057504832744598 at epoch: 20 and batch_num: 571\n",
      "Loss of train set: 0.29707765579223633 at epoch: 20 and batch_num: 572\n",
      "Loss of train set: 0.2299846112728119 at epoch: 20 and batch_num: 573\n",
      "Loss of train set: 0.27043408155441284 at epoch: 20 and batch_num: 574\n",
      "Loss of train set: 0.11860182136297226 at epoch: 20 and batch_num: 575\n",
      "Loss of train set: 0.20588204264640808 at epoch: 20 and batch_num: 576\n",
      "Loss of train set: 0.125650092959404 at epoch: 20 and batch_num: 577\n",
      "Loss of train set: 0.2530166804790497 at epoch: 20 and batch_num: 578\n",
      "Loss of train set: 0.5053409337997437 at epoch: 20 and batch_num: 579\n",
      "Loss of train set: 0.1781558245420456 at epoch: 20 and batch_num: 580\n",
      "Loss of train set: 0.16231295466423035 at epoch: 20 and batch_num: 581\n",
      "Loss of train set: 0.24497586488723755 at epoch: 20 and batch_num: 582\n",
      "Loss of train set: 0.31975093483924866 at epoch: 20 and batch_num: 583\n",
      "Loss of train set: 0.3261992335319519 at epoch: 20 and batch_num: 584\n",
      "Loss of train set: 0.19677382707595825 at epoch: 20 and batch_num: 585\n",
      "Loss of train set: 0.26271557807922363 at epoch: 20 and batch_num: 586\n",
      "Loss of train set: 0.2995682954788208 at epoch: 20 and batch_num: 587\n",
      "Loss of train set: 0.11539682000875473 at epoch: 20 and batch_num: 588\n",
      "Loss of train set: 0.27895665168762207 at epoch: 20 and batch_num: 589\n",
      "Loss of train set: 0.26899632811546326 at epoch: 20 and batch_num: 590\n",
      "Loss of train set: 0.28652387857437134 at epoch: 20 and batch_num: 591\n",
      "Loss of train set: 0.4079643785953522 at epoch: 20 and batch_num: 592\n",
      "Loss of train set: 0.22214436531066895 at epoch: 20 and batch_num: 593\n",
      "Loss of train set: 0.3478701710700989 at epoch: 20 and batch_num: 594\n",
      "Loss of train set: 0.2680957317352295 at epoch: 20 and batch_num: 595\n",
      "Loss of train set: 0.23486900329589844 at epoch: 20 and batch_num: 596\n",
      "Loss of train set: 0.4201000928878784 at epoch: 20 and batch_num: 597\n",
      "Loss of train set: 0.2994498610496521 at epoch: 20 and batch_num: 598\n",
      "Loss of train set: 0.30594319105148315 at epoch: 20 and batch_num: 599\n",
      "Loss of train set: 0.11858229339122772 at epoch: 20 and batch_num: 600\n",
      "Loss of train set: 0.2861204147338867 at epoch: 20 and batch_num: 601\n",
      "Loss of train set: 0.2986927628517151 at epoch: 20 and batch_num: 602\n",
      "Loss of train set: 0.2961186170578003 at epoch: 20 and batch_num: 603\n",
      "Loss of train set: 0.09681962430477142 at epoch: 20 and batch_num: 604\n",
      "Loss of train set: 0.2620852589607239 at epoch: 20 and batch_num: 605\n",
      "Loss of train set: 0.277505487203598 at epoch: 20 and batch_num: 606\n",
      "Loss of train set: 0.2667394280433655 at epoch: 20 and batch_num: 607\n",
      "Loss of train set: 0.30922967195510864 at epoch: 20 and batch_num: 608\n",
      "Loss of train set: 0.28216907382011414 at epoch: 20 and batch_num: 609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.22823166847229004 at epoch: 20 and batch_num: 610\n",
      "Loss of train set: 0.4195385277271271 at epoch: 20 and batch_num: 611\n",
      "Loss of train set: 0.25824442505836487 at epoch: 20 and batch_num: 612\n",
      "Loss of train set: 0.33488476276397705 at epoch: 20 and batch_num: 613\n",
      "Loss of train set: 0.15446434915065765 at epoch: 20 and batch_num: 614\n",
      "Loss of train set: 0.40777772665023804 at epoch: 20 and batch_num: 615\n",
      "Loss of train set: 0.18485930562019348 at epoch: 20 and batch_num: 616\n",
      "Loss of train set: 0.4183487296104431 at epoch: 20 and batch_num: 617\n",
      "Loss of train set: 0.18661317229270935 at epoch: 20 and batch_num: 618\n",
      "Loss of train set: 0.2139717936515808 at epoch: 20 and batch_num: 619\n",
      "Loss of train set: 0.3661732077598572 at epoch: 20 and batch_num: 620\n",
      "Loss of train set: 0.38864582777023315 at epoch: 20 and batch_num: 621\n",
      "Loss of train set: 0.3046104907989502 at epoch: 20 and batch_num: 622\n",
      "Loss of train set: 0.37948334217071533 at epoch: 20 and batch_num: 623\n",
      "Loss of train set: 0.22825440764427185 at epoch: 20 and batch_num: 624\n",
      "Loss of train set: 0.30540430545806885 at epoch: 20 and batch_num: 625\n",
      "Loss of train set: 0.4166758358478546 at epoch: 20 and batch_num: 626\n",
      "Loss of train set: 0.2540171444416046 at epoch: 20 and batch_num: 627\n",
      "Loss of train set: 0.3083539307117462 at epoch: 20 and batch_num: 628\n",
      "Loss of train set: 0.3696296215057373 at epoch: 20 and batch_num: 629\n",
      "Loss of train set: 0.2545775771141052 at epoch: 20 and batch_num: 630\n",
      "Loss of train set: 0.1999310851097107 at epoch: 20 and batch_num: 631\n",
      "Loss of train set: 0.2906474173069 at epoch: 20 and batch_num: 632\n",
      "Loss of train set: 0.24401865899562836 at epoch: 20 and batch_num: 633\n",
      "Loss of train set: 0.36640843749046326 at epoch: 20 and batch_num: 634\n",
      "Loss of train set: 0.21434079110622406 at epoch: 20 and batch_num: 635\n",
      "Loss of train set: 0.2195683866739273 at epoch: 20 and batch_num: 636\n",
      "Loss of train set: 0.3044065237045288 at epoch: 20 and batch_num: 637\n",
      "Loss of train set: 0.3135325014591217 at epoch: 20 and batch_num: 638\n",
      "Loss of train set: 0.27892136573791504 at epoch: 20 and batch_num: 639\n",
      "Loss of train set: 0.4018048644065857 at epoch: 20 and batch_num: 640\n",
      "Loss of train set: 0.3832360506057739 at epoch: 20 and batch_num: 641\n",
      "Loss of train set: 0.175789013504982 at epoch: 20 and batch_num: 642\n",
      "Loss of train set: 0.2932080328464508 at epoch: 20 and batch_num: 643\n",
      "Loss of train set: 0.26258784532546997 at epoch: 20 and batch_num: 644\n",
      "Loss of train set: 0.21742232143878937 at epoch: 20 and batch_num: 645\n",
      "Loss of train set: 0.23934374749660492 at epoch: 20 and batch_num: 646\n",
      "Loss of train set: 0.1972608119249344 at epoch: 20 and batch_num: 647\n",
      "Loss of train set: 0.23028847575187683 at epoch: 20 and batch_num: 648\n",
      "Loss of train set: 0.26840740442276 at epoch: 20 and batch_num: 649\n",
      "Loss of train set: 0.2964528799057007 at epoch: 20 and batch_num: 650\n",
      "Loss of train set: 0.2061939388513565 at epoch: 20 and batch_num: 651\n",
      "Loss of train set: 0.27633264660835266 at epoch: 20 and batch_num: 652\n",
      "Loss of train set: 0.27445656061172485 at epoch: 20 and batch_num: 653\n",
      "Loss of train set: 0.24346615374088287 at epoch: 20 and batch_num: 654\n",
      "Loss of train set: 0.14661480486392975 at epoch: 20 and batch_num: 655\n",
      "Loss of train set: 0.3765302896499634 at epoch: 20 and batch_num: 656\n",
      "Loss of train set: 0.31358763575553894 at epoch: 20 and batch_num: 657\n",
      "Loss of train set: 0.32728612422943115 at epoch: 20 and batch_num: 658\n",
      "Loss of train set: 0.15724173188209534 at epoch: 20 and batch_num: 659\n",
      "Loss of train set: 0.29669642448425293 at epoch: 20 and batch_num: 660\n",
      "Loss of train set: 0.24182987213134766 at epoch: 20 and batch_num: 661\n",
      "Loss of train set: 0.20975345373153687 at epoch: 20 and batch_num: 662\n",
      "Loss of train set: 0.43420249223709106 at epoch: 20 and batch_num: 663\n",
      "Loss of train set: 0.29505008459091187 at epoch: 20 and batch_num: 664\n",
      "Loss of train set: 0.27863332629203796 at epoch: 20 and batch_num: 665\n",
      "Loss of train set: 0.40156441926956177 at epoch: 20 and batch_num: 666\n",
      "Loss of train set: 0.16819289326667786 at epoch: 20 and batch_num: 667\n",
      "Loss of train set: 0.14302459359169006 at epoch: 20 and batch_num: 668\n",
      "Loss of train set: 0.3197973668575287 at epoch: 20 and batch_num: 669\n",
      "Loss of train set: 0.2739395499229431 at epoch: 20 and batch_num: 670\n",
      "Loss of train set: 0.3331109583377838 at epoch: 20 and batch_num: 671\n",
      "Loss of train set: 0.33627861738204956 at epoch: 20 and batch_num: 672\n",
      "Loss of train set: 0.19393157958984375 at epoch: 20 and batch_num: 673\n",
      "Loss of train set: 0.18785810470581055 at epoch: 20 and batch_num: 674\n",
      "Loss of train set: 0.27219706773757935 at epoch: 20 and batch_num: 675\n",
      "Loss of train set: 0.0981626808643341 at epoch: 20 and batch_num: 676\n",
      "Loss of train set: 0.31886541843414307 at epoch: 20 and batch_num: 677\n",
      "Loss of train set: 0.3302755355834961 at epoch: 20 and batch_num: 678\n",
      "Loss of train set: 0.4107712209224701 at epoch: 20 and batch_num: 679\n",
      "Loss of train set: 0.4837668240070343 at epoch: 20 and batch_num: 680\n",
      "Loss of train set: 0.26509711146354675 at epoch: 20 and batch_num: 681\n",
      "Loss of train set: 0.2516103982925415 at epoch: 20 and batch_num: 682\n",
      "Loss of train set: 0.12258924543857574 at epoch: 20 and batch_num: 683\n",
      "Loss of train set: 0.34859293699264526 at epoch: 20 and batch_num: 684\n",
      "Loss of train set: 0.17327916622161865 at epoch: 20 and batch_num: 685\n",
      "Loss of train set: 0.2194523811340332 at epoch: 20 and batch_num: 686\n",
      "Loss of train set: 0.23884598910808563 at epoch: 20 and batch_num: 687\n",
      "Loss of train set: 0.18929320573806763 at epoch: 20 and batch_num: 688\n",
      "Loss of train set: 0.2611324191093445 at epoch: 20 and batch_num: 689\n",
      "Loss of train set: 0.3301626443862915 at epoch: 20 and batch_num: 690\n",
      "Loss of train set: 0.13658323884010315 at epoch: 20 and batch_num: 691\n",
      "Loss of train set: 0.22358182072639465 at epoch: 20 and batch_num: 692\n",
      "Loss of train set: 0.32719990611076355 at epoch: 20 and batch_num: 693\n",
      "Loss of train set: 0.2749849259853363 at epoch: 20 and batch_num: 694\n",
      "Loss of train set: 0.23291921615600586 at epoch: 20 and batch_num: 695\n",
      "Loss of train set: 0.2839791476726532 at epoch: 20 and batch_num: 696\n",
      "Loss of train set: 0.226961150765419 at epoch: 20 and batch_num: 697\n",
      "Loss of train set: 0.15110841393470764 at epoch: 20 and batch_num: 698\n",
      "Loss of train set: 0.2895556688308716 at epoch: 20 and batch_num: 699\n",
      "Loss of train set: 0.27423030138015747 at epoch: 20 and batch_num: 700\n",
      "Loss of train set: 0.5018347501754761 at epoch: 20 and batch_num: 701\n",
      "Loss of train set: 0.2542402744293213 at epoch: 20 and batch_num: 702\n",
      "Loss of train set: 0.4414275884628296 at epoch: 20 and batch_num: 703\n",
      "Loss of train set: 0.2216200828552246 at epoch: 20 and batch_num: 704\n",
      "Loss of train set: 0.28900086879730225 at epoch: 20 and batch_num: 705\n",
      "Loss of train set: 0.3097224235534668 at epoch: 20 and batch_num: 706\n",
      "Loss of train set: 0.1836426854133606 at epoch: 20 and batch_num: 707\n",
      "Loss of train set: 0.19645220041275024 at epoch: 20 and batch_num: 708\n",
      "Loss of train set: 0.15989696979522705 at epoch: 20 and batch_num: 709\n",
      "Loss of train set: 0.20254653692245483 at epoch: 20 and batch_num: 710\n",
      "Loss of train set: 0.32758018374443054 at epoch: 20 and batch_num: 711\n",
      "Loss of train set: 0.3309707045555115 at epoch: 20 and batch_num: 712\n",
      "Loss of train set: 0.19015131890773773 at epoch: 20 and batch_num: 713\n",
      "Loss of train set: 0.23739618062973022 at epoch: 20 and batch_num: 714\n",
      "Loss of train set: 0.1901530623435974 at epoch: 20 and batch_num: 715\n",
      "Loss of train set: 0.19252213835716248 at epoch: 20 and batch_num: 716\n",
      "Loss of train set: 0.23268240690231323 at epoch: 20 and batch_num: 717\n",
      "Loss of train set: 0.1790383756160736 at epoch: 20 and batch_num: 718\n",
      "Loss of train set: 0.3299065828323364 at epoch: 20 and batch_num: 719\n",
      "Loss of train set: 0.2794572114944458 at epoch: 20 and batch_num: 720\n",
      "Loss of train set: 0.20162874460220337 at epoch: 20 and batch_num: 721\n",
      "Loss of train set: 0.3068299889564514 at epoch: 20 and batch_num: 722\n",
      "Loss of train set: 0.21789737045764923 at epoch: 20 and batch_num: 723\n",
      "Loss of train set: 0.1913072168827057 at epoch: 20 and batch_num: 724\n",
      "Loss of train set: 0.1514575183391571 at epoch: 20 and batch_num: 725\n",
      "Loss of train set: 0.536955714225769 at epoch: 20 and batch_num: 726\n",
      "Loss of train set: 0.18266050517559052 at epoch: 20 and batch_num: 727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2979768216609955 at epoch: 20 and batch_num: 728\n",
      "Loss of train set: 0.24922344088554382 at epoch: 20 and batch_num: 729\n",
      "Loss of train set: 0.19429278373718262 at epoch: 20 and batch_num: 730\n",
      "Loss of train set: 0.2847803831100464 at epoch: 20 and batch_num: 731\n",
      "Loss of train set: 0.28174805641174316 at epoch: 20 and batch_num: 732\n",
      "Loss of train set: 0.1943104863166809 at epoch: 20 and batch_num: 733\n",
      "Loss of train set: 0.17093124985694885 at epoch: 20 and batch_num: 734\n",
      "Loss of train set: 0.3115822672843933 at epoch: 20 and batch_num: 735\n",
      "Loss of train set: 0.34425389766693115 at epoch: 20 and batch_num: 736\n",
      "Loss of train set: 0.2390725314617157 at epoch: 20 and batch_num: 737\n",
      "Loss of train set: 0.36211535334587097 at epoch: 20 and batch_num: 738\n",
      "Loss of train set: 0.4063933193683624 at epoch: 20 and batch_num: 739\n",
      "Loss of train set: 0.19364044070243835 at epoch: 20 and batch_num: 740\n",
      "Loss of train set: 0.2589832544326782 at epoch: 20 and batch_num: 741\n",
      "Loss of train set: 0.34559816122055054 at epoch: 20 and batch_num: 742\n",
      "Loss of train set: 0.3715871572494507 at epoch: 20 and batch_num: 743\n",
      "Loss of train set: 0.3977031707763672 at epoch: 20 and batch_num: 744\n",
      "Loss of train set: 0.23228245973587036 at epoch: 20 and batch_num: 745\n",
      "Loss of train set: 0.25109922885894775 at epoch: 20 and batch_num: 746\n",
      "Loss of train set: 0.5193864107131958 at epoch: 20 and batch_num: 747\n",
      "Loss of train set: 0.2434796839952469 at epoch: 20 and batch_num: 748\n",
      "Loss of train set: 0.3643791675567627 at epoch: 20 and batch_num: 749\n",
      "Loss of train set: 0.13526609539985657 at epoch: 20 and batch_num: 750\n",
      "Loss of train set: 0.3152558505535126 at epoch: 20 and batch_num: 751\n",
      "Loss of train set: 0.2916119694709778 at epoch: 20 and batch_num: 752\n",
      "Loss of train set: 0.3881042003631592 at epoch: 20 and batch_num: 753\n",
      "Loss of train set: 0.2763555943965912 at epoch: 20 and batch_num: 754\n",
      "Loss of train set: 0.3579571545124054 at epoch: 20 and batch_num: 755\n",
      "Loss of train set: 0.3169393539428711 at epoch: 20 and batch_num: 756\n",
      "Loss of train set: 0.2582065463066101 at epoch: 20 and batch_num: 757\n",
      "Loss of train set: 0.32556724548339844 at epoch: 20 and batch_num: 758\n",
      "Loss of train set: 0.1649387627840042 at epoch: 20 and batch_num: 759\n",
      "Loss of train set: 0.31377702951431274 at epoch: 20 and batch_num: 760\n",
      "Loss of train set: 0.2648259401321411 at epoch: 20 and batch_num: 761\n",
      "Loss of train set: 0.4233540892601013 at epoch: 20 and batch_num: 762\n",
      "Loss of train set: 0.28287798166275024 at epoch: 20 and batch_num: 763\n",
      "Loss of train set: 0.20670127868652344 at epoch: 20 and batch_num: 764\n",
      "Loss of train set: 0.5040047764778137 at epoch: 20 and batch_num: 765\n",
      "Loss of train set: 0.17119932174682617 at epoch: 20 and batch_num: 766\n",
      "Loss of train set: 0.22682473063468933 at epoch: 20 and batch_num: 767\n",
      "Loss of train set: 0.3100578784942627 at epoch: 20 and batch_num: 768\n",
      "Loss of train set: 0.2317330539226532 at epoch: 20 and batch_num: 769\n",
      "Loss of train set: 0.10342106968164444 at epoch: 20 and batch_num: 770\n",
      "Loss of train set: 0.3813629746437073 at epoch: 20 and batch_num: 771\n",
      "Loss of train set: 0.29337313771247864 at epoch: 20 and batch_num: 772\n",
      "Loss of train set: 0.23630408942699432 at epoch: 20 and batch_num: 773\n",
      "Loss of train set: 0.19615785777568817 at epoch: 20 and batch_num: 774\n",
      "Loss of train set: 0.2365749180316925 at epoch: 20 and batch_num: 775\n",
      "Loss of train set: 0.43053561449050903 at epoch: 20 and batch_num: 776\n",
      "Loss of train set: 0.2639944553375244 at epoch: 20 and batch_num: 777\n",
      "Loss of train set: 0.26495102047920227 at epoch: 20 and batch_num: 778\n",
      "Loss of train set: 0.2800600230693817 at epoch: 20 and batch_num: 779\n",
      "Loss of train set: 0.34848490357398987 at epoch: 20 and batch_num: 780\n",
      "Loss of train set: 0.163992241024971 at epoch: 20 and batch_num: 781\n",
      "Loss of train set: 0.21030285954475403 at epoch: 20 and batch_num: 782\n",
      "Loss of train set: 0.36764639616012573 at epoch: 20 and batch_num: 783\n",
      "Loss of train set: 0.24223554134368896 at epoch: 20 and batch_num: 784\n",
      "Loss of train set: 0.3739812672138214 at epoch: 20 and batch_num: 785\n",
      "Loss of train set: 0.21786227822303772 at epoch: 20 and batch_num: 786\n",
      "Loss of train set: 0.3127858340740204 at epoch: 20 and batch_num: 787\n",
      "Loss of train set: 0.266664981842041 at epoch: 20 and batch_num: 788\n",
      "Loss of train set: 0.39846423268318176 at epoch: 20 and batch_num: 789\n",
      "Loss of train set: 0.22168153524398804 at epoch: 20 and batch_num: 790\n",
      "Loss of train set: 0.27767348289489746 at epoch: 20 and batch_num: 791\n",
      "Loss of train set: 0.582429826259613 at epoch: 20 and batch_num: 792\n",
      "Loss of train set: 0.23935982584953308 at epoch: 20 and batch_num: 793\n",
      "Loss of train set: 0.4216851592063904 at epoch: 20 and batch_num: 794\n",
      "Loss of train set: 0.25193506479263306 at epoch: 20 and batch_num: 795\n",
      "Loss of train set: 0.28332847356796265 at epoch: 20 and batch_num: 796\n",
      "Loss of train set: 0.3397838771343231 at epoch: 20 and batch_num: 797\n",
      "Loss of train set: 0.24929586052894592 at epoch: 20 and batch_num: 798\n",
      "Loss of train set: 0.20454415678977966 at epoch: 20 and batch_num: 799\n",
      "Loss of train set: 0.32769352197647095 at epoch: 20 and batch_num: 800\n",
      "Loss of train set: 0.30626875162124634 at epoch: 20 and batch_num: 801\n",
      "Loss of train set: 0.33084794878959656 at epoch: 20 and batch_num: 802\n",
      "Loss of train set: 0.17480403184890747 at epoch: 20 and batch_num: 803\n",
      "Loss of train set: 0.3469384014606476 at epoch: 20 and batch_num: 804\n",
      "Loss of train set: 0.2593047618865967 at epoch: 20 and batch_num: 805\n",
      "Loss of train set: 0.32229962944984436 at epoch: 20 and batch_num: 806\n",
      "Loss of train set: 0.46251189708709717 at epoch: 20 and batch_num: 807\n",
      "Loss of train set: 0.30131691694259644 at epoch: 20 and batch_num: 808\n",
      "Loss of train set: 0.37020641565322876 at epoch: 20 and batch_num: 809\n",
      "Loss of train set: 0.14582565426826477 at epoch: 20 and batch_num: 810\n",
      "Loss of train set: 0.4113953113555908 at epoch: 20 and batch_num: 811\n",
      "Loss of train set: 0.22058477997779846 at epoch: 20 and batch_num: 812\n",
      "Loss of train set: 0.25526902079582214 at epoch: 20 and batch_num: 813\n",
      "Loss of train set: 0.18581295013427734 at epoch: 20 and batch_num: 814\n",
      "Loss of train set: 0.36930331587791443 at epoch: 20 and batch_num: 815\n",
      "Loss of train set: 0.328879714012146 at epoch: 20 and batch_num: 816\n",
      "Loss of train set: 0.27311456203460693 at epoch: 20 and batch_num: 817\n",
      "Loss of train set: 0.37680429220199585 at epoch: 20 and batch_num: 818\n",
      "Loss of train set: 0.3013960123062134 at epoch: 20 and batch_num: 819\n",
      "Loss of train set: 0.37298041582107544 at epoch: 20 and batch_num: 820\n",
      "Loss of train set: 0.1887775957584381 at epoch: 20 and batch_num: 821\n",
      "Loss of train set: 0.30227020382881165 at epoch: 20 and batch_num: 822\n",
      "Loss of train set: 0.27846914529800415 at epoch: 20 and batch_num: 823\n",
      "Loss of train set: 0.4301999807357788 at epoch: 20 and batch_num: 824\n",
      "Loss of train set: 0.2499782294034958 at epoch: 20 and batch_num: 825\n",
      "Loss of train set: 0.39759427309036255 at epoch: 20 and batch_num: 826\n",
      "Loss of train set: 0.2176954746246338 at epoch: 20 and batch_num: 827\n",
      "Loss of train set: 0.2462901473045349 at epoch: 20 and batch_num: 828\n",
      "Loss of train set: 0.2930973768234253 at epoch: 20 and batch_num: 829\n",
      "Loss of train set: 0.16228975355625153 at epoch: 20 and batch_num: 830\n",
      "Loss of train set: 0.17829620838165283 at epoch: 20 and batch_num: 831\n",
      "Loss of train set: 0.3056945204734802 at epoch: 20 and batch_num: 832\n",
      "Loss of train set: 0.1502000391483307 at epoch: 20 and batch_num: 833\n",
      "Loss of train set: 0.2895164489746094 at epoch: 20 and batch_num: 834\n",
      "Loss of train set: 0.37508174777030945 at epoch: 20 and batch_num: 835\n",
      "Loss of train set: 0.4618780314922333 at epoch: 20 and batch_num: 836\n",
      "Loss of train set: 0.25297898054122925 at epoch: 20 and batch_num: 837\n",
      "Loss of train set: 0.42883116006851196 at epoch: 20 and batch_num: 838\n",
      "Loss of train set: 0.34864893555641174 at epoch: 20 and batch_num: 839\n",
      "Loss of train set: 0.27301931381225586 at epoch: 20 and batch_num: 840\n",
      "Loss of train set: 0.35163336992263794 at epoch: 20 and batch_num: 841\n",
      "Loss of train set: 0.24255864322185516 at epoch: 20 and batch_num: 842\n",
      "Loss of train set: 0.33013737201690674 at epoch: 20 and batch_num: 843\n",
      "Loss of train set: 0.2784385085105896 at epoch: 20 and batch_num: 844\n",
      "Loss of train set: 0.2449200302362442 at epoch: 20 and batch_num: 845\n",
      "Loss of train set: 0.33740881085395813 at epoch: 20 and batch_num: 846\n",
      "Loss of train set: 0.23780158162117004 at epoch: 20 and batch_num: 847\n",
      "Loss of train set: 0.15624606609344482 at epoch: 20 and batch_num: 848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.36980903148651123 at epoch: 20 and batch_num: 849\n",
      "Loss of train set: 0.19636914134025574 at epoch: 20 and batch_num: 850\n",
      "Loss of train set: 0.1979312300682068 at epoch: 20 and batch_num: 851\n",
      "Loss of train set: 0.20699521899223328 at epoch: 20 and batch_num: 852\n",
      "Loss of train set: 0.15029685199260712 at epoch: 20 and batch_num: 853\n",
      "Loss of train set: 0.4276968240737915 at epoch: 20 and batch_num: 854\n",
      "Loss of train set: 0.2744102478027344 at epoch: 20 and batch_num: 855\n",
      "Loss of train set: 0.5024073719978333 at epoch: 20 and batch_num: 856\n",
      "Loss of train set: 0.2329263985157013 at epoch: 20 and batch_num: 857\n",
      "Loss of train set: 0.23409195244312286 at epoch: 20 and batch_num: 858\n",
      "Loss of train set: 0.1994377076625824 at epoch: 20 and batch_num: 859\n",
      "Loss of train set: 0.2983109652996063 at epoch: 20 and batch_num: 860\n",
      "Loss of train set: 0.46207553148269653 at epoch: 20 and batch_num: 861\n",
      "Loss of train set: 0.2030315399169922 at epoch: 20 and batch_num: 862\n",
      "Loss of train set: 0.3655693531036377 at epoch: 20 and batch_num: 863\n",
      "Loss of train set: 0.2690107226371765 at epoch: 20 and batch_num: 864\n",
      "Loss of train set: 0.22943326830863953 at epoch: 20 and batch_num: 865\n",
      "Loss of train set: 0.3616226315498352 at epoch: 20 and batch_num: 866\n",
      "Loss of train set: 0.22224940359592438 at epoch: 20 and batch_num: 867\n",
      "Loss of train set: 0.3056170344352722 at epoch: 20 and batch_num: 868\n",
      "Loss of train set: 0.3759433925151825 at epoch: 20 and batch_num: 869\n",
      "Loss of train set: 0.16423921287059784 at epoch: 20 and batch_num: 870\n",
      "Loss of train set: 0.23793920874595642 at epoch: 20 and batch_num: 871\n",
      "Loss of train set: 0.26198530197143555 at epoch: 20 and batch_num: 872\n",
      "Loss of train set: 0.24276195466518402 at epoch: 20 and batch_num: 873\n",
      "Loss of train set: 0.21261996030807495 at epoch: 20 and batch_num: 874\n",
      "Loss of train set: 0.32143303751945496 at epoch: 20 and batch_num: 875\n",
      "Loss of train set: 0.2786754369735718 at epoch: 20 and batch_num: 876\n",
      "Loss of train set: 0.19389577209949493 at epoch: 20 and batch_num: 877\n",
      "Loss of train set: 0.4549047350883484 at epoch: 20 and batch_num: 878\n",
      "Loss of train set: 0.3817704916000366 at epoch: 20 and batch_num: 879\n",
      "Loss of train set: 0.26528793573379517 at epoch: 20 and batch_num: 880\n",
      "Loss of train set: 0.26158612966537476 at epoch: 20 and batch_num: 881\n",
      "Loss of train set: 0.22957608103752136 at epoch: 20 and batch_num: 882\n",
      "Loss of train set: 0.2685387134552002 at epoch: 20 and batch_num: 883\n",
      "Loss of train set: 0.3314172625541687 at epoch: 20 and batch_num: 884\n",
      "Loss of train set: 0.3416072130203247 at epoch: 20 and batch_num: 885\n",
      "Loss of train set: 0.32108932733535767 at epoch: 20 and batch_num: 886\n",
      "Loss of train set: 0.3791597783565521 at epoch: 20 and batch_num: 887\n",
      "Loss of train set: 0.29980793595314026 at epoch: 20 and batch_num: 888\n",
      "Loss of train set: 0.21163198351860046 at epoch: 20 and batch_num: 889\n",
      "Loss of train set: 0.3992044925689697 at epoch: 20 and batch_num: 890\n",
      "Loss of train set: 0.277809739112854 at epoch: 20 and batch_num: 891\n",
      "Loss of train set: 0.31377217173576355 at epoch: 20 and batch_num: 892\n",
      "Loss of train set: 0.21840599179267883 at epoch: 20 and batch_num: 893\n",
      "Loss of train set: 0.35583534836769104 at epoch: 20 and batch_num: 894\n",
      "Loss of train set: 0.2951207756996155 at epoch: 20 and batch_num: 895\n",
      "Loss of train set: 0.2537824213504791 at epoch: 20 and batch_num: 896\n",
      "Loss of train set: 0.467245489358902 at epoch: 20 and batch_num: 897\n",
      "Loss of train set: 0.41272300481796265 at epoch: 20 and batch_num: 898\n",
      "Loss of train set: 0.25542864203453064 at epoch: 20 and batch_num: 899\n",
      "Loss of train set: 0.2342013716697693 at epoch: 20 and batch_num: 900\n",
      "Loss of train set: 0.2797192335128784 at epoch: 20 and batch_num: 901\n",
      "Loss of train set: 0.4391991198062897 at epoch: 20 and batch_num: 902\n",
      "Loss of train set: 0.29321253299713135 at epoch: 20 and batch_num: 903\n",
      "Loss of train set: 0.2417377531528473 at epoch: 20 and batch_num: 904\n",
      "Loss of train set: 0.27805259823799133 at epoch: 20 and batch_num: 905\n",
      "Loss of train set: 0.2816651463508606 at epoch: 20 and batch_num: 906\n",
      "Loss of train set: 0.21181124448776245 at epoch: 20 and batch_num: 907\n",
      "Loss of train set: 0.38195696473121643 at epoch: 20 and batch_num: 908\n",
      "Loss of train set: 0.3886922299861908 at epoch: 20 and batch_num: 909\n",
      "Loss of train set: 0.3745512366294861 at epoch: 20 and batch_num: 910\n",
      "Loss of train set: 0.3789220452308655 at epoch: 20 and batch_num: 911\n",
      "Loss of train set: 0.25240328907966614 at epoch: 20 and batch_num: 912\n",
      "Loss of train set: 0.5602424144744873 at epoch: 20 and batch_num: 913\n",
      "Loss of train set: 0.2554181218147278 at epoch: 20 and batch_num: 914\n",
      "Loss of train set: 0.21893814206123352 at epoch: 20 and batch_num: 915\n",
      "Loss of train set: 0.28296926617622375 at epoch: 20 and batch_num: 916\n",
      "Loss of train set: 0.2537326216697693 at epoch: 20 and batch_num: 917\n",
      "Loss of train set: 0.3363780677318573 at epoch: 20 and batch_num: 918\n",
      "Loss of train set: 0.3450148105621338 at epoch: 20 and batch_num: 919\n",
      "Loss of train set: 0.37922346591949463 at epoch: 20 and batch_num: 920\n",
      "Loss of train set: 0.39522045850753784 at epoch: 20 and batch_num: 921\n",
      "Loss of train set: 0.21088281273841858 at epoch: 20 and batch_num: 922\n",
      "Loss of train set: 0.21762850880622864 at epoch: 20 and batch_num: 923\n",
      "Loss of train set: 0.35051941871643066 at epoch: 20 and batch_num: 924\n",
      "Loss of train set: 0.44596755504608154 at epoch: 20 and batch_num: 925\n",
      "Loss of train set: 0.34074878692626953 at epoch: 20 and batch_num: 926\n",
      "Loss of train set: 0.3951582908630371 at epoch: 20 and batch_num: 927\n",
      "Loss of train set: 0.4651607275009155 at epoch: 20 and batch_num: 928\n",
      "Loss of train set: 0.12874890863895416 at epoch: 20 and batch_num: 929\n",
      "Loss of train set: 0.15727774798870087 at epoch: 20 and batch_num: 930\n",
      "Loss of train set: 0.22687400877475739 at epoch: 20 and batch_num: 931\n",
      "Loss of train set: 0.3233169913291931 at epoch: 20 and batch_num: 932\n",
      "Loss of train set: 0.418012797832489 at epoch: 20 and batch_num: 933\n",
      "Loss of train set: 0.25878989696502686 at epoch: 20 and batch_num: 934\n",
      "Loss of train set: 0.38558757305145264 at epoch: 20 and batch_num: 935\n",
      "Loss of train set: 0.3612156808376312 at epoch: 20 and batch_num: 936\n",
      "Loss of train set: 0.2679043710231781 at epoch: 20 and batch_num: 937\n",
      "Accuracy of train set: 0.8970333333333333\n",
      "Loss of test set: 0.48237597942352295 at epoch: 20 and batch_num: 0\n",
      "Loss of test set: 0.474831223487854 at epoch: 20 and batch_num: 1\n",
      "Loss of test set: 0.44140923023223877 at epoch: 20 and batch_num: 2\n",
      "Loss of test set: 0.33031630516052246 at epoch: 20 and batch_num: 3\n",
      "Loss of test set: 0.21365797519683838 at epoch: 20 and batch_num: 4\n",
      "Loss of test set: 0.48429083824157715 at epoch: 20 and batch_num: 5\n",
      "Loss of test set: 0.486136794090271 at epoch: 20 and batch_num: 6\n",
      "Loss of test set: 0.25225508213043213 at epoch: 20 and batch_num: 7\n",
      "Loss of test set: 0.28277358412742615 at epoch: 20 and batch_num: 8\n",
      "Loss of test set: 0.3576740622520447 at epoch: 20 and batch_num: 9\n",
      "Loss of test set: 0.39976707100868225 at epoch: 20 and batch_num: 10\n",
      "Loss of test set: 0.3556290566921234 at epoch: 20 and batch_num: 11\n",
      "Loss of test set: 0.4758146405220032 at epoch: 20 and batch_num: 12\n",
      "Loss of test set: 0.2766762971878052 at epoch: 20 and batch_num: 13\n",
      "Loss of test set: 0.26884007453918457 at epoch: 20 and batch_num: 14\n",
      "Loss of test set: 0.36765700578689575 at epoch: 20 and batch_num: 15\n",
      "Loss of test set: 0.28852397203445435 at epoch: 20 and batch_num: 16\n",
      "Loss of test set: 0.4093729555606842 at epoch: 20 and batch_num: 17\n",
      "Loss of test set: 0.32538682222366333 at epoch: 20 and batch_num: 18\n",
      "Loss of test set: 0.42942753434181213 at epoch: 20 and batch_num: 19\n",
      "Loss of test set: 0.3515973687171936 at epoch: 20 and batch_num: 20\n",
      "Loss of test set: 0.3556528091430664 at epoch: 20 and batch_num: 21\n",
      "Loss of test set: 0.2896045744419098 at epoch: 20 and batch_num: 22\n",
      "Loss of test set: 0.27116402983665466 at epoch: 20 and batch_num: 23\n",
      "Loss of test set: 0.2928135395050049 at epoch: 20 and batch_num: 24\n",
      "Loss of test set: 0.3502786457538605 at epoch: 20 and batch_num: 25\n",
      "Loss of test set: 0.33175578713417053 at epoch: 20 and batch_num: 26\n",
      "Loss of test set: 0.38012707233428955 at epoch: 20 and batch_num: 27\n",
      "Loss of test set: 0.3692166209220886 at epoch: 20 and batch_num: 28\n",
      "Loss of test set: 0.23536694049835205 at epoch: 20 and batch_num: 29\n",
      "Loss of test set: 0.25270670652389526 at epoch: 20 and batch_num: 30\n",
      "Loss of test set: 0.2646353840827942 at epoch: 20 and batch_num: 31\n",
      "Loss of test set: 0.41524291038513184 at epoch: 20 and batch_num: 32\n",
      "Loss of test set: 0.3379950523376465 at epoch: 20 and batch_num: 33\n",
      "Loss of test set: 0.3675827383995056 at epoch: 20 and batch_num: 34\n",
      "Loss of test set: 0.2872009873390198 at epoch: 20 and batch_num: 35\n",
      "Loss of test set: 0.2943716049194336 at epoch: 20 and batch_num: 36\n",
      "Loss of test set: 0.217792809009552 at epoch: 20 and batch_num: 37\n",
      "Loss of test set: 0.5695543885231018 at epoch: 20 and batch_num: 38\n",
      "Loss of test set: 0.553164005279541 at epoch: 20 and batch_num: 39\n",
      "Loss of test set: 0.3748323619365692 at epoch: 20 and batch_num: 40\n",
      "Loss of test set: 0.3812095522880554 at epoch: 20 and batch_num: 41\n",
      "Loss of test set: 0.22257190942764282 at epoch: 20 and batch_num: 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.36328548192977905 at epoch: 20 and batch_num: 43\n",
      "Loss of test set: 0.6201293468475342 at epoch: 20 and batch_num: 44\n",
      "Loss of test set: 0.21981213986873627 at epoch: 20 and batch_num: 45\n",
      "Loss of test set: 0.4498835504055023 at epoch: 20 and batch_num: 46\n",
      "Loss of test set: 0.4075838327407837 at epoch: 20 and batch_num: 47\n",
      "Loss of test set: 0.23381075263023376 at epoch: 20 and batch_num: 48\n",
      "Loss of test set: 0.25853222608566284 at epoch: 20 and batch_num: 49\n",
      "Loss of test set: 0.5889629125595093 at epoch: 20 and batch_num: 50\n",
      "Loss of test set: 0.36022841930389404 at epoch: 20 and batch_num: 51\n",
      "Loss of test set: 0.4523937404155731 at epoch: 20 and batch_num: 52\n",
      "Loss of test set: 0.3181198239326477 at epoch: 20 and batch_num: 53\n",
      "Loss of test set: 0.27590835094451904 at epoch: 20 and batch_num: 54\n",
      "Loss of test set: 0.23921111226081848 at epoch: 20 and batch_num: 55\n",
      "Loss of test set: 0.46143823862075806 at epoch: 20 and batch_num: 56\n",
      "Loss of test set: 0.25401923060417175 at epoch: 20 and batch_num: 57\n",
      "Loss of test set: 0.40496668219566345 at epoch: 20 and batch_num: 58\n",
      "Loss of test set: 0.32728904485702515 at epoch: 20 and batch_num: 59\n",
      "Loss of test set: 0.11308099329471588 at epoch: 20 and batch_num: 60\n",
      "Loss of test set: 0.4497358202934265 at epoch: 20 and batch_num: 61\n",
      "Loss of test set: 0.4967610239982605 at epoch: 20 and batch_num: 62\n",
      "Loss of test set: 0.20167404413223267 at epoch: 20 and batch_num: 63\n",
      "Loss of test set: 0.23733553290367126 at epoch: 20 and batch_num: 64\n",
      "Loss of test set: 0.25863856077194214 at epoch: 20 and batch_num: 65\n",
      "Loss of test set: 0.3719385266304016 at epoch: 20 and batch_num: 66\n",
      "Loss of test set: 0.36509037017822266 at epoch: 20 and batch_num: 67\n",
      "Loss of test set: 0.20264333486557007 at epoch: 20 and batch_num: 68\n",
      "Loss of test set: 0.5945824980735779 at epoch: 20 and batch_num: 69\n",
      "Loss of test set: 0.32990965247154236 at epoch: 20 and batch_num: 70\n",
      "Loss of test set: 0.37565845251083374 at epoch: 20 and batch_num: 71\n",
      "Loss of test set: 0.3760700821876526 at epoch: 20 and batch_num: 72\n",
      "Loss of test set: 0.45630520582199097 at epoch: 20 and batch_num: 73\n",
      "Loss of test set: 0.2494552731513977 at epoch: 20 and batch_num: 74\n",
      "Loss of test set: 0.37649452686309814 at epoch: 20 and batch_num: 75\n",
      "Loss of test set: 0.42746663093566895 at epoch: 20 and batch_num: 76\n",
      "Loss of test set: 0.4426574110984802 at epoch: 20 and batch_num: 77\n",
      "Loss of test set: 0.35245296359062195 at epoch: 20 and batch_num: 78\n",
      "Loss of test set: 0.5511132478713989 at epoch: 20 and batch_num: 79\n",
      "Loss of test set: 0.40804171562194824 at epoch: 20 and batch_num: 80\n",
      "Loss of test set: 0.4003069996833801 at epoch: 20 and batch_num: 81\n",
      "Loss of test set: 0.2217033952474594 at epoch: 20 and batch_num: 82\n",
      "Loss of test set: 0.28891444206237793 at epoch: 20 and batch_num: 83\n",
      "Loss of test set: 0.514869749546051 at epoch: 20 and batch_num: 84\n",
      "Loss of test set: 0.29617151618003845 at epoch: 20 and batch_num: 85\n",
      "Loss of test set: 0.2523486912250519 at epoch: 20 and batch_num: 86\n",
      "Loss of test set: 0.3690321445465088 at epoch: 20 and batch_num: 87\n",
      "Loss of test set: 0.3910677433013916 at epoch: 20 and batch_num: 88\n",
      "Loss of test set: 0.4612736403942108 at epoch: 20 and batch_num: 89\n",
      "Loss of test set: 0.3354032635688782 at epoch: 20 and batch_num: 90\n",
      "Loss of test set: 0.3004438579082489 at epoch: 20 and batch_num: 91\n",
      "Loss of test set: 0.42239028215408325 at epoch: 20 and batch_num: 92\n",
      "Loss of test set: 0.41388896107673645 at epoch: 20 and batch_num: 93\n",
      "Loss of test set: 0.32204997539520264 at epoch: 20 and batch_num: 94\n",
      "Loss of test set: 0.45986121892929077 at epoch: 20 and batch_num: 95\n",
      "Loss of test set: 0.5279234647750854 at epoch: 20 and batch_num: 96\n",
      "Loss of test set: 0.26180753111839294 at epoch: 20 and batch_num: 97\n",
      "Loss of test set: 0.4181351661682129 at epoch: 20 and batch_num: 98\n",
      "Loss of test set: 0.2924934923648834 at epoch: 20 and batch_num: 99\n",
      "Loss of test set: 0.45035043358802795 at epoch: 20 and batch_num: 100\n",
      "Loss of test set: 0.2693614065647125 at epoch: 20 and batch_num: 101\n",
      "Loss of test set: 0.385515034198761 at epoch: 20 and batch_num: 102\n",
      "Loss of test set: 0.2643866539001465 at epoch: 20 and batch_num: 103\n",
      "Loss of test set: 0.4118046760559082 at epoch: 20 and batch_num: 104\n",
      "Loss of test set: 0.20088857412338257 at epoch: 20 and batch_num: 105\n",
      "Loss of test set: 0.3254498541355133 at epoch: 20 and batch_num: 106\n",
      "Loss of test set: 0.28705379366874695 at epoch: 20 and batch_num: 107\n",
      "Loss of test set: 0.336455762386322 at epoch: 20 and batch_num: 108\n",
      "Loss of test set: 0.19595468044281006 at epoch: 20 and batch_num: 109\n",
      "Loss of test set: 0.1757064163684845 at epoch: 20 and batch_num: 110\n",
      "Loss of test set: 0.3916183114051819 at epoch: 20 and batch_num: 111\n",
      "Loss of test set: 0.3534950911998749 at epoch: 20 and batch_num: 112\n",
      "Loss of test set: 0.42252039909362793 at epoch: 20 and batch_num: 113\n",
      "Loss of test set: 0.4304558038711548 at epoch: 20 and batch_num: 114\n",
      "Loss of test set: 0.2565748393535614 at epoch: 20 and batch_num: 115\n",
      "Loss of test set: 0.41691190004348755 at epoch: 20 and batch_num: 116\n",
      "Loss of test set: 0.24959325790405273 at epoch: 20 and batch_num: 117\n",
      "Loss of test set: 0.5455028414726257 at epoch: 20 and batch_num: 118\n",
      "Loss of test set: 0.2297699898481369 at epoch: 20 and batch_num: 119\n",
      "Loss of test set: 0.3343255817890167 at epoch: 20 and batch_num: 120\n",
      "Loss of test set: 0.3035554587841034 at epoch: 20 and batch_num: 121\n",
      "Loss of test set: 0.39504149556159973 at epoch: 20 and batch_num: 122\n",
      "Loss of test set: 0.1757226139307022 at epoch: 20 and batch_num: 123\n",
      "Loss of test set: 0.3541222810745239 at epoch: 20 and batch_num: 124\n",
      "Loss of test set: 0.3225559890270233 at epoch: 20 and batch_num: 125\n",
      "Loss of test set: 0.3063305914402008 at epoch: 20 and batch_num: 126\n",
      "Loss of test set: 0.3861095905303955 at epoch: 20 and batch_num: 127\n",
      "Loss of test set: 0.24974209070205688 at epoch: 20 and batch_num: 128\n",
      "Loss of test set: 0.36389121413230896 at epoch: 20 and batch_num: 129\n",
      "Loss of test set: 0.3546378016471863 at epoch: 20 and batch_num: 130\n",
      "Loss of test set: 0.3629913330078125 at epoch: 20 and batch_num: 131\n",
      "Loss of test set: 0.36000123620033264 at epoch: 20 and batch_num: 132\n",
      "Loss of test set: 0.18620535731315613 at epoch: 20 and batch_num: 133\n",
      "Loss of test set: 0.3465704917907715 at epoch: 20 and batch_num: 134\n",
      "Loss of test set: 0.5225294828414917 at epoch: 20 and batch_num: 135\n",
      "Loss of test set: 0.263688862323761 at epoch: 20 and batch_num: 136\n",
      "Loss of test set: 0.32292258739471436 at epoch: 20 and batch_num: 137\n",
      "Loss of test set: 0.2591038644313812 at epoch: 20 and batch_num: 138\n",
      "Loss of test set: 0.32822906970977783 at epoch: 20 and batch_num: 139\n",
      "Loss of test set: 0.33166855573654175 at epoch: 20 and batch_num: 140\n",
      "Loss of test set: 0.6767998933792114 at epoch: 20 and batch_num: 141\n",
      "Loss of test set: 0.5442519187927246 at epoch: 20 and batch_num: 142\n",
      "Loss of test set: 0.6935814619064331 at epoch: 20 and batch_num: 143\n",
      "Loss of test set: 0.24066105484962463 at epoch: 20 and batch_num: 144\n",
      "Loss of test set: 0.3457911014556885 at epoch: 20 and batch_num: 145\n",
      "Loss of test set: 0.43693915009498596 at epoch: 20 and batch_num: 146\n",
      "Loss of test set: 0.1985037922859192 at epoch: 20 and batch_num: 147\n",
      "Loss of test set: 0.41117438673973083 at epoch: 20 and batch_num: 148\n",
      "Loss of test set: 0.3413240611553192 at epoch: 20 and batch_num: 149\n",
      "Loss of test set: 0.2298375368118286 at epoch: 20 and batch_num: 150\n",
      "Loss of test set: 0.5525335073471069 at epoch: 20 and batch_num: 151\n",
      "Loss of test set: 0.4914955496788025 at epoch: 20 and batch_num: 152\n",
      "Loss of test set: 0.30777257680892944 at epoch: 20 and batch_num: 153\n",
      "Loss of test set: 0.23959361016750336 at epoch: 20 and batch_num: 154\n",
      "Loss of test set: 0.46351397037506104 at epoch: 20 and batch_num: 155\n",
      "Loss of test set: 0.1838965117931366 at epoch: 20 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8718\n",
      "Loss of train set: 0.20080825686454773 at epoch: 21 and batch_num: 0\n",
      "Loss of train set: 0.3164317011833191 at epoch: 21 and batch_num: 1\n",
      "Loss of train set: 0.33116385340690613 at epoch: 21 and batch_num: 2\n",
      "Loss of train set: 0.23940908908843994 at epoch: 21 and batch_num: 3\n",
      "Loss of train set: 0.23957780003547668 at epoch: 21 and batch_num: 4\n",
      "Loss of train set: 0.20889884233474731 at epoch: 21 and batch_num: 5\n",
      "Loss of train set: 0.13481146097183228 at epoch: 21 and batch_num: 6\n",
      "Loss of train set: 0.17804110050201416 at epoch: 21 and batch_num: 7\n",
      "Loss of train set: 0.29855209589004517 at epoch: 21 and batch_num: 8\n",
      "Loss of train set: 0.36078935861587524 at epoch: 21 and batch_num: 9\n",
      "Loss of train set: 0.25589656829833984 at epoch: 21 and batch_num: 10\n",
      "Loss of train set: 0.31936031579971313 at epoch: 21 and batch_num: 11\n",
      "Loss of train set: 0.1742739975452423 at epoch: 21 and batch_num: 12\n",
      "Loss of train set: 0.2388075888156891 at epoch: 21 and batch_num: 13\n",
      "Loss of train set: 0.17468619346618652 at epoch: 21 and batch_num: 14\n",
      "Loss of train set: 0.10340029001235962 at epoch: 21 and batch_num: 15\n",
      "Loss of train set: 0.2363176941871643 at epoch: 21 and batch_num: 16\n",
      "Loss of train set: 0.1669662594795227 at epoch: 21 and batch_num: 17\n",
      "Loss of train set: 0.21318970620632172 at epoch: 21 and batch_num: 18\n",
      "Loss of train set: 0.250785768032074 at epoch: 21 and batch_num: 19\n",
      "Loss of train set: 0.41824305057525635 at epoch: 21 and batch_num: 20\n",
      "Loss of train set: 0.33657991886138916 at epoch: 21 and batch_num: 21\n",
      "Loss of train set: 0.3446581959724426 at epoch: 21 and batch_num: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2348126471042633 at epoch: 21 and batch_num: 23\n",
      "Loss of train set: 0.3177412748336792 at epoch: 21 and batch_num: 24\n",
      "Loss of train set: 0.3113657832145691 at epoch: 21 and batch_num: 25\n",
      "Loss of train set: 0.34277549386024475 at epoch: 21 and batch_num: 26\n",
      "Loss of train set: 0.24836547672748566 at epoch: 21 and batch_num: 27\n",
      "Loss of train set: 0.33916619420051575 at epoch: 21 and batch_num: 28\n",
      "Loss of train set: 0.17581316828727722 at epoch: 21 and batch_num: 29\n",
      "Loss of train set: 0.21610206365585327 at epoch: 21 and batch_num: 30\n",
      "Loss of train set: 0.24158109724521637 at epoch: 21 and batch_num: 31\n",
      "Loss of train set: 0.2203374207019806 at epoch: 21 and batch_num: 32\n",
      "Loss of train set: 0.2078622579574585 at epoch: 21 and batch_num: 33\n",
      "Loss of train set: 0.2891453504562378 at epoch: 21 and batch_num: 34\n",
      "Loss of train set: 0.4800620377063751 at epoch: 21 and batch_num: 35\n",
      "Loss of train set: 0.30472153425216675 at epoch: 21 and batch_num: 36\n",
      "Loss of train set: 0.37901610136032104 at epoch: 21 and batch_num: 37\n",
      "Loss of train set: 0.25169113278388977 at epoch: 21 and batch_num: 38\n",
      "Loss of train set: 0.3006564974784851 at epoch: 21 and batch_num: 39\n",
      "Loss of train set: 0.3116588294506073 at epoch: 21 and batch_num: 40\n",
      "Loss of train set: 0.2527696490287781 at epoch: 21 and batch_num: 41\n",
      "Loss of train set: 0.3028719127178192 at epoch: 21 and batch_num: 42\n",
      "Loss of train set: 0.12291933596134186 at epoch: 21 and batch_num: 43\n",
      "Loss of train set: 0.28651583194732666 at epoch: 21 and batch_num: 44\n",
      "Loss of train set: 0.1750529408454895 at epoch: 21 and batch_num: 45\n",
      "Loss of train set: 0.23862378299236298 at epoch: 21 and batch_num: 46\n",
      "Loss of train set: 0.3888705372810364 at epoch: 21 and batch_num: 47\n",
      "Loss of train set: 0.19414645433425903 at epoch: 21 and batch_num: 48\n",
      "Loss of train set: 0.22915664315223694 at epoch: 21 and batch_num: 49\n",
      "Loss of train set: 0.3682723939418793 at epoch: 21 and batch_num: 50\n",
      "Loss of train set: 0.12391982227563858 at epoch: 21 and batch_num: 51\n",
      "Loss of train set: 0.2762225270271301 at epoch: 21 and batch_num: 52\n",
      "Loss of train set: 0.1877206563949585 at epoch: 21 and batch_num: 53\n",
      "Loss of train set: 0.25814658403396606 at epoch: 21 and batch_num: 54\n",
      "Loss of train set: 0.3440050482749939 at epoch: 21 and batch_num: 55\n",
      "Loss of train set: 0.24694903194904327 at epoch: 21 and batch_num: 56\n",
      "Loss of train set: 0.16308823227882385 at epoch: 21 and batch_num: 57\n",
      "Loss of train set: 0.17950335144996643 at epoch: 21 and batch_num: 58\n",
      "Loss of train set: 0.40321213006973267 at epoch: 21 and batch_num: 59\n",
      "Loss of train set: 0.37849217653274536 at epoch: 21 and batch_num: 60\n",
      "Loss of train set: 0.27744632959365845 at epoch: 21 and batch_num: 61\n",
      "Loss of train set: 0.21531794965267181 at epoch: 21 and batch_num: 62\n",
      "Loss of train set: 0.3575626313686371 at epoch: 21 and batch_num: 63\n",
      "Loss of train set: 0.2086002677679062 at epoch: 21 and batch_num: 64\n",
      "Loss of train set: 0.43269550800323486 at epoch: 21 and batch_num: 65\n",
      "Loss of train set: 0.251257061958313 at epoch: 21 and batch_num: 66\n",
      "Loss of train set: 0.26277071237564087 at epoch: 21 and batch_num: 67\n",
      "Loss of train set: 0.2458859384059906 at epoch: 21 and batch_num: 68\n",
      "Loss of train set: 0.174968421459198 at epoch: 21 and batch_num: 69\n",
      "Loss of train set: 0.4707775115966797 at epoch: 21 and batch_num: 70\n",
      "Loss of train set: 0.2433793693780899 at epoch: 21 and batch_num: 71\n",
      "Loss of train set: 0.2239869385957718 at epoch: 21 and batch_num: 72\n",
      "Loss of train set: 0.15091575682163239 at epoch: 21 and batch_num: 73\n",
      "Loss of train set: 0.4392279386520386 at epoch: 21 and batch_num: 74\n",
      "Loss of train set: 0.4102631211280823 at epoch: 21 and batch_num: 75\n",
      "Loss of train set: 0.36355167627334595 at epoch: 21 and batch_num: 76\n",
      "Loss of train set: 0.2988741993904114 at epoch: 21 and batch_num: 77\n",
      "Loss of train set: 0.41244006156921387 at epoch: 21 and batch_num: 78\n",
      "Loss of train set: 0.26334717869758606 at epoch: 21 and batch_num: 79\n",
      "Loss of train set: 0.26084426045417786 at epoch: 21 and batch_num: 80\n",
      "Loss of train set: 0.2852684259414673 at epoch: 21 and batch_num: 81\n",
      "Loss of train set: 0.3311120271682739 at epoch: 21 and batch_num: 82\n",
      "Loss of train set: 0.3391808867454529 at epoch: 21 and batch_num: 83\n",
      "Loss of train set: 0.23705561459064484 at epoch: 21 and batch_num: 84\n",
      "Loss of train set: 0.26673609018325806 at epoch: 21 and batch_num: 85\n",
      "Loss of train set: 0.39161357283592224 at epoch: 21 and batch_num: 86\n",
      "Loss of train set: 0.1854892373085022 at epoch: 21 and batch_num: 87\n",
      "Loss of train set: 0.3363604247570038 at epoch: 21 and batch_num: 88\n",
      "Loss of train set: 0.26768958568573 at epoch: 21 and batch_num: 89\n",
      "Loss of train set: 0.20838508009910583 at epoch: 21 and batch_num: 90\n",
      "Loss of train set: 0.18050524592399597 at epoch: 21 and batch_num: 91\n",
      "Loss of train set: 0.16253280639648438 at epoch: 21 and batch_num: 92\n",
      "Loss of train set: 0.20365062355995178 at epoch: 21 and batch_num: 93\n",
      "Loss of train set: 0.19921037554740906 at epoch: 21 and batch_num: 94\n",
      "Loss of train set: 0.3181263506412506 at epoch: 21 and batch_num: 95\n",
      "Loss of train set: 0.15863534808158875 at epoch: 21 and batch_num: 96\n",
      "Loss of train set: 0.14257225394248962 at epoch: 21 and batch_num: 97\n",
      "Loss of train set: 0.2414567768573761 at epoch: 21 and batch_num: 98\n",
      "Loss of train set: 0.2800321877002716 at epoch: 21 and batch_num: 99\n",
      "Loss of train set: 0.36726588010787964 at epoch: 21 and batch_num: 100\n",
      "Loss of train set: 0.2424236536026001 at epoch: 21 and batch_num: 101\n",
      "Loss of train set: 0.14626650512218475 at epoch: 21 and batch_num: 102\n",
      "Loss of train set: 0.2893553078174591 at epoch: 21 and batch_num: 103\n",
      "Loss of train set: 0.4729406237602234 at epoch: 21 and batch_num: 104\n",
      "Loss of train set: 0.1894741654396057 at epoch: 21 and batch_num: 105\n",
      "Loss of train set: 0.3744625151157379 at epoch: 21 and batch_num: 106\n",
      "Loss of train set: 0.3123641610145569 at epoch: 21 and batch_num: 107\n",
      "Loss of train set: 0.2801833152770996 at epoch: 21 and batch_num: 108\n",
      "Loss of train set: 0.23506787419319153 at epoch: 21 and batch_num: 109\n",
      "Loss of train set: 0.28274214267730713 at epoch: 21 and batch_num: 110\n",
      "Loss of train set: 0.339386910200119 at epoch: 21 and batch_num: 111\n",
      "Loss of train set: 0.26165252923965454 at epoch: 21 and batch_num: 112\n",
      "Loss of train set: 0.13924740254878998 at epoch: 21 and batch_num: 113\n",
      "Loss of train set: 0.21211296319961548 at epoch: 21 and batch_num: 114\n",
      "Loss of train set: 0.29117536544799805 at epoch: 21 and batch_num: 115\n",
      "Loss of train set: 0.3680366277694702 at epoch: 21 and batch_num: 116\n",
      "Loss of train set: 0.27613261342048645 at epoch: 21 and batch_num: 117\n",
      "Loss of train set: 0.3448947072029114 at epoch: 21 and batch_num: 118\n",
      "Loss of train set: 0.23907257616519928 at epoch: 21 and batch_num: 119\n",
      "Loss of train set: 0.1302313506603241 at epoch: 21 and batch_num: 120\n",
      "Loss of train set: 0.1993199586868286 at epoch: 21 and batch_num: 121\n",
      "Loss of train set: 0.18582658469676971 at epoch: 21 and batch_num: 122\n",
      "Loss of train set: 0.2368440330028534 at epoch: 21 and batch_num: 123\n",
      "Loss of train set: 0.34549611806869507 at epoch: 21 and batch_num: 124\n",
      "Loss of train set: 0.23608529567718506 at epoch: 21 and batch_num: 125\n",
      "Loss of train set: 0.24036355316638947 at epoch: 21 and batch_num: 126\n",
      "Loss of train set: 0.27919507026672363 at epoch: 21 and batch_num: 127\n",
      "Loss of train set: 0.311026930809021 at epoch: 21 and batch_num: 128\n",
      "Loss of train set: 0.2833399176597595 at epoch: 21 and batch_num: 129\n",
      "Loss of train set: 0.34230297803878784 at epoch: 21 and batch_num: 130\n",
      "Loss of train set: 0.3757280707359314 at epoch: 21 and batch_num: 131\n",
      "Loss of train set: 0.13132700324058533 at epoch: 21 and batch_num: 132\n",
      "Loss of train set: 0.25735580921173096 at epoch: 21 and batch_num: 133\n",
      "Loss of train set: 0.255761981010437 at epoch: 21 and batch_num: 134\n",
      "Loss of train set: 0.1825794279575348 at epoch: 21 and batch_num: 135\n",
      "Loss of train set: 0.22666631639003754 at epoch: 21 and batch_num: 136\n",
      "Loss of train set: 0.42034950852394104 at epoch: 21 and batch_num: 137\n",
      "Loss of train set: 0.3828381896018982 at epoch: 21 and batch_num: 138\n",
      "Loss of train set: 0.3000143766403198 at epoch: 21 and batch_num: 139\n",
      "Loss of train set: 0.22315968573093414 at epoch: 21 and batch_num: 140\n",
      "Loss of train set: 0.22213448584079742 at epoch: 21 and batch_num: 141\n",
      "Loss of train set: 0.35185104608535767 at epoch: 21 and batch_num: 142\n",
      "Loss of train set: 0.2903023064136505 at epoch: 21 and batch_num: 143\n",
      "Loss of train set: 0.1316550076007843 at epoch: 21 and batch_num: 144\n",
      "Loss of train set: 0.3133417069911957 at epoch: 21 and batch_num: 145\n",
      "Loss of train set: 0.31883805990219116 at epoch: 21 and batch_num: 146\n",
      "Loss of train set: 0.3063717484474182 at epoch: 21 and batch_num: 147\n",
      "Loss of train set: 0.32079076766967773 at epoch: 21 and batch_num: 148\n",
      "Loss of train set: 0.27654731273651123 at epoch: 21 and batch_num: 149\n",
      "Loss of train set: 0.30058205127716064 at epoch: 21 and batch_num: 150\n",
      "Loss of train set: 0.22330313920974731 at epoch: 21 and batch_num: 151\n",
      "Loss of train set: 0.4223668575286865 at epoch: 21 and batch_num: 152\n",
      "Loss of train set: 0.338451087474823 at epoch: 21 and batch_num: 153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.16354802250862122 at epoch: 21 and batch_num: 154\n",
      "Loss of train set: 0.17700764536857605 at epoch: 21 and batch_num: 155\n",
      "Loss of train set: 0.45315712690353394 at epoch: 21 and batch_num: 156\n",
      "Loss of train set: 0.30518612265586853 at epoch: 21 and batch_num: 157\n",
      "Loss of train set: 0.3293613791465759 at epoch: 21 and batch_num: 158\n",
      "Loss of train set: 0.27616560459136963 at epoch: 21 and batch_num: 159\n",
      "Loss of train set: 0.23198974132537842 at epoch: 21 and batch_num: 160\n",
      "Loss of train set: 0.24700604379177094 at epoch: 21 and batch_num: 161\n",
      "Loss of train set: 0.22690604627132416 at epoch: 21 and batch_num: 162\n",
      "Loss of train set: 0.17659799754619598 at epoch: 21 and batch_num: 163\n",
      "Loss of train set: 0.20259928703308105 at epoch: 21 and batch_num: 164\n",
      "Loss of train set: 0.38280826807022095 at epoch: 21 and batch_num: 165\n",
      "Loss of train set: 0.202815443277359 at epoch: 21 and batch_num: 166\n",
      "Loss of train set: 0.4961967468261719 at epoch: 21 and batch_num: 167\n",
      "Loss of train set: 0.18424853682518005 at epoch: 21 and batch_num: 168\n",
      "Loss of train set: 0.3177123963832855 at epoch: 21 and batch_num: 169\n",
      "Loss of train set: 0.25026217103004456 at epoch: 21 and batch_num: 170\n",
      "Loss of train set: 0.16300475597381592 at epoch: 21 and batch_num: 171\n",
      "Loss of train set: 0.21460559964179993 at epoch: 21 and batch_num: 172\n",
      "Loss of train set: 0.2657792866230011 at epoch: 21 and batch_num: 173\n",
      "Loss of train set: 0.2919311821460724 at epoch: 21 and batch_num: 174\n",
      "Loss of train set: 0.45390045642852783 at epoch: 21 and batch_num: 175\n",
      "Loss of train set: 0.3793064057826996 at epoch: 21 and batch_num: 176\n",
      "Loss of train set: 0.2844587564468384 at epoch: 21 and batch_num: 177\n",
      "Loss of train set: 0.25080084800720215 at epoch: 21 and batch_num: 178\n",
      "Loss of train set: 0.34981030225753784 at epoch: 21 and batch_num: 179\n",
      "Loss of train set: 0.4445164203643799 at epoch: 21 and batch_num: 180\n",
      "Loss of train set: 0.18223342299461365 at epoch: 21 and batch_num: 181\n",
      "Loss of train set: 0.23322853446006775 at epoch: 21 and batch_num: 182\n",
      "Loss of train set: 0.3297349214553833 at epoch: 21 and batch_num: 183\n",
      "Loss of train set: 0.2763350009918213 at epoch: 21 and batch_num: 184\n",
      "Loss of train set: 0.3331451416015625 at epoch: 21 and batch_num: 185\n",
      "Loss of train set: 0.2733322083950043 at epoch: 21 and batch_num: 186\n",
      "Loss of train set: 0.49631768465042114 at epoch: 21 and batch_num: 187\n",
      "Loss of train set: 0.25074833631515503 at epoch: 21 and batch_num: 188\n",
      "Loss of train set: 0.18973970413208008 at epoch: 21 and batch_num: 189\n",
      "Loss of train set: 0.30560022592544556 at epoch: 21 and batch_num: 190\n",
      "Loss of train set: 0.34478408098220825 at epoch: 21 and batch_num: 191\n",
      "Loss of train set: 0.3687018156051636 at epoch: 21 and batch_num: 192\n",
      "Loss of train set: 0.20121096074581146 at epoch: 21 and batch_num: 193\n",
      "Loss of train set: 0.15061169862747192 at epoch: 21 and batch_num: 194\n",
      "Loss of train set: 0.2374931275844574 at epoch: 21 and batch_num: 195\n",
      "Loss of train set: 0.2939501702785492 at epoch: 21 and batch_num: 196\n",
      "Loss of train set: 0.18989287316799164 at epoch: 21 and batch_num: 197\n",
      "Loss of train set: 0.19860847294330597 at epoch: 21 and batch_num: 198\n",
      "Loss of train set: 0.2118360698223114 at epoch: 21 and batch_num: 199\n",
      "Loss of train set: 0.404219388961792 at epoch: 21 and batch_num: 200\n",
      "Loss of train set: 0.26671600341796875 at epoch: 21 and batch_num: 201\n",
      "Loss of train set: 0.3948037624359131 at epoch: 21 and batch_num: 202\n",
      "Loss of train set: 0.21861426532268524 at epoch: 21 and batch_num: 203\n",
      "Loss of train set: 0.24513943493366241 at epoch: 21 and batch_num: 204\n",
      "Loss of train set: 0.31121182441711426 at epoch: 21 and batch_num: 205\n",
      "Loss of train set: 0.23528295755386353 at epoch: 21 and batch_num: 206\n",
      "Loss of train set: 0.27331072092056274 at epoch: 21 and batch_num: 207\n",
      "Loss of train set: 0.2029557228088379 at epoch: 21 and batch_num: 208\n",
      "Loss of train set: 0.3975057005882263 at epoch: 21 and batch_num: 209\n",
      "Loss of train set: 0.14705462753772736 at epoch: 21 and batch_num: 210\n",
      "Loss of train set: 0.20488429069519043 at epoch: 21 and batch_num: 211\n",
      "Loss of train set: 0.32641613483428955 at epoch: 21 and batch_num: 212\n",
      "Loss of train set: 0.2371804416179657 at epoch: 21 and batch_num: 213\n",
      "Loss of train set: 0.3980535566806793 at epoch: 21 and batch_num: 214\n",
      "Loss of train set: 0.2682161033153534 at epoch: 21 and batch_num: 215\n",
      "Loss of train set: 0.38653069734573364 at epoch: 21 and batch_num: 216\n",
      "Loss of train set: 0.19277536869049072 at epoch: 21 and batch_num: 217\n",
      "Loss of train set: 0.3623749613761902 at epoch: 21 and batch_num: 218\n",
      "Loss of train set: 0.20958027243614197 at epoch: 21 and batch_num: 219\n",
      "Loss of train set: 0.368276983499527 at epoch: 21 and batch_num: 220\n",
      "Loss of train set: 0.22970890998840332 at epoch: 21 and batch_num: 221\n",
      "Loss of train set: 0.18773293495178223 at epoch: 21 and batch_num: 222\n",
      "Loss of train set: 0.17499837279319763 at epoch: 21 and batch_num: 223\n",
      "Loss of train set: 0.2004522681236267 at epoch: 21 and batch_num: 224\n",
      "Loss of train set: 0.2982785105705261 at epoch: 21 and batch_num: 225\n",
      "Loss of train set: 0.264573335647583 at epoch: 21 and batch_num: 226\n",
      "Loss of train set: 0.3065038323402405 at epoch: 21 and batch_num: 227\n",
      "Loss of train set: 0.14853549003601074 at epoch: 21 and batch_num: 228\n",
      "Loss of train set: 0.2296963930130005 at epoch: 21 and batch_num: 229\n",
      "Loss of train set: 0.20857873558998108 at epoch: 21 and batch_num: 230\n",
      "Loss of train set: 0.18366852402687073 at epoch: 21 and batch_num: 231\n",
      "Loss of train set: 0.1743498146533966 at epoch: 21 and batch_num: 232\n",
      "Loss of train set: 0.15541590750217438 at epoch: 21 and batch_num: 233\n",
      "Loss of train set: 0.29138046503067017 at epoch: 21 and batch_num: 234\n",
      "Loss of train set: 0.2518564462661743 at epoch: 21 and batch_num: 235\n",
      "Loss of train set: 0.2922404110431671 at epoch: 21 and batch_num: 236\n",
      "Loss of train set: 0.22457526624202728 at epoch: 21 and batch_num: 237\n",
      "Loss of train set: 0.35356464982032776 at epoch: 21 and batch_num: 238\n",
      "Loss of train set: 0.2521684765815735 at epoch: 21 and batch_num: 239\n",
      "Loss of train set: 0.4742497205734253 at epoch: 21 and batch_num: 240\n",
      "Loss of train set: 0.44355958700180054 at epoch: 21 and batch_num: 241\n",
      "Loss of train set: 0.4007253646850586 at epoch: 21 and batch_num: 242\n",
      "Loss of train set: 0.13614538311958313 at epoch: 21 and batch_num: 243\n",
      "Loss of train set: 0.34684625267982483 at epoch: 21 and batch_num: 244\n",
      "Loss of train set: 0.2511356472969055 at epoch: 21 and batch_num: 245\n",
      "Loss of train set: 0.5255679488182068 at epoch: 21 and batch_num: 246\n",
      "Loss of train set: 0.43028920888900757 at epoch: 21 and batch_num: 247\n",
      "Loss of train set: 0.12609735131263733 at epoch: 21 and batch_num: 248\n",
      "Loss of train set: 0.24042683839797974 at epoch: 21 and batch_num: 249\n",
      "Loss of train set: 0.10119503736495972 at epoch: 21 and batch_num: 250\n",
      "Loss of train set: 0.2387186735868454 at epoch: 21 and batch_num: 251\n",
      "Loss of train set: 0.1983037292957306 at epoch: 21 and batch_num: 252\n",
      "Loss of train set: 0.33416834473609924 at epoch: 21 and batch_num: 253\n",
      "Loss of train set: 0.33806467056274414 at epoch: 21 and batch_num: 254\n",
      "Loss of train set: 0.2605978548526764 at epoch: 21 and batch_num: 255\n",
      "Loss of train set: 0.26840323209762573 at epoch: 21 and batch_num: 256\n",
      "Loss of train set: 0.38342729210853577 at epoch: 21 and batch_num: 257\n",
      "Loss of train set: 0.2582353353500366 at epoch: 21 and batch_num: 258\n",
      "Loss of train set: 0.13301745057106018 at epoch: 21 and batch_num: 259\n",
      "Loss of train set: 0.2621843218803406 at epoch: 21 and batch_num: 260\n",
      "Loss of train set: 0.24577365815639496 at epoch: 21 and batch_num: 261\n",
      "Loss of train set: 0.253692090511322 at epoch: 21 and batch_num: 262\n",
      "Loss of train set: 0.30918604135513306 at epoch: 21 and batch_num: 263\n",
      "Loss of train set: 0.19772914052009583 at epoch: 21 and batch_num: 264\n",
      "Loss of train set: 0.2158929407596588 at epoch: 21 and batch_num: 265\n",
      "Loss of train set: 0.3138221502304077 at epoch: 21 and batch_num: 266\n",
      "Loss of train set: 0.3099595308303833 at epoch: 21 and batch_num: 267\n",
      "Loss of train set: 0.21325881779193878 at epoch: 21 and batch_num: 268\n",
      "Loss of train set: 0.3284755349159241 at epoch: 21 and batch_num: 269\n",
      "Loss of train set: 0.36850738525390625 at epoch: 21 and batch_num: 270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3937203288078308 at epoch: 21 and batch_num: 271\n",
      "Loss of train set: 0.29037797451019287 at epoch: 21 and batch_num: 272\n",
      "Loss of train set: 0.21777909994125366 at epoch: 21 and batch_num: 273\n",
      "Loss of train set: 0.4185340404510498 at epoch: 21 and batch_num: 274\n",
      "Loss of train set: 0.34272316098213196 at epoch: 21 and batch_num: 275\n",
      "Loss of train set: 0.1876368522644043 at epoch: 21 and batch_num: 276\n",
      "Loss of train set: 0.3363988995552063 at epoch: 21 and batch_num: 277\n",
      "Loss of train set: 0.14718225598335266 at epoch: 21 and batch_num: 278\n",
      "Loss of train set: 0.268982470035553 at epoch: 21 and batch_num: 279\n",
      "Loss of train set: 0.23471632599830627 at epoch: 21 and batch_num: 280\n",
      "Loss of train set: 0.4131384491920471 at epoch: 21 and batch_num: 281\n",
      "Loss of train set: 0.25796568393707275 at epoch: 21 and batch_num: 282\n",
      "Loss of train set: 0.3293079435825348 at epoch: 21 and batch_num: 283\n",
      "Loss of train set: 0.4256404638290405 at epoch: 21 and batch_num: 284\n",
      "Loss of train set: 0.3599400520324707 at epoch: 21 and batch_num: 285\n",
      "Loss of train set: 0.5447313189506531 at epoch: 21 and batch_num: 286\n",
      "Loss of train set: 0.4306928515434265 at epoch: 21 and batch_num: 287\n",
      "Loss of train set: 0.3365422487258911 at epoch: 21 and batch_num: 288\n",
      "Loss of train set: 0.2860259413719177 at epoch: 21 and batch_num: 289\n",
      "Loss of train set: 0.2441786825656891 at epoch: 21 and batch_num: 290\n",
      "Loss of train set: 0.2582751512527466 at epoch: 21 and batch_num: 291\n",
      "Loss of train set: 0.2196405977010727 at epoch: 21 and batch_num: 292\n",
      "Loss of train set: 0.21070709824562073 at epoch: 21 and batch_num: 293\n",
      "Loss of train set: 0.34985998272895813 at epoch: 21 and batch_num: 294\n",
      "Loss of train set: 0.20901402831077576 at epoch: 21 and batch_num: 295\n",
      "Loss of train set: 0.16515317559242249 at epoch: 21 and batch_num: 296\n",
      "Loss of train set: 0.12697450816631317 at epoch: 21 and batch_num: 297\n",
      "Loss of train set: 0.2897510528564453 at epoch: 21 and batch_num: 298\n",
      "Loss of train set: 0.2532374858856201 at epoch: 21 and batch_num: 299\n",
      "Loss of train set: 0.16734465956687927 at epoch: 21 and batch_num: 300\n",
      "Loss of train set: 0.17898766696453094 at epoch: 21 and batch_num: 301\n",
      "Loss of train set: 0.3521766662597656 at epoch: 21 and batch_num: 302\n",
      "Loss of train set: 0.19399474561214447 at epoch: 21 and batch_num: 303\n",
      "Loss of train set: 0.2797343134880066 at epoch: 21 and batch_num: 304\n",
      "Loss of train set: 0.10209834575653076 at epoch: 21 and batch_num: 305\n",
      "Loss of train set: 0.24091598391532898 at epoch: 21 and batch_num: 306\n",
      "Loss of train set: 0.31875357031822205 at epoch: 21 and batch_num: 307\n",
      "Loss of train set: 0.36840611696243286 at epoch: 21 and batch_num: 308\n",
      "Loss of train set: 0.3198249936103821 at epoch: 21 and batch_num: 309\n",
      "Loss of train set: 0.252454936504364 at epoch: 21 and batch_num: 310\n",
      "Loss of train set: 0.29432225227355957 at epoch: 21 and batch_num: 311\n",
      "Loss of train set: 0.38486969470977783 at epoch: 21 and batch_num: 312\n",
      "Loss of train set: 0.29286593198776245 at epoch: 21 and batch_num: 313\n",
      "Loss of train set: 0.3899083733558655 at epoch: 21 and batch_num: 314\n",
      "Loss of train set: 0.21198342740535736 at epoch: 21 and batch_num: 315\n",
      "Loss of train set: 0.3332934081554413 at epoch: 21 and batch_num: 316\n",
      "Loss of train set: 0.2590947449207306 at epoch: 21 and batch_num: 317\n",
      "Loss of train set: 0.22777432203292847 at epoch: 21 and batch_num: 318\n",
      "Loss of train set: 0.5074738264083862 at epoch: 21 and batch_num: 319\n",
      "Loss of train set: 0.14894837141036987 at epoch: 21 and batch_num: 320\n",
      "Loss of train set: 0.17106014490127563 at epoch: 21 and batch_num: 321\n",
      "Loss of train set: 0.3300327658653259 at epoch: 21 and batch_num: 322\n",
      "Loss of train set: 0.5212022066116333 at epoch: 21 and batch_num: 323\n",
      "Loss of train set: 0.3041960597038269 at epoch: 21 and batch_num: 324\n",
      "Loss of train set: 0.1969471573829651 at epoch: 21 and batch_num: 325\n",
      "Loss of train set: 0.35142168402671814 at epoch: 21 and batch_num: 326\n",
      "Loss of train set: 0.3161836266517639 at epoch: 21 and batch_num: 327\n",
      "Loss of train set: 0.44966304302215576 at epoch: 21 and batch_num: 328\n",
      "Loss of train set: 0.30198389291763306 at epoch: 21 and batch_num: 329\n",
      "Loss of train set: 0.1432730257511139 at epoch: 21 and batch_num: 330\n",
      "Loss of train set: 0.20998354256153107 at epoch: 21 and batch_num: 331\n",
      "Loss of train set: 0.649079442024231 at epoch: 21 and batch_num: 332\n",
      "Loss of train set: 0.26470687985420227 at epoch: 21 and batch_num: 333\n",
      "Loss of train set: 0.24164125323295593 at epoch: 21 and batch_num: 334\n",
      "Loss of train set: 0.2522544860839844 at epoch: 21 and batch_num: 335\n",
      "Loss of train set: 0.20245519280433655 at epoch: 21 and batch_num: 336\n",
      "Loss of train set: 0.19614940881729126 at epoch: 21 and batch_num: 337\n",
      "Loss of train set: 0.3343018591403961 at epoch: 21 and batch_num: 338\n",
      "Loss of train set: 0.28155750036239624 at epoch: 21 and batch_num: 339\n",
      "Loss of train set: 0.33488380908966064 at epoch: 21 and batch_num: 340\n",
      "Loss of train set: 0.3180769383907318 at epoch: 21 and batch_num: 341\n",
      "Loss of train set: 0.2950294017791748 at epoch: 21 and batch_num: 342\n",
      "Loss of train set: 0.3725844621658325 at epoch: 21 and batch_num: 343\n",
      "Loss of train set: 0.1585715413093567 at epoch: 21 and batch_num: 344\n",
      "Loss of train set: 0.2173086702823639 at epoch: 21 and batch_num: 345\n",
      "Loss of train set: 0.3611224293708801 at epoch: 21 and batch_num: 346\n",
      "Loss of train set: 0.23518115282058716 at epoch: 21 and batch_num: 347\n",
      "Loss of train set: 0.09579049795866013 at epoch: 21 and batch_num: 348\n",
      "Loss of train set: 0.26226240396499634 at epoch: 21 and batch_num: 349\n",
      "Loss of train set: 0.18505041301250458 at epoch: 21 and batch_num: 350\n",
      "Loss of train set: 0.2737593650817871 at epoch: 21 and batch_num: 351\n",
      "Loss of train set: 0.2207513302564621 at epoch: 21 and batch_num: 352\n",
      "Loss of train set: 0.20598384737968445 at epoch: 21 and batch_num: 353\n",
      "Loss of train set: 0.2145141065120697 at epoch: 21 and batch_num: 354\n",
      "Loss of train set: 0.3786645829677582 at epoch: 21 and batch_num: 355\n",
      "Loss of train set: 0.29143422842025757 at epoch: 21 and batch_num: 356\n",
      "Loss of train set: 0.25650104880332947 at epoch: 21 and batch_num: 357\n",
      "Loss of train set: 0.3327551782131195 at epoch: 21 and batch_num: 358\n",
      "Loss of train set: 0.5013225078582764 at epoch: 21 and batch_num: 359\n",
      "Loss of train set: 0.1778448075056076 at epoch: 21 and batch_num: 360\n",
      "Loss of train set: 0.277861624956131 at epoch: 21 and batch_num: 361\n",
      "Loss of train set: 0.1504392921924591 at epoch: 21 and batch_num: 362\n",
      "Loss of train set: 0.34154653549194336 at epoch: 21 and batch_num: 363\n",
      "Loss of train set: 0.20409110188484192 at epoch: 21 and batch_num: 364\n",
      "Loss of train set: 0.33296412229537964 at epoch: 21 and batch_num: 365\n",
      "Loss of train set: 0.1699979305267334 at epoch: 21 and batch_num: 366\n",
      "Loss of train set: 0.32055753469467163 at epoch: 21 and batch_num: 367\n",
      "Loss of train set: 0.3848796486854553 at epoch: 21 and batch_num: 368\n",
      "Loss of train set: 0.25764578580856323 at epoch: 21 and batch_num: 369\n",
      "Loss of train set: 0.24525918066501617 at epoch: 21 and batch_num: 370\n",
      "Loss of train set: 0.17971783876419067 at epoch: 21 and batch_num: 371\n",
      "Loss of train set: 0.380850613117218 at epoch: 21 and batch_num: 372\n",
      "Loss of train set: 0.33715397119522095 at epoch: 21 and batch_num: 373\n",
      "Loss of train set: 0.2097156047821045 at epoch: 21 and batch_num: 374\n",
      "Loss of train set: 0.3014068603515625 at epoch: 21 and batch_num: 375\n",
      "Loss of train set: 0.2675621509552002 at epoch: 21 and batch_num: 376\n",
      "Loss of train set: 0.3550001382827759 at epoch: 21 and batch_num: 377\n",
      "Loss of train set: 0.29207783937454224 at epoch: 21 and batch_num: 378\n",
      "Loss of train set: 0.24086406826972961 at epoch: 21 and batch_num: 379\n",
      "Loss of train set: 0.40308722853660583 at epoch: 21 and batch_num: 380\n",
      "Loss of train set: 0.46243342757225037 at epoch: 21 and batch_num: 381\n",
      "Loss of train set: 0.25797683000564575 at epoch: 21 and batch_num: 382\n",
      "Loss of train set: 0.24202775955200195 at epoch: 21 and batch_num: 383\n",
      "Loss of train set: 0.28544050455093384 at epoch: 21 and batch_num: 384\n",
      "Loss of train set: 0.2970995008945465 at epoch: 21 and batch_num: 385\n",
      "Loss of train set: 0.20961111783981323 at epoch: 21 and batch_num: 386\n",
      "Loss of train set: 0.21539154648780823 at epoch: 21 and batch_num: 387\n",
      "Loss of train set: 0.3647844195365906 at epoch: 21 and batch_num: 388\n",
      "Loss of train set: 0.29345521330833435 at epoch: 21 and batch_num: 389\n",
      "Loss of train set: 0.23567301034927368 at epoch: 21 and batch_num: 390\n",
      "Loss of train set: 0.3312787711620331 at epoch: 21 and batch_num: 391\n",
      "Loss of train set: 0.31056731939315796 at epoch: 21 and batch_num: 392\n",
      "Loss of train set: 0.17196567356586456 at epoch: 21 and batch_num: 393\n",
      "Loss of train set: 0.2891388535499573 at epoch: 21 and batch_num: 394\n",
      "Loss of train set: 0.5915650129318237 at epoch: 21 and batch_num: 395\n",
      "Loss of train set: 0.24051296710968018 at epoch: 21 and batch_num: 396\n",
      "Loss of train set: 0.22649995982646942 at epoch: 21 and batch_num: 397\n",
      "Loss of train set: 0.3709595799446106 at epoch: 21 and batch_num: 398\n",
      "Loss of train set: 0.3058331608772278 at epoch: 21 and batch_num: 399\n",
      "Loss of train set: 0.40689003467559814 at epoch: 21 and batch_num: 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.23307004570960999 at epoch: 21 and batch_num: 401\n",
      "Loss of train set: 0.2474730759859085 at epoch: 21 and batch_num: 402\n",
      "Loss of train set: 0.15662431716918945 at epoch: 21 and batch_num: 403\n",
      "Loss of train set: 0.4242827594280243 at epoch: 21 and batch_num: 404\n",
      "Loss of train set: 0.2581048011779785 at epoch: 21 and batch_num: 405\n",
      "Loss of train set: 0.17180758714675903 at epoch: 21 and batch_num: 406\n",
      "Loss of train set: 0.2503308653831482 at epoch: 21 and batch_num: 407\n",
      "Loss of train set: 0.2725449800491333 at epoch: 21 and batch_num: 408\n",
      "Loss of train set: 0.3390584886074066 at epoch: 21 and batch_num: 409\n",
      "Loss of train set: 0.35583874583244324 at epoch: 21 and batch_num: 410\n",
      "Loss of train set: 0.5503800511360168 at epoch: 21 and batch_num: 411\n",
      "Loss of train set: 0.3366895318031311 at epoch: 21 and batch_num: 412\n",
      "Loss of train set: 0.17822441458702087 at epoch: 21 and batch_num: 413\n",
      "Loss of train set: 0.2994205951690674 at epoch: 21 and batch_num: 414\n",
      "Loss of train set: 0.16209661960601807 at epoch: 21 and batch_num: 415\n",
      "Loss of train set: 0.29363226890563965 at epoch: 21 and batch_num: 416\n",
      "Loss of train set: 0.30002906918525696 at epoch: 21 and batch_num: 417\n",
      "Loss of train set: 0.1884286105632782 at epoch: 21 and batch_num: 418\n",
      "Loss of train set: 0.280396431684494 at epoch: 21 and batch_num: 419\n",
      "Loss of train set: 0.3952135145664215 at epoch: 21 and batch_num: 420\n",
      "Loss of train set: 0.2774519920349121 at epoch: 21 and batch_num: 421\n",
      "Loss of train set: 0.2501162886619568 at epoch: 21 and batch_num: 422\n",
      "Loss of train set: 0.3624258041381836 at epoch: 21 and batch_num: 423\n",
      "Loss of train set: 0.23424682021141052 at epoch: 21 and batch_num: 424\n",
      "Loss of train set: 0.22497493028640747 at epoch: 21 and batch_num: 425\n",
      "Loss of train set: 0.25188368558883667 at epoch: 21 and batch_num: 426\n",
      "Loss of train set: 0.21801941096782684 at epoch: 21 and batch_num: 427\n",
      "Loss of train set: 0.3593199551105499 at epoch: 21 and batch_num: 428\n",
      "Loss of train set: 0.31173527240753174 at epoch: 21 and batch_num: 429\n",
      "Loss of train set: 0.29173406958580017 at epoch: 21 and batch_num: 430\n",
      "Loss of train set: 0.3058501183986664 at epoch: 21 and batch_num: 431\n",
      "Loss of train set: 0.33945727348327637 at epoch: 21 and batch_num: 432\n",
      "Loss of train set: 0.550061821937561 at epoch: 21 and batch_num: 433\n",
      "Loss of train set: 0.3276717960834503 at epoch: 21 and batch_num: 434\n",
      "Loss of train set: 0.4784896969795227 at epoch: 21 and batch_num: 435\n",
      "Loss of train set: 0.18095386028289795 at epoch: 21 and batch_num: 436\n",
      "Loss of train set: 0.12714943289756775 at epoch: 21 and batch_num: 437\n",
      "Loss of train set: 0.28362154960632324 at epoch: 21 and batch_num: 438\n",
      "Loss of train set: 0.13490033149719238 at epoch: 21 and batch_num: 439\n",
      "Loss of train set: 0.20046120882034302 at epoch: 21 and batch_num: 440\n",
      "Loss of train set: 0.32852402329444885 at epoch: 21 and batch_num: 441\n",
      "Loss of train set: 0.16318053007125854 at epoch: 21 and batch_num: 442\n",
      "Loss of train set: 0.26340052485466003 at epoch: 21 and batch_num: 443\n",
      "Loss of train set: 0.4337839186191559 at epoch: 21 and batch_num: 444\n",
      "Loss of train set: 0.2615792453289032 at epoch: 21 and batch_num: 445\n",
      "Loss of train set: 0.23689398169517517 at epoch: 21 and batch_num: 446\n",
      "Loss of train set: 0.31567585468292236 at epoch: 21 and batch_num: 447\n",
      "Loss of train set: 0.23198774456977844 at epoch: 21 and batch_num: 448\n",
      "Loss of train set: 0.3458818197250366 at epoch: 21 and batch_num: 449\n",
      "Loss of train set: 0.32010704278945923 at epoch: 21 and batch_num: 450\n",
      "Loss of train set: 0.45683950185775757 at epoch: 21 and batch_num: 451\n",
      "Loss of train set: 0.26725947856903076 at epoch: 21 and batch_num: 452\n",
      "Loss of train set: 0.20200586318969727 at epoch: 21 and batch_num: 453\n",
      "Loss of train set: 0.38150811195373535 at epoch: 21 and batch_num: 454\n",
      "Loss of train set: 0.21500816941261292 at epoch: 21 and batch_num: 455\n",
      "Loss of train set: 0.28708070516586304 at epoch: 21 and batch_num: 456\n",
      "Loss of train set: 0.3210892677307129 at epoch: 21 and batch_num: 457\n",
      "Loss of train set: 0.3178301453590393 at epoch: 21 and batch_num: 458\n",
      "Loss of train set: 0.24714016914367676 at epoch: 21 and batch_num: 459\n",
      "Loss of train set: 0.26918134093284607 at epoch: 21 and batch_num: 460\n",
      "Loss of train set: 0.3650110065937042 at epoch: 21 and batch_num: 461\n",
      "Loss of train set: 0.4511309862136841 at epoch: 21 and batch_num: 462\n",
      "Loss of train set: 0.3276095390319824 at epoch: 21 and batch_num: 463\n",
      "Loss of train set: 0.27069711685180664 at epoch: 21 and batch_num: 464\n",
      "Loss of train set: 0.42870032787323 at epoch: 21 and batch_num: 465\n",
      "Loss of train set: 0.27733251452445984 at epoch: 21 and batch_num: 466\n",
      "Loss of train set: 0.2930252254009247 at epoch: 21 and batch_num: 467\n",
      "Loss of train set: 0.3133084774017334 at epoch: 21 and batch_num: 468\n",
      "Loss of train set: 0.416473388671875 at epoch: 21 and batch_num: 469\n",
      "Loss of train set: 0.3119317293167114 at epoch: 21 and batch_num: 470\n",
      "Loss of train set: 0.31044599413871765 at epoch: 21 and batch_num: 471\n",
      "Loss of train set: 0.34963685274124146 at epoch: 21 and batch_num: 472\n",
      "Loss of train set: 0.16516657173633575 at epoch: 21 and batch_num: 473\n",
      "Loss of train set: 0.3010917007923126 at epoch: 21 and batch_num: 474\n",
      "Loss of train set: 0.29610252380371094 at epoch: 21 and batch_num: 475\n",
      "Loss of train set: 0.42896175384521484 at epoch: 21 and batch_num: 476\n",
      "Loss of train set: 0.256700724363327 at epoch: 21 and batch_num: 477\n",
      "Loss of train set: 0.314902663230896 at epoch: 21 and batch_num: 478\n",
      "Loss of train set: 0.35781458020210266 at epoch: 21 and batch_num: 479\n",
      "Loss of train set: 0.27164265513420105 at epoch: 21 and batch_num: 480\n",
      "Loss of train set: 0.2310604453086853 at epoch: 21 and batch_num: 481\n",
      "Loss of train set: 0.30437207221984863 at epoch: 21 and batch_num: 482\n",
      "Loss of train set: 0.35703808069229126 at epoch: 21 and batch_num: 483\n",
      "Loss of train set: 0.5100085139274597 at epoch: 21 and batch_num: 484\n",
      "Loss of train set: 0.3867186903953552 at epoch: 21 and batch_num: 485\n",
      "Loss of train set: 0.48070836067199707 at epoch: 21 and batch_num: 486\n",
      "Loss of train set: 0.45482319593429565 at epoch: 21 and batch_num: 487\n",
      "Loss of train set: 0.25019699335098267 at epoch: 21 and batch_num: 488\n",
      "Loss of train set: 0.33272218704223633 at epoch: 21 and batch_num: 489\n",
      "Loss of train set: 0.3253058195114136 at epoch: 21 and batch_num: 490\n",
      "Loss of train set: 0.21239838004112244 at epoch: 21 and batch_num: 491\n",
      "Loss of train set: 0.37675541639328003 at epoch: 21 and batch_num: 492\n",
      "Loss of train set: 0.3184490203857422 at epoch: 21 and batch_num: 493\n",
      "Loss of train set: 0.29464900493621826 at epoch: 21 and batch_num: 494\n",
      "Loss of train set: 0.2295021265745163 at epoch: 21 and batch_num: 495\n",
      "Loss of train set: 0.20756568014621735 at epoch: 21 and batch_num: 496\n",
      "Loss of train set: 0.17421771585941315 at epoch: 21 and batch_num: 497\n",
      "Loss of train set: 0.21348534524440765 at epoch: 21 and batch_num: 498\n",
      "Loss of train set: 0.18218505382537842 at epoch: 21 and batch_num: 499\n",
      "Loss of train set: 0.5525285601615906 at epoch: 21 and batch_num: 500\n",
      "Loss of train set: 0.3014426827430725 at epoch: 21 and batch_num: 501\n",
      "Loss of train set: 0.31863710284233093 at epoch: 21 and batch_num: 502\n",
      "Loss of train set: 0.23315130174160004 at epoch: 21 and batch_num: 503\n",
      "Loss of train set: 0.21850425004959106 at epoch: 21 and batch_num: 504\n",
      "Loss of train set: 0.2530217170715332 at epoch: 21 and batch_num: 505\n",
      "Loss of train set: 0.3726913630962372 at epoch: 21 and batch_num: 506\n",
      "Loss of train set: 0.1876337081193924 at epoch: 21 and batch_num: 507\n",
      "Loss of train set: 0.28987717628479004 at epoch: 21 and batch_num: 508\n",
      "Loss of train set: 0.2071271538734436 at epoch: 21 and batch_num: 509\n",
      "Loss of train set: 0.2880256772041321 at epoch: 21 and batch_num: 510\n",
      "Loss of train set: 0.1376054286956787 at epoch: 21 and batch_num: 511\n",
      "Loss of train set: 0.23736567795276642 at epoch: 21 and batch_num: 512\n",
      "Loss of train set: 0.17728574573993683 at epoch: 21 and batch_num: 513\n",
      "Loss of train set: 0.22766678035259247 at epoch: 21 and batch_num: 514\n",
      "Loss of train set: 0.2639773190021515 at epoch: 21 and batch_num: 515\n",
      "Loss of train set: 0.24618543684482574 at epoch: 21 and batch_num: 516\n",
      "Loss of train set: 0.30430614948272705 at epoch: 21 and batch_num: 517\n",
      "Loss of train set: 0.20561563968658447 at epoch: 21 and batch_num: 518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.201076477766037 at epoch: 21 and batch_num: 519\n",
      "Loss of train set: 0.24970892071723938 at epoch: 21 and batch_num: 520\n",
      "Loss of train set: 0.2277415692806244 at epoch: 21 and batch_num: 521\n",
      "Loss of train set: 0.23998263478279114 at epoch: 21 and batch_num: 522\n",
      "Loss of train set: 0.3386099934577942 at epoch: 21 and batch_num: 523\n",
      "Loss of train set: 0.12870590388774872 at epoch: 21 and batch_num: 524\n",
      "Loss of train set: 0.3754376471042633 at epoch: 21 and batch_num: 525\n",
      "Loss of train set: 0.2722168564796448 at epoch: 21 and batch_num: 526\n",
      "Loss of train set: 0.12637652456760406 at epoch: 21 and batch_num: 527\n",
      "Loss of train set: 0.4585065245628357 at epoch: 21 and batch_num: 528\n",
      "Loss of train set: 0.2772338390350342 at epoch: 21 and batch_num: 529\n",
      "Loss of train set: 0.2969043552875519 at epoch: 21 and batch_num: 530\n",
      "Loss of train set: 0.22475071251392365 at epoch: 21 and batch_num: 531\n",
      "Loss of train set: 0.19414415955543518 at epoch: 21 and batch_num: 532\n",
      "Loss of train set: 0.19995331764221191 at epoch: 21 and batch_num: 533\n",
      "Loss of train set: 0.42730650305747986 at epoch: 21 and batch_num: 534\n",
      "Loss of train set: 0.23375043272972107 at epoch: 21 and batch_num: 535\n",
      "Loss of train set: 0.28288280963897705 at epoch: 21 and batch_num: 536\n",
      "Loss of train set: 0.28791385889053345 at epoch: 21 and batch_num: 537\n",
      "Loss of train set: 0.3811892867088318 at epoch: 21 and batch_num: 538\n",
      "Loss of train set: 0.2884793281555176 at epoch: 21 and batch_num: 539\n",
      "Loss of train set: 0.31647777557373047 at epoch: 21 and batch_num: 540\n",
      "Loss of train set: 0.2958061099052429 at epoch: 21 and batch_num: 541\n",
      "Loss of train set: 0.1728893667459488 at epoch: 21 and batch_num: 542\n",
      "Loss of train set: 0.16448014974594116 at epoch: 21 and batch_num: 543\n",
      "Loss of train set: 0.4078447222709656 at epoch: 21 and batch_num: 544\n",
      "Loss of train set: 0.2111583799123764 at epoch: 21 and batch_num: 545\n",
      "Loss of train set: 0.36715954542160034 at epoch: 21 and batch_num: 546\n",
      "Loss of train set: 0.2789413332939148 at epoch: 21 and batch_num: 547\n",
      "Loss of train set: 0.2269132286310196 at epoch: 21 and batch_num: 548\n",
      "Loss of train set: 0.320593923330307 at epoch: 21 and batch_num: 549\n",
      "Loss of train set: 0.15204361081123352 at epoch: 21 and batch_num: 550\n",
      "Loss of train set: 0.22222715616226196 at epoch: 21 and batch_num: 551\n",
      "Loss of train set: 0.2735649049282074 at epoch: 21 and batch_num: 552\n",
      "Loss of train set: 0.259213924407959 at epoch: 21 and batch_num: 553\n",
      "Loss of train set: 0.4596744179725647 at epoch: 21 and batch_num: 554\n",
      "Loss of train set: 0.20927302539348602 at epoch: 21 and batch_num: 555\n",
      "Loss of train set: 0.2288699448108673 at epoch: 21 and batch_num: 556\n",
      "Loss of train set: 0.20450706779956818 at epoch: 21 and batch_num: 557\n",
      "Loss of train set: 0.27597570419311523 at epoch: 21 and batch_num: 558\n",
      "Loss of train set: 0.2641003131866455 at epoch: 21 and batch_num: 559\n",
      "Loss of train set: 0.23656439781188965 at epoch: 21 and batch_num: 560\n",
      "Loss of train set: 0.22292320430278778 at epoch: 21 and batch_num: 561\n",
      "Loss of train set: 0.33663302659988403 at epoch: 21 and batch_num: 562\n",
      "Loss of train set: 0.28063270449638367 at epoch: 21 and batch_num: 563\n",
      "Loss of train set: 0.1903211623430252 at epoch: 21 and batch_num: 564\n",
      "Loss of train set: 0.35545629262924194 at epoch: 21 and batch_num: 565\n",
      "Loss of train set: 0.22011791169643402 at epoch: 21 and batch_num: 566\n",
      "Loss of train set: 0.1869772970676422 at epoch: 21 and batch_num: 567\n",
      "Loss of train set: 0.31001928448677063 at epoch: 21 and batch_num: 568\n",
      "Loss of train set: 0.3027929961681366 at epoch: 21 and batch_num: 569\n",
      "Loss of train set: 0.33849436044692993 at epoch: 21 and batch_num: 570\n",
      "Loss of train set: 0.2438410520553589 at epoch: 21 and batch_num: 571\n",
      "Loss of train set: 0.15489575266838074 at epoch: 21 and batch_num: 572\n",
      "Loss of train set: 0.2584293484687805 at epoch: 21 and batch_num: 573\n",
      "Loss of train set: 0.2945026457309723 at epoch: 21 and batch_num: 574\n",
      "Loss of train set: 0.39381301403045654 at epoch: 21 and batch_num: 575\n",
      "Loss of train set: 0.35444873571395874 at epoch: 21 and batch_num: 576\n",
      "Loss of train set: 0.2716054618358612 at epoch: 21 and batch_num: 577\n",
      "Loss of train set: 0.25983208417892456 at epoch: 21 and batch_num: 578\n",
      "Loss of train set: 0.28288501501083374 at epoch: 21 and batch_num: 579\n",
      "Loss of train set: 0.21808332204818726 at epoch: 21 and batch_num: 580\n",
      "Loss of train set: 0.3797990679740906 at epoch: 21 and batch_num: 581\n",
      "Loss of train set: 0.2727794945240021 at epoch: 21 and batch_num: 582\n",
      "Loss of train set: 0.1943725049495697 at epoch: 21 and batch_num: 583\n",
      "Loss of train set: 0.32181280851364136 at epoch: 21 and batch_num: 584\n",
      "Loss of train set: 0.3025432825088501 at epoch: 21 and batch_num: 585\n",
      "Loss of train set: 0.20616042613983154 at epoch: 21 and batch_num: 586\n",
      "Loss of train set: 0.20837515592575073 at epoch: 21 and batch_num: 587\n",
      "Loss of train set: 0.42866307497024536 at epoch: 21 and batch_num: 588\n",
      "Loss of train set: 0.2946675717830658 at epoch: 21 and batch_num: 589\n",
      "Loss of train set: 0.18966636061668396 at epoch: 21 and batch_num: 590\n",
      "Loss of train set: 0.26929306983947754 at epoch: 21 and batch_num: 591\n",
      "Loss of train set: 0.10305479168891907 at epoch: 21 and batch_num: 592\n",
      "Loss of train set: 0.2774788737297058 at epoch: 21 and batch_num: 593\n",
      "Loss of train set: 0.29008451104164124 at epoch: 21 and batch_num: 594\n",
      "Loss of train set: 0.2627011239528656 at epoch: 21 and batch_num: 595\n",
      "Loss of train set: 0.44893303513526917 at epoch: 21 and batch_num: 596\n",
      "Loss of train set: 0.3668779730796814 at epoch: 21 and batch_num: 597\n",
      "Loss of train set: 0.23993514478206635 at epoch: 21 and batch_num: 598\n",
      "Loss of train set: 0.4361848831176758 at epoch: 21 and batch_num: 599\n",
      "Loss of train set: 0.23043686151504517 at epoch: 21 and batch_num: 600\n",
      "Loss of train set: 0.2543119788169861 at epoch: 21 and batch_num: 601\n",
      "Loss of train set: 0.2495088279247284 at epoch: 21 and batch_num: 602\n",
      "Loss of train set: 0.4466792047023773 at epoch: 21 and batch_num: 603\n",
      "Loss of train set: 0.20168595016002655 at epoch: 21 and batch_num: 604\n",
      "Loss of train set: 0.3524244427680969 at epoch: 21 and batch_num: 605\n",
      "Loss of train set: 0.40455353260040283 at epoch: 21 and batch_num: 606\n",
      "Loss of train set: 0.2889467775821686 at epoch: 21 and batch_num: 607\n",
      "Loss of train set: 0.1984429657459259 at epoch: 21 and batch_num: 608\n",
      "Loss of train set: 0.29332518577575684 at epoch: 21 and batch_num: 609\n",
      "Loss of train set: 0.235086590051651 at epoch: 21 and batch_num: 610\n",
      "Loss of train set: 0.17035450041294098 at epoch: 21 and batch_num: 611\n",
      "Loss of train set: 0.23096208274364471 at epoch: 21 and batch_num: 612\n",
      "Loss of train set: 0.24170131981372833 at epoch: 21 and batch_num: 613\n",
      "Loss of train set: 0.226526141166687 at epoch: 21 and batch_num: 614\n",
      "Loss of train set: 0.4519784450531006 at epoch: 21 and batch_num: 615\n",
      "Loss of train set: 0.39939039945602417 at epoch: 21 and batch_num: 616\n",
      "Loss of train set: 0.2600897550582886 at epoch: 21 and batch_num: 617\n",
      "Loss of train set: 0.24481560289859772 at epoch: 21 and batch_num: 618\n",
      "Loss of train set: 0.22500281035900116 at epoch: 21 and batch_num: 619\n",
      "Loss of train set: 0.34601181745529175 at epoch: 21 and batch_num: 620\n",
      "Loss of train set: 0.15654799342155457 at epoch: 21 and batch_num: 621\n",
      "Loss of train set: 0.32846206426620483 at epoch: 21 and batch_num: 622\n",
      "Loss of train set: 0.1327599287033081 at epoch: 21 and batch_num: 623\n",
      "Loss of train set: 0.23296228051185608 at epoch: 21 and batch_num: 624\n",
      "Loss of train set: 0.21144068241119385 at epoch: 21 and batch_num: 625\n",
      "Loss of train set: 0.2819414734840393 at epoch: 21 and batch_num: 626\n",
      "Loss of train set: 0.29096534848213196 at epoch: 21 and batch_num: 627\n",
      "Loss of train set: 0.21105751395225525 at epoch: 21 and batch_num: 628\n",
      "Loss of train set: 0.20374299585819244 at epoch: 21 and batch_num: 629\n",
      "Loss of train set: 0.30956190824508667 at epoch: 21 and batch_num: 630\n",
      "Loss of train set: 0.2832472622394562 at epoch: 21 and batch_num: 631\n",
      "Loss of train set: 0.16293293237686157 at epoch: 21 and batch_num: 632\n",
      "Loss of train set: 0.29836535453796387 at epoch: 21 and batch_num: 633\n",
      "Loss of train set: 0.4693470597267151 at epoch: 21 and batch_num: 634\n",
      "Loss of train set: 0.2893076539039612 at epoch: 21 and batch_num: 635\n",
      "Loss of train set: 0.22688956558704376 at epoch: 21 and batch_num: 636\n",
      "Loss of train set: 0.14850836992263794 at epoch: 21 and batch_num: 637\n",
      "Loss of train set: 0.11461874842643738 at epoch: 21 and batch_num: 638\n",
      "Loss of train set: 0.23683494329452515 at epoch: 21 and batch_num: 639\n",
      "Loss of train set: 0.24759045243263245 at epoch: 21 and batch_num: 640\n",
      "Loss of train set: 0.39339157938957214 at epoch: 21 and batch_num: 641\n",
      "Loss of train set: 0.34665000438690186 at epoch: 21 and batch_num: 642\n",
      "Loss of train set: 0.2938340902328491 at epoch: 21 and batch_num: 643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3431164026260376 at epoch: 21 and batch_num: 644\n",
      "Loss of train set: 0.20347991585731506 at epoch: 21 and batch_num: 645\n",
      "Loss of train set: 0.572380006313324 at epoch: 21 and batch_num: 646\n",
      "Loss of train set: 0.32352572679519653 at epoch: 21 and batch_num: 647\n",
      "Loss of train set: 0.218581885099411 at epoch: 21 and batch_num: 648\n",
      "Loss of train set: 0.1971363127231598 at epoch: 21 and batch_num: 649\n",
      "Loss of train set: 0.26433515548706055 at epoch: 21 and batch_num: 650\n",
      "Loss of train set: 0.27335184812545776 at epoch: 21 and batch_num: 651\n",
      "Loss of train set: 0.31934255361557007 at epoch: 21 and batch_num: 652\n",
      "Loss of train set: 0.3013705611228943 at epoch: 21 and batch_num: 653\n",
      "Loss of train set: 0.3145533502101898 at epoch: 21 and batch_num: 654\n",
      "Loss of train set: 0.3548594117164612 at epoch: 21 and batch_num: 655\n",
      "Loss of train set: 0.2953474223613739 at epoch: 21 and batch_num: 656\n",
      "Loss of train set: 0.1624017357826233 at epoch: 21 and batch_num: 657\n",
      "Loss of train set: 0.314855694770813 at epoch: 21 and batch_num: 658\n",
      "Loss of train set: 0.40546008944511414 at epoch: 21 and batch_num: 659\n",
      "Loss of train set: 0.3024907112121582 at epoch: 21 and batch_num: 660\n",
      "Loss of train set: 0.20865657925605774 at epoch: 21 and batch_num: 661\n",
      "Loss of train set: 0.20579937100410461 at epoch: 21 and batch_num: 662\n",
      "Loss of train set: 0.2666456997394562 at epoch: 21 and batch_num: 663\n",
      "Loss of train set: 0.25387728214263916 at epoch: 21 and batch_num: 664\n",
      "Loss of train set: 0.32683825492858887 at epoch: 21 and batch_num: 665\n",
      "Loss of train set: 0.20914992690086365 at epoch: 21 and batch_num: 666\n",
      "Loss of train set: 0.4488183856010437 at epoch: 21 and batch_num: 667\n",
      "Loss of train set: 0.25748777389526367 at epoch: 21 and batch_num: 668\n",
      "Loss of train set: 0.23322080075740814 at epoch: 21 and batch_num: 669\n",
      "Loss of train set: 0.2987220585346222 at epoch: 21 and batch_num: 670\n",
      "Loss of train set: 0.23328934609889984 at epoch: 21 and batch_num: 671\n",
      "Loss of train set: 0.1067868247628212 at epoch: 21 and batch_num: 672\n",
      "Loss of train set: 0.23262064158916473 at epoch: 21 and batch_num: 673\n",
      "Loss of train set: 0.3789314031600952 at epoch: 21 and batch_num: 674\n",
      "Loss of train set: 0.19328777492046356 at epoch: 21 and batch_num: 675\n",
      "Loss of train set: 0.19340041279792786 at epoch: 21 and batch_num: 676\n",
      "Loss of train set: 0.19832277297973633 at epoch: 21 and batch_num: 677\n",
      "Loss of train set: 0.3380582332611084 at epoch: 21 and batch_num: 678\n",
      "Loss of train set: 0.18186604976654053 at epoch: 21 and batch_num: 679\n",
      "Loss of train set: 0.3848050832748413 at epoch: 21 and batch_num: 680\n",
      "Loss of train set: 0.28519535064697266 at epoch: 21 and batch_num: 681\n",
      "Loss of train set: 0.2566181421279907 at epoch: 21 and batch_num: 682\n",
      "Loss of train set: 0.23586362600326538 at epoch: 21 and batch_num: 683\n",
      "Loss of train set: 0.23372268676757812 at epoch: 21 and batch_num: 684\n",
      "Loss of train set: 0.22285594046115875 at epoch: 21 and batch_num: 685\n",
      "Loss of train set: 0.2706291377544403 at epoch: 21 and batch_num: 686\n",
      "Loss of train set: 0.3610042929649353 at epoch: 21 and batch_num: 687\n",
      "Loss of train set: 0.2889043986797333 at epoch: 21 and batch_num: 688\n",
      "Loss of train set: 0.22581008076667786 at epoch: 21 and batch_num: 689\n",
      "Loss of train set: 0.18506862223148346 at epoch: 21 and batch_num: 690\n",
      "Loss of train set: 0.18980620801448822 at epoch: 21 and batch_num: 691\n",
      "Loss of train set: 0.15038911998271942 at epoch: 21 and batch_num: 692\n",
      "Loss of train set: 0.1908196210861206 at epoch: 21 and batch_num: 693\n",
      "Loss of train set: 0.5345306396484375 at epoch: 21 and batch_num: 694\n",
      "Loss of train set: 0.22687050700187683 at epoch: 21 and batch_num: 695\n",
      "Loss of train set: 0.3301719129085541 at epoch: 21 and batch_num: 696\n",
      "Loss of train set: 0.22138233482837677 at epoch: 21 and batch_num: 697\n",
      "Loss of train set: 0.280152291059494 at epoch: 21 and batch_num: 698\n",
      "Loss of train set: 0.19962510466575623 at epoch: 21 and batch_num: 699\n",
      "Loss of train set: 0.17039981484413147 at epoch: 21 and batch_num: 700\n",
      "Loss of train set: 0.20763695240020752 at epoch: 21 and batch_num: 701\n",
      "Loss of train set: 0.375186562538147 at epoch: 21 and batch_num: 702\n",
      "Loss of train set: 0.27134475111961365 at epoch: 21 and batch_num: 703\n",
      "Loss of train set: 0.3550037741661072 at epoch: 21 and batch_num: 704\n",
      "Loss of train set: 0.16834266483783722 at epoch: 21 and batch_num: 705\n",
      "Loss of train set: 0.2440578043460846 at epoch: 21 and batch_num: 706\n",
      "Loss of train set: 0.32522228360176086 at epoch: 21 and batch_num: 707\n",
      "Loss of train set: 0.20849043130874634 at epoch: 21 and batch_num: 708\n",
      "Loss of train set: 0.22432748973369598 at epoch: 21 and batch_num: 709\n",
      "Loss of train set: 0.17652973532676697 at epoch: 21 and batch_num: 710\n",
      "Loss of train set: 0.2861897945404053 at epoch: 21 and batch_num: 711\n",
      "Loss of train set: 0.2910436689853668 at epoch: 21 and batch_num: 712\n",
      "Loss of train set: 0.1825769990682602 at epoch: 21 and batch_num: 713\n",
      "Loss of train set: 0.3676019310951233 at epoch: 21 and batch_num: 714\n",
      "Loss of train set: 0.2726171314716339 at epoch: 21 and batch_num: 715\n",
      "Loss of train set: 0.1301005780696869 at epoch: 21 and batch_num: 716\n",
      "Loss of train set: 0.2741989493370056 at epoch: 21 and batch_num: 717\n",
      "Loss of train set: 0.15871548652648926 at epoch: 21 and batch_num: 718\n",
      "Loss of train set: 0.2350238412618637 at epoch: 21 and batch_num: 719\n",
      "Loss of train set: 0.29796385765075684 at epoch: 21 and batch_num: 720\n",
      "Loss of train set: 0.17489495873451233 at epoch: 21 and batch_num: 721\n",
      "Loss of train set: 0.23767930269241333 at epoch: 21 and batch_num: 722\n",
      "Loss of train set: 0.3215354084968567 at epoch: 21 and batch_num: 723\n",
      "Loss of train set: 0.31611359119415283 at epoch: 21 and batch_num: 724\n",
      "Loss of train set: 0.20329609513282776 at epoch: 21 and batch_num: 725\n",
      "Loss of train set: 0.15877914428710938 at epoch: 21 and batch_num: 726\n",
      "Loss of train set: 0.2725328207015991 at epoch: 21 and batch_num: 727\n",
      "Loss of train set: 0.3193921446800232 at epoch: 21 and batch_num: 728\n",
      "Loss of train set: 0.1972711682319641 at epoch: 21 and batch_num: 729\n",
      "Loss of train set: 0.1423482596874237 at epoch: 21 and batch_num: 730\n",
      "Loss of train set: 0.2535278797149658 at epoch: 21 and batch_num: 731\n",
      "Loss of train set: 0.6025692224502563 at epoch: 21 and batch_num: 732\n",
      "Loss of train set: 0.26402172446250916 at epoch: 21 and batch_num: 733\n",
      "Loss of train set: 0.3017352223396301 at epoch: 21 and batch_num: 734\n",
      "Loss of train set: 0.4940289855003357 at epoch: 21 and batch_num: 735\n",
      "Loss of train set: 0.3221702575683594 at epoch: 21 and batch_num: 736\n",
      "Loss of train set: 0.29440584778785706 at epoch: 21 and batch_num: 737\n",
      "Loss of train set: 0.39871251583099365 at epoch: 21 and batch_num: 738\n",
      "Loss of train set: 0.4413038492202759 at epoch: 21 and batch_num: 739\n",
      "Loss of train set: 0.2092888504266739 at epoch: 21 and batch_num: 740\n",
      "Loss of train set: 0.17948684096336365 at epoch: 21 and batch_num: 741\n",
      "Loss of train set: 0.26487457752227783 at epoch: 21 and batch_num: 742\n",
      "Loss of train set: 0.17771944403648376 at epoch: 21 and batch_num: 743\n",
      "Loss of train set: 0.4338859021663666 at epoch: 21 and batch_num: 744\n",
      "Loss of train set: 0.2683877944946289 at epoch: 21 and batch_num: 745\n",
      "Loss of train set: 0.4633064568042755 at epoch: 21 and batch_num: 746\n",
      "Loss of train set: 0.20476841926574707 at epoch: 21 and batch_num: 747\n",
      "Loss of train set: 0.3358713686466217 at epoch: 21 and batch_num: 748\n",
      "Loss of train set: 0.263933002948761 at epoch: 21 and batch_num: 749\n",
      "Loss of train set: 0.1613786816596985 at epoch: 21 and batch_num: 750\n",
      "Loss of train set: 0.12469495832920074 at epoch: 21 and batch_num: 751\n",
      "Loss of train set: 0.3891860842704773 at epoch: 21 and batch_num: 752\n",
      "Loss of train set: 0.178457111120224 at epoch: 21 and batch_num: 753\n",
      "Loss of train set: 0.18747910857200623 at epoch: 21 and batch_num: 754\n",
      "Loss of train set: 0.14055576920509338 at epoch: 21 and batch_num: 755\n",
      "Loss of train set: 0.31537702679634094 at epoch: 21 and batch_num: 756\n",
      "Loss of train set: 0.4283352792263031 at epoch: 21 and batch_num: 757\n",
      "Loss of train set: 0.3201746344566345 at epoch: 21 and batch_num: 758\n",
      "Loss of train set: 0.1625104546546936 at epoch: 21 and batch_num: 759\n",
      "Loss of train set: 0.24084991216659546 at epoch: 21 and batch_num: 760\n",
      "Loss of train set: 0.21938356757164001 at epoch: 21 and batch_num: 761\n",
      "Loss of train set: 0.3112817406654358 at epoch: 21 and batch_num: 762\n",
      "Loss of train set: 0.5202085971832275 at epoch: 21 and batch_num: 763\n",
      "Loss of train set: 0.4919344484806061 at epoch: 21 and batch_num: 764\n",
      "Loss of train set: 0.3404805064201355 at epoch: 21 and batch_num: 765\n",
      "Loss of train set: 0.2525128424167633 at epoch: 21 and batch_num: 766\n",
      "Loss of train set: 0.4940633177757263 at epoch: 21 and batch_num: 767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.4172123074531555 at epoch: 21 and batch_num: 768\n",
      "Loss of train set: 0.26819801330566406 at epoch: 21 and batch_num: 769\n",
      "Loss of train set: 0.21034634113311768 at epoch: 21 and batch_num: 770\n",
      "Loss of train set: 0.2709287703037262 at epoch: 21 and batch_num: 771\n",
      "Loss of train set: 0.2228769063949585 at epoch: 21 and batch_num: 772\n",
      "Loss of train set: 0.28085237741470337 at epoch: 21 and batch_num: 773\n",
      "Loss of train set: 0.4252077043056488 at epoch: 21 and batch_num: 774\n",
      "Loss of train set: 0.20929604768753052 at epoch: 21 and batch_num: 775\n",
      "Loss of train set: 0.4160960614681244 at epoch: 21 and batch_num: 776\n",
      "Loss of train set: 0.22492378950119019 at epoch: 21 and batch_num: 777\n",
      "Loss of train set: 0.42719900608062744 at epoch: 21 and batch_num: 778\n",
      "Loss of train set: 0.3675631880760193 at epoch: 21 and batch_num: 779\n",
      "Loss of train set: 0.2426709234714508 at epoch: 21 and batch_num: 780\n",
      "Loss of train set: 0.4028542637825012 at epoch: 21 and batch_num: 781\n",
      "Loss of train set: 0.2212112843990326 at epoch: 21 and batch_num: 782\n",
      "Loss of train set: 0.1540476232767105 at epoch: 21 and batch_num: 783\n",
      "Loss of train set: 0.2378028929233551 at epoch: 21 and batch_num: 784\n",
      "Loss of train set: 0.17817169427871704 at epoch: 21 and batch_num: 785\n",
      "Loss of train set: 0.1721910834312439 at epoch: 21 and batch_num: 786\n",
      "Loss of train set: 0.2961047291755676 at epoch: 21 and batch_num: 787\n",
      "Loss of train set: 0.2851414084434509 at epoch: 21 and batch_num: 788\n",
      "Loss of train set: 0.2609400451183319 at epoch: 21 and batch_num: 789\n",
      "Loss of train set: 0.11932377517223358 at epoch: 21 and batch_num: 790\n",
      "Loss of train set: 0.20414063334465027 at epoch: 21 and batch_num: 791\n",
      "Loss of train set: 0.20754776895046234 at epoch: 21 and batch_num: 792\n",
      "Loss of train set: 0.23026853799819946 at epoch: 21 and batch_num: 793\n",
      "Loss of train set: 0.32960039377212524 at epoch: 21 and batch_num: 794\n",
      "Loss of train set: 0.1635872721672058 at epoch: 21 and batch_num: 795\n",
      "Loss of train set: 0.33719491958618164 at epoch: 21 and batch_num: 796\n",
      "Loss of train set: 0.17348280549049377 at epoch: 21 and batch_num: 797\n",
      "Loss of train set: 0.3154619336128235 at epoch: 21 and batch_num: 798\n",
      "Loss of train set: 0.2230353057384491 at epoch: 21 and batch_num: 799\n",
      "Loss of train set: 0.1595892757177353 at epoch: 21 and batch_num: 800\n",
      "Loss of train set: 0.41612935066223145 at epoch: 21 and batch_num: 801\n",
      "Loss of train set: 0.2704673409461975 at epoch: 21 and batch_num: 802\n",
      "Loss of train set: 0.2647519111633301 at epoch: 21 and batch_num: 803\n",
      "Loss of train set: 0.24319544434547424 at epoch: 21 and batch_num: 804\n",
      "Loss of train set: 0.36580890417099 at epoch: 21 and batch_num: 805\n",
      "Loss of train set: 0.3001776933670044 at epoch: 21 and batch_num: 806\n",
      "Loss of train set: 0.214036762714386 at epoch: 21 and batch_num: 807\n",
      "Loss of train set: 0.28034740686416626 at epoch: 21 and batch_num: 808\n",
      "Loss of train set: 0.3054310977458954 at epoch: 21 and batch_num: 809\n",
      "Loss of train set: 0.3331316411495209 at epoch: 21 and batch_num: 810\n",
      "Loss of train set: 0.2962912619113922 at epoch: 21 and batch_num: 811\n",
      "Loss of train set: 0.24262972176074982 at epoch: 21 and batch_num: 812\n",
      "Loss of train set: 0.45741739869117737 at epoch: 21 and batch_num: 813\n",
      "Loss of train set: 0.2603807747364044 at epoch: 21 and batch_num: 814\n",
      "Loss of train set: 0.296497106552124 at epoch: 21 and batch_num: 815\n",
      "Loss of train set: 0.301907479763031 at epoch: 21 and batch_num: 816\n",
      "Loss of train set: 0.32995712757110596 at epoch: 21 and batch_num: 817\n",
      "Loss of train set: 0.22170600295066833 at epoch: 21 and batch_num: 818\n",
      "Loss of train set: 0.37799692153930664 at epoch: 21 and batch_num: 819\n",
      "Loss of train set: 0.37049466371536255 at epoch: 21 and batch_num: 820\n",
      "Loss of train set: 0.27955615520477295 at epoch: 21 and batch_num: 821\n",
      "Loss of train set: 0.33332499861717224 at epoch: 21 and batch_num: 822\n",
      "Loss of train set: 0.28477829694747925 at epoch: 21 and batch_num: 823\n",
      "Loss of train set: 0.3139870762825012 at epoch: 21 and batch_num: 824\n",
      "Loss of train set: 0.2720506191253662 at epoch: 21 and batch_num: 825\n",
      "Loss of train set: 0.2566823661327362 at epoch: 21 and batch_num: 826\n",
      "Loss of train set: 0.26326221227645874 at epoch: 21 and batch_num: 827\n",
      "Loss of train set: 0.25984349846839905 at epoch: 21 and batch_num: 828\n",
      "Loss of train set: 0.29861241579055786 at epoch: 21 and batch_num: 829\n",
      "Loss of train set: 0.32474270462989807 at epoch: 21 and batch_num: 830\n",
      "Loss of train set: 0.2293097823858261 at epoch: 21 and batch_num: 831\n",
      "Loss of train set: 0.4660063087940216 at epoch: 21 and batch_num: 832\n",
      "Loss of train set: 0.16458317637443542 at epoch: 21 and batch_num: 833\n",
      "Loss of train set: 0.26073023676872253 at epoch: 21 and batch_num: 834\n",
      "Loss of train set: 0.18063420057296753 at epoch: 21 and batch_num: 835\n",
      "Loss of train set: 0.260872483253479 at epoch: 21 and batch_num: 836\n",
      "Loss of train set: 0.24894016981124878 at epoch: 21 and batch_num: 837\n",
      "Loss of train set: 0.17864766716957092 at epoch: 21 and batch_num: 838\n",
      "Loss of train set: 0.26539692282676697 at epoch: 21 and batch_num: 839\n",
      "Loss of train set: 0.23257961869239807 at epoch: 21 and batch_num: 840\n",
      "Loss of train set: 0.21377766132354736 at epoch: 21 and batch_num: 841\n",
      "Loss of train set: 0.2231794148683548 at epoch: 21 and batch_num: 842\n",
      "Loss of train set: 0.1862841248512268 at epoch: 21 and batch_num: 843\n",
      "Loss of train set: 0.22540122270584106 at epoch: 21 and batch_num: 844\n",
      "Loss of train set: 0.31455522775650024 at epoch: 21 and batch_num: 845\n",
      "Loss of train set: 0.3971247673034668 at epoch: 21 and batch_num: 846\n",
      "Loss of train set: 0.2629413604736328 at epoch: 21 and batch_num: 847\n",
      "Loss of train set: 0.44778764247894287 at epoch: 21 and batch_num: 848\n",
      "Loss of train set: 0.24415314197540283 at epoch: 21 and batch_num: 849\n",
      "Loss of train set: 0.320631206035614 at epoch: 21 and batch_num: 850\n",
      "Loss of train set: 0.27470117807388306 at epoch: 21 and batch_num: 851\n",
      "Loss of train set: 0.2675245702266693 at epoch: 21 and batch_num: 852\n",
      "Loss of train set: 0.2587686777114868 at epoch: 21 and batch_num: 853\n",
      "Loss of train set: 0.2797994613647461 at epoch: 21 and batch_num: 854\n",
      "Loss of train set: 0.20277966558933258 at epoch: 21 and batch_num: 855\n",
      "Loss of train set: 0.18140092492103577 at epoch: 21 and batch_num: 856\n",
      "Loss of train set: 0.4609208106994629 at epoch: 21 and batch_num: 857\n",
      "Loss of train set: 0.23671390116214752 at epoch: 21 and batch_num: 858\n",
      "Loss of train set: 0.14628708362579346 at epoch: 21 and batch_num: 859\n",
      "Loss of train set: 0.29221653938293457 at epoch: 21 and batch_num: 860\n",
      "Loss of train set: 0.3263705372810364 at epoch: 21 and batch_num: 861\n",
      "Loss of train set: 0.2812938392162323 at epoch: 21 and batch_num: 862\n",
      "Loss of train set: 0.2783096432685852 at epoch: 21 and batch_num: 863\n",
      "Loss of train set: 0.33207666873931885 at epoch: 21 and batch_num: 864\n",
      "Loss of train set: 0.5522201657295227 at epoch: 21 and batch_num: 865\n",
      "Loss of train set: 0.19369526207447052 at epoch: 21 and batch_num: 866\n",
      "Loss of train set: 0.31366631388664246 at epoch: 21 and batch_num: 867\n",
      "Loss of train set: 0.2699066400527954 at epoch: 21 and batch_num: 868\n",
      "Loss of train set: 0.1990700215101242 at epoch: 21 and batch_num: 869\n",
      "Loss of train set: 0.21492654085159302 at epoch: 21 and batch_num: 870\n",
      "Loss of train set: 0.2528459429740906 at epoch: 21 and batch_num: 871\n",
      "Loss of train set: 0.24993237853050232 at epoch: 21 and batch_num: 872\n",
      "Loss of train set: 0.4121794104576111 at epoch: 21 and batch_num: 873\n",
      "Loss of train set: 0.1932869851589203 at epoch: 21 and batch_num: 874\n",
      "Loss of train set: 0.36822038888931274 at epoch: 21 and batch_num: 875\n",
      "Loss of train set: 0.2737378478050232 at epoch: 21 and batch_num: 876\n",
      "Loss of train set: 0.270003080368042 at epoch: 21 and batch_num: 877\n",
      "Loss of train set: 0.37414640188217163 at epoch: 21 and batch_num: 878\n",
      "Loss of train set: 0.31911778450012207 at epoch: 21 and batch_num: 879\n",
      "Loss of train set: 0.2927704453468323 at epoch: 21 and batch_num: 880\n",
      "Loss of train set: 0.1534005105495453 at epoch: 21 and batch_num: 881\n",
      "Loss of train set: 0.23716795444488525 at epoch: 21 and batch_num: 882\n",
      "Loss of train set: 0.42145732045173645 at epoch: 21 and batch_num: 883\n",
      "Loss of train set: 0.14043346047401428 at epoch: 21 and batch_num: 884\n",
      "Loss of train set: 0.27618226408958435 at epoch: 21 and batch_num: 885\n",
      "Loss of train set: 0.3577570915222168 at epoch: 21 and batch_num: 886\n",
      "Loss of train set: 0.2738053798675537 at epoch: 21 and batch_num: 887\n",
      "Loss of train set: 0.19879117608070374 at epoch: 21 and batch_num: 888\n",
      "Loss of train set: 0.27548614144325256 at epoch: 21 and batch_num: 889\n",
      "Loss of train set: 0.2712487578392029 at epoch: 21 and batch_num: 890\n",
      "Loss of train set: 0.21886786818504333 at epoch: 21 and batch_num: 891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.38336634635925293 at epoch: 21 and batch_num: 892\n",
      "Loss of train set: 0.3631978929042816 at epoch: 21 and batch_num: 893\n",
      "Loss of train set: 0.2503293752670288 at epoch: 21 and batch_num: 894\n",
      "Loss of train set: 0.23419293761253357 at epoch: 21 and batch_num: 895\n",
      "Loss of train set: 0.3583618998527527 at epoch: 21 and batch_num: 896\n",
      "Loss of train set: 0.30369192361831665 at epoch: 21 and batch_num: 897\n",
      "Loss of train set: 0.30204641819000244 at epoch: 21 and batch_num: 898\n",
      "Loss of train set: 0.1879492700099945 at epoch: 21 and batch_num: 899\n",
      "Loss of train set: 0.2498365193605423 at epoch: 21 and batch_num: 900\n",
      "Loss of train set: 0.5651792287826538 at epoch: 21 and batch_num: 901\n",
      "Loss of train set: 0.3411861062049866 at epoch: 21 and batch_num: 902\n",
      "Loss of train set: 0.26798781752586365 at epoch: 21 and batch_num: 903\n",
      "Loss of train set: 0.13750621676445007 at epoch: 21 and batch_num: 904\n",
      "Loss of train set: 0.39195722341537476 at epoch: 21 and batch_num: 905\n",
      "Loss of train set: 0.27843156456947327 at epoch: 21 and batch_num: 906\n",
      "Loss of train set: 0.2919805347919464 at epoch: 21 and batch_num: 907\n",
      "Loss of train set: 0.15878528356552124 at epoch: 21 and batch_num: 908\n",
      "Loss of train set: 0.4378332197666168 at epoch: 21 and batch_num: 909\n",
      "Loss of train set: 0.26918894052505493 at epoch: 21 and batch_num: 910\n",
      "Loss of train set: 0.43429821729660034 at epoch: 21 and batch_num: 911\n",
      "Loss of train set: 0.34761208295822144 at epoch: 21 and batch_num: 912\n",
      "Loss of train set: 0.22232724726200104 at epoch: 21 and batch_num: 913\n",
      "Loss of train set: 0.2455894649028778 at epoch: 21 and batch_num: 914\n",
      "Loss of train set: 0.2772107422351837 at epoch: 21 and batch_num: 915\n",
      "Loss of train set: 0.3991716206073761 at epoch: 21 and batch_num: 916\n",
      "Loss of train set: 0.3068446218967438 at epoch: 21 and batch_num: 917\n",
      "Loss of train set: 0.3190678358078003 at epoch: 21 and batch_num: 918\n",
      "Loss of train set: 0.3234117329120636 at epoch: 21 and batch_num: 919\n",
      "Loss of train set: 0.40419918298721313 at epoch: 21 and batch_num: 920\n",
      "Loss of train set: 0.2361522912979126 at epoch: 21 and batch_num: 921\n",
      "Loss of train set: 0.3426939845085144 at epoch: 21 and batch_num: 922\n",
      "Loss of train set: 0.2510014474391937 at epoch: 21 and batch_num: 923\n",
      "Loss of train set: 0.14874562621116638 at epoch: 21 and batch_num: 924\n",
      "Loss of train set: 0.23056179285049438 at epoch: 21 and batch_num: 925\n",
      "Loss of train set: 0.2968785762786865 at epoch: 21 and batch_num: 926\n",
      "Loss of train set: 0.39176905155181885 at epoch: 21 and batch_num: 927\n",
      "Loss of train set: 0.2397301197052002 at epoch: 21 and batch_num: 928\n",
      "Loss of train set: 0.3004373610019684 at epoch: 21 and batch_num: 929\n",
      "Loss of train set: 0.2516743540763855 at epoch: 21 and batch_num: 930\n",
      "Loss of train set: 0.14676016569137573 at epoch: 21 and batch_num: 931\n",
      "Loss of train set: 0.32020509243011475 at epoch: 21 and batch_num: 932\n",
      "Loss of train set: 0.3140048384666443 at epoch: 21 and batch_num: 933\n",
      "Loss of train set: 0.3603326976299286 at epoch: 21 and batch_num: 934\n",
      "Loss of train set: 0.3337281346321106 at epoch: 21 and batch_num: 935\n",
      "Loss of train set: 0.26276111602783203 at epoch: 21 and batch_num: 936\n",
      "Loss of train set: 0.27267318964004517 at epoch: 21 and batch_num: 937\n",
      "Accuracy of train set: 0.8989833333333334\n",
      "Loss of test set: 0.5687318444252014 at epoch: 21 and batch_num: 0\n",
      "Loss of test set: 0.20253269374370575 at epoch: 21 and batch_num: 1\n",
      "Loss of test set: 0.48166772723197937 at epoch: 21 and batch_num: 2\n",
      "Loss of test set: 0.2977933883666992 at epoch: 21 and batch_num: 3\n",
      "Loss of test set: 0.26352792978286743 at epoch: 21 and batch_num: 4\n",
      "Loss of test set: 0.48726576566696167 at epoch: 21 and batch_num: 5\n",
      "Loss of test set: 0.24281229078769684 at epoch: 21 and batch_num: 6\n",
      "Loss of test set: 0.37812626361846924 at epoch: 21 and batch_num: 7\n",
      "Loss of test set: 0.2480381429195404 at epoch: 21 and batch_num: 8\n",
      "Loss of test set: 0.3667827546596527 at epoch: 21 and batch_num: 9\n",
      "Loss of test set: 0.310855507850647 at epoch: 21 and batch_num: 10\n",
      "Loss of test set: 0.4931999742984772 at epoch: 21 and batch_num: 11\n",
      "Loss of test set: 0.25059810280799866 at epoch: 21 and batch_num: 12\n",
      "Loss of test set: 0.4461195170879364 at epoch: 21 and batch_num: 13\n",
      "Loss of test set: 0.4422260522842407 at epoch: 21 and batch_num: 14\n",
      "Loss of test set: 0.39059704542160034 at epoch: 21 and batch_num: 15\n",
      "Loss of test set: 0.37594208121299744 at epoch: 21 and batch_num: 16\n",
      "Loss of test set: 0.5204036235809326 at epoch: 21 and batch_num: 17\n",
      "Loss of test set: 0.3620271384716034 at epoch: 21 and batch_num: 18\n",
      "Loss of test set: 0.297808974981308 at epoch: 21 and batch_num: 19\n",
      "Loss of test set: 0.43626585602760315 at epoch: 21 and batch_num: 20\n",
      "Loss of test set: 0.2884714901447296 at epoch: 21 and batch_num: 21\n",
      "Loss of test set: 0.3820766508579254 at epoch: 21 and batch_num: 22\n",
      "Loss of test set: 0.3159599304199219 at epoch: 21 and batch_num: 23\n",
      "Loss of test set: 0.3801501989364624 at epoch: 21 and batch_num: 24\n",
      "Loss of test set: 0.4593851566314697 at epoch: 21 and batch_num: 25\n",
      "Loss of test set: 0.6882556080818176 at epoch: 21 and batch_num: 26\n",
      "Loss of test set: 0.5281335115432739 at epoch: 21 and batch_num: 27\n",
      "Loss of test set: 0.4571988582611084 at epoch: 21 and batch_num: 28\n",
      "Loss of test set: 0.2361791729927063 at epoch: 21 and batch_num: 29\n",
      "Loss of test set: 0.2937415838241577 at epoch: 21 and batch_num: 30\n",
      "Loss of test set: 0.49272701144218445 at epoch: 21 and batch_num: 31\n",
      "Loss of test set: 0.3627488315105438 at epoch: 21 and batch_num: 32\n",
      "Loss of test set: 0.33056285977363586 at epoch: 21 and batch_num: 33\n",
      "Loss of test set: 0.28023794293403625 at epoch: 21 and batch_num: 34\n",
      "Loss of test set: 0.29602527618408203 at epoch: 21 and batch_num: 35\n",
      "Loss of test set: 0.36976736783981323 at epoch: 21 and batch_num: 36\n",
      "Loss of test set: 0.47550469636917114 at epoch: 21 and batch_num: 37\n",
      "Loss of test set: 0.35103610157966614 at epoch: 21 and batch_num: 38\n",
      "Loss of test set: 0.333914190530777 at epoch: 21 and batch_num: 39\n",
      "Loss of test set: 0.33803901076316833 at epoch: 21 and batch_num: 40\n",
      "Loss of test set: 0.4705117344856262 at epoch: 21 and batch_num: 41\n",
      "Loss of test set: 0.3766832649707794 at epoch: 21 and batch_num: 42\n",
      "Loss of test set: 0.2625359296798706 at epoch: 21 and batch_num: 43\n",
      "Loss of test set: 0.40813660621643066 at epoch: 21 and batch_num: 44\n",
      "Loss of test set: 0.44249993562698364 at epoch: 21 and batch_num: 45\n",
      "Loss of test set: 0.23787948489189148 at epoch: 21 and batch_num: 46\n",
      "Loss of test set: 0.41888704895973206 at epoch: 21 and batch_num: 47\n",
      "Loss of test set: 0.3121413290500641 at epoch: 21 and batch_num: 48\n",
      "Loss of test set: 0.31873854994773865 at epoch: 21 and batch_num: 49\n",
      "Loss of test set: 0.39031851291656494 at epoch: 21 and batch_num: 50\n",
      "Loss of test set: 0.3235031068325043 at epoch: 21 and batch_num: 51\n",
      "Loss of test set: 0.4792669415473938 at epoch: 21 and batch_num: 52\n",
      "Loss of test set: 0.21197691559791565 at epoch: 21 and batch_num: 53\n",
      "Loss of test set: 0.4251430630683899 at epoch: 21 and batch_num: 54\n",
      "Loss of test set: 0.5034708976745605 at epoch: 21 and batch_num: 55\n",
      "Loss of test set: 0.2476317435503006 at epoch: 21 and batch_num: 56\n",
      "Loss of test set: 0.30754297971725464 at epoch: 21 and batch_num: 57\n",
      "Loss of test set: 0.336159884929657 at epoch: 21 and batch_num: 58\n",
      "Loss of test set: 0.2659865617752075 at epoch: 21 and batch_num: 59\n",
      "Loss of test set: 0.29185983538627625 at epoch: 21 and batch_num: 60\n",
      "Loss of test set: 0.278877854347229 at epoch: 21 and batch_num: 61\n",
      "Loss of test set: 0.4145476818084717 at epoch: 21 and batch_num: 62\n",
      "Loss of test set: 0.38411957025527954 at epoch: 21 and batch_num: 63\n",
      "Loss of test set: 0.3371622562408447 at epoch: 21 and batch_num: 64\n",
      "Loss of test set: 0.3300916850566864 at epoch: 21 and batch_num: 65\n",
      "Loss of test set: 0.4140232801437378 at epoch: 21 and batch_num: 66\n",
      "Loss of test set: 0.3630666136741638 at epoch: 21 and batch_num: 67\n",
      "Loss of test set: 0.4524533748626709 at epoch: 21 and batch_num: 68\n",
      "Loss of test set: 0.34877967834472656 at epoch: 21 and batch_num: 69\n",
      "Loss of test set: 0.27253657579421997 at epoch: 21 and batch_num: 70\n",
      "Loss of test set: 0.35865944623947144 at epoch: 21 and batch_num: 71\n",
      "Loss of test set: 0.4522205591201782 at epoch: 21 and batch_num: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.26801633834838867 at epoch: 21 and batch_num: 73\n",
      "Loss of test set: 0.5845942497253418 at epoch: 21 and batch_num: 74\n",
      "Loss of test set: 0.5377159714698792 at epoch: 21 and batch_num: 75\n",
      "Loss of test set: 0.4159991443157196 at epoch: 21 and batch_num: 76\n",
      "Loss of test set: 0.45181748270988464 at epoch: 21 and batch_num: 77\n",
      "Loss of test set: 0.18613474071025848 at epoch: 21 and batch_num: 78\n",
      "Loss of test set: 0.4420151710510254 at epoch: 21 and batch_num: 79\n",
      "Loss of test set: 0.3445626497268677 at epoch: 21 and batch_num: 80\n",
      "Loss of test set: 0.31032678484916687 at epoch: 21 and batch_num: 81\n",
      "Loss of test set: 0.41817647218704224 at epoch: 21 and batch_num: 82\n",
      "Loss of test set: 0.4519604444503784 at epoch: 21 and batch_num: 83\n",
      "Loss of test set: 0.3944008946418762 at epoch: 21 and batch_num: 84\n",
      "Loss of test set: 0.2549859583377838 at epoch: 21 and batch_num: 85\n",
      "Loss of test set: 0.2598172426223755 at epoch: 21 and batch_num: 86\n",
      "Loss of test set: 0.39970773458480835 at epoch: 21 and batch_num: 87\n",
      "Loss of test set: 0.5313863158226013 at epoch: 21 and batch_num: 88\n",
      "Loss of test set: 0.3319782614707947 at epoch: 21 and batch_num: 89\n",
      "Loss of test set: 0.2658301293849945 at epoch: 21 and batch_num: 90\n",
      "Loss of test set: 0.3117671012878418 at epoch: 21 and batch_num: 91\n",
      "Loss of test set: 0.3062645196914673 at epoch: 21 and batch_num: 92\n",
      "Loss of test set: 0.3862874507904053 at epoch: 21 and batch_num: 93\n",
      "Loss of test set: 0.5607727766036987 at epoch: 21 and batch_num: 94\n",
      "Loss of test set: 0.36434808373451233 at epoch: 21 and batch_num: 95\n",
      "Loss of test set: 0.31355440616607666 at epoch: 21 and batch_num: 96\n",
      "Loss of test set: 0.5383307933807373 at epoch: 21 and batch_num: 97\n",
      "Loss of test set: 0.15658053755760193 at epoch: 21 and batch_num: 98\n",
      "Loss of test set: 0.45498764514923096 at epoch: 21 and batch_num: 99\n",
      "Loss of test set: 0.4030207395553589 at epoch: 21 and batch_num: 100\n",
      "Loss of test set: 0.3375106155872345 at epoch: 21 and batch_num: 101\n",
      "Loss of test set: 0.5239243507385254 at epoch: 21 and batch_num: 102\n",
      "Loss of test set: 0.28783148527145386 at epoch: 21 and batch_num: 103\n",
      "Loss of test set: 0.6379040479660034 at epoch: 21 and batch_num: 104\n",
      "Loss of test set: 0.49065613746643066 at epoch: 21 and batch_num: 105\n",
      "Loss of test set: 0.28803926706314087 at epoch: 21 and batch_num: 106\n",
      "Loss of test set: 0.4322088956832886 at epoch: 21 and batch_num: 107\n",
      "Loss of test set: 0.28402993083000183 at epoch: 21 and batch_num: 108\n",
      "Loss of test set: 0.5824744701385498 at epoch: 21 and batch_num: 109\n",
      "Loss of test set: 0.42745551466941833 at epoch: 21 and batch_num: 110\n",
      "Loss of test set: 0.4603497385978699 at epoch: 21 and batch_num: 111\n",
      "Loss of test set: 0.4011193513870239 at epoch: 21 and batch_num: 112\n",
      "Loss of test set: 0.3742605745792389 at epoch: 21 and batch_num: 113\n",
      "Loss of test set: 0.27191251516342163 at epoch: 21 and batch_num: 114\n",
      "Loss of test set: 0.40088462829589844 at epoch: 21 and batch_num: 115\n",
      "Loss of test set: 0.4583545923233032 at epoch: 21 and batch_num: 116\n",
      "Loss of test set: 0.4688502252101898 at epoch: 21 and batch_num: 117\n",
      "Loss of test set: 0.3083236515522003 at epoch: 21 and batch_num: 118\n",
      "Loss of test set: 0.4523830711841583 at epoch: 21 and batch_num: 119\n",
      "Loss of test set: 0.5028280019760132 at epoch: 21 and batch_num: 120\n",
      "Loss of test set: 0.3504980802536011 at epoch: 21 and batch_num: 121\n",
      "Loss of test set: 0.19482609629631042 at epoch: 21 and batch_num: 122\n",
      "Loss of test set: 0.1554834246635437 at epoch: 21 and batch_num: 123\n",
      "Loss of test set: 0.3702011704444885 at epoch: 21 and batch_num: 124\n",
      "Loss of test set: 0.14443406462669373 at epoch: 21 and batch_num: 125\n",
      "Loss of test set: 0.3975887894630432 at epoch: 21 and batch_num: 126\n",
      "Loss of test set: 0.38489091396331787 at epoch: 21 and batch_num: 127\n",
      "Loss of test set: 0.30933189392089844 at epoch: 21 and batch_num: 128\n",
      "Loss of test set: 0.5381494164466858 at epoch: 21 and batch_num: 129\n",
      "Loss of test set: 0.4218622148036957 at epoch: 21 and batch_num: 130\n",
      "Loss of test set: 0.49236029386520386 at epoch: 21 and batch_num: 131\n",
      "Loss of test set: 0.4207943379878998 at epoch: 21 and batch_num: 132\n",
      "Loss of test set: 0.4448471963405609 at epoch: 21 and batch_num: 133\n",
      "Loss of test set: 0.5428217649459839 at epoch: 21 and batch_num: 134\n",
      "Loss of test set: 0.3550450801849365 at epoch: 21 and batch_num: 135\n",
      "Loss of test set: 0.23910081386566162 at epoch: 21 and batch_num: 136\n",
      "Loss of test set: 0.5104610323905945 at epoch: 21 and batch_num: 137\n",
      "Loss of test set: 0.44153913855552673 at epoch: 21 and batch_num: 138\n",
      "Loss of test set: 0.5032061338424683 at epoch: 21 and batch_num: 139\n",
      "Loss of test set: 0.31672096252441406 at epoch: 21 and batch_num: 140\n",
      "Loss of test set: 0.2741906940937042 at epoch: 21 and batch_num: 141\n",
      "Loss of test set: 0.5060584545135498 at epoch: 21 and batch_num: 142\n",
      "Loss of test set: 0.31887465715408325 at epoch: 21 and batch_num: 143\n",
      "Loss of test set: 0.4625051021575928 at epoch: 21 and batch_num: 144\n",
      "Loss of test set: 0.3991376757621765 at epoch: 21 and batch_num: 145\n",
      "Loss of test set: 0.4244579076766968 at epoch: 21 and batch_num: 146\n",
      "Loss of test set: 0.38218629360198975 at epoch: 21 and batch_num: 147\n",
      "Loss of test set: 0.4337524473667145 at epoch: 21 and batch_num: 148\n",
      "Loss of test set: 0.3531951308250427 at epoch: 21 and batch_num: 149\n",
      "Loss of test set: 0.3639637529850006 at epoch: 21 and batch_num: 150\n",
      "Loss of test set: 0.3179546594619751 at epoch: 21 and batch_num: 151\n",
      "Loss of test set: 0.47490429878234863 at epoch: 21 and batch_num: 152\n",
      "Loss of test set: 0.37869030237197876 at epoch: 21 and batch_num: 153\n",
      "Loss of test set: 0.36180800199508667 at epoch: 21 and batch_num: 154\n",
      "Loss of test set: 0.40561428666114807 at epoch: 21 and batch_num: 155\n",
      "Loss of test set: 0.24477915465831757 at epoch: 21 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8602\n",
      "Loss of train set: 0.422121524810791 at epoch: 22 and batch_num: 0\n",
      "Loss of train set: 0.3575647473335266 at epoch: 22 and batch_num: 1\n",
      "Loss of train set: 0.3075354993343353 at epoch: 22 and batch_num: 2\n",
      "Loss of train set: 0.18817424774169922 at epoch: 22 and batch_num: 3\n",
      "Loss of train set: 0.26875197887420654 at epoch: 22 and batch_num: 4\n",
      "Loss of train set: 0.4220564365386963 at epoch: 22 and batch_num: 5\n",
      "Loss of train set: 0.22203095257282257 at epoch: 22 and batch_num: 6\n",
      "Loss of train set: 0.22294816374778748 at epoch: 22 and batch_num: 7\n",
      "Loss of train set: 0.13797935843467712 at epoch: 22 and batch_num: 8\n",
      "Loss of train set: 0.20183029770851135 at epoch: 22 and batch_num: 9\n",
      "Loss of train set: 0.21127110719680786 at epoch: 22 and batch_num: 10\n",
      "Loss of train set: 0.2573786675930023 at epoch: 22 and batch_num: 11\n",
      "Loss of train set: 0.1657296121120453 at epoch: 22 and batch_num: 12\n",
      "Loss of train set: 0.23133355379104614 at epoch: 22 and batch_num: 13\n",
      "Loss of train set: 0.20021387934684753 at epoch: 22 and batch_num: 14\n",
      "Loss of train set: 0.38835614919662476 at epoch: 22 and batch_num: 15\n",
      "Loss of train set: 0.3555419445037842 at epoch: 22 and batch_num: 16\n",
      "Loss of train set: 0.33748388290405273 at epoch: 22 and batch_num: 17\n",
      "Loss of train set: 0.3149067163467407 at epoch: 22 and batch_num: 18\n",
      "Loss of train set: 0.3127055764198303 at epoch: 22 and batch_num: 19\n",
      "Loss of train set: 0.32112908363342285 at epoch: 22 and batch_num: 20\n",
      "Loss of train set: 0.18512214720249176 at epoch: 22 and batch_num: 21\n",
      "Loss of train set: 0.17677918076515198 at epoch: 22 and batch_num: 22\n",
      "Loss of train set: 0.25639674067497253 at epoch: 22 and batch_num: 23\n",
      "Loss of train set: 0.2806442975997925 at epoch: 22 and batch_num: 24\n",
      "Loss of train set: 0.4029538035392761 at epoch: 22 and batch_num: 25\n",
      "Loss of train set: 0.26543545722961426 at epoch: 22 and batch_num: 26\n",
      "Loss of train set: 0.30769023299217224 at epoch: 22 and batch_num: 27\n",
      "Loss of train set: 0.27305474877357483 at epoch: 22 and batch_num: 28\n",
      "Loss of train set: 0.17591974139213562 at epoch: 22 and batch_num: 29\n",
      "Loss of train set: 0.22062280774116516 at epoch: 22 and batch_num: 30\n",
      "Loss of train set: 0.28445863723754883 at epoch: 22 and batch_num: 31\n",
      "Loss of train set: 0.41703784465789795 at epoch: 22 and batch_num: 32\n",
      "Loss of train set: 0.280428022146225 at epoch: 22 and batch_num: 33\n",
      "Loss of train set: 0.2685169577598572 at epoch: 22 and batch_num: 34\n",
      "Loss of train set: 0.31560710072517395 at epoch: 22 and batch_num: 35\n",
      "Loss of train set: 0.26876285672187805 at epoch: 22 and batch_num: 36\n",
      "Loss of train set: 0.2748018801212311 at epoch: 22 and batch_num: 37\n",
      "Loss of train set: 0.22582846879959106 at epoch: 22 and batch_num: 38\n",
      "Loss of train set: 0.36280357837677 at epoch: 22 and batch_num: 39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.18694955110549927 at epoch: 22 and batch_num: 40\n",
      "Loss of train set: 0.3397201895713806 at epoch: 22 and batch_num: 41\n",
      "Loss of train set: 0.206893652677536 at epoch: 22 and batch_num: 42\n",
      "Loss of train set: 0.23974938690662384 at epoch: 22 and batch_num: 43\n",
      "Loss of train set: 0.20051097869873047 at epoch: 22 and batch_num: 44\n",
      "Loss of train set: 0.3083345890045166 at epoch: 22 and batch_num: 45\n",
      "Loss of train set: 0.20899829268455505 at epoch: 22 and batch_num: 46\n",
      "Loss of train set: 0.19806471467018127 at epoch: 22 and batch_num: 47\n",
      "Loss of train set: 0.25882259011268616 at epoch: 22 and batch_num: 48\n",
      "Loss of train set: 0.24869975447654724 at epoch: 22 and batch_num: 49\n",
      "Loss of train set: 0.2412724792957306 at epoch: 22 and batch_num: 50\n",
      "Loss of train set: 0.13402798771858215 at epoch: 22 and batch_num: 51\n",
      "Loss of train set: 0.24179081618785858 at epoch: 22 and batch_num: 52\n",
      "Loss of train set: 0.30296528339385986 at epoch: 22 and batch_num: 53\n",
      "Loss of train set: 0.13380183279514313 at epoch: 22 and batch_num: 54\n",
      "Loss of train set: 0.30774080753326416 at epoch: 22 and batch_num: 55\n",
      "Loss of train set: 0.1387403905391693 at epoch: 22 and batch_num: 56\n",
      "Loss of train set: 0.2451585829257965 at epoch: 22 and batch_num: 57\n",
      "Loss of train set: 0.36959320306777954 at epoch: 22 and batch_num: 58\n",
      "Loss of train set: 0.22022709250450134 at epoch: 22 and batch_num: 59\n",
      "Loss of train set: 0.24141386151313782 at epoch: 22 and batch_num: 60\n",
      "Loss of train set: 0.20182256400585175 at epoch: 22 and batch_num: 61\n",
      "Loss of train set: 0.33637356758117676 at epoch: 22 and batch_num: 62\n",
      "Loss of train set: 0.3002437651157379 at epoch: 22 and batch_num: 63\n",
      "Loss of train set: 0.2756292521953583 at epoch: 22 and batch_num: 64\n",
      "Loss of train set: 0.26349207758903503 at epoch: 22 and batch_num: 65\n",
      "Loss of train set: 0.44736966490745544 at epoch: 22 and batch_num: 66\n",
      "Loss of train set: 0.15352624654769897 at epoch: 22 and batch_num: 67\n",
      "Loss of train set: 0.2668963074684143 at epoch: 22 and batch_num: 68\n",
      "Loss of train set: 0.21990150213241577 at epoch: 22 and batch_num: 69\n",
      "Loss of train set: 0.3455161452293396 at epoch: 22 and batch_num: 70\n",
      "Loss of train set: 0.39360883831977844 at epoch: 22 and batch_num: 71\n",
      "Loss of train set: 0.2978299558162689 at epoch: 22 and batch_num: 72\n",
      "Loss of train set: 0.2162039875984192 at epoch: 22 and batch_num: 73\n",
      "Loss of train set: 0.21120308339595795 at epoch: 22 and batch_num: 74\n",
      "Loss of train set: 0.18602347373962402 at epoch: 22 and batch_num: 75\n",
      "Loss of train set: 0.19740691781044006 at epoch: 22 and batch_num: 76\n",
      "Loss of train set: 0.34334155917167664 at epoch: 22 and batch_num: 77\n",
      "Loss of train set: 0.21489858627319336 at epoch: 22 and batch_num: 78\n",
      "Loss of train set: 0.1731829047203064 at epoch: 22 and batch_num: 79\n",
      "Loss of train set: 0.2336788773536682 at epoch: 22 and batch_num: 80\n",
      "Loss of train set: 0.18598118424415588 at epoch: 22 and batch_num: 81\n",
      "Loss of train set: 0.24505043029785156 at epoch: 22 and batch_num: 82\n",
      "Loss of train set: 0.3525354266166687 at epoch: 22 and batch_num: 83\n",
      "Loss of train set: 0.27314573526382446 at epoch: 22 and batch_num: 84\n",
      "Loss of train set: 0.26654917001724243 at epoch: 22 and batch_num: 85\n",
      "Loss of train set: 0.40510010719299316 at epoch: 22 and batch_num: 86\n",
      "Loss of train set: 0.3530282974243164 at epoch: 22 and batch_num: 87\n",
      "Loss of train set: 0.35917219519615173 at epoch: 22 and batch_num: 88\n",
      "Loss of train set: 0.2986072897911072 at epoch: 22 and batch_num: 89\n",
      "Loss of train set: 0.36852091550827026 at epoch: 22 and batch_num: 90\n",
      "Loss of train set: 0.3297485113143921 at epoch: 22 and batch_num: 91\n",
      "Loss of train set: 0.23839986324310303 at epoch: 22 and batch_num: 92\n",
      "Loss of train set: 0.27672451734542847 at epoch: 22 and batch_num: 93\n",
      "Loss of train set: 0.25379639863967896 at epoch: 22 and batch_num: 94\n",
      "Loss of train set: 0.24324482679367065 at epoch: 22 and batch_num: 95\n",
      "Loss of train set: 0.2600298821926117 at epoch: 22 and batch_num: 96\n",
      "Loss of train set: 0.1863316297531128 at epoch: 22 and batch_num: 97\n",
      "Loss of train set: 0.2047073096036911 at epoch: 22 and batch_num: 98\n",
      "Loss of train set: 0.3178669810295105 at epoch: 22 and batch_num: 99\n",
      "Loss of train set: 0.4920775890350342 at epoch: 22 and batch_num: 100\n",
      "Loss of train set: 0.31396597623825073 at epoch: 22 and batch_num: 101\n",
      "Loss of train set: 0.4089891016483307 at epoch: 22 and batch_num: 102\n",
      "Loss of train set: 0.11373619735240936 at epoch: 22 and batch_num: 103\n",
      "Loss of train set: 0.2284841686487198 at epoch: 22 and batch_num: 104\n",
      "Loss of train set: 0.35096949338912964 at epoch: 22 and batch_num: 105\n",
      "Loss of train set: 0.32589343190193176 at epoch: 22 and batch_num: 106\n",
      "Loss of train set: 0.22840598225593567 at epoch: 22 and batch_num: 107\n",
      "Loss of train set: 0.3002106547355652 at epoch: 22 and batch_num: 108\n",
      "Loss of train set: 0.2544098198413849 at epoch: 22 and batch_num: 109\n",
      "Loss of train set: 0.31780433654785156 at epoch: 22 and batch_num: 110\n",
      "Loss of train set: 0.2306780368089676 at epoch: 22 and batch_num: 111\n",
      "Loss of train set: 0.1472536027431488 at epoch: 22 and batch_num: 112\n",
      "Loss of train set: 0.27076250314712524 at epoch: 22 and batch_num: 113\n",
      "Loss of train set: 0.21766918897628784 at epoch: 22 and batch_num: 114\n",
      "Loss of train set: 0.1546323597431183 at epoch: 22 and batch_num: 115\n",
      "Loss of train set: 0.26569879055023193 at epoch: 22 and batch_num: 116\n",
      "Loss of train set: 0.21163995563983917 at epoch: 22 and batch_num: 117\n",
      "Loss of train set: 0.1859125792980194 at epoch: 22 and batch_num: 118\n",
      "Loss of train set: 0.3212524950504303 at epoch: 22 and batch_num: 119\n",
      "Loss of train set: 0.1822482943534851 at epoch: 22 and batch_num: 120\n",
      "Loss of train set: 0.20445941388607025 at epoch: 22 and batch_num: 121\n",
      "Loss of train set: 0.17662836611270905 at epoch: 22 and batch_num: 122\n",
      "Loss of train set: 0.3943805694580078 at epoch: 22 and batch_num: 123\n",
      "Loss of train set: 0.3803328275680542 at epoch: 22 and batch_num: 124\n",
      "Loss of train set: 0.20138928294181824 at epoch: 22 and batch_num: 125\n",
      "Loss of train set: 0.19947603344917297 at epoch: 22 and batch_num: 126\n",
      "Loss of train set: 0.29329994320869446 at epoch: 22 and batch_num: 127\n",
      "Loss of train set: 0.3633265793323517 at epoch: 22 and batch_num: 128\n",
      "Loss of train set: 0.1953485608100891 at epoch: 22 and batch_num: 129\n",
      "Loss of train set: 0.36441850662231445 at epoch: 22 and batch_num: 130\n",
      "Loss of train set: 0.22103172540664673 at epoch: 22 and batch_num: 131\n",
      "Loss of train set: 0.284106969833374 at epoch: 22 and batch_num: 132\n",
      "Loss of train set: 0.46064066886901855 at epoch: 22 and batch_num: 133\n",
      "Loss of train set: 0.3431432843208313 at epoch: 22 and batch_num: 134\n",
      "Loss of train set: 0.3528572916984558 at epoch: 22 and batch_num: 135\n",
      "Loss of train set: 0.23469658195972443 at epoch: 22 and batch_num: 136\n",
      "Loss of train set: 0.21559152007102966 at epoch: 22 and batch_num: 137\n",
      "Loss of train set: 0.30580002069473267 at epoch: 22 and batch_num: 138\n",
      "Loss of train set: 0.1742585301399231 at epoch: 22 and batch_num: 139\n",
      "Loss of train set: 0.31364554166793823 at epoch: 22 and batch_num: 140\n",
      "Loss of train set: 0.1745220422744751 at epoch: 22 and batch_num: 141\n",
      "Loss of train set: 0.3070327639579773 at epoch: 22 and batch_num: 142\n",
      "Loss of train set: 0.16767928004264832 at epoch: 22 and batch_num: 143\n",
      "Loss of train set: 0.13925568759441376 at epoch: 22 and batch_num: 144\n",
      "Loss of train set: 0.2560120224952698 at epoch: 22 and batch_num: 145\n",
      "Loss of train set: 0.1662520170211792 at epoch: 22 and batch_num: 146\n",
      "Loss of train set: 0.3154575228691101 at epoch: 22 and batch_num: 147\n",
      "Loss of train set: 0.2123776078224182 at epoch: 22 and batch_num: 148\n",
      "Loss of train set: 0.26665717363357544 at epoch: 22 and batch_num: 149\n",
      "Loss of train set: 0.3553324341773987 at epoch: 22 and batch_num: 150\n",
      "Loss of train set: 0.29684120416641235 at epoch: 22 and batch_num: 151\n",
      "Loss of train set: 0.5395059585571289 at epoch: 22 and batch_num: 152\n",
      "Loss of train set: 0.2449198216199875 at epoch: 22 and batch_num: 153\n",
      "Loss of train set: 0.3974306881427765 at epoch: 22 and batch_num: 154\n",
      "Loss of train set: 0.2484588325023651 at epoch: 22 and batch_num: 155\n",
      "Loss of train set: 0.28375518321990967 at epoch: 22 and batch_num: 156\n",
      "Loss of train set: 0.2915996313095093 at epoch: 22 and batch_num: 157\n",
      "Loss of train set: 0.17656254768371582 at epoch: 22 and batch_num: 158\n",
      "Loss of train set: 0.25495123863220215 at epoch: 22 and batch_num: 159\n",
      "Loss of train set: 0.3906968832015991 at epoch: 22 and batch_num: 160\n",
      "Loss of train set: 0.3258836567401886 at epoch: 22 and batch_num: 161\n",
      "Loss of train set: 0.32602572441101074 at epoch: 22 and batch_num: 162\n",
      "Loss of train set: 0.25703829526901245 at epoch: 22 and batch_num: 163\n",
      "Loss of train set: 0.37976008653640747 at epoch: 22 and batch_num: 164\n",
      "Loss of train set: 0.25087982416152954 at epoch: 22 and batch_num: 165\n",
      "Loss of train set: 0.2766695022583008 at epoch: 22 and batch_num: 166\n",
      "Loss of train set: 0.4481552839279175 at epoch: 22 and batch_num: 167\n",
      "Loss of train set: 0.2506418228149414 at epoch: 22 and batch_num: 168\n",
      "Loss of train set: 0.22728557884693146 at epoch: 22 and batch_num: 169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2226337045431137 at epoch: 22 and batch_num: 170\n",
      "Loss of train set: 0.24420735239982605 at epoch: 22 and batch_num: 171\n",
      "Loss of train set: 0.23966997861862183 at epoch: 22 and batch_num: 172\n",
      "Loss of train set: 0.26010748744010925 at epoch: 22 and batch_num: 173\n",
      "Loss of train set: 0.2563335597515106 at epoch: 22 and batch_num: 174\n",
      "Loss of train set: 0.30666062235832214 at epoch: 22 and batch_num: 175\n",
      "Loss of train set: 0.5068696141242981 at epoch: 22 and batch_num: 176\n",
      "Loss of train set: 0.160871684551239 at epoch: 22 and batch_num: 177\n",
      "Loss of train set: 0.23315706849098206 at epoch: 22 and batch_num: 178\n",
      "Loss of train set: 0.13545985519886017 at epoch: 22 and batch_num: 179\n",
      "Loss of train set: 0.3881850242614746 at epoch: 22 and batch_num: 180\n",
      "Loss of train set: 0.3476852774620056 at epoch: 22 and batch_num: 181\n",
      "Loss of train set: 0.4800848662853241 at epoch: 22 and batch_num: 182\n",
      "Loss of train set: 0.275525838136673 at epoch: 22 and batch_num: 183\n",
      "Loss of train set: 0.23291340470314026 at epoch: 22 and batch_num: 184\n",
      "Loss of train set: 0.13710139691829681 at epoch: 22 and batch_num: 185\n",
      "Loss of train set: 0.2525136470794678 at epoch: 22 and batch_num: 186\n",
      "Loss of train set: 0.2536507844924927 at epoch: 22 and batch_num: 187\n",
      "Loss of train set: 0.1601000279188156 at epoch: 22 and batch_num: 188\n",
      "Loss of train set: 0.14555521309375763 at epoch: 22 and batch_num: 189\n",
      "Loss of train set: 0.23154404759407043 at epoch: 22 and batch_num: 190\n",
      "Loss of train set: 0.407940149307251 at epoch: 22 and batch_num: 191\n",
      "Loss of train set: 0.22174018621444702 at epoch: 22 and batch_num: 192\n",
      "Loss of train set: 0.20187948644161224 at epoch: 22 and batch_num: 193\n",
      "Loss of train set: 0.13491004705429077 at epoch: 22 and batch_num: 194\n",
      "Loss of train set: 0.3019232451915741 at epoch: 22 and batch_num: 195\n",
      "Loss of train set: 0.4856496751308441 at epoch: 22 and batch_num: 196\n",
      "Loss of train set: 0.2524256706237793 at epoch: 22 and batch_num: 197\n",
      "Loss of train set: 0.2154810130596161 at epoch: 22 and batch_num: 198\n",
      "Loss of train set: 0.2785695195198059 at epoch: 22 and batch_num: 199\n",
      "Loss of train set: 0.2706603407859802 at epoch: 22 and batch_num: 200\n",
      "Loss of train set: 0.2892986238002777 at epoch: 22 and batch_num: 201\n",
      "Loss of train set: 0.3136971592903137 at epoch: 22 and batch_num: 202\n",
      "Loss of train set: 0.23753632605075836 at epoch: 22 and batch_num: 203\n",
      "Loss of train set: 0.4094099998474121 at epoch: 22 and batch_num: 204\n",
      "Loss of train set: 0.22044801712036133 at epoch: 22 and batch_num: 205\n",
      "Loss of train set: 0.3555644154548645 at epoch: 22 and batch_num: 206\n",
      "Loss of train set: 0.39160460233688354 at epoch: 22 and batch_num: 207\n",
      "Loss of train set: 0.3023863434791565 at epoch: 22 and batch_num: 208\n",
      "Loss of train set: 0.24336224794387817 at epoch: 22 and batch_num: 209\n",
      "Loss of train set: 0.5151236057281494 at epoch: 22 and batch_num: 210\n",
      "Loss of train set: 0.21988636255264282 at epoch: 22 and batch_num: 211\n",
      "Loss of train set: 0.19467486441135406 at epoch: 22 and batch_num: 212\n",
      "Loss of train set: 0.2275029718875885 at epoch: 22 and batch_num: 213\n",
      "Loss of train set: 0.21562589704990387 at epoch: 22 and batch_num: 214\n",
      "Loss of train set: 0.3154592216014862 at epoch: 22 and batch_num: 215\n",
      "Loss of train set: 0.2403208315372467 at epoch: 22 and batch_num: 216\n",
      "Loss of train set: 0.24684782326221466 at epoch: 22 and batch_num: 217\n",
      "Loss of train set: 0.23058843612670898 at epoch: 22 and batch_num: 218\n",
      "Loss of train set: 0.2541930675506592 at epoch: 22 and batch_num: 219\n",
      "Loss of train set: 0.19810034334659576 at epoch: 22 and batch_num: 220\n",
      "Loss of train set: 0.42599987983703613 at epoch: 22 and batch_num: 221\n",
      "Loss of train set: 0.3541126549243927 at epoch: 22 and batch_num: 222\n",
      "Loss of train set: 0.2739364504814148 at epoch: 22 and batch_num: 223\n",
      "Loss of train set: 0.385694295167923 at epoch: 22 and batch_num: 224\n",
      "Loss of train set: 0.2812489867210388 at epoch: 22 and batch_num: 225\n",
      "Loss of train set: 0.23720693588256836 at epoch: 22 and batch_num: 226\n",
      "Loss of train set: 0.2156887650489807 at epoch: 22 and batch_num: 227\n",
      "Loss of train set: 0.3069937527179718 at epoch: 22 and batch_num: 228\n",
      "Loss of train set: 0.38242921233177185 at epoch: 22 and batch_num: 229\n",
      "Loss of train set: 0.24507707357406616 at epoch: 22 and batch_num: 230\n",
      "Loss of train set: 0.28860968351364136 at epoch: 22 and batch_num: 231\n",
      "Loss of train set: 0.44135695695877075 at epoch: 22 and batch_num: 232\n",
      "Loss of train set: 0.1335352659225464 at epoch: 22 and batch_num: 233\n",
      "Loss of train set: 0.3057878017425537 at epoch: 22 and batch_num: 234\n",
      "Loss of train set: 0.2998605966567993 at epoch: 22 and batch_num: 235\n",
      "Loss of train set: 0.23525066673755646 at epoch: 22 and batch_num: 236\n",
      "Loss of train set: 0.21186549961566925 at epoch: 22 and batch_num: 237\n",
      "Loss of train set: 0.3504946231842041 at epoch: 22 and batch_num: 238\n",
      "Loss of train set: 0.18178626894950867 at epoch: 22 and batch_num: 239\n",
      "Loss of train set: 0.312430739402771 at epoch: 22 and batch_num: 240\n",
      "Loss of train set: 0.3185172975063324 at epoch: 22 and batch_num: 241\n",
      "Loss of train set: 0.22046636044979095 at epoch: 22 and batch_num: 242\n",
      "Loss of train set: 0.2657772898674011 at epoch: 22 and batch_num: 243\n",
      "Loss of train set: 0.22232550382614136 at epoch: 22 and batch_num: 244\n",
      "Loss of train set: 0.237462118268013 at epoch: 22 and batch_num: 245\n",
      "Loss of train set: 0.4105093479156494 at epoch: 22 and batch_num: 246\n",
      "Loss of train set: 0.2162456512451172 at epoch: 22 and batch_num: 247\n",
      "Loss of train set: 0.3146008849143982 at epoch: 22 and batch_num: 248\n",
      "Loss of train set: 0.1641249656677246 at epoch: 22 and batch_num: 249\n",
      "Loss of train set: 0.6324777603149414 at epoch: 22 and batch_num: 250\n",
      "Loss of train set: 0.28588658571243286 at epoch: 22 and batch_num: 251\n",
      "Loss of train set: 0.28502795100212097 at epoch: 22 and batch_num: 252\n",
      "Loss of train set: 0.32227882742881775 at epoch: 22 and batch_num: 253\n",
      "Loss of train set: 0.3504078984260559 at epoch: 22 and batch_num: 254\n",
      "Loss of train set: 0.1487852931022644 at epoch: 22 and batch_num: 255\n",
      "Loss of train set: 0.2194153368473053 at epoch: 22 and batch_num: 256\n",
      "Loss of train set: 0.42807167768478394 at epoch: 22 and batch_num: 257\n",
      "Loss of train set: 0.24303653836250305 at epoch: 22 and batch_num: 258\n",
      "Loss of train set: 0.2204103171825409 at epoch: 22 and batch_num: 259\n",
      "Loss of train set: 0.28369516134262085 at epoch: 22 and batch_num: 260\n",
      "Loss of train set: 0.11662696301937103 at epoch: 22 and batch_num: 261\n",
      "Loss of train set: 0.2251737117767334 at epoch: 22 and batch_num: 262\n",
      "Loss of train set: 0.30189162492752075 at epoch: 22 and batch_num: 263\n",
      "Loss of train set: 0.29906585812568665 at epoch: 22 and batch_num: 264\n",
      "Loss of train set: 0.2940260171890259 at epoch: 22 and batch_num: 265\n",
      "Loss of train set: 0.18457093834877014 at epoch: 22 and batch_num: 266\n",
      "Loss of train set: 0.2837666869163513 at epoch: 22 and batch_num: 267\n",
      "Loss of train set: 0.35133904218673706 at epoch: 22 and batch_num: 268\n",
      "Loss of train set: 0.18264883756637573 at epoch: 22 and batch_num: 269\n",
      "Loss of train set: 0.3256704807281494 at epoch: 22 and batch_num: 270\n",
      "Loss of train set: 0.29878050088882446 at epoch: 22 and batch_num: 271\n",
      "Loss of train set: 0.346393346786499 at epoch: 22 and batch_num: 272\n",
      "Loss of train set: 0.27423471212387085 at epoch: 22 and batch_num: 273\n",
      "Loss of train set: 0.13427802920341492 at epoch: 22 and batch_num: 274\n",
      "Loss of train set: 0.12239588797092438 at epoch: 22 and batch_num: 275\n",
      "Loss of train set: 0.2741580009460449 at epoch: 22 and batch_num: 276\n",
      "Loss of train set: 0.225900799036026 at epoch: 22 and batch_num: 277\n",
      "Loss of train set: 0.21830977499485016 at epoch: 22 and batch_num: 278\n",
      "Loss of train set: 0.26307955384254456 at epoch: 22 and batch_num: 279\n",
      "Loss of train set: 0.19018496572971344 at epoch: 22 and batch_num: 280\n",
      "Loss of train set: 0.15090806782245636 at epoch: 22 and batch_num: 281\n",
      "Loss of train set: 0.2834318280220032 at epoch: 22 and batch_num: 282\n",
      "Loss of train set: 0.37483152747154236 at epoch: 22 and batch_num: 283\n",
      "Loss of train set: 0.3416363000869751 at epoch: 22 and batch_num: 284\n",
      "Loss of train set: 0.3389964699745178 at epoch: 22 and batch_num: 285\n",
      "Loss of train set: 0.23172004520893097 at epoch: 22 and batch_num: 286\n",
      "Loss of train set: 0.22814252972602844 at epoch: 22 and batch_num: 287\n",
      "Loss of train set: 0.222934752702713 at epoch: 22 and batch_num: 288\n",
      "Loss of train set: 0.3029443621635437 at epoch: 22 and batch_num: 289\n",
      "Loss of train set: 0.27803438901901245 at epoch: 22 and batch_num: 290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2595418393611908 at epoch: 22 and batch_num: 291\n",
      "Loss of train set: 0.43841123580932617 at epoch: 22 and batch_num: 292\n",
      "Loss of train set: 0.23354756832122803 at epoch: 22 and batch_num: 293\n",
      "Loss of train set: 0.20258642733097076 at epoch: 22 and batch_num: 294\n",
      "Loss of train set: 0.23876923322677612 at epoch: 22 and batch_num: 295\n",
      "Loss of train set: 0.22676829993724823 at epoch: 22 and batch_num: 296\n",
      "Loss of train set: 0.3181731700897217 at epoch: 22 and batch_num: 297\n",
      "Loss of train set: 0.12635841965675354 at epoch: 22 and batch_num: 298\n",
      "Loss of train set: 0.25622615218162537 at epoch: 22 and batch_num: 299\n",
      "Loss of train set: 0.30998900532722473 at epoch: 22 and batch_num: 300\n",
      "Loss of train set: 0.4132015109062195 at epoch: 22 and batch_num: 301\n",
      "Loss of train set: 0.282591849565506 at epoch: 22 and batch_num: 302\n",
      "Loss of train set: 0.28771069645881653 at epoch: 22 and batch_num: 303\n",
      "Loss of train set: 0.2812194228172302 at epoch: 22 and batch_num: 304\n",
      "Loss of train set: 0.25553280115127563 at epoch: 22 and batch_num: 305\n",
      "Loss of train set: 0.39513054490089417 at epoch: 22 and batch_num: 306\n",
      "Loss of train set: 0.33788740634918213 at epoch: 22 and batch_num: 307\n",
      "Loss of train set: 0.3650372624397278 at epoch: 22 and batch_num: 308\n",
      "Loss of train set: 0.2842189073562622 at epoch: 22 and batch_num: 309\n",
      "Loss of train set: 0.22374719381332397 at epoch: 22 and batch_num: 310\n",
      "Loss of train set: 0.2725656032562256 at epoch: 22 and batch_num: 311\n",
      "Loss of train set: 0.2482050508260727 at epoch: 22 and batch_num: 312\n",
      "Loss of train set: 0.32662105560302734 at epoch: 22 and batch_num: 313\n",
      "Loss of train set: 0.2522363066673279 at epoch: 22 and batch_num: 314\n",
      "Loss of train set: 0.26516976952552795 at epoch: 22 and batch_num: 315\n",
      "Loss of train set: 0.1740349978208542 at epoch: 22 and batch_num: 316\n",
      "Loss of train set: 0.3690758943557739 at epoch: 22 and batch_num: 317\n",
      "Loss of train set: 0.18141081929206848 at epoch: 22 and batch_num: 318\n",
      "Loss of train set: 0.22774091362953186 at epoch: 22 and batch_num: 319\n",
      "Loss of train set: 0.2532673180103302 at epoch: 22 and batch_num: 320\n",
      "Loss of train set: 0.20020648837089539 at epoch: 22 and batch_num: 321\n",
      "Loss of train set: 0.2510960102081299 at epoch: 22 and batch_num: 322\n",
      "Loss of train set: 0.2511809766292572 at epoch: 22 and batch_num: 323\n",
      "Loss of train set: 0.5075486302375793 at epoch: 22 and batch_num: 324\n",
      "Loss of train set: 0.3656938076019287 at epoch: 22 and batch_num: 325\n",
      "Loss of train set: 0.17318472266197205 at epoch: 22 and batch_num: 326\n",
      "Loss of train set: 0.3988546133041382 at epoch: 22 and batch_num: 327\n",
      "Loss of train set: 0.23833228647708893 at epoch: 22 and batch_num: 328\n",
      "Loss of train set: 0.2805130183696747 at epoch: 22 and batch_num: 329\n",
      "Loss of train set: 0.23359845578670502 at epoch: 22 and batch_num: 330\n",
      "Loss of train set: 0.27577799558639526 at epoch: 22 and batch_num: 331\n",
      "Loss of train set: 0.22271141409873962 at epoch: 22 and batch_num: 332\n",
      "Loss of train set: 0.17807286977767944 at epoch: 22 and batch_num: 333\n",
      "Loss of train set: 0.3198114037513733 at epoch: 22 and batch_num: 334\n",
      "Loss of train set: 0.20360279083251953 at epoch: 22 and batch_num: 335\n",
      "Loss of train set: 0.15075868368148804 at epoch: 22 and batch_num: 336\n",
      "Loss of train set: 0.39363691210746765 at epoch: 22 and batch_num: 337\n",
      "Loss of train set: 0.18302257359027863 at epoch: 22 and batch_num: 338\n",
      "Loss of train set: 0.3959130644798279 at epoch: 22 and batch_num: 339\n",
      "Loss of train set: 0.36632049083709717 at epoch: 22 and batch_num: 340\n",
      "Loss of train set: 0.35918429493904114 at epoch: 22 and batch_num: 341\n",
      "Loss of train set: 0.3842661678791046 at epoch: 22 and batch_num: 342\n",
      "Loss of train set: 0.30751097202301025 at epoch: 22 and batch_num: 343\n",
      "Loss of train set: 0.2887451648712158 at epoch: 22 and batch_num: 344\n",
      "Loss of train set: 0.42017894983291626 at epoch: 22 and batch_num: 345\n",
      "Loss of train set: 0.19762864708900452 at epoch: 22 and batch_num: 346\n",
      "Loss of train set: 0.2209455519914627 at epoch: 22 and batch_num: 347\n",
      "Loss of train set: 0.14499489963054657 at epoch: 22 and batch_num: 348\n",
      "Loss of train set: 0.21090523898601532 at epoch: 22 and batch_num: 349\n",
      "Loss of train set: 0.2467716783285141 at epoch: 22 and batch_num: 350\n",
      "Loss of train set: 0.24697771668434143 at epoch: 22 and batch_num: 351\n",
      "Loss of train set: 0.16675639152526855 at epoch: 22 and batch_num: 352\n",
      "Loss of train set: 0.22821807861328125 at epoch: 22 and batch_num: 353\n",
      "Loss of train set: 0.1520291268825531 at epoch: 22 and batch_num: 354\n",
      "Loss of train set: 0.3052407503128052 at epoch: 22 and batch_num: 355\n",
      "Loss of train set: 0.25712233781814575 at epoch: 22 and batch_num: 356\n",
      "Loss of train set: 0.39046511054039 at epoch: 22 and batch_num: 357\n",
      "Loss of train set: 0.28046855330467224 at epoch: 22 and batch_num: 358\n",
      "Loss of train set: 0.1809869259595871 at epoch: 22 and batch_num: 359\n",
      "Loss of train set: 0.14970190823078156 at epoch: 22 and batch_num: 360\n",
      "Loss of train set: 0.37680912017822266 at epoch: 22 and batch_num: 361\n",
      "Loss of train set: 0.19160126149654388 at epoch: 22 and batch_num: 362\n",
      "Loss of train set: 0.4447556138038635 at epoch: 22 and batch_num: 363\n",
      "Loss of train set: 0.3752968907356262 at epoch: 22 and batch_num: 364\n",
      "Loss of train set: 0.26657241582870483 at epoch: 22 and batch_num: 365\n",
      "Loss of train set: 0.20147551596164703 at epoch: 22 and batch_num: 366\n",
      "Loss of train set: 0.3148272633552551 at epoch: 22 and batch_num: 367\n",
      "Loss of train set: 0.30421921610832214 at epoch: 22 and batch_num: 368\n",
      "Loss of train set: 0.24454861879348755 at epoch: 22 and batch_num: 369\n",
      "Loss of train set: 0.17743656039237976 at epoch: 22 and batch_num: 370\n",
      "Loss of train set: 0.34350454807281494 at epoch: 22 and batch_num: 371\n",
      "Loss of train set: 0.20877428352832794 at epoch: 22 and batch_num: 372\n",
      "Loss of train set: 0.27690058946609497 at epoch: 22 and batch_num: 373\n",
      "Loss of train set: 0.2702561914920807 at epoch: 22 and batch_num: 374\n",
      "Loss of train set: 0.3587400019168854 at epoch: 22 and batch_num: 375\n",
      "Loss of train set: 0.24678650498390198 at epoch: 22 and batch_num: 376\n",
      "Loss of train set: 0.45374614000320435 at epoch: 22 and batch_num: 377\n",
      "Loss of train set: 0.1479547917842865 at epoch: 22 and batch_num: 378\n",
      "Loss of train set: 0.23976968228816986 at epoch: 22 and batch_num: 379\n",
      "Loss of train set: 0.22132906317710876 at epoch: 22 and batch_num: 380\n",
      "Loss of train set: 0.30447471141815186 at epoch: 22 and batch_num: 381\n",
      "Loss of train set: 0.3427543640136719 at epoch: 22 and batch_num: 382\n",
      "Loss of train set: 0.18347035348415375 at epoch: 22 and batch_num: 383\n",
      "Loss of train set: 0.17133426666259766 at epoch: 22 and batch_num: 384\n",
      "Loss of train set: 0.2623150050640106 at epoch: 22 and batch_num: 385\n",
      "Loss of train set: 0.14131516218185425 at epoch: 22 and batch_num: 386\n",
      "Loss of train set: 0.2600105404853821 at epoch: 22 and batch_num: 387\n",
      "Loss of train set: 0.17789395153522491 at epoch: 22 and batch_num: 388\n",
      "Loss of train set: 0.21369656920433044 at epoch: 22 and batch_num: 389\n",
      "Loss of train set: 0.23065145313739777 at epoch: 22 and batch_num: 390\n",
      "Loss of train set: 0.2632583975791931 at epoch: 22 and batch_num: 391\n",
      "Loss of train set: 0.30775898694992065 at epoch: 22 and batch_num: 392\n",
      "Loss of train set: 0.22462303936481476 at epoch: 22 and batch_num: 393\n",
      "Loss of train set: 0.2073754370212555 at epoch: 22 and batch_num: 394\n",
      "Loss of train set: 0.3999062776565552 at epoch: 22 and batch_num: 395\n",
      "Loss of train set: 0.2646518051624298 at epoch: 22 and batch_num: 396\n",
      "Loss of train set: 0.3096504807472229 at epoch: 22 and batch_num: 397\n",
      "Loss of train set: 0.2707948088645935 at epoch: 22 and batch_num: 398\n",
      "Loss of train set: 0.1571180522441864 at epoch: 22 and batch_num: 399\n",
      "Loss of train set: 0.26133817434310913 at epoch: 22 and batch_num: 400\n",
      "Loss of train set: 0.2758558392524719 at epoch: 22 and batch_num: 401\n",
      "Loss of train set: 0.18277491629123688 at epoch: 22 and batch_num: 402\n",
      "Loss of train set: 0.28039538860321045 at epoch: 22 and batch_num: 403\n",
      "Loss of train set: 0.255183607339859 at epoch: 22 and batch_num: 404\n",
      "Loss of train set: 0.20890507102012634 at epoch: 22 and batch_num: 405\n",
      "Loss of train set: 0.25217685103416443 at epoch: 22 and batch_num: 406\n",
      "Loss of train set: 0.2518349289894104 at epoch: 22 and batch_num: 407\n",
      "Loss of train set: 0.323757529258728 at epoch: 22 and batch_num: 408\n",
      "Loss of train set: 0.2042391002178192 at epoch: 22 and batch_num: 409\n",
      "Loss of train set: 0.3082922697067261 at epoch: 22 and batch_num: 410\n",
      "Loss of train set: 0.23316913843154907 at epoch: 22 and batch_num: 411\n",
      "Loss of train set: 0.13845562934875488 at epoch: 22 and batch_num: 412\n",
      "Loss of train set: 0.1613881140947342 at epoch: 22 and batch_num: 413\n",
      "Loss of train set: 0.2292003333568573 at epoch: 22 and batch_num: 414\n",
      "Loss of train set: 0.2111678421497345 at epoch: 22 and batch_num: 415\n",
      "Loss of train set: 0.15331250429153442 at epoch: 22 and batch_num: 416\n",
      "Loss of train set: 0.33258113265037537 at epoch: 22 and batch_num: 417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.18718288838863373 at epoch: 22 and batch_num: 418\n",
      "Loss of train set: 0.3086009919643402 at epoch: 22 and batch_num: 419\n",
      "Loss of train set: 0.4183591902256012 at epoch: 22 and batch_num: 420\n",
      "Loss of train set: 0.3339446187019348 at epoch: 22 and batch_num: 421\n",
      "Loss of train set: 0.3231763243675232 at epoch: 22 and batch_num: 422\n",
      "Loss of train set: 0.2699509859085083 at epoch: 22 and batch_num: 423\n",
      "Loss of train set: 0.2801819145679474 at epoch: 22 and batch_num: 424\n",
      "Loss of train set: 0.363233745098114 at epoch: 22 and batch_num: 425\n",
      "Loss of train set: 0.1971825361251831 at epoch: 22 and batch_num: 426\n",
      "Loss of train set: 0.22786396741867065 at epoch: 22 and batch_num: 427\n",
      "Loss of train set: 0.2378455400466919 at epoch: 22 and batch_num: 428\n",
      "Loss of train set: 0.20356948673725128 at epoch: 22 and batch_num: 429\n",
      "Loss of train set: 0.290127694606781 at epoch: 22 and batch_num: 430\n",
      "Loss of train set: 0.19212409853935242 at epoch: 22 and batch_num: 431\n",
      "Loss of train set: 0.1452585905790329 at epoch: 22 and batch_num: 432\n",
      "Loss of train set: 0.2385481297969818 at epoch: 22 and batch_num: 433\n",
      "Loss of train set: 0.16038930416107178 at epoch: 22 and batch_num: 434\n",
      "Loss of train set: 0.20696817338466644 at epoch: 22 and batch_num: 435\n",
      "Loss of train set: 0.4296554923057556 at epoch: 22 and batch_num: 436\n",
      "Loss of train set: 0.1985984593629837 at epoch: 22 and batch_num: 437\n",
      "Loss of train set: 0.30875280499458313 at epoch: 22 and batch_num: 438\n",
      "Loss of train set: 0.2753201425075531 at epoch: 22 and batch_num: 439\n",
      "Loss of train set: 0.164396271109581 at epoch: 22 and batch_num: 440\n",
      "Loss of train set: 0.14710044860839844 at epoch: 22 and batch_num: 441\n",
      "Loss of train set: 0.29213595390319824 at epoch: 22 and batch_num: 442\n",
      "Loss of train set: 0.3352876901626587 at epoch: 22 and batch_num: 443\n",
      "Loss of train set: 0.21033737063407898 at epoch: 22 and batch_num: 444\n",
      "Loss of train set: 0.3314778804779053 at epoch: 22 and batch_num: 445\n",
      "Loss of train set: 0.27341774106025696 at epoch: 22 and batch_num: 446\n",
      "Loss of train set: 0.2083444595336914 at epoch: 22 and batch_num: 447\n",
      "Loss of train set: 0.22615471482276917 at epoch: 22 and batch_num: 448\n",
      "Loss of train set: 0.3376833498477936 at epoch: 22 and batch_num: 449\n",
      "Loss of train set: 0.26828181743621826 at epoch: 22 and batch_num: 450\n",
      "Loss of train set: 0.39770281314849854 at epoch: 22 and batch_num: 451\n",
      "Loss of train set: 0.163528710603714 at epoch: 22 and batch_num: 452\n",
      "Loss of train set: 0.2834021747112274 at epoch: 22 and batch_num: 453\n",
      "Loss of train set: 0.2641364336013794 at epoch: 22 and batch_num: 454\n",
      "Loss of train set: 0.1312050223350525 at epoch: 22 and batch_num: 455\n",
      "Loss of train set: 0.2794630825519562 at epoch: 22 and batch_num: 456\n",
      "Loss of train set: 0.22249966859817505 at epoch: 22 and batch_num: 457\n",
      "Loss of train set: 0.4147183299064636 at epoch: 22 and batch_num: 458\n",
      "Loss of train set: 0.38149625062942505 at epoch: 22 and batch_num: 459\n",
      "Loss of train set: 0.2005065232515335 at epoch: 22 and batch_num: 460\n",
      "Loss of train set: 0.30833134055137634 at epoch: 22 and batch_num: 461\n",
      "Loss of train set: 0.18432091176509857 at epoch: 22 and batch_num: 462\n",
      "Loss of train set: 0.39563554525375366 at epoch: 22 and batch_num: 463\n",
      "Loss of train set: 0.3301158547401428 at epoch: 22 and batch_num: 464\n",
      "Loss of train set: 0.33490505814552307 at epoch: 22 and batch_num: 465\n",
      "Loss of train set: 0.24849779903888702 at epoch: 22 and batch_num: 466\n",
      "Loss of train set: 0.3228573799133301 at epoch: 22 and batch_num: 467\n",
      "Loss of train set: 0.29269689321517944 at epoch: 22 and batch_num: 468\n",
      "Loss of train set: 0.4106646180152893 at epoch: 22 and batch_num: 469\n",
      "Loss of train set: 0.30110812187194824 at epoch: 22 and batch_num: 470\n",
      "Loss of train set: 0.3041800856590271 at epoch: 22 and batch_num: 471\n",
      "Loss of train set: 0.2928485870361328 at epoch: 22 and batch_num: 472\n",
      "Loss of train set: 0.30240005254745483 at epoch: 22 and batch_num: 473\n",
      "Loss of train set: 0.2795252799987793 at epoch: 22 and batch_num: 474\n",
      "Loss of train set: 0.33546575903892517 at epoch: 22 and batch_num: 475\n",
      "Loss of train set: 0.46931201219558716 at epoch: 22 and batch_num: 476\n",
      "Loss of train set: 0.21395455300807953 at epoch: 22 and batch_num: 477\n",
      "Loss of train set: 0.278830885887146 at epoch: 22 and batch_num: 478\n",
      "Loss of train set: 0.22757473587989807 at epoch: 22 and batch_num: 479\n",
      "Loss of train set: 0.27520421147346497 at epoch: 22 and batch_num: 480\n",
      "Loss of train set: 0.18343469500541687 at epoch: 22 and batch_num: 481\n",
      "Loss of train set: 0.17687764763832092 at epoch: 22 and batch_num: 482\n",
      "Loss of train set: 0.3115418553352356 at epoch: 22 and batch_num: 483\n",
      "Loss of train set: 0.1850900501012802 at epoch: 22 and batch_num: 484\n",
      "Loss of train set: 0.24690058827400208 at epoch: 22 and batch_num: 485\n",
      "Loss of train set: 0.37616342306137085 at epoch: 22 and batch_num: 486\n",
      "Loss of train set: 0.22588062286376953 at epoch: 22 and batch_num: 487\n",
      "Loss of train set: 0.1993609219789505 at epoch: 22 and batch_num: 488\n",
      "Loss of train set: 0.2327958196401596 at epoch: 22 and batch_num: 489\n",
      "Loss of train set: 0.37751877307891846 at epoch: 22 and batch_num: 490\n",
      "Loss of train set: 0.341450035572052 at epoch: 22 and batch_num: 491\n",
      "Loss of train set: 0.2323264479637146 at epoch: 22 and batch_num: 492\n",
      "Loss of train set: 0.301224946975708 at epoch: 22 and batch_num: 493\n",
      "Loss of train set: 0.352888286113739 at epoch: 22 and batch_num: 494\n",
      "Loss of train set: 0.43362918496131897 at epoch: 22 and batch_num: 495\n",
      "Loss of train set: 0.4422018527984619 at epoch: 22 and batch_num: 496\n",
      "Loss of train set: 0.2693599462509155 at epoch: 22 and batch_num: 497\n",
      "Loss of train set: 0.2792559266090393 at epoch: 22 and batch_num: 498\n",
      "Loss of train set: 0.32828596234321594 at epoch: 22 and batch_num: 499\n",
      "Loss of train set: 0.3070533573627472 at epoch: 22 and batch_num: 500\n",
      "Loss of train set: 0.10632580518722534 at epoch: 22 and batch_num: 501\n",
      "Loss of train set: 0.17068511247634888 at epoch: 22 and batch_num: 502\n",
      "Loss of train set: 0.32898327708244324 at epoch: 22 and batch_num: 503\n",
      "Loss of train set: 0.31656545400619507 at epoch: 22 and batch_num: 504\n",
      "Loss of train set: 0.293384850025177 at epoch: 22 and batch_num: 505\n",
      "Loss of train set: 0.20235171914100647 at epoch: 22 and batch_num: 506\n",
      "Loss of train set: 0.385724812746048 at epoch: 22 and batch_num: 507\n",
      "Loss of train set: 0.21137253940105438 at epoch: 22 and batch_num: 508\n",
      "Loss of train set: 0.2665365934371948 at epoch: 22 and batch_num: 509\n",
      "Loss of train set: 0.3125828802585602 at epoch: 22 and batch_num: 510\n",
      "Loss of train set: 0.4600212275981903 at epoch: 22 and batch_num: 511\n",
      "Loss of train set: 0.31930699944496155 at epoch: 22 and batch_num: 512\n",
      "Loss of train set: 0.35751545429229736 at epoch: 22 and batch_num: 513\n",
      "Loss of train set: 0.2504246234893799 at epoch: 22 and batch_num: 514\n",
      "Loss of train set: 0.2138473391532898 at epoch: 22 and batch_num: 515\n",
      "Loss of train set: 0.3416140079498291 at epoch: 22 and batch_num: 516\n",
      "Loss of train set: 0.4141113758087158 at epoch: 22 and batch_num: 517\n",
      "Loss of train set: 0.2337370365858078 at epoch: 22 and batch_num: 518\n",
      "Loss of train set: 0.30676573514938354 at epoch: 22 and batch_num: 519\n",
      "Loss of train set: 0.17624056339263916 at epoch: 22 and batch_num: 520\n",
      "Loss of train set: 0.28065264225006104 at epoch: 22 and batch_num: 521\n",
      "Loss of train set: 0.2899278998374939 at epoch: 22 and batch_num: 522\n",
      "Loss of train set: 0.20320141315460205 at epoch: 22 and batch_num: 523\n",
      "Loss of train set: 0.31369900703430176 at epoch: 22 and batch_num: 524\n",
      "Loss of train set: 0.24078404903411865 at epoch: 22 and batch_num: 525\n",
      "Loss of train set: 0.33556067943573 at epoch: 22 and batch_num: 526\n",
      "Loss of train set: 0.35569241642951965 at epoch: 22 and batch_num: 527\n",
      "Loss of train set: 0.2750079929828644 at epoch: 22 and batch_num: 528\n",
      "Loss of train set: 0.3751295804977417 at epoch: 22 and batch_num: 529\n",
      "Loss of train set: 0.24815699458122253 at epoch: 22 and batch_num: 530\n",
      "Loss of train set: 0.30732712149620056 at epoch: 22 and batch_num: 531\n",
      "Loss of train set: 0.10057500749826431 at epoch: 22 and batch_num: 532\n",
      "Loss of train set: 0.27726495265960693 at epoch: 22 and batch_num: 533\n",
      "Loss of train set: 0.31915226578712463 at epoch: 22 and batch_num: 534\n",
      "Loss of train set: 0.30809515714645386 at epoch: 22 and batch_num: 535\n",
      "Loss of train set: 0.2950247526168823 at epoch: 22 and batch_num: 536\n",
      "Loss of train set: 0.18642663955688477 at epoch: 22 and batch_num: 537\n",
      "Loss of train set: 0.16385382413864136 at epoch: 22 and batch_num: 538\n",
      "Loss of train set: 0.26657095551490784 at epoch: 22 and batch_num: 539\n",
      "Loss of train set: 0.24900400638580322 at epoch: 22 and batch_num: 540\n",
      "Loss of train set: 0.1476910561323166 at epoch: 22 and batch_num: 541\n",
      "Loss of train set: 0.3721773028373718 at epoch: 22 and batch_num: 542\n",
      "Loss of train set: 0.33993130922317505 at epoch: 22 and batch_num: 543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2390766739845276 at epoch: 22 and batch_num: 544\n",
      "Loss of train set: 0.27085500955581665 at epoch: 22 and batch_num: 545\n",
      "Loss of train set: 0.19811877608299255 at epoch: 22 and batch_num: 546\n",
      "Loss of train set: 0.3589427173137665 at epoch: 22 and batch_num: 547\n",
      "Loss of train set: 0.36549532413482666 at epoch: 22 and batch_num: 548\n",
      "Loss of train set: 0.21929004788398743 at epoch: 22 and batch_num: 549\n",
      "Loss of train set: 0.20173513889312744 at epoch: 22 and batch_num: 550\n",
      "Loss of train set: 0.26739364862442017 at epoch: 22 and batch_num: 551\n",
      "Loss of train set: 0.3870640993118286 at epoch: 22 and batch_num: 552\n",
      "Loss of train set: 0.3925369381904602 at epoch: 22 and batch_num: 553\n",
      "Loss of train set: 0.1237536370754242 at epoch: 22 and batch_num: 554\n",
      "Loss of train set: 0.2825911343097687 at epoch: 22 and batch_num: 555\n",
      "Loss of train set: 0.15514199435710907 at epoch: 22 and batch_num: 556\n",
      "Loss of train set: 0.12368698418140411 at epoch: 22 and batch_num: 557\n",
      "Loss of train set: 0.35041892528533936 at epoch: 22 and batch_num: 558\n",
      "Loss of train set: 0.27243244647979736 at epoch: 22 and batch_num: 559\n",
      "Loss of train set: 0.3841128349304199 at epoch: 22 and batch_num: 560\n",
      "Loss of train set: 0.18895578384399414 at epoch: 22 and batch_num: 561\n",
      "Loss of train set: 0.27638354897499084 at epoch: 22 and batch_num: 562\n",
      "Loss of train set: 0.25923487544059753 at epoch: 22 and batch_num: 563\n",
      "Loss of train set: 0.3069724440574646 at epoch: 22 and batch_num: 564\n",
      "Loss of train set: 0.16086485981941223 at epoch: 22 and batch_num: 565\n",
      "Loss of train set: 0.2578932046890259 at epoch: 22 and batch_num: 566\n",
      "Loss of train set: 0.3046485185623169 at epoch: 22 and batch_num: 567\n",
      "Loss of train set: 0.2362108826637268 at epoch: 22 and batch_num: 568\n",
      "Loss of train set: 0.19526255130767822 at epoch: 22 and batch_num: 569\n",
      "Loss of train set: 0.3086528182029724 at epoch: 22 and batch_num: 570\n",
      "Loss of train set: 0.1889292150735855 at epoch: 22 and batch_num: 571\n",
      "Loss of train set: 0.18559780716896057 at epoch: 22 and batch_num: 572\n",
      "Loss of train set: 0.5112482309341431 at epoch: 22 and batch_num: 573\n",
      "Loss of train set: 0.2627856135368347 at epoch: 22 and batch_num: 574\n",
      "Loss of train set: 0.436679482460022 at epoch: 22 and batch_num: 575\n",
      "Loss of train set: 0.4387024939060211 at epoch: 22 and batch_num: 576\n",
      "Loss of train set: 0.28639715909957886 at epoch: 22 and batch_num: 577\n",
      "Loss of train set: 0.1634521186351776 at epoch: 22 and batch_num: 578\n",
      "Loss of train set: 0.12969180941581726 at epoch: 22 and batch_num: 579\n",
      "Loss of train set: 0.38971224427223206 at epoch: 22 and batch_num: 580\n",
      "Loss of train set: 0.17869439721107483 at epoch: 22 and batch_num: 581\n",
      "Loss of train set: 0.16744908690452576 at epoch: 22 and batch_num: 582\n",
      "Loss of train set: 0.21346203982830048 at epoch: 22 and batch_num: 583\n",
      "Loss of train set: 0.31700804829597473 at epoch: 22 and batch_num: 584\n",
      "Loss of train set: 0.33478623628616333 at epoch: 22 and batch_num: 585\n",
      "Loss of train set: 0.2431621551513672 at epoch: 22 and batch_num: 586\n",
      "Loss of train set: 0.1436423808336258 at epoch: 22 and batch_num: 587\n",
      "Loss of train set: 0.23079508543014526 at epoch: 22 and batch_num: 588\n",
      "Loss of train set: 0.19196082651615143 at epoch: 22 and batch_num: 589\n",
      "Loss of train set: 0.4111313819885254 at epoch: 22 and batch_num: 590\n",
      "Loss of train set: 0.4218686819076538 at epoch: 22 and batch_num: 591\n",
      "Loss of train set: 0.25373032689094543 at epoch: 22 and batch_num: 592\n",
      "Loss of train set: 0.22198177874088287 at epoch: 22 and batch_num: 593\n",
      "Loss of train set: 0.3256363868713379 at epoch: 22 and batch_num: 594\n",
      "Loss of train set: 0.27223849296569824 at epoch: 22 and batch_num: 595\n",
      "Loss of train set: 0.3761589825153351 at epoch: 22 and batch_num: 596\n",
      "Loss of train set: 0.21393543481826782 at epoch: 22 and batch_num: 597\n",
      "Loss of train set: 0.3104683458805084 at epoch: 22 and batch_num: 598\n",
      "Loss of train set: 0.4004030227661133 at epoch: 22 and batch_num: 599\n",
      "Loss of train set: 0.3230094313621521 at epoch: 22 and batch_num: 600\n",
      "Loss of train set: 0.4458746910095215 at epoch: 22 and batch_num: 601\n",
      "Loss of train set: 0.16839559376239777 at epoch: 22 and batch_num: 602\n",
      "Loss of train set: 0.34233641624450684 at epoch: 22 and batch_num: 603\n",
      "Loss of train set: 0.1867680847644806 at epoch: 22 and batch_num: 604\n",
      "Loss of train set: 0.41859084367752075 at epoch: 22 and batch_num: 605\n",
      "Loss of train set: 0.39112383127212524 at epoch: 22 and batch_num: 606\n",
      "Loss of train set: 0.4444730877876282 at epoch: 22 and batch_num: 607\n",
      "Loss of train set: 0.31643611192703247 at epoch: 22 and batch_num: 608\n",
      "Loss of train set: 0.2768271267414093 at epoch: 22 and batch_num: 609\n",
      "Loss of train set: 0.218712717294693 at epoch: 22 and batch_num: 610\n",
      "Loss of train set: 0.19400404393672943 at epoch: 22 and batch_num: 611\n",
      "Loss of train set: 0.23371592164039612 at epoch: 22 and batch_num: 612\n",
      "Loss of train set: 0.418188214302063 at epoch: 22 and batch_num: 613\n",
      "Loss of train set: 0.12944231927394867 at epoch: 22 and batch_num: 614\n",
      "Loss of train set: 0.2977427542209625 at epoch: 22 and batch_num: 615\n",
      "Loss of train set: 0.28673145174980164 at epoch: 22 and batch_num: 616\n",
      "Loss of train set: 0.390778124332428 at epoch: 22 and batch_num: 617\n",
      "Loss of train set: 0.34771570563316345 at epoch: 22 and batch_num: 618\n",
      "Loss of train set: 0.22620636224746704 at epoch: 22 and batch_num: 619\n",
      "Loss of train set: 0.23476219177246094 at epoch: 22 and batch_num: 620\n",
      "Loss of train set: 0.26051512360572815 at epoch: 22 and batch_num: 621\n",
      "Loss of train set: 0.32163721323013306 at epoch: 22 and batch_num: 622\n",
      "Loss of train set: 0.3329612612724304 at epoch: 22 and batch_num: 623\n",
      "Loss of train set: 0.22750893235206604 at epoch: 22 and batch_num: 624\n",
      "Loss of train set: 0.15881553292274475 at epoch: 22 and batch_num: 625\n",
      "Loss of train set: 0.17325086891651154 at epoch: 22 and batch_num: 626\n",
      "Loss of train set: 0.4385965168476105 at epoch: 22 and batch_num: 627\n",
      "Loss of train set: 0.2683241367340088 at epoch: 22 and batch_num: 628\n",
      "Loss of train set: 0.15076911449432373 at epoch: 22 and batch_num: 629\n",
      "Loss of train set: 0.1767757534980774 at epoch: 22 and batch_num: 630\n",
      "Loss of train set: 0.2310563623905182 at epoch: 22 and batch_num: 631\n",
      "Loss of train set: 0.20146828889846802 at epoch: 22 and batch_num: 632\n",
      "Loss of train set: 0.34111136198043823 at epoch: 22 and batch_num: 633\n",
      "Loss of train set: 0.1773310899734497 at epoch: 22 and batch_num: 634\n",
      "Loss of train set: 0.28705543279647827 at epoch: 22 and batch_num: 635\n",
      "Loss of train set: 0.20513930916786194 at epoch: 22 and batch_num: 636\n",
      "Loss of train set: 0.2984662353992462 at epoch: 22 and batch_num: 637\n",
      "Loss of train set: 0.24170595407485962 at epoch: 22 and batch_num: 638\n",
      "Loss of train set: 0.37839287519454956 at epoch: 22 and batch_num: 639\n",
      "Loss of train set: 0.27005454897880554 at epoch: 22 and batch_num: 640\n",
      "Loss of train set: 0.21155139803886414 at epoch: 22 and batch_num: 641\n",
      "Loss of train set: 0.22048087418079376 at epoch: 22 and batch_num: 642\n",
      "Loss of train set: 0.2506210207939148 at epoch: 22 and batch_num: 643\n",
      "Loss of train set: 0.23379279673099518 at epoch: 22 and batch_num: 644\n",
      "Loss of train set: 0.26870959997177124 at epoch: 22 and batch_num: 645\n",
      "Loss of train set: 0.24886402487754822 at epoch: 22 and batch_num: 646\n",
      "Loss of train set: 0.20865535736083984 at epoch: 22 and batch_num: 647\n",
      "Loss of train set: 0.1860455870628357 at epoch: 22 and batch_num: 648\n",
      "Loss of train set: 0.12372181564569473 at epoch: 22 and batch_num: 649\n",
      "Loss of train set: 0.2398872971534729 at epoch: 22 and batch_num: 650\n",
      "Loss of train set: 0.4920387268066406 at epoch: 22 and batch_num: 651\n",
      "Loss of train set: 0.2878541648387909 at epoch: 22 and batch_num: 652\n",
      "Loss of train set: 0.2599339187145233 at epoch: 22 and batch_num: 653\n",
      "Loss of train set: 0.31859397888183594 at epoch: 22 and batch_num: 654\n",
      "Loss of train set: 0.4225409924983978 at epoch: 22 and batch_num: 655\n",
      "Loss of train set: 0.20016656816005707 at epoch: 22 and batch_num: 656\n",
      "Loss of train set: 0.18354569375514984 at epoch: 22 and batch_num: 657\n",
      "Loss of train set: 0.3516557514667511 at epoch: 22 and batch_num: 658\n",
      "Loss of train set: 0.224821075797081 at epoch: 22 and batch_num: 659\n",
      "Loss of train set: 0.28954771161079407 at epoch: 22 and batch_num: 660\n",
      "Loss of train set: 0.19752156734466553 at epoch: 22 and batch_num: 661\n",
      "Loss of train set: 0.2792619466781616 at epoch: 22 and batch_num: 662\n",
      "Loss of train set: 0.19352078437805176 at epoch: 22 and batch_num: 663\n",
      "Loss of train set: 0.1637192666530609 at epoch: 22 and batch_num: 664\n",
      "Loss of train set: 0.2710927128791809 at epoch: 22 and batch_num: 665\n",
      "Loss of train set: 0.380606472492218 at epoch: 22 and batch_num: 666\n",
      "Loss of train set: 0.5038774013519287 at epoch: 22 and batch_num: 667\n",
      "Loss of train set: 0.2990280091762543 at epoch: 22 and batch_num: 668\n",
      "Loss of train set: 0.24973106384277344 at epoch: 22 and batch_num: 669\n",
      "Loss of train set: 0.39809995889663696 at epoch: 22 and batch_num: 670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2533581852912903 at epoch: 22 and batch_num: 671\n",
      "Loss of train set: 0.4541243314743042 at epoch: 22 and batch_num: 672\n",
      "Loss of train set: 0.401751846075058 at epoch: 22 and batch_num: 673\n",
      "Loss of train set: 0.34183475375175476 at epoch: 22 and batch_num: 674\n",
      "Loss of train set: 0.19829273223876953 at epoch: 22 and batch_num: 675\n",
      "Loss of train set: 0.4026784300804138 at epoch: 22 and batch_num: 676\n",
      "Loss of train set: 0.49221545457839966 at epoch: 22 and batch_num: 677\n",
      "Loss of train set: 0.33787238597869873 at epoch: 22 and batch_num: 678\n",
      "Loss of train set: 0.36598294973373413 at epoch: 22 and batch_num: 679\n",
      "Loss of train set: 0.1963658630847931 at epoch: 22 and batch_num: 680\n",
      "Loss of train set: 0.23293162882328033 at epoch: 22 and batch_num: 681\n",
      "Loss of train set: 0.3306013345718384 at epoch: 22 and batch_num: 682\n",
      "Loss of train set: 0.34695056080818176 at epoch: 22 and batch_num: 683\n",
      "Loss of train set: 0.2734980285167694 at epoch: 22 and batch_num: 684\n",
      "Loss of train set: 0.4018099904060364 at epoch: 22 and batch_num: 685\n",
      "Loss of train set: 0.3081663250923157 at epoch: 22 and batch_num: 686\n",
      "Loss of train set: 0.22836998105049133 at epoch: 22 and batch_num: 687\n",
      "Loss of train set: 0.28846848011016846 at epoch: 22 and batch_num: 688\n",
      "Loss of train set: 0.32562485337257385 at epoch: 22 and batch_num: 689\n",
      "Loss of train set: 0.2264997512102127 at epoch: 22 and batch_num: 690\n",
      "Loss of train set: 0.2270677387714386 at epoch: 22 and batch_num: 691\n",
      "Loss of train set: 0.1486576646566391 at epoch: 22 and batch_num: 692\n",
      "Loss of train set: 0.48883000016212463 at epoch: 22 and batch_num: 693\n",
      "Loss of train set: 0.22901731729507446 at epoch: 22 and batch_num: 694\n",
      "Loss of train set: 0.2620544731616974 at epoch: 22 and batch_num: 695\n",
      "Loss of train set: 0.17487186193466187 at epoch: 22 and batch_num: 696\n",
      "Loss of train set: 0.16659663617610931 at epoch: 22 and batch_num: 697\n",
      "Loss of train set: 0.20930656790733337 at epoch: 22 and batch_num: 698\n",
      "Loss of train set: 0.22785156965255737 at epoch: 22 and batch_num: 699\n",
      "Loss of train set: 0.24405887722969055 at epoch: 22 and batch_num: 700\n",
      "Loss of train set: 0.238113135099411 at epoch: 22 and batch_num: 701\n",
      "Loss of train set: 0.2533532381057739 at epoch: 22 and batch_num: 702\n",
      "Loss of train set: 0.22585785388946533 at epoch: 22 and batch_num: 703\n",
      "Loss of train set: 0.25608399510383606 at epoch: 22 and batch_num: 704\n",
      "Loss of train set: 0.17876851558685303 at epoch: 22 and batch_num: 705\n",
      "Loss of train set: 0.23374320566654205 at epoch: 22 and batch_num: 706\n",
      "Loss of train set: 0.20047006011009216 at epoch: 22 and batch_num: 707\n",
      "Loss of train set: 0.39689603447914124 at epoch: 22 and batch_num: 708\n",
      "Loss of train set: 0.306379497051239 at epoch: 22 and batch_num: 709\n",
      "Loss of train set: 0.3006414771080017 at epoch: 22 and batch_num: 710\n",
      "Loss of train set: 0.39499562978744507 at epoch: 22 and batch_num: 711\n",
      "Loss of train set: 0.13926658034324646 at epoch: 22 and batch_num: 712\n",
      "Loss of train set: 0.2013891041278839 at epoch: 22 and batch_num: 713\n",
      "Loss of train set: 0.3467809855937958 at epoch: 22 and batch_num: 714\n",
      "Loss of train set: 0.3774963319301605 at epoch: 22 and batch_num: 715\n",
      "Loss of train set: 0.3498557209968567 at epoch: 22 and batch_num: 716\n",
      "Loss of train set: 0.2650858759880066 at epoch: 22 and batch_num: 717\n",
      "Loss of train set: 0.31960225105285645 at epoch: 22 and batch_num: 718\n",
      "Loss of train set: 0.2516377568244934 at epoch: 22 and batch_num: 719\n",
      "Loss of train set: 0.12524178624153137 at epoch: 22 and batch_num: 720\n",
      "Loss of train set: 0.22700750827789307 at epoch: 22 and batch_num: 721\n",
      "Loss of train set: 0.24975502490997314 at epoch: 22 and batch_num: 722\n",
      "Loss of train set: 0.3022858202457428 at epoch: 22 and batch_num: 723\n",
      "Loss of train set: 0.4332340955734253 at epoch: 22 and batch_num: 724\n",
      "Loss of train set: 0.35313594341278076 at epoch: 22 and batch_num: 725\n",
      "Loss of train set: 0.24933074414730072 at epoch: 22 and batch_num: 726\n",
      "Loss of train set: 0.16273269057273865 at epoch: 22 and batch_num: 727\n",
      "Loss of train set: 0.33055347204208374 at epoch: 22 and batch_num: 728\n",
      "Loss of train set: 0.2788025736808777 at epoch: 22 and batch_num: 729\n",
      "Loss of train set: 0.29097282886505127 at epoch: 22 and batch_num: 730\n",
      "Loss of train set: 0.29602301120758057 at epoch: 22 and batch_num: 731\n",
      "Loss of train set: 0.3062766194343567 at epoch: 22 and batch_num: 732\n",
      "Loss of train set: 0.23052263259887695 at epoch: 22 and batch_num: 733\n",
      "Loss of train set: 0.20790064334869385 at epoch: 22 and batch_num: 734\n",
      "Loss of train set: 0.322740375995636 at epoch: 22 and batch_num: 735\n",
      "Loss of train set: 0.4108056426048279 at epoch: 22 and batch_num: 736\n",
      "Loss of train set: 0.2563105821609497 at epoch: 22 and batch_num: 737\n",
      "Loss of train set: 0.3892659544944763 at epoch: 22 and batch_num: 738\n",
      "Loss of train set: 0.24879410862922668 at epoch: 22 and batch_num: 739\n",
      "Loss of train set: 0.2278015911579132 at epoch: 22 and batch_num: 740\n",
      "Loss of train set: 0.1936722695827484 at epoch: 22 and batch_num: 741\n",
      "Loss of train set: 0.2866681218147278 at epoch: 22 and batch_num: 742\n",
      "Loss of train set: 0.3743712902069092 at epoch: 22 and batch_num: 743\n",
      "Loss of train set: 0.24037067592144012 at epoch: 22 and batch_num: 744\n",
      "Loss of train set: 0.426097571849823 at epoch: 22 and batch_num: 745\n",
      "Loss of train set: 0.2910970449447632 at epoch: 22 and batch_num: 746\n",
      "Loss of train set: 0.37773561477661133 at epoch: 22 and batch_num: 747\n",
      "Loss of train set: 0.3123178482055664 at epoch: 22 and batch_num: 748\n",
      "Loss of train set: 0.16755786538124084 at epoch: 22 and batch_num: 749\n",
      "Loss of train set: 0.35519587993621826 at epoch: 22 and batch_num: 750\n",
      "Loss of train set: 0.42746701836586 at epoch: 22 and batch_num: 751\n",
      "Loss of train set: 0.1327982097864151 at epoch: 22 and batch_num: 752\n",
      "Loss of train set: 0.18671643733978271 at epoch: 22 and batch_num: 753\n",
      "Loss of train set: 0.22777079045772552 at epoch: 22 and batch_num: 754\n",
      "Loss of train set: 0.23300626873970032 at epoch: 22 and batch_num: 755\n",
      "Loss of train set: 0.4848614037036896 at epoch: 22 and batch_num: 756\n",
      "Loss of train set: 0.24205920100212097 at epoch: 22 and batch_num: 757\n",
      "Loss of train set: 0.2939578890800476 at epoch: 22 and batch_num: 758\n",
      "Loss of train set: 0.15461713075637817 at epoch: 22 and batch_num: 759\n",
      "Loss of train set: 0.3123224377632141 at epoch: 22 and batch_num: 760\n",
      "Loss of train set: 0.28908205032348633 at epoch: 22 and batch_num: 761\n",
      "Loss of train set: 0.23590996861457825 at epoch: 22 and batch_num: 762\n",
      "Loss of train set: 0.18103662133216858 at epoch: 22 and batch_num: 763\n",
      "Loss of train set: 0.2819281220436096 at epoch: 22 and batch_num: 764\n",
      "Loss of train set: 0.3195520341396332 at epoch: 22 and batch_num: 765\n",
      "Loss of train set: 0.19926559925079346 at epoch: 22 and batch_num: 766\n",
      "Loss of train set: 0.1684926301240921 at epoch: 22 and batch_num: 767\n",
      "Loss of train set: 0.23848602175712585 at epoch: 22 and batch_num: 768\n",
      "Loss of train set: 0.2827184796333313 at epoch: 22 and batch_num: 769\n",
      "Loss of train set: 0.3554534316062927 at epoch: 22 and batch_num: 770\n",
      "Loss of train set: 0.45261597633361816 at epoch: 22 and batch_num: 771\n",
      "Loss of train set: 0.39670515060424805 at epoch: 22 and batch_num: 772\n",
      "Loss of train set: 0.2700168788433075 at epoch: 22 and batch_num: 773\n",
      "Loss of train set: 0.3001744747161865 at epoch: 22 and batch_num: 774\n",
      "Loss of train set: 0.2722625136375427 at epoch: 22 and batch_num: 775\n",
      "Loss of train set: 0.29111218452453613 at epoch: 22 and batch_num: 776\n",
      "Loss of train set: 0.36936241388320923 at epoch: 22 and batch_num: 777\n",
      "Loss of train set: 0.24659548699855804 at epoch: 22 and batch_num: 778\n",
      "Loss of train set: 0.15625065565109253 at epoch: 22 and batch_num: 779\n",
      "Loss of train set: 0.39417022466659546 at epoch: 22 and batch_num: 780\n",
      "Loss of train set: 0.2722015678882599 at epoch: 22 and batch_num: 781\n",
      "Loss of train set: 0.34186530113220215 at epoch: 22 and batch_num: 782\n",
      "Loss of train set: 0.31038329005241394 at epoch: 22 and batch_num: 783\n",
      "Loss of train set: 0.26177290081977844 at epoch: 22 and batch_num: 784\n",
      "Loss of train set: 0.2876459062099457 at epoch: 22 and batch_num: 785\n",
      "Loss of train set: 0.2727508246898651 at epoch: 22 and batch_num: 786\n",
      "Loss of train set: 0.314507395029068 at epoch: 22 and batch_num: 787\n",
      "Loss of train set: 0.3499263823032379 at epoch: 22 and batch_num: 788\n",
      "Loss of train set: 0.3378203511238098 at epoch: 22 and batch_num: 789\n",
      "Loss of train set: 0.18352435529232025 at epoch: 22 and batch_num: 790\n",
      "Loss of train set: 0.4741806983947754 at epoch: 22 and batch_num: 791\n",
      "Loss of train set: 0.14344224333763123 at epoch: 22 and batch_num: 792\n",
      "Loss of train set: 0.3578418493270874 at epoch: 22 and batch_num: 793\n",
      "Loss of train set: 0.2143564075231552 at epoch: 22 and batch_num: 794\n",
      "Loss of train set: 0.2732214629650116 at epoch: 22 and batch_num: 795\n",
      "Loss of train set: 0.3676666021347046 at epoch: 22 and batch_num: 796\n",
      "Loss of train set: 0.1679382026195526 at epoch: 22 and batch_num: 797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.39688611030578613 at epoch: 22 and batch_num: 798\n",
      "Loss of train set: 0.24648790061473846 at epoch: 22 and batch_num: 799\n",
      "Loss of train set: 0.25764894485473633 at epoch: 22 and batch_num: 800\n",
      "Loss of train set: 0.2320617288351059 at epoch: 22 and batch_num: 801\n",
      "Loss of train set: 0.24241340160369873 at epoch: 22 and batch_num: 802\n",
      "Loss of train set: 0.37278836965560913 at epoch: 22 and batch_num: 803\n",
      "Loss of train set: 0.18593181669712067 at epoch: 22 and batch_num: 804\n",
      "Loss of train set: 0.3849353790283203 at epoch: 22 and batch_num: 805\n",
      "Loss of train set: 0.43246546387672424 at epoch: 22 and batch_num: 806\n",
      "Loss of train set: 0.3348083198070526 at epoch: 22 and batch_num: 807\n",
      "Loss of train set: 0.28176653385162354 at epoch: 22 and batch_num: 808\n",
      "Loss of train set: 0.2857319712638855 at epoch: 22 and batch_num: 809\n",
      "Loss of train set: 0.2633918821811676 at epoch: 22 and batch_num: 810\n",
      "Loss of train set: 0.34853631258010864 at epoch: 22 and batch_num: 811\n",
      "Loss of train set: 0.3072740137577057 at epoch: 22 and batch_num: 812\n",
      "Loss of train set: 0.21345095336437225 at epoch: 22 and batch_num: 813\n",
      "Loss of train set: 0.378854900598526 at epoch: 22 and batch_num: 814\n",
      "Loss of train set: 0.215126633644104 at epoch: 22 and batch_num: 815\n",
      "Loss of train set: 0.2670655846595764 at epoch: 22 and batch_num: 816\n",
      "Loss of train set: 0.27532005310058594 at epoch: 22 and batch_num: 817\n",
      "Loss of train set: 0.32804322242736816 at epoch: 22 and batch_num: 818\n",
      "Loss of train set: 0.2637554407119751 at epoch: 22 and batch_num: 819\n",
      "Loss of train set: 0.41397935152053833 at epoch: 22 and batch_num: 820\n",
      "Loss of train set: 0.4176143407821655 at epoch: 22 and batch_num: 821\n",
      "Loss of train set: 0.6424451470375061 at epoch: 22 and batch_num: 822\n",
      "Loss of train set: 0.4016525149345398 at epoch: 22 and batch_num: 823\n",
      "Loss of train set: 0.3146553337574005 at epoch: 22 and batch_num: 824\n",
      "Loss of train set: 0.3500711917877197 at epoch: 22 and batch_num: 825\n",
      "Loss of train set: 0.451494425535202 at epoch: 22 and batch_num: 826\n",
      "Loss of train set: 0.32265016436576843 at epoch: 22 and batch_num: 827\n",
      "Loss of train set: 0.19570434093475342 at epoch: 22 and batch_num: 828\n",
      "Loss of train set: 0.24411872029304504 at epoch: 22 and batch_num: 829\n",
      "Loss of train set: 0.1584862321615219 at epoch: 22 and batch_num: 830\n",
      "Loss of train set: 0.4206255078315735 at epoch: 22 and batch_num: 831\n",
      "Loss of train set: 0.1797686666250229 at epoch: 22 and batch_num: 832\n",
      "Loss of train set: 0.35797956585884094 at epoch: 22 and batch_num: 833\n",
      "Loss of train set: 0.2500815689563751 at epoch: 22 and batch_num: 834\n",
      "Loss of train set: 0.17757642269134521 at epoch: 22 and batch_num: 835\n",
      "Loss of train set: 0.18803879618644714 at epoch: 22 and batch_num: 836\n",
      "Loss of train set: 0.27338871359825134 at epoch: 22 and batch_num: 837\n",
      "Loss of train set: 0.15601547062397003 at epoch: 22 and batch_num: 838\n",
      "Loss of train set: 0.30116379261016846 at epoch: 22 and batch_num: 839\n",
      "Loss of train set: 0.33962810039520264 at epoch: 22 and batch_num: 840\n",
      "Loss of train set: 0.23987120389938354 at epoch: 22 and batch_num: 841\n",
      "Loss of train set: 0.2127150595188141 at epoch: 22 and batch_num: 842\n",
      "Loss of train set: 0.2785947322845459 at epoch: 22 and batch_num: 843\n",
      "Loss of train set: 0.27871018648147583 at epoch: 22 and batch_num: 844\n",
      "Loss of train set: 0.43111464381217957 at epoch: 22 and batch_num: 845\n",
      "Loss of train set: 0.3565162420272827 at epoch: 22 and batch_num: 846\n",
      "Loss of train set: 0.17013567686080933 at epoch: 22 and batch_num: 847\n",
      "Loss of train set: 0.2667278051376343 at epoch: 22 and batch_num: 848\n",
      "Loss of train set: 0.29053759574890137 at epoch: 22 and batch_num: 849\n",
      "Loss of train set: 0.37116551399230957 at epoch: 22 and batch_num: 850\n",
      "Loss of train set: 0.3546495735645294 at epoch: 22 and batch_num: 851\n",
      "Loss of train set: 0.3053057789802551 at epoch: 22 and batch_num: 852\n",
      "Loss of train set: 0.29725271463394165 at epoch: 22 and batch_num: 853\n",
      "Loss of train set: 0.3316357731819153 at epoch: 22 and batch_num: 854\n",
      "Loss of train set: 0.18013018369674683 at epoch: 22 and batch_num: 855\n",
      "Loss of train set: 0.3031464219093323 at epoch: 22 and batch_num: 856\n",
      "Loss of train set: 0.257852166891098 at epoch: 22 and batch_num: 857\n",
      "Loss of train set: 0.3190421760082245 at epoch: 22 and batch_num: 858\n",
      "Loss of train set: 0.28178343176841736 at epoch: 22 and batch_num: 859\n",
      "Loss of train set: 0.36830592155456543 at epoch: 22 and batch_num: 860\n",
      "Loss of train set: 0.3990435004234314 at epoch: 22 and batch_num: 861\n",
      "Loss of train set: 0.27377328276634216 at epoch: 22 and batch_num: 862\n",
      "Loss of train set: 0.20026013255119324 at epoch: 22 and batch_num: 863\n",
      "Loss of train set: 0.28647303581237793 at epoch: 22 and batch_num: 864\n",
      "Loss of train set: 0.24317888915538788 at epoch: 22 and batch_num: 865\n",
      "Loss of train set: 0.21615472435951233 at epoch: 22 and batch_num: 866\n",
      "Loss of train set: 0.30376991629600525 at epoch: 22 and batch_num: 867\n",
      "Loss of train set: 0.4183059334754944 at epoch: 22 and batch_num: 868\n",
      "Loss of train set: 0.23406532406806946 at epoch: 22 and batch_num: 869\n",
      "Loss of train set: 0.16865386068820953 at epoch: 22 and batch_num: 870\n",
      "Loss of train set: 0.2901507616043091 at epoch: 22 and batch_num: 871\n",
      "Loss of train set: 0.30323874950408936 at epoch: 22 and batch_num: 872\n",
      "Loss of train set: 0.44479238986968994 at epoch: 22 and batch_num: 873\n",
      "Loss of train set: 0.3730262517929077 at epoch: 22 and batch_num: 874\n",
      "Loss of train set: 0.1646048128604889 at epoch: 22 and batch_num: 875\n",
      "Loss of train set: 0.18046727776527405 at epoch: 22 and batch_num: 876\n",
      "Loss of train set: 0.35233601927757263 at epoch: 22 and batch_num: 877\n",
      "Loss of train set: 0.40247756242752075 at epoch: 22 and batch_num: 878\n",
      "Loss of train set: 0.31026023626327515 at epoch: 22 and batch_num: 879\n",
      "Loss of train set: 0.2918579578399658 at epoch: 22 and batch_num: 880\n",
      "Loss of train set: 0.3116331398487091 at epoch: 22 and batch_num: 881\n",
      "Loss of train set: 0.2221914529800415 at epoch: 22 and batch_num: 882\n",
      "Loss of train set: 0.22038587927818298 at epoch: 22 and batch_num: 883\n",
      "Loss of train set: 0.21938756108283997 at epoch: 22 and batch_num: 884\n",
      "Loss of train set: 0.31767529249191284 at epoch: 22 and batch_num: 885\n",
      "Loss of train set: 0.34602922201156616 at epoch: 22 and batch_num: 886\n",
      "Loss of train set: 0.33335304260253906 at epoch: 22 and batch_num: 887\n",
      "Loss of train set: 0.3355044424533844 at epoch: 22 and batch_num: 888\n",
      "Loss of train set: 0.39767688512802124 at epoch: 22 and batch_num: 889\n",
      "Loss of train set: 0.38763219118118286 at epoch: 22 and batch_num: 890\n",
      "Loss of train set: 0.26462727785110474 at epoch: 22 and batch_num: 891\n",
      "Loss of train set: 0.20530173182487488 at epoch: 22 and batch_num: 892\n",
      "Loss of train set: 0.30427658557891846 at epoch: 22 and batch_num: 893\n",
      "Loss of train set: 0.2579748332500458 at epoch: 22 and batch_num: 894\n",
      "Loss of train set: 0.14951497316360474 at epoch: 22 and batch_num: 895\n",
      "Loss of train set: 0.42807087302207947 at epoch: 22 and batch_num: 896\n",
      "Loss of train set: 0.22263804078102112 at epoch: 22 and batch_num: 897\n",
      "Loss of train set: 0.19775700569152832 at epoch: 22 and batch_num: 898\n",
      "Loss of train set: 0.19539868831634521 at epoch: 22 and batch_num: 899\n",
      "Loss of train set: 0.23832973837852478 at epoch: 22 and batch_num: 900\n",
      "Loss of train set: 0.27111294865608215 at epoch: 22 and batch_num: 901\n",
      "Loss of train set: 0.24163733422756195 at epoch: 22 and batch_num: 902\n",
      "Loss of train set: 0.25009262561798096 at epoch: 22 and batch_num: 903\n",
      "Loss of train set: 0.361671507358551 at epoch: 22 and batch_num: 904\n",
      "Loss of train set: 0.4121255576610565 at epoch: 22 and batch_num: 905\n",
      "Loss of train set: 0.1272059679031372 at epoch: 22 and batch_num: 906\n",
      "Loss of train set: 0.1531641185283661 at epoch: 22 and batch_num: 907\n",
      "Loss of train set: 0.1695271134376526 at epoch: 22 and batch_num: 908\n",
      "Loss of train set: 0.34636008739471436 at epoch: 22 and batch_num: 909\n",
      "Loss of train set: 0.25188589096069336 at epoch: 22 and batch_num: 910\n",
      "Loss of train set: 0.20051588118076324 at epoch: 22 and batch_num: 911\n",
      "Loss of train set: 0.31119152903556824 at epoch: 22 and batch_num: 912\n",
      "Loss of train set: 0.35084447264671326 at epoch: 22 and batch_num: 913\n",
      "Loss of train set: 0.386340856552124 at epoch: 22 and batch_num: 914\n",
      "Loss of train set: 0.3363187313079834 at epoch: 22 and batch_num: 915\n",
      "Loss of train set: 0.1362367868423462 at epoch: 22 and batch_num: 916\n",
      "Loss of train set: 0.26945531368255615 at epoch: 22 and batch_num: 917\n",
      "Loss of train set: 0.2690357565879822 at epoch: 22 and batch_num: 918\n",
      "Loss of train set: 0.3140353262424469 at epoch: 22 and batch_num: 919\n",
      "Loss of train set: 0.1993286907672882 at epoch: 22 and batch_num: 920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2032061070203781 at epoch: 22 and batch_num: 921\n",
      "Loss of train set: 0.20403364300727844 at epoch: 22 and batch_num: 922\n",
      "Loss of train set: 0.19110088050365448 at epoch: 22 and batch_num: 923\n",
      "Loss of train set: 0.2874926030635834 at epoch: 22 and batch_num: 924\n",
      "Loss of train set: 0.4236811697483063 at epoch: 22 and batch_num: 925\n",
      "Loss of train set: 0.27712443470954895 at epoch: 22 and batch_num: 926\n",
      "Loss of train set: 0.26705676317214966 at epoch: 22 and batch_num: 927\n",
      "Loss of train set: 0.2889612317085266 at epoch: 22 and batch_num: 928\n",
      "Loss of train set: 0.19027400016784668 at epoch: 22 and batch_num: 929\n",
      "Loss of train set: 0.27873894572257996 at epoch: 22 and batch_num: 930\n",
      "Loss of train set: 0.300692081451416 at epoch: 22 and batch_num: 931\n",
      "Loss of train set: 0.17678874731063843 at epoch: 22 and batch_num: 932\n",
      "Loss of train set: 0.43457189202308655 at epoch: 22 and batch_num: 933\n",
      "Loss of train set: 0.15558770298957825 at epoch: 22 and batch_num: 934\n",
      "Loss of train set: 0.18338321149349213 at epoch: 22 and batch_num: 935\n",
      "Loss of train set: 0.15253981947898865 at epoch: 22 and batch_num: 936\n",
      "Loss of train set: 0.16794061660766602 at epoch: 22 and batch_num: 937\n",
      "Accuracy of train set: 0.89935\n",
      "Loss of test set: 0.3033350110054016 at epoch: 22 and batch_num: 0\n",
      "Loss of test set: 0.4421117603778839 at epoch: 22 and batch_num: 1\n",
      "Loss of test set: 0.3071260154247284 at epoch: 22 and batch_num: 2\n",
      "Loss of test set: 0.3160167336463928 at epoch: 22 and batch_num: 3\n",
      "Loss of test set: 0.2533046305179596 at epoch: 22 and batch_num: 4\n",
      "Loss of test set: 0.3817344307899475 at epoch: 22 and batch_num: 5\n",
      "Loss of test set: 0.2669716477394104 at epoch: 22 and batch_num: 6\n",
      "Loss of test set: 0.5288325548171997 at epoch: 22 and batch_num: 7\n",
      "Loss of test set: 0.24082602560520172 at epoch: 22 and batch_num: 8\n",
      "Loss of test set: 0.3097561001777649 at epoch: 22 and batch_num: 9\n",
      "Loss of test set: 0.144005686044693 at epoch: 22 and batch_num: 10\n",
      "Loss of test set: 0.3117757737636566 at epoch: 22 and batch_num: 11\n",
      "Loss of test set: 0.5345317125320435 at epoch: 22 and batch_num: 12\n",
      "Loss of test set: 0.29947710037231445 at epoch: 22 and batch_num: 13\n",
      "Loss of test set: 0.4332096576690674 at epoch: 22 and batch_num: 14\n",
      "Loss of test set: 0.45746493339538574 at epoch: 22 and batch_num: 15\n",
      "Loss of test set: 0.3003235459327698 at epoch: 22 and batch_num: 16\n",
      "Loss of test set: 0.34769633412361145 at epoch: 22 and batch_num: 17\n",
      "Loss of test set: 0.2644790709018707 at epoch: 22 and batch_num: 18\n",
      "Loss of test set: 0.39598387479782104 at epoch: 22 and batch_num: 19\n",
      "Loss of test set: 0.32561665773391724 at epoch: 22 and batch_num: 20\n",
      "Loss of test set: 0.33413809537887573 at epoch: 22 and batch_num: 21\n",
      "Loss of test set: 0.33312833309173584 at epoch: 22 and batch_num: 22\n",
      "Loss of test set: 0.4242832362651825 at epoch: 22 and batch_num: 23\n",
      "Loss of test set: 0.2283795028924942 at epoch: 22 and batch_num: 24\n",
      "Loss of test set: 0.1212802529335022 at epoch: 22 and batch_num: 25\n",
      "Loss of test set: 0.4570636749267578 at epoch: 22 and batch_num: 26\n",
      "Loss of test set: 0.43619590997695923 at epoch: 22 and batch_num: 27\n",
      "Loss of test set: 0.5162335634231567 at epoch: 22 and batch_num: 28\n",
      "Loss of test set: 0.2786027789115906 at epoch: 22 and batch_num: 29\n",
      "Loss of test set: 0.1427898108959198 at epoch: 22 and batch_num: 30\n",
      "Loss of test set: 0.33987489342689514 at epoch: 22 and batch_num: 31\n",
      "Loss of test set: 0.19862303137779236 at epoch: 22 and batch_num: 32\n",
      "Loss of test set: 0.40006589889526367 at epoch: 22 and batch_num: 33\n",
      "Loss of test set: 0.37644389271736145 at epoch: 22 and batch_num: 34\n",
      "Loss of test set: 0.28566694259643555 at epoch: 22 and batch_num: 35\n",
      "Loss of test set: 0.3912424147129059 at epoch: 22 and batch_num: 36\n",
      "Loss of test set: 0.33627673983573914 at epoch: 22 and batch_num: 37\n",
      "Loss of test set: 0.257251501083374 at epoch: 22 and batch_num: 38\n",
      "Loss of test set: 0.39064761996269226 at epoch: 22 and batch_num: 39\n",
      "Loss of test set: 0.22414253652095795 at epoch: 22 and batch_num: 40\n",
      "Loss of test set: 0.6407545804977417 at epoch: 22 and batch_num: 41\n",
      "Loss of test set: 0.2611868977546692 at epoch: 22 and batch_num: 42\n",
      "Loss of test set: 0.18104055523872375 at epoch: 22 and batch_num: 43\n",
      "Loss of test set: 0.5155037641525269 at epoch: 22 and batch_num: 44\n",
      "Loss of test set: 0.2909144163131714 at epoch: 22 and batch_num: 45\n",
      "Loss of test set: 0.46177709102630615 at epoch: 22 and batch_num: 46\n",
      "Loss of test set: 0.4190520644187927 at epoch: 22 and batch_num: 47\n",
      "Loss of test set: 0.45205238461494446 at epoch: 22 and batch_num: 48\n",
      "Loss of test set: 0.3636893332004547 at epoch: 22 and batch_num: 49\n",
      "Loss of test set: 0.37372785806655884 at epoch: 22 and batch_num: 50\n",
      "Loss of test set: 0.16767168045043945 at epoch: 22 and batch_num: 51\n",
      "Loss of test set: 0.3388633131980896 at epoch: 22 and batch_num: 52\n",
      "Loss of test set: 0.38459911942481995 at epoch: 22 and batch_num: 53\n",
      "Loss of test set: 0.3340643346309662 at epoch: 22 and batch_num: 54\n",
      "Loss of test set: 0.25868308544158936 at epoch: 22 and batch_num: 55\n",
      "Loss of test set: 0.21611523628234863 at epoch: 22 and batch_num: 56\n",
      "Loss of test set: 0.23017042875289917 at epoch: 22 and batch_num: 57\n",
      "Loss of test set: 0.4319130778312683 at epoch: 22 and batch_num: 58\n",
      "Loss of test set: 0.20274655520915985 at epoch: 22 and batch_num: 59\n",
      "Loss of test set: 0.3874092698097229 at epoch: 22 and batch_num: 60\n",
      "Loss of test set: 0.3032585382461548 at epoch: 22 and batch_num: 61\n",
      "Loss of test set: 0.4148916006088257 at epoch: 22 and batch_num: 62\n",
      "Loss of test set: 0.2611229419708252 at epoch: 22 and batch_num: 63\n",
      "Loss of test set: 0.22562214732170105 at epoch: 22 and batch_num: 64\n",
      "Loss of test set: 0.3103345036506653 at epoch: 22 and batch_num: 65\n",
      "Loss of test set: 0.4630255103111267 at epoch: 22 and batch_num: 66\n",
      "Loss of test set: 0.44576045870780945 at epoch: 22 and batch_num: 67\n",
      "Loss of test set: 0.4470803737640381 at epoch: 22 and batch_num: 68\n",
      "Loss of test set: 0.34876805543899536 at epoch: 22 and batch_num: 69\n",
      "Loss of test set: 0.35200512409210205 at epoch: 22 and batch_num: 70\n",
      "Loss of test set: 0.3201422095298767 at epoch: 22 and batch_num: 71\n",
      "Loss of test set: 0.3059207797050476 at epoch: 22 and batch_num: 72\n",
      "Loss of test set: 0.3031728267669678 at epoch: 22 and batch_num: 73\n",
      "Loss of test set: 0.2678567171096802 at epoch: 22 and batch_num: 74\n",
      "Loss of test set: 0.30094704031944275 at epoch: 22 and batch_num: 75\n",
      "Loss of test set: 0.5079249143600464 at epoch: 22 and batch_num: 76\n",
      "Loss of test set: 0.4195902943611145 at epoch: 22 and batch_num: 77\n",
      "Loss of test set: 0.37278443574905396 at epoch: 22 and batch_num: 78\n",
      "Loss of test set: 0.24716947972774506 at epoch: 22 and batch_num: 79\n",
      "Loss of test set: 0.35423317551612854 at epoch: 22 and batch_num: 80\n",
      "Loss of test set: 0.5838069915771484 at epoch: 22 and batch_num: 81\n",
      "Loss of test set: 0.3166084885597229 at epoch: 22 and batch_num: 82\n",
      "Loss of test set: 0.41287463903427124 at epoch: 22 and batch_num: 83\n",
      "Loss of test set: 0.2944512367248535 at epoch: 22 and batch_num: 84\n",
      "Loss of test set: 0.2359047681093216 at epoch: 22 and batch_num: 85\n",
      "Loss of test set: 0.3196307420730591 at epoch: 22 and batch_num: 86\n",
      "Loss of test set: 0.3594689667224884 at epoch: 22 and batch_num: 87\n",
      "Loss of test set: 0.23822271823883057 at epoch: 22 and batch_num: 88\n",
      "Loss of test set: 0.27182579040527344 at epoch: 22 and batch_num: 89\n",
      "Loss of test set: 0.3083699941635132 at epoch: 22 and batch_num: 90\n",
      "Loss of test set: 0.3208194375038147 at epoch: 22 and batch_num: 91\n",
      "Loss of test set: 0.18146763741970062 at epoch: 22 and batch_num: 92\n",
      "Loss of test set: 0.25287556648254395 at epoch: 22 and batch_num: 93\n",
      "Loss of test set: 0.21875524520874023 at epoch: 22 and batch_num: 94\n",
      "Loss of test set: 0.3057807683944702 at epoch: 22 and batch_num: 95\n",
      "Loss of test set: 0.4394020736217499 at epoch: 22 and batch_num: 96\n",
      "Loss of test set: 0.33828216791152954 at epoch: 22 and batch_num: 97\n",
      "Loss of test set: 0.45323216915130615 at epoch: 22 and batch_num: 98\n",
      "Loss of test set: 0.20031285285949707 at epoch: 22 and batch_num: 99\n",
      "Loss of test set: 0.2749602198600769 at epoch: 22 and batch_num: 100\n",
      "Loss of test set: 0.46385449171066284 at epoch: 22 and batch_num: 101\n",
      "Loss of test set: 0.2988564372062683 at epoch: 22 and batch_num: 102\n",
      "Loss of test set: 0.3752191364765167 at epoch: 22 and batch_num: 103\n",
      "Loss of test set: 0.4569617509841919 at epoch: 22 and batch_num: 104\n",
      "Loss of test set: 0.2507975995540619 at epoch: 22 and batch_num: 105\n",
      "Loss of test set: 0.45997315645217896 at epoch: 22 and batch_num: 106\n",
      "Loss of test set: 0.42749372124671936 at epoch: 22 and batch_num: 107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.24417829513549805 at epoch: 22 and batch_num: 108\n",
      "Loss of test set: 0.39997637271881104 at epoch: 22 and batch_num: 109\n",
      "Loss of test set: 0.257510781288147 at epoch: 22 and batch_num: 110\n",
      "Loss of test set: 0.41174960136413574 at epoch: 22 and batch_num: 111\n",
      "Loss of test set: 0.30258867144584656 at epoch: 22 and batch_num: 112\n",
      "Loss of test set: 0.2915192246437073 at epoch: 22 and batch_num: 113\n",
      "Loss of test set: 0.2220306396484375 at epoch: 22 and batch_num: 114\n",
      "Loss of test set: 0.37183862924575806 at epoch: 22 and batch_num: 115\n",
      "Loss of test set: 0.4283842146396637 at epoch: 22 and batch_num: 116\n",
      "Loss of test set: 0.22109432518482208 at epoch: 22 and batch_num: 117\n",
      "Loss of test set: 0.3479803204536438 at epoch: 22 and batch_num: 118\n",
      "Loss of test set: 0.38559508323669434 at epoch: 22 and batch_num: 119\n",
      "Loss of test set: 0.24473759531974792 at epoch: 22 and batch_num: 120\n",
      "Loss of test set: 0.2047160267829895 at epoch: 22 and batch_num: 121\n",
      "Loss of test set: 0.1708175539970398 at epoch: 22 and batch_num: 122\n",
      "Loss of test set: 0.36850395798683167 at epoch: 22 and batch_num: 123\n",
      "Loss of test set: 0.19275778532028198 at epoch: 22 and batch_num: 124\n",
      "Loss of test set: 0.5376284122467041 at epoch: 22 and batch_num: 125\n",
      "Loss of test set: 0.4202384352684021 at epoch: 22 and batch_num: 126\n",
      "Loss of test set: 0.3277350068092346 at epoch: 22 and batch_num: 127\n",
      "Loss of test set: 0.37841615080833435 at epoch: 22 and batch_num: 128\n",
      "Loss of test set: 0.41313058137893677 at epoch: 22 and batch_num: 129\n",
      "Loss of test set: 0.45777761936187744 at epoch: 22 and batch_num: 130\n",
      "Loss of test set: 0.26652511954307556 at epoch: 22 and batch_num: 131\n",
      "Loss of test set: 0.5336097478866577 at epoch: 22 and batch_num: 132\n",
      "Loss of test set: 0.34063607454299927 at epoch: 22 and batch_num: 133\n",
      "Loss of test set: 0.3770506680011749 at epoch: 22 and batch_num: 134\n",
      "Loss of test set: 0.3678804934024811 at epoch: 22 and batch_num: 135\n",
      "Loss of test set: 0.32115256786346436 at epoch: 22 and batch_num: 136\n",
      "Loss of test set: 0.284169465303421 at epoch: 22 and batch_num: 137\n",
      "Loss of test set: 0.40081727504730225 at epoch: 22 and batch_num: 138\n",
      "Loss of test set: 0.2585659325122833 at epoch: 22 and batch_num: 139\n",
      "Loss of test set: 0.5031970739364624 at epoch: 22 and batch_num: 140\n",
      "Loss of test set: 0.33119067549705505 at epoch: 22 and batch_num: 141\n",
      "Loss of test set: 0.5610600113868713 at epoch: 22 and batch_num: 142\n",
      "Loss of test set: 0.2811832129955292 at epoch: 22 and batch_num: 143\n",
      "Loss of test set: 0.22374802827835083 at epoch: 22 and batch_num: 144\n",
      "Loss of test set: 0.47769829630851746 at epoch: 22 and batch_num: 145\n",
      "Loss of test set: 0.22171875834465027 at epoch: 22 and batch_num: 146\n",
      "Loss of test set: 0.30291062593460083 at epoch: 22 and batch_num: 147\n",
      "Loss of test set: 0.22579345107078552 at epoch: 22 and batch_num: 148\n",
      "Loss of test set: 0.2840905785560608 at epoch: 22 and batch_num: 149\n",
      "Loss of test set: 0.37535756826400757 at epoch: 22 and batch_num: 150\n",
      "Loss of test set: 0.6901180148124695 at epoch: 22 and batch_num: 151\n",
      "Loss of test set: 0.24850042164325714 at epoch: 22 and batch_num: 152\n",
      "Loss of test set: 0.5218439698219299 at epoch: 22 and batch_num: 153\n",
      "Loss of test set: 0.20865283906459808 at epoch: 22 and batch_num: 154\n",
      "Loss of test set: 0.3848508596420288 at epoch: 22 and batch_num: 155\n",
      "Loss of test set: 0.2721669375896454 at epoch: 22 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8781\n",
      "Loss of train set: 0.25235414505004883 at epoch: 23 and batch_num: 0\n",
      "Loss of train set: 0.3601107597351074 at epoch: 23 and batch_num: 1\n",
      "Loss of train set: 0.3359617292881012 at epoch: 23 and batch_num: 2\n",
      "Loss of train set: 0.27258729934692383 at epoch: 23 and batch_num: 3\n",
      "Loss of train set: 0.3314138948917389 at epoch: 23 and batch_num: 4\n",
      "Loss of train set: 0.2897340655326843 at epoch: 23 and batch_num: 5\n",
      "Loss of train set: 0.33694541454315186 at epoch: 23 and batch_num: 6\n",
      "Loss of train set: 0.20330578088760376 at epoch: 23 and batch_num: 7\n",
      "Loss of train set: 0.1636313647031784 at epoch: 23 and batch_num: 8\n",
      "Loss of train set: 0.24345991015434265 at epoch: 23 and batch_num: 9\n",
      "Loss of train set: 0.21321925520896912 at epoch: 23 and batch_num: 10\n",
      "Loss of train set: 0.2313835620880127 at epoch: 23 and batch_num: 11\n",
      "Loss of train set: 0.10924699902534485 at epoch: 23 and batch_num: 12\n",
      "Loss of train set: 0.1952190101146698 at epoch: 23 and batch_num: 13\n",
      "Loss of train set: 0.3307911455631256 at epoch: 23 and batch_num: 14\n",
      "Loss of train set: 0.2945273518562317 at epoch: 23 and batch_num: 15\n",
      "Loss of train set: 0.15706385672092438 at epoch: 23 and batch_num: 16\n",
      "Loss of train set: 0.16562646627426147 at epoch: 23 and batch_num: 17\n",
      "Loss of train set: 0.24284711480140686 at epoch: 23 and batch_num: 18\n",
      "Loss of train set: 0.31648099422454834 at epoch: 23 and batch_num: 19\n",
      "Loss of train set: 0.31192904710769653 at epoch: 23 and batch_num: 20\n",
      "Loss of train set: 0.17738553881645203 at epoch: 23 and batch_num: 21\n",
      "Loss of train set: 0.2071382701396942 at epoch: 23 and batch_num: 22\n",
      "Loss of train set: 0.2592355012893677 at epoch: 23 and batch_num: 23\n",
      "Loss of train set: 0.2888126075267792 at epoch: 23 and batch_num: 24\n",
      "Loss of train set: 0.21028044819831848 at epoch: 23 and batch_num: 25\n",
      "Loss of train set: 0.38964560627937317 at epoch: 23 and batch_num: 26\n",
      "Loss of train set: 0.15384739637374878 at epoch: 23 and batch_num: 27\n",
      "Loss of train set: 0.28065669536590576 at epoch: 23 and batch_num: 28\n",
      "Loss of train set: 0.18305042386054993 at epoch: 23 and batch_num: 29\n",
      "Loss of train set: 0.39002150297164917 at epoch: 23 and batch_num: 30\n",
      "Loss of train set: 0.3244870901107788 at epoch: 23 and batch_num: 31\n",
      "Loss of train set: 0.3931828439235687 at epoch: 23 and batch_num: 32\n",
      "Loss of train set: 0.20809444785118103 at epoch: 23 and batch_num: 33\n",
      "Loss of train set: 0.25104713439941406 at epoch: 23 and batch_num: 34\n",
      "Loss of train set: 0.18978333473205566 at epoch: 23 and batch_num: 35\n",
      "Loss of train set: 0.314799427986145 at epoch: 23 and batch_num: 36\n",
      "Loss of train set: 0.24128754436969757 at epoch: 23 and batch_num: 37\n",
      "Loss of train set: 0.26482829451560974 at epoch: 23 and batch_num: 38\n",
      "Loss of train set: 0.23363323509693146 at epoch: 23 and batch_num: 39\n",
      "Loss of train set: 0.17475640773773193 at epoch: 23 and batch_num: 40\n",
      "Loss of train set: 0.19563651084899902 at epoch: 23 and batch_num: 41\n",
      "Loss of train set: 0.3360944986343384 at epoch: 23 and batch_num: 42\n",
      "Loss of train set: 0.3259921967983246 at epoch: 23 and batch_num: 43\n",
      "Loss of train set: 0.19036328792572021 at epoch: 23 and batch_num: 44\n",
      "Loss of train set: 0.13749417662620544 at epoch: 23 and batch_num: 45\n",
      "Loss of train set: 0.19247882068157196 at epoch: 23 and batch_num: 46\n",
      "Loss of train set: 0.295737087726593 at epoch: 23 and batch_num: 47\n",
      "Loss of train set: 0.2002754807472229 at epoch: 23 and batch_num: 48\n",
      "Loss of train set: 0.14389929175376892 at epoch: 23 and batch_num: 49\n",
      "Loss of train set: 0.19493886828422546 at epoch: 23 and batch_num: 50\n",
      "Loss of train set: 0.2306535542011261 at epoch: 23 and batch_num: 51\n",
      "Loss of train set: 0.1794496476650238 at epoch: 23 and batch_num: 52\n",
      "Loss of train set: 0.17557454109191895 at epoch: 23 and batch_num: 53\n",
      "Loss of train set: 0.3505938947200775 at epoch: 23 and batch_num: 54\n",
      "Loss of train set: 0.2694641947746277 at epoch: 23 and batch_num: 55\n",
      "Loss of train set: 0.2739790976047516 at epoch: 23 and batch_num: 56\n",
      "Loss of train set: 0.19295385479927063 at epoch: 23 and batch_num: 57\n",
      "Loss of train set: 0.2337525486946106 at epoch: 23 and batch_num: 58\n",
      "Loss of train set: 0.26644641160964966 at epoch: 23 and batch_num: 59\n",
      "Loss of train set: 0.1903100311756134 at epoch: 23 and batch_num: 60\n",
      "Loss of train set: 0.20060598850250244 at epoch: 23 and batch_num: 61\n",
      "Loss of train set: 0.22537675499916077 at epoch: 23 and batch_num: 62\n",
      "Loss of train set: 0.12494991719722748 at epoch: 23 and batch_num: 63\n",
      "Loss of train set: 0.23723740875720978 at epoch: 23 and batch_num: 64\n",
      "Loss of train set: 0.2507896423339844 at epoch: 23 and batch_num: 65\n",
      "Loss of train set: 0.33742761611938477 at epoch: 23 and batch_num: 66\n",
      "Loss of train set: 0.280253142118454 at epoch: 23 and batch_num: 67\n",
      "Loss of train set: 0.16097089648246765 at epoch: 23 and batch_num: 68\n",
      "Loss of train set: 0.2578684389591217 at epoch: 23 and batch_num: 69\n",
      "Loss of train set: 0.15637479722499847 at epoch: 23 and batch_num: 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2646930515766144 at epoch: 23 and batch_num: 71\n",
      "Loss of train set: 0.14278516173362732 at epoch: 23 and batch_num: 72\n",
      "Loss of train set: 0.40366441011428833 at epoch: 23 and batch_num: 73\n",
      "Loss of train set: 0.43173643946647644 at epoch: 23 and batch_num: 74\n",
      "Loss of train set: 0.24047061800956726 at epoch: 23 and batch_num: 75\n",
      "Loss of train set: 0.2171260416507721 at epoch: 23 and batch_num: 76\n",
      "Loss of train set: 0.20656579732894897 at epoch: 23 and batch_num: 77\n",
      "Loss of train set: 0.2413300722837448 at epoch: 23 and batch_num: 78\n",
      "Loss of train set: 0.186122328042984 at epoch: 23 and batch_num: 79\n",
      "Loss of train set: 0.3791496753692627 at epoch: 23 and batch_num: 80\n",
      "Loss of train set: 0.27392739057540894 at epoch: 23 and batch_num: 81\n",
      "Loss of train set: 0.21591687202453613 at epoch: 23 and batch_num: 82\n",
      "Loss of train set: 0.17247208952903748 at epoch: 23 and batch_num: 83\n",
      "Loss of train set: 0.1536623239517212 at epoch: 23 and batch_num: 84\n",
      "Loss of train set: 0.2706857919692993 at epoch: 23 and batch_num: 85\n",
      "Loss of train set: 0.48054155707359314 at epoch: 23 and batch_num: 86\n",
      "Loss of train set: 0.3514305353164673 at epoch: 23 and batch_num: 87\n",
      "Loss of train set: 0.2704676389694214 at epoch: 23 and batch_num: 88\n",
      "Loss of train set: 0.24935518205165863 at epoch: 23 and batch_num: 89\n",
      "Loss of train set: 0.36052238941192627 at epoch: 23 and batch_num: 90\n",
      "Loss of train set: 0.3010375499725342 at epoch: 23 and batch_num: 91\n",
      "Loss of train set: 0.24642498791217804 at epoch: 23 and batch_num: 92\n",
      "Loss of train set: 0.24747316539287567 at epoch: 23 and batch_num: 93\n",
      "Loss of train set: 0.27085256576538086 at epoch: 23 and batch_num: 94\n",
      "Loss of train set: 0.1941070556640625 at epoch: 23 and batch_num: 95\n",
      "Loss of train set: 0.2680773437023163 at epoch: 23 and batch_num: 96\n",
      "Loss of train set: 0.18081116676330566 at epoch: 23 and batch_num: 97\n",
      "Loss of train set: 0.2008654624223709 at epoch: 23 and batch_num: 98\n",
      "Loss of train set: 0.2483886480331421 at epoch: 23 and batch_num: 99\n",
      "Loss of train set: 0.20439651608467102 at epoch: 23 and batch_num: 100\n",
      "Loss of train set: 0.21641805768013 at epoch: 23 and batch_num: 101\n",
      "Loss of train set: 0.26373592019081116 at epoch: 23 and batch_num: 102\n",
      "Loss of train set: 0.29322272539138794 at epoch: 23 and batch_num: 103\n",
      "Loss of train set: 0.38125064969062805 at epoch: 23 and batch_num: 104\n",
      "Loss of train set: 0.3275644779205322 at epoch: 23 and batch_num: 105\n",
      "Loss of train set: 0.2478846311569214 at epoch: 23 and batch_num: 106\n",
      "Loss of train set: 0.46298080682754517 at epoch: 23 and batch_num: 107\n",
      "Loss of train set: 0.34637880325317383 at epoch: 23 and batch_num: 108\n",
      "Loss of train set: 0.2410021722316742 at epoch: 23 and batch_num: 109\n",
      "Loss of train set: 0.16994738578796387 at epoch: 23 and batch_num: 110\n",
      "Loss of train set: 0.39210498332977295 at epoch: 23 and batch_num: 111\n",
      "Loss of train set: 0.20710614323616028 at epoch: 23 and batch_num: 112\n",
      "Loss of train set: 0.26094168424606323 at epoch: 23 and batch_num: 113\n",
      "Loss of train set: 0.38885998725891113 at epoch: 23 and batch_num: 114\n",
      "Loss of train set: 0.28462955355644226 at epoch: 23 and batch_num: 115\n",
      "Loss of train set: 0.5251872539520264 at epoch: 23 and batch_num: 116\n",
      "Loss of train set: 0.1674993634223938 at epoch: 23 and batch_num: 117\n",
      "Loss of train set: 0.2555932402610779 at epoch: 23 and batch_num: 118\n",
      "Loss of train set: 0.31816840171813965 at epoch: 23 and batch_num: 119\n",
      "Loss of train set: 0.4925786256790161 at epoch: 23 and batch_num: 120\n",
      "Loss of train set: 0.28828662633895874 at epoch: 23 and batch_num: 121\n",
      "Loss of train set: 0.23214343190193176 at epoch: 23 and batch_num: 122\n",
      "Loss of train set: 0.24662259221076965 at epoch: 23 and batch_num: 123\n",
      "Loss of train set: 0.17193174362182617 at epoch: 23 and batch_num: 124\n",
      "Loss of train set: 0.280913770198822 at epoch: 23 and batch_num: 125\n",
      "Loss of train set: 0.24635577201843262 at epoch: 23 and batch_num: 126\n",
      "Loss of train set: 0.3359653353691101 at epoch: 23 and batch_num: 127\n",
      "Loss of train set: 0.19474825263023376 at epoch: 23 and batch_num: 128\n",
      "Loss of train set: 0.290446937084198 at epoch: 23 and batch_num: 129\n",
      "Loss of train set: 0.36408764123916626 at epoch: 23 and batch_num: 130\n",
      "Loss of train set: 0.27695703506469727 at epoch: 23 and batch_num: 131\n",
      "Loss of train set: 0.23773767054080963 at epoch: 23 and batch_num: 132\n",
      "Loss of train set: 0.25212177634239197 at epoch: 23 and batch_num: 133\n",
      "Loss of train set: 0.2364587038755417 at epoch: 23 and batch_num: 134\n",
      "Loss of train set: 0.24085073173046112 at epoch: 23 and batch_num: 135\n",
      "Loss of train set: 0.25599583983421326 at epoch: 23 and batch_num: 136\n",
      "Loss of train set: 0.20083987712860107 at epoch: 23 and batch_num: 137\n",
      "Loss of train set: 0.1829240918159485 at epoch: 23 and batch_num: 138\n",
      "Loss of train set: 0.30215370655059814 at epoch: 23 and batch_num: 139\n",
      "Loss of train set: 0.21459037065505981 at epoch: 23 and batch_num: 140\n",
      "Loss of train set: 0.35911428928375244 at epoch: 23 and batch_num: 141\n",
      "Loss of train set: 0.1769096553325653 at epoch: 23 and batch_num: 142\n",
      "Loss of train set: 0.20663511753082275 at epoch: 23 and batch_num: 143\n",
      "Loss of train set: 0.2445215880870819 at epoch: 23 and batch_num: 144\n",
      "Loss of train set: 0.27036166191101074 at epoch: 23 and batch_num: 145\n",
      "Loss of train set: 0.1236545741558075 at epoch: 23 and batch_num: 146\n",
      "Loss of train set: 0.35113176703453064 at epoch: 23 and batch_num: 147\n",
      "Loss of train set: 0.26248109340667725 at epoch: 23 and batch_num: 148\n",
      "Loss of train set: 0.2646990418434143 at epoch: 23 and batch_num: 149\n",
      "Loss of train set: 0.18975231051445007 at epoch: 23 and batch_num: 150\n",
      "Loss of train set: 0.23584437370300293 at epoch: 23 and batch_num: 151\n",
      "Loss of train set: 0.3640083074569702 at epoch: 23 and batch_num: 152\n",
      "Loss of train set: 0.261207640171051 at epoch: 23 and batch_num: 153\n",
      "Loss of train set: 0.3534207344055176 at epoch: 23 and batch_num: 154\n",
      "Loss of train set: 0.21550239622592926 at epoch: 23 and batch_num: 155\n",
      "Loss of train set: 0.27658528089523315 at epoch: 23 and batch_num: 156\n",
      "Loss of train set: 0.24805110692977905 at epoch: 23 and batch_num: 157\n",
      "Loss of train set: 0.33708611130714417 at epoch: 23 and batch_num: 158\n",
      "Loss of train set: 0.22544533014297485 at epoch: 23 and batch_num: 159\n",
      "Loss of train set: 0.4008055329322815 at epoch: 23 and batch_num: 160\n",
      "Loss of train set: 0.23297299444675446 at epoch: 23 and batch_num: 161\n",
      "Loss of train set: 0.2159629762172699 at epoch: 23 and batch_num: 162\n",
      "Loss of train set: 0.38228946924209595 at epoch: 23 and batch_num: 163\n",
      "Loss of train set: 0.2769846022129059 at epoch: 23 and batch_num: 164\n",
      "Loss of train set: 0.2630561590194702 at epoch: 23 and batch_num: 165\n",
      "Loss of train set: 0.24543607234954834 at epoch: 23 and batch_num: 166\n",
      "Loss of train set: 0.18933668732643127 at epoch: 23 and batch_num: 167\n",
      "Loss of train set: 0.23110456764698029 at epoch: 23 and batch_num: 168\n",
      "Loss of train set: 0.1350845843553543 at epoch: 23 and batch_num: 169\n",
      "Loss of train set: 0.09703386574983597 at epoch: 23 and batch_num: 170\n",
      "Loss of train set: 0.19182847440242767 at epoch: 23 and batch_num: 171\n",
      "Loss of train set: 0.3647565245628357 at epoch: 23 and batch_num: 172\n",
      "Loss of train set: 0.09247838705778122 at epoch: 23 and batch_num: 173\n",
      "Loss of train set: 0.24708418548107147 at epoch: 23 and batch_num: 174\n",
      "Loss of train set: 0.30586981773376465 at epoch: 23 and batch_num: 175\n",
      "Loss of train set: 0.2608969807624817 at epoch: 23 and batch_num: 176\n",
      "Loss of train set: 0.21415671706199646 at epoch: 23 and batch_num: 177\n",
      "Loss of train set: 0.1660844087600708 at epoch: 23 and batch_num: 178\n",
      "Loss of train set: 0.264702707529068 at epoch: 23 and batch_num: 179\n",
      "Loss of train set: 0.24260272085666656 at epoch: 23 and batch_num: 180\n",
      "Loss of train set: 0.15347589552402496 at epoch: 23 and batch_num: 181\n",
      "Loss of train set: 0.2270822376012802 at epoch: 23 and batch_num: 182\n",
      "Loss of train set: 0.26076626777648926 at epoch: 23 and batch_num: 183\n",
      "Loss of train set: 0.37017083168029785 at epoch: 23 and batch_num: 184\n",
      "Loss of train set: 0.1411479413509369 at epoch: 23 and batch_num: 185\n",
      "Loss of train set: 0.42989060282707214 at epoch: 23 and batch_num: 186\n",
      "Loss of train set: 0.24306616187095642 at epoch: 23 and batch_num: 187\n",
      "Loss of train set: 0.26170170307159424 at epoch: 23 and batch_num: 188\n",
      "Loss of train set: 0.29636693000793457 at epoch: 23 and batch_num: 189\n",
      "Loss of train set: 0.23048555850982666 at epoch: 23 and batch_num: 190\n",
      "Loss of train set: 0.20023606717586517 at epoch: 23 and batch_num: 191\n",
      "Loss of train set: 0.37322548031806946 at epoch: 23 and batch_num: 192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.6465269327163696 at epoch: 23 and batch_num: 193\n",
      "Loss of train set: 0.24382701516151428 at epoch: 23 and batch_num: 194\n",
      "Loss of train set: 0.3337319791316986 at epoch: 23 and batch_num: 195\n",
      "Loss of train set: 0.22507795691490173 at epoch: 23 and batch_num: 196\n",
      "Loss of train set: 0.3545182943344116 at epoch: 23 and batch_num: 197\n",
      "Loss of train set: 0.2339666187763214 at epoch: 23 and batch_num: 198\n",
      "Loss of train set: 0.15442392230033875 at epoch: 23 and batch_num: 199\n",
      "Loss of train set: 0.24867543578147888 at epoch: 23 and batch_num: 200\n",
      "Loss of train set: 0.3349107801914215 at epoch: 23 and batch_num: 201\n",
      "Loss of train set: 0.4043368101119995 at epoch: 23 and batch_num: 202\n",
      "Loss of train set: 0.19917838275432587 at epoch: 23 and batch_num: 203\n",
      "Loss of train set: 0.33825281262397766 at epoch: 23 and batch_num: 204\n",
      "Loss of train set: 0.35761743783950806 at epoch: 23 and batch_num: 205\n",
      "Loss of train set: 0.6196838617324829 at epoch: 23 and batch_num: 206\n",
      "Loss of train set: 0.2760559022426605 at epoch: 23 and batch_num: 207\n",
      "Loss of train set: 0.26458436250686646 at epoch: 23 and batch_num: 208\n",
      "Loss of train set: 0.212105393409729 at epoch: 23 and batch_num: 209\n",
      "Loss of train set: 0.372775673866272 at epoch: 23 and batch_num: 210\n",
      "Loss of train set: 0.13688711822032928 at epoch: 23 and batch_num: 211\n",
      "Loss of train set: 0.29790759086608887 at epoch: 23 and batch_num: 212\n",
      "Loss of train set: 0.26882970333099365 at epoch: 23 and batch_num: 213\n",
      "Loss of train set: 0.12413188815116882 at epoch: 23 and batch_num: 214\n",
      "Loss of train set: 0.32426199316978455 at epoch: 23 and batch_num: 215\n",
      "Loss of train set: 0.25570109486579895 at epoch: 23 and batch_num: 216\n",
      "Loss of train set: 0.10364165902137756 at epoch: 23 and batch_num: 217\n",
      "Loss of train set: 0.3147895038127899 at epoch: 23 and batch_num: 218\n",
      "Loss of train set: 0.29531529545783997 at epoch: 23 and batch_num: 219\n",
      "Loss of train set: 0.26834237575531006 at epoch: 23 and batch_num: 220\n",
      "Loss of train set: 0.4051504135131836 at epoch: 23 and batch_num: 221\n",
      "Loss of train set: 0.15098078548908234 at epoch: 23 and batch_num: 222\n",
      "Loss of train set: 0.3134876787662506 at epoch: 23 and batch_num: 223\n",
      "Loss of train set: 0.3086495101451874 at epoch: 23 and batch_num: 224\n",
      "Loss of train set: 0.2380061149597168 at epoch: 23 and batch_num: 225\n",
      "Loss of train set: 0.36936086416244507 at epoch: 23 and batch_num: 226\n",
      "Loss of train set: 0.24138528108596802 at epoch: 23 and batch_num: 227\n",
      "Loss of train set: 0.296838641166687 at epoch: 23 and batch_num: 228\n",
      "Loss of train set: 0.20646065473556519 at epoch: 23 and batch_num: 229\n",
      "Loss of train set: 0.2854766845703125 at epoch: 23 and batch_num: 230\n",
      "Loss of train set: 0.1405978500843048 at epoch: 23 and batch_num: 231\n",
      "Loss of train set: 0.39294755458831787 at epoch: 23 and batch_num: 232\n",
      "Loss of train set: 0.21763701736927032 at epoch: 23 and batch_num: 233\n",
      "Loss of train set: 0.3424656093120575 at epoch: 23 and batch_num: 234\n",
      "Loss of train set: 0.38457170128822327 at epoch: 23 and batch_num: 235\n",
      "Loss of train set: 0.3450922966003418 at epoch: 23 and batch_num: 236\n",
      "Loss of train set: 0.3310800790786743 at epoch: 23 and batch_num: 237\n",
      "Loss of train set: 0.23387955129146576 at epoch: 23 and batch_num: 238\n",
      "Loss of train set: 0.211552232503891 at epoch: 23 and batch_num: 239\n",
      "Loss of train set: 0.21456289291381836 at epoch: 23 and batch_num: 240\n",
      "Loss of train set: 0.18271729350090027 at epoch: 23 and batch_num: 241\n",
      "Loss of train set: 0.2127450704574585 at epoch: 23 and batch_num: 242\n",
      "Loss of train set: 0.3381745517253876 at epoch: 23 and batch_num: 243\n",
      "Loss of train set: 0.33725133538246155 at epoch: 23 and batch_num: 244\n",
      "Loss of train set: 0.2617764174938202 at epoch: 23 and batch_num: 245\n",
      "Loss of train set: 0.3492498993873596 at epoch: 23 and batch_num: 246\n",
      "Loss of train set: 0.22134485840797424 at epoch: 23 and batch_num: 247\n",
      "Loss of train set: 0.19237764179706573 at epoch: 23 and batch_num: 248\n",
      "Loss of train set: 0.18633714318275452 at epoch: 23 and batch_num: 249\n",
      "Loss of train set: 0.16378253698349 at epoch: 23 and batch_num: 250\n",
      "Loss of train set: 0.3576231002807617 at epoch: 23 and batch_num: 251\n",
      "Loss of train set: 0.21017128229141235 at epoch: 23 and batch_num: 252\n",
      "Loss of train set: 0.20902124047279358 at epoch: 23 and batch_num: 253\n",
      "Loss of train set: 0.2642260789871216 at epoch: 23 and batch_num: 254\n",
      "Loss of train set: 0.23485910892486572 at epoch: 23 and batch_num: 255\n",
      "Loss of train set: 0.3135744035243988 at epoch: 23 and batch_num: 256\n",
      "Loss of train set: 0.16002345085144043 at epoch: 23 and batch_num: 257\n",
      "Loss of train set: 0.3289182186126709 at epoch: 23 and batch_num: 258\n",
      "Loss of train set: 0.24490350484848022 at epoch: 23 and batch_num: 259\n",
      "Loss of train set: 0.2648324966430664 at epoch: 23 and batch_num: 260\n",
      "Loss of train set: 0.2398158609867096 at epoch: 23 and batch_num: 261\n",
      "Loss of train set: 0.18090534210205078 at epoch: 23 and batch_num: 262\n",
      "Loss of train set: 0.23819103837013245 at epoch: 23 and batch_num: 263\n",
      "Loss of train set: 0.39949700236320496 at epoch: 23 and batch_num: 264\n",
      "Loss of train set: 0.12798024713993073 at epoch: 23 and batch_num: 265\n",
      "Loss of train set: 0.25455978512763977 at epoch: 23 and batch_num: 266\n",
      "Loss of train set: 0.29227808117866516 at epoch: 23 and batch_num: 267\n",
      "Loss of train set: 0.19380445778369904 at epoch: 23 and batch_num: 268\n",
      "Loss of train set: 0.23025771975517273 at epoch: 23 and batch_num: 269\n",
      "Loss of train set: 0.2507314682006836 at epoch: 23 and batch_num: 270\n",
      "Loss of train set: 0.18072445690631866 at epoch: 23 and batch_num: 271\n",
      "Loss of train set: 0.3706139922142029 at epoch: 23 and batch_num: 272\n",
      "Loss of train set: 0.16199630498886108 at epoch: 23 and batch_num: 273\n",
      "Loss of train set: 0.1379721760749817 at epoch: 23 and batch_num: 274\n",
      "Loss of train set: 0.267715722322464 at epoch: 23 and batch_num: 275\n",
      "Loss of train set: 0.2653927803039551 at epoch: 23 and batch_num: 276\n",
      "Loss of train set: 0.18102428317070007 at epoch: 23 and batch_num: 277\n",
      "Loss of train set: 0.25841522216796875 at epoch: 23 and batch_num: 278\n",
      "Loss of train set: 0.3591490387916565 at epoch: 23 and batch_num: 279\n",
      "Loss of train set: 0.1979893445968628 at epoch: 23 and batch_num: 280\n",
      "Loss of train set: 0.13908842206001282 at epoch: 23 and batch_num: 281\n",
      "Loss of train set: 0.26785051822662354 at epoch: 23 and batch_num: 282\n",
      "Loss of train set: 0.22690144181251526 at epoch: 23 and batch_num: 283\n",
      "Loss of train set: 0.32048285007476807 at epoch: 23 and batch_num: 284\n",
      "Loss of train set: 0.28492438793182373 at epoch: 23 and batch_num: 285\n",
      "Loss of train set: 0.2086978256702423 at epoch: 23 and batch_num: 286\n",
      "Loss of train set: 0.31208300590515137 at epoch: 23 and batch_num: 287\n",
      "Loss of train set: 0.19061501324176788 at epoch: 23 and batch_num: 288\n",
      "Loss of train set: 0.25445735454559326 at epoch: 23 and batch_num: 289\n",
      "Loss of train set: 0.35853311419487 at epoch: 23 and batch_num: 290\n",
      "Loss of train set: 0.2859838604927063 at epoch: 23 and batch_num: 291\n",
      "Loss of train set: 0.32623234391212463 at epoch: 23 and batch_num: 292\n",
      "Loss of train set: 0.3606126010417938 at epoch: 23 and batch_num: 293\n",
      "Loss of train set: 0.2863408923149109 at epoch: 23 and batch_num: 294\n",
      "Loss of train set: 0.2542143166065216 at epoch: 23 and batch_num: 295\n",
      "Loss of train set: 0.20887145400047302 at epoch: 23 and batch_num: 296\n",
      "Loss of train set: 0.27559182047843933 at epoch: 23 and batch_num: 297\n",
      "Loss of train set: 0.3181828558444977 at epoch: 23 and batch_num: 298\n",
      "Loss of train set: 0.2516414523124695 at epoch: 23 and batch_num: 299\n",
      "Loss of train set: 0.3556213080883026 at epoch: 23 and batch_num: 300\n",
      "Loss of train set: 0.27010515332221985 at epoch: 23 and batch_num: 301\n",
      "Loss of train set: 0.4837527275085449 at epoch: 23 and batch_num: 302\n",
      "Loss of train set: 0.31998130679130554 at epoch: 23 and batch_num: 303\n",
      "Loss of train set: 0.3907909095287323 at epoch: 23 and batch_num: 304\n",
      "Loss of train set: 0.301618367433548 at epoch: 23 and batch_num: 305\n",
      "Loss of train set: 0.575298011302948 at epoch: 23 and batch_num: 306\n",
      "Loss of train set: 0.3968569040298462 at epoch: 23 and batch_num: 307\n",
      "Loss of train set: 0.33147525787353516 at epoch: 23 and batch_num: 308\n",
      "Loss of train set: 0.17090648412704468 at epoch: 23 and batch_num: 309\n",
      "Loss of train set: 0.33604589104652405 at epoch: 23 and batch_num: 310\n",
      "Loss of train set: 0.2672281861305237 at epoch: 23 and batch_num: 311\n",
      "Loss of train set: 0.26940059661865234 at epoch: 23 and batch_num: 312\n",
      "Loss of train set: 0.2738543152809143 at epoch: 23 and batch_num: 313\n",
      "Loss of train set: 0.2787245213985443 at epoch: 23 and batch_num: 314\n",
      "Loss of train set: 0.32518163323402405 at epoch: 23 and batch_num: 315\n",
      "Loss of train set: 0.18849033117294312 at epoch: 23 and batch_num: 316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.2853582501411438 at epoch: 23 and batch_num: 317\n",
      "Loss of train set: 0.11940371245145798 at epoch: 23 and batch_num: 318\n",
      "Loss of train set: 0.22102786600589752 at epoch: 23 and batch_num: 319\n",
      "Loss of train set: 0.18802417814731598 at epoch: 23 and batch_num: 320\n",
      "Loss of train set: 0.23877228796482086 at epoch: 23 and batch_num: 321\n",
      "Loss of train set: 0.1814955472946167 at epoch: 23 and batch_num: 322\n",
      "Loss of train set: 0.2655065953731537 at epoch: 23 and batch_num: 323\n",
      "Loss of train set: 0.3309054672718048 at epoch: 23 and batch_num: 324\n",
      "Loss of train set: 0.09779150784015656 at epoch: 23 and batch_num: 325\n",
      "Loss of train set: 0.23649229109287262 at epoch: 23 and batch_num: 326\n",
      "Loss of train set: 0.2345007061958313 at epoch: 23 and batch_num: 327\n",
      "Loss of train set: 0.2749921977519989 at epoch: 23 and batch_num: 328\n",
      "Loss of train set: 0.13505475223064423 at epoch: 23 and batch_num: 329\n",
      "Loss of train set: 0.22022750973701477 at epoch: 23 and batch_num: 330\n",
      "Loss of train set: 0.2603098750114441 at epoch: 23 and batch_num: 331\n",
      "Loss of train set: 0.28152450919151306 at epoch: 23 and batch_num: 332\n",
      "Loss of train set: 0.182247132062912 at epoch: 23 and batch_num: 333\n",
      "Loss of train set: 0.5080942511558533 at epoch: 23 and batch_num: 334\n",
      "Loss of train set: 0.5137515068054199 at epoch: 23 and batch_num: 335\n",
      "Loss of train set: 0.3974892795085907 at epoch: 23 and batch_num: 336\n",
      "Loss of train set: 0.23577691614627838 at epoch: 23 and batch_num: 337\n",
      "Loss of train set: 0.3475807309150696 at epoch: 23 and batch_num: 338\n",
      "Loss of train set: 0.31983447074890137 at epoch: 23 and batch_num: 339\n",
      "Loss of train set: 0.23569327592849731 at epoch: 23 and batch_num: 340\n",
      "Loss of train set: 0.26182955503463745 at epoch: 23 and batch_num: 341\n",
      "Loss of train set: 0.29389771819114685 at epoch: 23 and batch_num: 342\n",
      "Loss of train set: 0.2228279858827591 at epoch: 23 and batch_num: 343\n",
      "Loss of train set: 0.2966527044773102 at epoch: 23 and batch_num: 344\n",
      "Loss of train set: 0.12103241682052612 at epoch: 23 and batch_num: 345\n",
      "Loss of train set: 0.3079881966114044 at epoch: 23 and batch_num: 346\n",
      "Loss of train set: 0.352988600730896 at epoch: 23 and batch_num: 347\n",
      "Loss of train set: 0.3059762418270111 at epoch: 23 and batch_num: 348\n",
      "Loss of train set: 0.2667463421821594 at epoch: 23 and batch_num: 349\n",
      "Loss of train set: 0.20524734258651733 at epoch: 23 and batch_num: 350\n",
      "Loss of train set: 0.27773669362068176 at epoch: 23 and batch_num: 351\n",
      "Loss of train set: 0.24289968609809875 at epoch: 23 and batch_num: 352\n",
      "Loss of train set: 0.20034103095531464 at epoch: 23 and batch_num: 353\n",
      "Loss of train set: 0.4334162175655365 at epoch: 23 and batch_num: 354\n",
      "Loss of train set: 0.357968807220459 at epoch: 23 and batch_num: 355\n",
      "Loss of train set: 0.2731584906578064 at epoch: 23 and batch_num: 356\n",
      "Loss of train set: 0.2412450611591339 at epoch: 23 and batch_num: 357\n",
      "Loss of train set: 0.2473914623260498 at epoch: 23 and batch_num: 358\n",
      "Loss of train set: 0.49862146377563477 at epoch: 23 and batch_num: 359\n",
      "Loss of train set: 0.14182308316230774 at epoch: 23 and batch_num: 360\n",
      "Loss of train set: 0.13420647382736206 at epoch: 23 and batch_num: 361\n",
      "Loss of train set: 0.26300719380378723 at epoch: 23 and batch_num: 362\n",
      "Loss of train set: 0.5660619139671326 at epoch: 23 and batch_num: 363\n",
      "Loss of train set: 0.4521198868751526 at epoch: 23 and batch_num: 364\n",
      "Loss of train set: 0.25679853558540344 at epoch: 23 and batch_num: 365\n",
      "Loss of train set: 0.17880763113498688 at epoch: 23 and batch_num: 366\n",
      "Loss of train set: 0.22395798563957214 at epoch: 23 and batch_num: 367\n",
      "Loss of train set: 0.3508610129356384 at epoch: 23 and batch_num: 368\n",
      "Loss of train set: 0.17188523709774017 at epoch: 23 and batch_num: 369\n",
      "Loss of train set: 0.33677446842193604 at epoch: 23 and batch_num: 370\n",
      "Loss of train set: 0.2791225016117096 at epoch: 23 and batch_num: 371\n",
      "Loss of train set: 0.3120998740196228 at epoch: 23 and batch_num: 372\n",
      "Loss of train set: 0.18931981921195984 at epoch: 23 and batch_num: 373\n",
      "Loss of train set: 0.38318657875061035 at epoch: 23 and batch_num: 374\n",
      "Loss of train set: 0.2119974046945572 at epoch: 23 and batch_num: 375\n",
      "Loss of train set: 0.24190913140773773 at epoch: 23 and batch_num: 376\n",
      "Loss of train set: 0.3427618145942688 at epoch: 23 and batch_num: 377\n",
      "Loss of train set: 0.22979314625263214 at epoch: 23 and batch_num: 378\n",
      "Loss of train set: 0.16376063227653503 at epoch: 23 and batch_num: 379\n",
      "Loss of train set: 0.4205993115901947 at epoch: 23 and batch_num: 380\n",
      "Loss of train set: 0.2596655786037445 at epoch: 23 and batch_num: 381\n",
      "Loss of train set: 0.28882330656051636 at epoch: 23 and batch_num: 382\n",
      "Loss of train set: 0.22947701811790466 at epoch: 23 and batch_num: 383\n",
      "Loss of train set: 0.31207963824272156 at epoch: 23 and batch_num: 384\n",
      "Loss of train set: 0.2068285346031189 at epoch: 23 and batch_num: 385\n",
      "Loss of train set: 0.2045387476682663 at epoch: 23 and batch_num: 386\n",
      "Loss of train set: 0.19351081550121307 at epoch: 23 and batch_num: 387\n",
      "Loss of train set: 0.23556718230247498 at epoch: 23 and batch_num: 388\n",
      "Loss of train set: 0.2926609218120575 at epoch: 23 and batch_num: 389\n",
      "Loss of train set: 0.3068111538887024 at epoch: 23 and batch_num: 390\n",
      "Loss of train set: 0.7242851257324219 at epoch: 23 and batch_num: 391\n",
      "Loss of train set: 0.4209514260292053 at epoch: 23 and batch_num: 392\n",
      "Loss of train set: 0.22653332352638245 at epoch: 23 and batch_num: 393\n",
      "Loss of train set: 0.3340676724910736 at epoch: 23 and batch_num: 394\n",
      "Loss of train set: 0.24293015897274017 at epoch: 23 and batch_num: 395\n",
      "Loss of train set: 0.21477724611759186 at epoch: 23 and batch_num: 396\n",
      "Loss of train set: 0.30104678869247437 at epoch: 23 and batch_num: 397\n",
      "Loss of train set: 0.4702117443084717 at epoch: 23 and batch_num: 398\n",
      "Loss of train set: 0.31333717703819275 at epoch: 23 and batch_num: 399\n",
      "Loss of train set: 0.2494688779115677 at epoch: 23 and batch_num: 400\n",
      "Loss of train set: 0.3267415165901184 at epoch: 23 and batch_num: 401\n",
      "Loss of train set: 0.3278699517250061 at epoch: 23 and batch_num: 402\n",
      "Loss of train set: 0.3599264621734619 at epoch: 23 and batch_num: 403\n",
      "Loss of train set: 0.12874455749988556 at epoch: 23 and batch_num: 404\n",
      "Loss of train set: 0.24966666102409363 at epoch: 23 and batch_num: 405\n",
      "Loss of train set: 0.20955973863601685 at epoch: 23 and batch_num: 406\n",
      "Loss of train set: 0.3459148406982422 at epoch: 23 and batch_num: 407\n",
      "Loss of train set: 0.45161956548690796 at epoch: 23 and batch_num: 408\n",
      "Loss of train set: 0.19011440873146057 at epoch: 23 and batch_num: 409\n",
      "Loss of train set: 0.4177078604698181 at epoch: 23 and batch_num: 410\n",
      "Loss of train set: 0.26592501997947693 at epoch: 23 and batch_num: 411\n",
      "Loss of train set: 0.2796010375022888 at epoch: 23 and batch_num: 412\n",
      "Loss of train set: 0.48910242319107056 at epoch: 23 and batch_num: 413\n",
      "Loss of train set: 0.3372928500175476 at epoch: 23 and batch_num: 414\n",
      "Loss of train set: 0.25410887598991394 at epoch: 23 and batch_num: 415\n",
      "Loss of train set: 0.3030565083026886 at epoch: 23 and batch_num: 416\n",
      "Loss of train set: 0.36391013860702515 at epoch: 23 and batch_num: 417\n",
      "Loss of train set: 0.1799326390028 at epoch: 23 and batch_num: 418\n",
      "Loss of train set: 0.26793229579925537 at epoch: 23 and batch_num: 419\n",
      "Loss of train set: 0.2692681849002838 at epoch: 23 and batch_num: 420\n",
      "Loss of train set: 0.18977731466293335 at epoch: 23 and batch_num: 421\n",
      "Loss of train set: 0.11998478323221207 at epoch: 23 and batch_num: 422\n",
      "Loss of train set: 0.2906242609024048 at epoch: 23 and batch_num: 423\n",
      "Loss of train set: 0.2469719648361206 at epoch: 23 and batch_num: 424\n",
      "Loss of train set: 0.1637183427810669 at epoch: 23 and batch_num: 425\n",
      "Loss of train set: 0.13511548936367035 at epoch: 23 and batch_num: 426\n",
      "Loss of train set: 0.16931013762950897 at epoch: 23 and batch_num: 427\n",
      "Loss of train set: 0.35589897632598877 at epoch: 23 and batch_num: 428\n",
      "Loss of train set: 0.3237554728984833 at epoch: 23 and batch_num: 429\n",
      "Loss of train set: 0.2631232440471649 at epoch: 23 and batch_num: 430\n",
      "Loss of train set: 0.20686931908130646 at epoch: 23 and batch_num: 431\n",
      "Loss of train set: 0.18750156462192535 at epoch: 23 and batch_num: 432\n",
      "Loss of train set: 0.3443810045719147 at epoch: 23 and batch_num: 433\n",
      "Loss of train set: 0.26362860202789307 at epoch: 23 and batch_num: 434\n",
      "Loss of train set: 0.3596874475479126 at epoch: 23 and batch_num: 435\n",
      "Loss of train set: 0.30053138732910156 at epoch: 23 and batch_num: 436\n",
      "Loss of train set: 0.28357723355293274 at epoch: 23 and batch_num: 437\n",
      "Loss of train set: 0.3298942744731903 at epoch: 23 and batch_num: 438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3381045460700989 at epoch: 23 and batch_num: 439\n",
      "Loss of train set: 0.25594455003738403 at epoch: 23 and batch_num: 440\n",
      "Loss of train set: 0.4250129461288452 at epoch: 23 and batch_num: 441\n",
      "Loss of train set: 0.15955865383148193 at epoch: 23 and batch_num: 442\n",
      "Loss of train set: 0.2498956024646759 at epoch: 23 and batch_num: 443\n",
      "Loss of train set: 0.13932503759860992 at epoch: 23 and batch_num: 444\n",
      "Loss of train set: 0.24964143335819244 at epoch: 23 and batch_num: 445\n",
      "Loss of train set: 0.4045712947845459 at epoch: 23 and batch_num: 446\n",
      "Loss of train set: 0.2574308514595032 at epoch: 23 and batch_num: 447\n",
      "Loss of train set: 0.3757103979587555 at epoch: 23 and batch_num: 448\n",
      "Loss of train set: 0.34543776512145996 at epoch: 23 and batch_num: 449\n",
      "Loss of train set: 0.21722546219825745 at epoch: 23 and batch_num: 450\n",
      "Loss of train set: 0.29343360662460327 at epoch: 23 and batch_num: 451\n",
      "Loss of train set: 0.26956087350845337 at epoch: 23 and batch_num: 452\n",
      "Loss of train set: 0.22800123691558838 at epoch: 23 and batch_num: 453\n",
      "Loss of train set: 0.2458062767982483 at epoch: 23 and batch_num: 454\n",
      "Loss of train set: 0.2937920391559601 at epoch: 23 and batch_num: 455\n",
      "Loss of train set: 0.2667618989944458 at epoch: 23 and batch_num: 456\n",
      "Loss of train set: 0.36699366569519043 at epoch: 23 and batch_num: 457\n",
      "Loss of train set: 0.34830260276794434 at epoch: 23 and batch_num: 458\n",
      "Loss of train set: 0.20899155735969543 at epoch: 23 and batch_num: 459\n",
      "Loss of train set: 0.24960806965827942 at epoch: 23 and batch_num: 460\n",
      "Loss of train set: 0.21807861328125 at epoch: 23 and batch_num: 461\n",
      "Loss of train set: 0.27937793731689453 at epoch: 23 and batch_num: 462\n",
      "Loss of train set: 0.15336616337299347 at epoch: 23 and batch_num: 463\n",
      "Loss of train set: 0.3375898003578186 at epoch: 23 and batch_num: 464\n",
      "Loss of train set: 0.2912539541721344 at epoch: 23 and batch_num: 465\n",
      "Loss of train set: 0.32590875029563904 at epoch: 23 and batch_num: 466\n",
      "Loss of train set: 0.5225251913070679 at epoch: 23 and batch_num: 467\n",
      "Loss of train set: 0.20840051770210266 at epoch: 23 and batch_num: 468\n",
      "Loss of train set: 0.31686463952064514 at epoch: 23 and batch_num: 469\n",
      "Loss of train set: 0.18194396793842316 at epoch: 23 and batch_num: 470\n",
      "Loss of train set: 0.1503186821937561 at epoch: 23 and batch_num: 471\n",
      "Loss of train set: 0.37444448471069336 at epoch: 23 and batch_num: 472\n",
      "Loss of train set: 0.18920698761940002 at epoch: 23 and batch_num: 473\n",
      "Loss of train set: 0.30735933780670166 at epoch: 23 and batch_num: 474\n",
      "Loss of train set: 0.34237319231033325 at epoch: 23 and batch_num: 475\n",
      "Loss of train set: 0.41556891798973083 at epoch: 23 and batch_num: 476\n",
      "Loss of train set: 0.26682016253471375 at epoch: 23 and batch_num: 477\n",
      "Loss of train set: 0.21590867638587952 at epoch: 23 and batch_num: 478\n",
      "Loss of train set: 0.29268354177474976 at epoch: 23 and batch_num: 479\n",
      "Loss of train set: 0.19994959235191345 at epoch: 23 and batch_num: 480\n",
      "Loss of train set: 0.32627353072166443 at epoch: 23 and batch_num: 481\n",
      "Loss of train set: 0.32196617126464844 at epoch: 23 and batch_num: 482\n",
      "Loss of train set: 0.45093634724617004 at epoch: 23 and batch_num: 483\n",
      "Loss of train set: 0.188128262758255 at epoch: 23 and batch_num: 484\n",
      "Loss of train set: 0.2584100365638733 at epoch: 23 and batch_num: 485\n",
      "Loss of train set: 0.33457839488983154 at epoch: 23 and batch_num: 486\n",
      "Loss of train set: 0.33352333307266235 at epoch: 23 and batch_num: 487\n",
      "Loss of train set: 0.16247442364692688 at epoch: 23 and batch_num: 488\n",
      "Loss of train set: 0.3332774043083191 at epoch: 23 and batch_num: 489\n",
      "Loss of train set: 0.3712422251701355 at epoch: 23 and batch_num: 490\n",
      "Loss of train set: 0.306674063205719 at epoch: 23 and batch_num: 491\n",
      "Loss of train set: 0.23560622334480286 at epoch: 23 and batch_num: 492\n",
      "Loss of train set: 0.3921954929828644 at epoch: 23 and batch_num: 493\n",
      "Loss of train set: 0.24525654315948486 at epoch: 23 and batch_num: 494\n",
      "Loss of train set: 0.2834917902946472 at epoch: 23 and batch_num: 495\n",
      "Loss of train set: 0.28312069177627563 at epoch: 23 and batch_num: 496\n",
      "Loss of train set: 0.2564195394515991 at epoch: 23 and batch_num: 497\n",
      "Loss of train set: 0.4324696660041809 at epoch: 23 and batch_num: 498\n",
      "Loss of train set: 0.4045718312263489 at epoch: 23 and batch_num: 499\n",
      "Loss of train set: 0.31190258264541626 at epoch: 23 and batch_num: 500\n",
      "Loss of train set: 0.15444110333919525 at epoch: 23 and batch_num: 501\n",
      "Loss of train set: 0.14552418887615204 at epoch: 23 and batch_num: 502\n",
      "Loss of train set: 0.18973587453365326 at epoch: 23 and batch_num: 503\n",
      "Loss of train set: 0.20578092336654663 at epoch: 23 and batch_num: 504\n",
      "Loss of train set: 0.39946287870407104 at epoch: 23 and batch_num: 505\n",
      "Loss of train set: 0.16106179356575012 at epoch: 23 and batch_num: 506\n",
      "Loss of train set: 0.20016145706176758 at epoch: 23 and batch_num: 507\n",
      "Loss of train set: 0.32818740606307983 at epoch: 23 and batch_num: 508\n",
      "Loss of train set: 0.1780523657798767 at epoch: 23 and batch_num: 509\n",
      "Loss of train set: 0.483287513256073 at epoch: 23 and batch_num: 510\n",
      "Loss of train set: 0.29048043489456177 at epoch: 23 and batch_num: 511\n",
      "Loss of train set: 0.3624701499938965 at epoch: 23 and batch_num: 512\n",
      "Loss of train set: 0.29867252707481384 at epoch: 23 and batch_num: 513\n",
      "Loss of train set: 0.25248920917510986 at epoch: 23 and batch_num: 514\n",
      "Loss of train set: 0.2986536920070648 at epoch: 23 and batch_num: 515\n",
      "Loss of train set: 0.3765682578086853 at epoch: 23 and batch_num: 516\n",
      "Loss of train set: 0.21131430566310883 at epoch: 23 and batch_num: 517\n",
      "Loss of train set: 0.3930090665817261 at epoch: 23 and batch_num: 518\n",
      "Loss of train set: 0.2883704900741577 at epoch: 23 and batch_num: 519\n",
      "Loss of train set: 0.35176408290863037 at epoch: 23 and batch_num: 520\n",
      "Loss of train set: 0.27904829382896423 at epoch: 23 and batch_num: 521\n",
      "Loss of train set: 0.34344929456710815 at epoch: 23 and batch_num: 522\n",
      "Loss of train set: 0.4258235991001129 at epoch: 23 and batch_num: 523\n",
      "Loss of train set: 0.3794689178466797 at epoch: 23 and batch_num: 524\n",
      "Loss of train set: 0.32170015573501587 at epoch: 23 and batch_num: 525\n",
      "Loss of train set: 0.2025061845779419 at epoch: 23 and batch_num: 526\n",
      "Loss of train set: 0.2725805640220642 at epoch: 23 and batch_num: 527\n",
      "Loss of train set: 0.2886236310005188 at epoch: 23 and batch_num: 528\n",
      "Loss of train set: 0.3747575879096985 at epoch: 23 and batch_num: 529\n",
      "Loss of train set: 0.2789734899997711 at epoch: 23 and batch_num: 530\n",
      "Loss of train set: 0.36899763345718384 at epoch: 23 and batch_num: 531\n",
      "Loss of train set: 0.47678208351135254 at epoch: 23 and batch_num: 532\n",
      "Loss of train set: 0.203121617436409 at epoch: 23 and batch_num: 533\n",
      "Loss of train set: 0.41329678893089294 at epoch: 23 and batch_num: 534\n",
      "Loss of train set: 0.3930824100971222 at epoch: 23 and batch_num: 535\n",
      "Loss of train set: 0.3115299642086029 at epoch: 23 and batch_num: 536\n",
      "Loss of train set: 0.26948148012161255 at epoch: 23 and batch_num: 537\n",
      "Loss of train set: 0.3367411196231842 at epoch: 23 and batch_num: 538\n",
      "Loss of train set: 0.247277170419693 at epoch: 23 and batch_num: 539\n",
      "Loss of train set: 0.40368568897247314 at epoch: 23 and batch_num: 540\n",
      "Loss of train set: 0.2488718330860138 at epoch: 23 and batch_num: 541\n",
      "Loss of train set: 0.23625177145004272 at epoch: 23 and batch_num: 542\n",
      "Loss of train set: 0.21163907647132874 at epoch: 23 and batch_num: 543\n",
      "Loss of train set: 0.2682141065597534 at epoch: 23 and batch_num: 544\n",
      "Loss of train set: 0.259502649307251 at epoch: 23 and batch_num: 545\n",
      "Loss of train set: 0.3336559534072876 at epoch: 23 and batch_num: 546\n",
      "Loss of train set: 0.30516761541366577 at epoch: 23 and batch_num: 547\n",
      "Loss of train set: 0.44188252091407776 at epoch: 23 and batch_num: 548\n",
      "Loss of train set: 0.14916399121284485 at epoch: 23 and batch_num: 549\n",
      "Loss of train set: 0.2945975661277771 at epoch: 23 and batch_num: 550\n",
      "Loss of train set: 0.18838591873645782 at epoch: 23 and batch_num: 551\n",
      "Loss of train set: 0.15675458312034607 at epoch: 23 and batch_num: 552\n",
      "Loss of train set: 0.16358132660388947 at epoch: 23 and batch_num: 553\n",
      "Loss of train set: 0.2711382210254669 at epoch: 23 and batch_num: 554\n",
      "Loss of train set: 0.1994694471359253 at epoch: 23 and batch_num: 555\n",
      "Loss of train set: 0.22948795557022095 at epoch: 23 and batch_num: 556\n",
      "Loss of train set: 0.40407025814056396 at epoch: 23 and batch_num: 557\n",
      "Loss of train set: 0.32574325799942017 at epoch: 23 and batch_num: 558\n",
      "Loss of train set: 0.23516735434532166 at epoch: 23 and batch_num: 559\n",
      "Loss of train set: 0.5210322141647339 at epoch: 23 and batch_num: 560\n",
      "Loss of train set: 0.31848177313804626 at epoch: 23 and batch_num: 561\n",
      "Loss of train set: 0.34372949600219727 at epoch: 23 and batch_num: 562\n",
      "Loss of train set: 0.2058820128440857 at epoch: 23 and batch_num: 563\n",
      "Loss of train set: 0.48392874002456665 at epoch: 23 and batch_num: 564\n",
      "Loss of train set: 0.2626507878303528 at epoch: 23 and batch_num: 565\n",
      "Loss of train set: 0.33942461013793945 at epoch: 23 and batch_num: 566\n",
      "Loss of train set: 0.26978063583374023 at epoch: 23 and batch_num: 567\n",
      "Loss of train set: 0.19060295820236206 at epoch: 23 and batch_num: 568\n",
      "Loss of train set: 0.2542237341403961 at epoch: 23 and batch_num: 569\n",
      "Loss of train set: 0.15931905806064606 at epoch: 23 and batch_num: 570\n",
      "Loss of train set: 0.3466184437274933 at epoch: 23 and batch_num: 571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.1639661192893982 at epoch: 23 and batch_num: 572\n",
      "Loss of train set: 0.2249910682439804 at epoch: 23 and batch_num: 573\n",
      "Loss of train set: 0.18380960822105408 at epoch: 23 and batch_num: 574\n",
      "Loss of train set: 0.322476327419281 at epoch: 23 and batch_num: 575\n",
      "Loss of train set: 0.17010173201560974 at epoch: 23 and batch_num: 576\n",
      "Loss of train set: 0.3173865079879761 at epoch: 23 and batch_num: 577\n",
      "Loss of train set: 0.3384108543395996 at epoch: 23 and batch_num: 578\n",
      "Loss of train set: 0.1659657061100006 at epoch: 23 and batch_num: 579\n",
      "Loss of train set: 0.23893846571445465 at epoch: 23 and batch_num: 580\n",
      "Loss of train set: 0.26270461082458496 at epoch: 23 and batch_num: 581\n",
      "Loss of train set: 0.13487987220287323 at epoch: 23 and batch_num: 582\n",
      "Loss of train set: 0.20992836356163025 at epoch: 23 and batch_num: 583\n",
      "Loss of train set: 0.14976827800273895 at epoch: 23 and batch_num: 584\n",
      "Loss of train set: 0.17338719964027405 at epoch: 23 and batch_num: 585\n",
      "Loss of train set: 0.23225060105323792 at epoch: 23 and batch_num: 586\n",
      "Loss of train set: 0.32213276624679565 at epoch: 23 and batch_num: 587\n",
      "Loss of train set: 0.3444879651069641 at epoch: 23 and batch_num: 588\n",
      "Loss of train set: 0.3405688405036926 at epoch: 23 and batch_num: 589\n",
      "Loss of train set: 0.24513143301010132 at epoch: 23 and batch_num: 590\n",
      "Loss of train set: 0.5155991911888123 at epoch: 23 and batch_num: 591\n",
      "Loss of train set: 0.31552553176879883 at epoch: 23 and batch_num: 592\n",
      "Loss of train set: 0.28469324111938477 at epoch: 23 and batch_num: 593\n",
      "Loss of train set: 0.18625979125499725 at epoch: 23 and batch_num: 594\n",
      "Loss of train set: 0.32318827509880066 at epoch: 23 and batch_num: 595\n",
      "Loss of train set: 0.2125914990901947 at epoch: 23 and batch_num: 596\n",
      "Loss of train set: 0.21822616457939148 at epoch: 23 and batch_num: 597\n",
      "Loss of train set: 0.19453540444374084 at epoch: 23 and batch_num: 598\n",
      "Loss of train set: 0.25223177671432495 at epoch: 23 and batch_num: 599\n",
      "Loss of train set: 0.27637407183647156 at epoch: 23 and batch_num: 600\n",
      "Loss of train set: 0.20870061218738556 at epoch: 23 and batch_num: 601\n",
      "Loss of train set: 0.23927465081214905 at epoch: 23 and batch_num: 602\n",
      "Loss of train set: 0.24638377130031586 at epoch: 23 and batch_num: 603\n",
      "Loss of train set: 0.24370498955249786 at epoch: 23 and batch_num: 604\n",
      "Loss of train set: 0.26699090003967285 at epoch: 23 and batch_num: 605\n",
      "Loss of train set: 0.10366781055927277 at epoch: 23 and batch_num: 606\n",
      "Loss of train set: 0.37549540400505066 at epoch: 23 and batch_num: 607\n",
      "Loss of train set: 0.301644891500473 at epoch: 23 and batch_num: 608\n",
      "Loss of train set: 0.23649218678474426 at epoch: 23 and batch_num: 609\n",
      "Loss of train set: 0.27670055627822876 at epoch: 23 and batch_num: 610\n",
      "Loss of train set: 0.21113291382789612 at epoch: 23 and batch_num: 611\n",
      "Loss of train set: 0.20921730995178223 at epoch: 23 and batch_num: 612\n",
      "Loss of train set: 0.13162365555763245 at epoch: 23 and batch_num: 613\n",
      "Loss of train set: 0.2736722528934479 at epoch: 23 and batch_num: 614\n",
      "Loss of train set: 0.2329772710800171 at epoch: 23 and batch_num: 615\n",
      "Loss of train set: 0.28732287883758545 at epoch: 23 and batch_num: 616\n",
      "Loss of train set: 0.3105696737766266 at epoch: 23 and batch_num: 617\n",
      "Loss of train set: 0.39253175258636475 at epoch: 23 and batch_num: 618\n",
      "Loss of train set: 0.2237592339515686 at epoch: 23 and batch_num: 619\n",
      "Loss of train set: 0.2939801514148712 at epoch: 23 and batch_num: 620\n",
      "Loss of train set: 0.2794837951660156 at epoch: 23 and batch_num: 621\n",
      "Loss of train set: 0.273123562335968 at epoch: 23 and batch_num: 622\n",
      "Loss of train set: 0.25539326667785645 at epoch: 23 and batch_num: 623\n",
      "Loss of train set: 0.12100761383771896 at epoch: 23 and batch_num: 624\n",
      "Loss of train set: 0.4040457010269165 at epoch: 23 and batch_num: 625\n",
      "Loss of train set: 0.2113000750541687 at epoch: 23 and batch_num: 626\n",
      "Loss of train set: 0.2056054025888443 at epoch: 23 and batch_num: 627\n",
      "Loss of train set: 0.2053641378879547 at epoch: 23 and batch_num: 628\n",
      "Loss of train set: 0.24846793711185455 at epoch: 23 and batch_num: 629\n",
      "Loss of train set: 0.16324006021022797 at epoch: 23 and batch_num: 630\n",
      "Loss of train set: 0.21962834894657135 at epoch: 23 and batch_num: 631\n",
      "Loss of train set: 0.20201721787452698 at epoch: 23 and batch_num: 632\n",
      "Loss of train set: 0.37977495789527893 at epoch: 23 and batch_num: 633\n",
      "Loss of train set: 0.31663140654563904 at epoch: 23 and batch_num: 634\n",
      "Loss of train set: 0.2997809052467346 at epoch: 23 and batch_num: 635\n",
      "Loss of train set: 0.21765953302383423 at epoch: 23 and batch_num: 636\n",
      "Loss of train set: 0.25983288884162903 at epoch: 23 and batch_num: 637\n",
      "Loss of train set: 0.27681827545166016 at epoch: 23 and batch_num: 638\n",
      "Loss of train set: 0.2769734561443329 at epoch: 23 and batch_num: 639\n",
      "Loss of train set: 0.21437451243400574 at epoch: 23 and batch_num: 640\n",
      "Loss of train set: 0.38675037026405334 at epoch: 23 and batch_num: 641\n",
      "Loss of train set: 0.4403340816497803 at epoch: 23 and batch_num: 642\n",
      "Loss of train set: 0.14423251152038574 at epoch: 23 and batch_num: 643\n",
      "Loss of train set: 0.2599855363368988 at epoch: 23 and batch_num: 644\n",
      "Loss of train set: 0.2470584660768509 at epoch: 23 and batch_num: 645\n",
      "Loss of train set: 0.17209169268608093 at epoch: 23 and batch_num: 646\n",
      "Loss of train set: 0.2876710295677185 at epoch: 23 and batch_num: 647\n",
      "Loss of train set: 0.18272793292999268 at epoch: 23 and batch_num: 648\n",
      "Loss of train set: 0.40636587142944336 at epoch: 23 and batch_num: 649\n",
      "Loss of train set: 0.22430235147476196 at epoch: 23 and batch_num: 650\n",
      "Loss of train set: 0.3456839919090271 at epoch: 23 and batch_num: 651\n",
      "Loss of train set: 0.23092985153198242 at epoch: 23 and batch_num: 652\n",
      "Loss of train set: 0.1621522307395935 at epoch: 23 and batch_num: 653\n",
      "Loss of train set: 0.2536398470401764 at epoch: 23 and batch_num: 654\n",
      "Loss of train set: 0.463792622089386 at epoch: 23 and batch_num: 655\n",
      "Loss of train set: 0.21566051244735718 at epoch: 23 and batch_num: 656\n",
      "Loss of train set: 0.29818758368492126 at epoch: 23 and batch_num: 657\n",
      "Loss of train set: 0.1950196623802185 at epoch: 23 and batch_num: 658\n",
      "Loss of train set: 0.5377613306045532 at epoch: 23 and batch_num: 659\n",
      "Loss of train set: 0.16576248407363892 at epoch: 23 and batch_num: 660\n",
      "Loss of train set: 0.22311276197433472 at epoch: 23 and batch_num: 661\n",
      "Loss of train set: 0.2158801555633545 at epoch: 23 and batch_num: 662\n",
      "Loss of train set: 0.197389617562294 at epoch: 23 and batch_num: 663\n",
      "Loss of train set: 0.2678914964199066 at epoch: 23 and batch_num: 664\n",
      "Loss of train set: 0.38638269901275635 at epoch: 23 and batch_num: 665\n",
      "Loss of train set: 0.2142038345336914 at epoch: 23 and batch_num: 666\n",
      "Loss of train set: 0.24364431202411652 at epoch: 23 and batch_num: 667\n",
      "Loss of train set: 0.4765653610229492 at epoch: 23 and batch_num: 668\n",
      "Loss of train set: 0.16845399141311646 at epoch: 23 and batch_num: 669\n",
      "Loss of train set: 0.28718167543411255 at epoch: 23 and batch_num: 670\n",
      "Loss of train set: 0.26023149490356445 at epoch: 23 and batch_num: 671\n",
      "Loss of train set: 0.42289304733276367 at epoch: 23 and batch_num: 672\n",
      "Loss of train set: 0.135800302028656 at epoch: 23 and batch_num: 673\n",
      "Loss of train set: 0.23612284660339355 at epoch: 23 and batch_num: 674\n",
      "Loss of train set: 0.39196744561195374 at epoch: 23 and batch_num: 675\n",
      "Loss of train set: 0.1564168930053711 at epoch: 23 and batch_num: 676\n",
      "Loss of train set: 0.3133251368999481 at epoch: 23 and batch_num: 677\n",
      "Loss of train set: 0.17548510432243347 at epoch: 23 and batch_num: 678\n",
      "Loss of train set: 0.23355817794799805 at epoch: 23 and batch_num: 679\n",
      "Loss of train set: 0.3821265995502472 at epoch: 23 and batch_num: 680\n",
      "Loss of train set: 0.32675230503082275 at epoch: 23 and batch_num: 681\n",
      "Loss of train set: 0.31426069140434265 at epoch: 23 and batch_num: 682\n",
      "Loss of train set: 0.2901752293109894 at epoch: 23 and batch_num: 683\n",
      "Loss of train set: 0.2662404775619507 at epoch: 23 and batch_num: 684\n",
      "Loss of train set: 0.28409290313720703 at epoch: 23 and batch_num: 685\n",
      "Loss of train set: 0.17115508019924164 at epoch: 23 and batch_num: 686\n",
      "Loss of train set: 0.32910096645355225 at epoch: 23 and batch_num: 687\n",
      "Loss of train set: 0.16431143879890442 at epoch: 23 and batch_num: 688\n",
      "Loss of train set: 0.2710575461387634 at epoch: 23 and batch_num: 689\n",
      "Loss of train set: 0.37046098709106445 at epoch: 23 and batch_num: 690\n",
      "Loss of train set: 0.2609579563140869 at epoch: 23 and batch_num: 691\n",
      "Loss of train set: 0.42503923177719116 at epoch: 23 and batch_num: 692\n",
      "Loss of train set: 0.2963446378707886 at epoch: 23 and batch_num: 693\n",
      "Loss of train set: 0.3428399860858917 at epoch: 23 and batch_num: 694\n",
      "Loss of train set: 0.2355770766735077 at epoch: 23 and batch_num: 695\n",
      "Loss of train set: 0.1983884871006012 at epoch: 23 and batch_num: 696\n",
      "Loss of train set: 0.20181208848953247 at epoch: 23 and batch_num: 697\n",
      "Loss of train set: 0.24423596262931824 at epoch: 23 and batch_num: 698\n",
      "Loss of train set: 0.362938791513443 at epoch: 23 and batch_num: 699\n",
      "Loss of train set: 0.3941487669944763 at epoch: 23 and batch_num: 700\n",
      "Loss of train set: 0.4130527377128601 at epoch: 23 and batch_num: 701\n",
      "Loss of train set: 0.4461486041545868 at epoch: 23 and batch_num: 702\n",
      "Loss of train set: 0.2703523635864258 at epoch: 23 and batch_num: 703\n",
      "Loss of train set: 0.18331663310527802 at epoch: 23 and batch_num: 704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.39573991298675537 at epoch: 23 and batch_num: 705\n",
      "Loss of train set: 0.2961011230945587 at epoch: 23 and batch_num: 706\n",
      "Loss of train set: 0.23835670948028564 at epoch: 23 and batch_num: 707\n",
      "Loss of train set: 0.2074250876903534 at epoch: 23 and batch_num: 708\n",
      "Loss of train set: 0.14316025376319885 at epoch: 23 and batch_num: 709\n",
      "Loss of train set: 0.23058930039405823 at epoch: 23 and batch_num: 710\n",
      "Loss of train set: 0.19228008389472961 at epoch: 23 and batch_num: 711\n",
      "Loss of train set: 0.21937626600265503 at epoch: 23 and batch_num: 712\n",
      "Loss of train set: 0.24255797266960144 at epoch: 23 and batch_num: 713\n",
      "Loss of train set: 0.3692758083343506 at epoch: 23 and batch_num: 714\n",
      "Loss of train set: 0.22621171176433563 at epoch: 23 and batch_num: 715\n",
      "Loss of train set: 0.3119407892227173 at epoch: 23 and batch_num: 716\n",
      "Loss of train set: 0.36815783381462097 at epoch: 23 and batch_num: 717\n",
      "Loss of train set: 0.14053554832935333 at epoch: 23 and batch_num: 718\n",
      "Loss of train set: 0.1832544505596161 at epoch: 23 and batch_num: 719\n",
      "Loss of train set: 0.2149772197008133 at epoch: 23 and batch_num: 720\n",
      "Loss of train set: 0.13099268078804016 at epoch: 23 and batch_num: 721\n",
      "Loss of train set: 0.19656714797019958 at epoch: 23 and batch_num: 722\n",
      "Loss of train set: 0.24541112780570984 at epoch: 23 and batch_num: 723\n",
      "Loss of train set: 0.24354776740074158 at epoch: 23 and batch_num: 724\n",
      "Loss of train set: 0.2250959277153015 at epoch: 23 and batch_num: 725\n",
      "Loss of train set: 0.19259805977344513 at epoch: 23 and batch_num: 726\n",
      "Loss of train set: 0.31289422512054443 at epoch: 23 and batch_num: 727\n",
      "Loss of train set: 0.44629883766174316 at epoch: 23 and batch_num: 728\n",
      "Loss of train set: 0.24699819087982178 at epoch: 23 and batch_num: 729\n",
      "Loss of train set: 0.18264850974082947 at epoch: 23 and batch_num: 730\n",
      "Loss of train set: 0.30692997574806213 at epoch: 23 and batch_num: 731\n",
      "Loss of train set: 0.3856615424156189 at epoch: 23 and batch_num: 732\n",
      "Loss of train set: 0.4312272369861603 at epoch: 23 and batch_num: 733\n",
      "Loss of train set: 0.36380672454833984 at epoch: 23 and batch_num: 734\n",
      "Loss of train set: 0.39011406898498535 at epoch: 23 and batch_num: 735\n",
      "Loss of train set: 0.35913169384002686 at epoch: 23 and batch_num: 736\n",
      "Loss of train set: 0.2397862821817398 at epoch: 23 and batch_num: 737\n",
      "Loss of train set: 0.27042073011398315 at epoch: 23 and batch_num: 738\n",
      "Loss of train set: 0.15379655361175537 at epoch: 23 and batch_num: 739\n",
      "Loss of train set: 0.2578834891319275 at epoch: 23 and batch_num: 740\n",
      "Loss of train set: 0.23551446199417114 at epoch: 23 and batch_num: 741\n",
      "Loss of train set: 0.23804008960723877 at epoch: 23 and batch_num: 742\n",
      "Loss of train set: 0.5239669680595398 at epoch: 23 and batch_num: 743\n",
      "Loss of train set: 0.25637906789779663 at epoch: 23 and batch_num: 744\n",
      "Loss of train set: 0.23781363666057587 at epoch: 23 and batch_num: 745\n",
      "Loss of train set: 0.35193702578544617 at epoch: 23 and batch_num: 746\n",
      "Loss of train set: 0.1905955672264099 at epoch: 23 and batch_num: 747\n",
      "Loss of train set: 0.363148033618927 at epoch: 23 and batch_num: 748\n",
      "Loss of train set: 0.2285289764404297 at epoch: 23 and batch_num: 749\n",
      "Loss of train set: 0.25358837842941284 at epoch: 23 and batch_num: 750\n",
      "Loss of train set: 0.26052209734916687 at epoch: 23 and batch_num: 751\n",
      "Loss of train set: 0.1924571692943573 at epoch: 23 and batch_num: 752\n",
      "Loss of train set: 0.1742824912071228 at epoch: 23 and batch_num: 753\n",
      "Loss of train set: 0.39413154125213623 at epoch: 23 and batch_num: 754\n",
      "Loss of train set: 0.23457936942577362 at epoch: 23 and batch_num: 755\n",
      "Loss of train set: 0.1934703141450882 at epoch: 23 and batch_num: 756\n",
      "Loss of train set: 0.3272221088409424 at epoch: 23 and batch_num: 757\n",
      "Loss of train set: 0.22692623734474182 at epoch: 23 and batch_num: 758\n",
      "Loss of train set: 0.1788148581981659 at epoch: 23 and batch_num: 759\n",
      "Loss of train set: 0.2225886732339859 at epoch: 23 and batch_num: 760\n",
      "Loss of train set: 0.36834120750427246 at epoch: 23 and batch_num: 761\n",
      "Loss of train set: 0.23278221487998962 at epoch: 23 and batch_num: 762\n",
      "Loss of train set: 0.3913098871707916 at epoch: 23 and batch_num: 763\n",
      "Loss of train set: 0.37931203842163086 at epoch: 23 and batch_num: 764\n",
      "Loss of train set: 0.28408297896385193 at epoch: 23 and batch_num: 765\n",
      "Loss of train set: 0.18770256638526917 at epoch: 23 and batch_num: 766\n",
      "Loss of train set: 0.23560583591461182 at epoch: 23 and batch_num: 767\n",
      "Loss of train set: 0.2225484400987625 at epoch: 23 and batch_num: 768\n",
      "Loss of train set: 0.29230448603630066 at epoch: 23 and batch_num: 769\n",
      "Loss of train set: 0.17828640341758728 at epoch: 23 and batch_num: 770\n",
      "Loss of train set: 0.22570952773094177 at epoch: 23 and batch_num: 771\n",
      "Loss of train set: 0.16857635974884033 at epoch: 23 and batch_num: 772\n",
      "Loss of train set: 0.24472416937351227 at epoch: 23 and batch_num: 773\n",
      "Loss of train set: 0.3180582523345947 at epoch: 23 and batch_num: 774\n",
      "Loss of train set: 0.24784046411514282 at epoch: 23 and batch_num: 775\n",
      "Loss of train set: 0.20748460292816162 at epoch: 23 and batch_num: 776\n",
      "Loss of train set: 0.2782790958881378 at epoch: 23 and batch_num: 777\n",
      "Loss of train set: 0.3170573115348816 at epoch: 23 and batch_num: 778\n",
      "Loss of train set: 0.2710105776786804 at epoch: 23 and batch_num: 779\n",
      "Loss of train set: 0.256923645734787 at epoch: 23 and batch_num: 780\n",
      "Loss of train set: 0.5787780284881592 at epoch: 23 and batch_num: 781\n",
      "Loss of train set: 0.27836787700653076 at epoch: 23 and batch_num: 782\n",
      "Loss of train set: 0.39126908779144287 at epoch: 23 and batch_num: 783\n",
      "Loss of train set: 0.21103155612945557 at epoch: 23 and batch_num: 784\n",
      "Loss of train set: 0.2389025092124939 at epoch: 23 and batch_num: 785\n",
      "Loss of train set: 0.1939048320055008 at epoch: 23 and batch_num: 786\n",
      "Loss of train set: 0.1414160132408142 at epoch: 23 and batch_num: 787\n",
      "Loss of train set: 0.4002315402030945 at epoch: 23 and batch_num: 788\n",
      "Loss of train set: 0.29197412729263306 at epoch: 23 and batch_num: 789\n",
      "Loss of train set: 0.29207560420036316 at epoch: 23 and batch_num: 790\n",
      "Loss of train set: 0.2998724579811096 at epoch: 23 and batch_num: 791\n",
      "Loss of train set: 0.35158929228782654 at epoch: 23 and batch_num: 792\n",
      "Loss of train set: 0.34030085802078247 at epoch: 23 and batch_num: 793\n",
      "Loss of train set: 0.202126145362854 at epoch: 23 and batch_num: 794\n",
      "Loss of train set: 0.19376835227012634 at epoch: 23 and batch_num: 795\n",
      "Loss of train set: 0.27240797877311707 at epoch: 23 and batch_num: 796\n",
      "Loss of train set: 0.21577969193458557 at epoch: 23 and batch_num: 797\n",
      "Loss of train set: 0.3119960427284241 at epoch: 23 and batch_num: 798\n",
      "Loss of train set: 0.2821650803089142 at epoch: 23 and batch_num: 799\n",
      "Loss of train set: 0.35616984963417053 at epoch: 23 and batch_num: 800\n",
      "Loss of train set: 0.29287365078926086 at epoch: 23 and batch_num: 801\n",
      "Loss of train set: 0.1565113067626953 at epoch: 23 and batch_num: 802\n",
      "Loss of train set: 0.11377347260713577 at epoch: 23 and batch_num: 803\n",
      "Loss of train set: 0.14296141266822815 at epoch: 23 and batch_num: 804\n",
      "Loss of train set: 0.3507915735244751 at epoch: 23 and batch_num: 805\n",
      "Loss of train set: 0.27719545364379883 at epoch: 23 and batch_num: 806\n",
      "Loss of train set: 0.43722057342529297 at epoch: 23 and batch_num: 807\n",
      "Loss of train set: 0.21088410913944244 at epoch: 23 and batch_num: 808\n",
      "Loss of train set: 0.2544921636581421 at epoch: 23 and batch_num: 809\n",
      "Loss of train set: 0.11260028928518295 at epoch: 23 and batch_num: 810\n",
      "Loss of train set: 0.19608668982982635 at epoch: 23 and batch_num: 811\n",
      "Loss of train set: 0.28037914633750916 at epoch: 23 and batch_num: 812\n",
      "Loss of train set: 0.2650717794895172 at epoch: 23 and batch_num: 813\n",
      "Loss of train set: 0.22975830733776093 at epoch: 23 and batch_num: 814\n",
      "Loss of train set: 0.19978633522987366 at epoch: 23 and batch_num: 815\n",
      "Loss of train set: 0.1720888316631317 at epoch: 23 and batch_num: 816\n",
      "Loss of train set: 0.36601975560188293 at epoch: 23 and batch_num: 817\n",
      "Loss of train set: 0.2128865271806717 at epoch: 23 and batch_num: 818\n",
      "Loss of train set: 0.3398134708404541 at epoch: 23 and batch_num: 819\n",
      "Loss of train set: 0.21773222088813782 at epoch: 23 and batch_num: 820\n",
      "Loss of train set: 0.2561055123806 at epoch: 23 and batch_num: 821\n",
      "Loss of train set: 0.252797394990921 at epoch: 23 and batch_num: 822\n",
      "Loss of train set: 0.1455610692501068 at epoch: 23 and batch_num: 823\n",
      "Loss of train set: 0.18918602168560028 at epoch: 23 and batch_num: 824\n",
      "Loss of train set: 0.27379119396209717 at epoch: 23 and batch_num: 825\n",
      "Loss of train set: 0.26198744773864746 at epoch: 23 and batch_num: 826\n",
      "Loss of train set: 0.3004658818244934 at epoch: 23 and batch_num: 827\n",
      "Loss of train set: 0.4926493167877197 at epoch: 23 and batch_num: 828\n",
      "Loss of train set: 0.31042277812957764 at epoch: 23 and batch_num: 829\n",
      "Loss of train set: 0.2679007649421692 at epoch: 23 and batch_num: 830\n",
      "Loss of train set: 0.22252187132835388 at epoch: 23 and batch_num: 831\n",
      "Loss of train set: 0.18813146650791168 at epoch: 23 and batch_num: 832\n",
      "Loss of train set: 0.1808261275291443 at epoch: 23 and batch_num: 833\n",
      "Loss of train set: 0.291248083114624 at epoch: 23 and batch_num: 834\n",
      "Loss of train set: 0.22126725316047668 at epoch: 23 and batch_num: 835\n",
      "Loss of train set: 0.3262023329734802 at epoch: 23 and batch_num: 836\n",
      "Loss of train set: 0.28315508365631104 at epoch: 23 and batch_num: 837\n",
      "Loss of train set: 0.17167842388153076 at epoch: 23 and batch_num: 838\n",
      "Loss of train set: 0.3625096380710602 at epoch: 23 and batch_num: 839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.46842119097709656 at epoch: 23 and batch_num: 840\n",
      "Loss of train set: 0.23130179941654205 at epoch: 23 and batch_num: 841\n",
      "Loss of train set: 0.19930186867713928 at epoch: 23 and batch_num: 842\n",
      "Loss of train set: 0.3521192669868469 at epoch: 23 and batch_num: 843\n",
      "Loss of train set: 0.30122101306915283 at epoch: 23 and batch_num: 844\n",
      "Loss of train set: 0.29165855050086975 at epoch: 23 and batch_num: 845\n",
      "Loss of train set: 0.23022189736366272 at epoch: 23 and batch_num: 846\n",
      "Loss of train set: 0.2838118076324463 at epoch: 23 and batch_num: 847\n",
      "Loss of train set: 0.3379855155944824 at epoch: 23 and batch_num: 848\n",
      "Loss of train set: 0.2130553126335144 at epoch: 23 and batch_num: 849\n",
      "Loss of train set: 0.3340076804161072 at epoch: 23 and batch_num: 850\n",
      "Loss of train set: 0.27344250679016113 at epoch: 23 and batch_num: 851\n",
      "Loss of train set: 0.5018881559371948 at epoch: 23 and batch_num: 852\n",
      "Loss of train set: 0.17288000881671906 at epoch: 23 and batch_num: 853\n",
      "Loss of train set: 0.1937824785709381 at epoch: 23 and batch_num: 854\n",
      "Loss of train set: 0.3033730685710907 at epoch: 23 and batch_num: 855\n",
      "Loss of train set: 0.38867419958114624 at epoch: 23 and batch_num: 856\n",
      "Loss of train set: 0.22958335280418396 at epoch: 23 and batch_num: 857\n",
      "Loss of train set: 0.3355948328971863 at epoch: 23 and batch_num: 858\n",
      "Loss of train set: 0.30441710352897644 at epoch: 23 and batch_num: 859\n",
      "Loss of train set: 0.22518882155418396 at epoch: 23 and batch_num: 860\n",
      "Loss of train set: 0.20150403678417206 at epoch: 23 and batch_num: 861\n",
      "Loss of train set: 0.39683830738067627 at epoch: 23 and batch_num: 862\n",
      "Loss of train set: 0.5445506572723389 at epoch: 23 and batch_num: 863\n",
      "Loss of train set: 0.2658720314502716 at epoch: 23 and batch_num: 864\n",
      "Loss of train set: 0.34034109115600586 at epoch: 23 and batch_num: 865\n",
      "Loss of train set: 0.42275652289390564 at epoch: 23 and batch_num: 866\n",
      "Loss of train set: 0.2988725006580353 at epoch: 23 and batch_num: 867\n",
      "Loss of train set: 0.22704041004180908 at epoch: 23 and batch_num: 868\n",
      "Loss of train set: 0.3255815804004669 at epoch: 23 and batch_num: 869\n",
      "Loss of train set: 0.3184390068054199 at epoch: 23 and batch_num: 870\n",
      "Loss of train set: 0.1343875229358673 at epoch: 23 and batch_num: 871\n",
      "Loss of train set: 0.22177936136722565 at epoch: 23 and batch_num: 872\n",
      "Loss of train set: 0.4226202666759491 at epoch: 23 and batch_num: 873\n",
      "Loss of train set: 0.2821800410747528 at epoch: 23 and batch_num: 874\n",
      "Loss of train set: 0.19619211554527283 at epoch: 23 and batch_num: 875\n",
      "Loss of train set: 0.249961256980896 at epoch: 23 and batch_num: 876\n",
      "Loss of train set: 0.16142581403255463 at epoch: 23 and batch_num: 877\n",
      "Loss of train set: 0.24389350414276123 at epoch: 23 and batch_num: 878\n",
      "Loss of train set: 0.2942456007003784 at epoch: 23 and batch_num: 879\n",
      "Loss of train set: 0.3276812434196472 at epoch: 23 and batch_num: 880\n",
      "Loss of train set: 0.3203567862510681 at epoch: 23 and batch_num: 881\n",
      "Loss of train set: 0.3034105598926544 at epoch: 23 and batch_num: 882\n",
      "Loss of train set: 0.3671516478061676 at epoch: 23 and batch_num: 883\n",
      "Loss of train set: 0.2581299841403961 at epoch: 23 and batch_num: 884\n",
      "Loss of train set: 0.2538043260574341 at epoch: 23 and batch_num: 885\n",
      "Loss of train set: 0.2583712637424469 at epoch: 23 and batch_num: 886\n",
      "Loss of train set: 0.18838509917259216 at epoch: 23 and batch_num: 887\n",
      "Loss of train set: 0.13776224851608276 at epoch: 23 and batch_num: 888\n",
      "Loss of train set: 0.33787763118743896 at epoch: 23 and batch_num: 889\n",
      "Loss of train set: 0.24951700866222382 at epoch: 23 and batch_num: 890\n",
      "Loss of train set: 0.3514954149723053 at epoch: 23 and batch_num: 891\n",
      "Loss of train set: 0.2535325884819031 at epoch: 23 and batch_num: 892\n",
      "Loss of train set: 0.2409902662038803 at epoch: 23 and batch_num: 893\n",
      "Loss of train set: 0.1985974758863449 at epoch: 23 and batch_num: 894\n",
      "Loss of train set: 0.3565469980239868 at epoch: 23 and batch_num: 895\n",
      "Loss of train set: 0.25778523087501526 at epoch: 23 and batch_num: 896\n",
      "Loss of train set: 0.15076659619808197 at epoch: 23 and batch_num: 897\n",
      "Loss of train set: 0.34885260462760925 at epoch: 23 and batch_num: 898\n",
      "Loss of train set: 0.2582765519618988 at epoch: 23 and batch_num: 899\n",
      "Loss of train set: 0.40936556458473206 at epoch: 23 and batch_num: 900\n",
      "Loss of train set: 0.2610870897769928 at epoch: 23 and batch_num: 901\n",
      "Loss of train set: 0.19795694947242737 at epoch: 23 and batch_num: 902\n",
      "Loss of train set: 0.37026217579841614 at epoch: 23 and batch_num: 903\n",
      "Loss of train set: 0.22449448704719543 at epoch: 23 and batch_num: 904\n",
      "Loss of train set: 0.2241406887769699 at epoch: 23 and batch_num: 905\n",
      "Loss of train set: 0.3525991141796112 at epoch: 23 and batch_num: 906\n",
      "Loss of train set: 0.3687349259853363 at epoch: 23 and batch_num: 907\n",
      "Loss of train set: 0.31691816449165344 at epoch: 23 and batch_num: 908\n",
      "Loss of train set: 0.2037041038274765 at epoch: 23 and batch_num: 909\n",
      "Loss of train set: 0.40037810802459717 at epoch: 23 and batch_num: 910\n",
      "Loss of train set: 0.30804967880249023 at epoch: 23 and batch_num: 911\n",
      "Loss of train set: 0.2456635981798172 at epoch: 23 and batch_num: 912\n",
      "Loss of train set: 0.3265560269355774 at epoch: 23 and batch_num: 913\n",
      "Loss of train set: 0.26315417885780334 at epoch: 23 and batch_num: 914\n",
      "Loss of train set: 0.17798137664794922 at epoch: 23 and batch_num: 915\n",
      "Loss of train set: 0.31042569875717163 at epoch: 23 and batch_num: 916\n",
      "Loss of train set: 0.40974119305610657 at epoch: 23 and batch_num: 917\n",
      "Loss of train set: 0.21824976801872253 at epoch: 23 and batch_num: 918\n",
      "Loss of train set: 0.28567177057266235 at epoch: 23 and batch_num: 919\n",
      "Loss of train set: 0.29611364006996155 at epoch: 23 and batch_num: 920\n",
      "Loss of train set: 0.3276117742061615 at epoch: 23 and batch_num: 921\n",
      "Loss of train set: 0.30861136317253113 at epoch: 23 and batch_num: 922\n",
      "Loss of train set: 0.23793762922286987 at epoch: 23 and batch_num: 923\n",
      "Loss of train set: 0.16266095638275146 at epoch: 23 and batch_num: 924\n",
      "Loss of train set: 0.31921952962875366 at epoch: 23 and batch_num: 925\n",
      "Loss of train set: 0.24919399619102478 at epoch: 23 and batch_num: 926\n",
      "Loss of train set: 0.15714457631111145 at epoch: 23 and batch_num: 927\n",
      "Loss of train set: 0.34059110283851624 at epoch: 23 and batch_num: 928\n",
      "Loss of train set: 0.1849287748336792 at epoch: 23 and batch_num: 929\n",
      "Loss of train set: 0.37388208508491516 at epoch: 23 and batch_num: 930\n",
      "Loss of train set: 0.3182428777217865 at epoch: 23 and batch_num: 931\n",
      "Loss of train set: 0.2878420054912567 at epoch: 23 and batch_num: 932\n",
      "Loss of train set: 0.22499100863933563 at epoch: 23 and batch_num: 933\n",
      "Loss of train set: 0.25181227922439575 at epoch: 23 and batch_num: 934\n",
      "Loss of train set: 0.21545834839344025 at epoch: 23 and batch_num: 935\n",
      "Loss of train set: 0.4129595458507538 at epoch: 23 and batch_num: 936\n",
      "Loss of train set: 0.057667892426252365 at epoch: 23 and batch_num: 937\n",
      "Accuracy of train set: 0.90125\n",
      "Loss of test set: 0.33960044384002686 at epoch: 23 and batch_num: 0\n",
      "Loss of test set: 0.3702203333377838 at epoch: 23 and batch_num: 1\n",
      "Loss of test set: 0.25781527161598206 at epoch: 23 and batch_num: 2\n",
      "Loss of test set: 0.3326277732849121 at epoch: 23 and batch_num: 3\n",
      "Loss of test set: 0.40445423126220703 at epoch: 23 and batch_num: 4\n",
      "Loss of test set: 0.30030903220176697 at epoch: 23 and batch_num: 5\n",
      "Loss of test set: 0.4943685531616211 at epoch: 23 and batch_num: 6\n",
      "Loss of test set: 0.43688273429870605 at epoch: 23 and batch_num: 7\n",
      "Loss of test set: 0.35724788904190063 at epoch: 23 and batch_num: 8\n",
      "Loss of test set: 0.5659005641937256 at epoch: 23 and batch_num: 9\n",
      "Loss of test set: 0.3488374352455139 at epoch: 23 and batch_num: 10\n",
      "Loss of test set: 0.4274912178516388 at epoch: 23 and batch_num: 11\n",
      "Loss of test set: 0.1624772548675537 at epoch: 23 and batch_num: 12\n",
      "Loss of test set: 0.24747927486896515 at epoch: 23 and batch_num: 13\n",
      "Loss of test set: 0.4733158349990845 at epoch: 23 and batch_num: 14\n",
      "Loss of test set: 0.3579411804676056 at epoch: 23 and batch_num: 15\n",
      "Loss of test set: 0.30824002623558044 at epoch: 23 and batch_num: 16\n",
      "Loss of test set: 0.29723799228668213 at epoch: 23 and batch_num: 17\n",
      "Loss of test set: 0.17507511377334595 at epoch: 23 and batch_num: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.48075026273727417 at epoch: 23 and batch_num: 19\n",
      "Loss of test set: 0.2777959704399109 at epoch: 23 and batch_num: 20\n",
      "Loss of test set: 0.2987542450428009 at epoch: 23 and batch_num: 21\n",
      "Loss of test set: 0.29090940952301025 at epoch: 23 and batch_num: 22\n",
      "Loss of test set: 0.23798850178718567 at epoch: 23 and batch_num: 23\n",
      "Loss of test set: 0.27289879322052 at epoch: 23 and batch_num: 24\n",
      "Loss of test set: 0.17447049915790558 at epoch: 23 and batch_num: 25\n",
      "Loss of test set: 0.48340943455696106 at epoch: 23 and batch_num: 26\n",
      "Loss of test set: 0.4105265140533447 at epoch: 23 and batch_num: 27\n",
      "Loss of test set: 0.3615248501300812 at epoch: 23 and batch_num: 28\n",
      "Loss of test set: 0.2894350290298462 at epoch: 23 and batch_num: 29\n",
      "Loss of test set: 0.3421901762485504 at epoch: 23 and batch_num: 30\n",
      "Loss of test set: 0.49893510341644287 at epoch: 23 and batch_num: 31\n",
      "Loss of test set: 0.24822692573070526 at epoch: 23 and batch_num: 32\n",
      "Loss of test set: 0.3352369964122772 at epoch: 23 and batch_num: 33\n",
      "Loss of test set: 0.3091813623905182 at epoch: 23 and batch_num: 34\n",
      "Loss of test set: 0.3650248050689697 at epoch: 23 and batch_num: 35\n",
      "Loss of test set: 0.4515690803527832 at epoch: 23 and batch_num: 36\n",
      "Loss of test set: 0.4815438687801361 at epoch: 23 and batch_num: 37\n",
      "Loss of test set: 0.36766278743743896 at epoch: 23 and batch_num: 38\n",
      "Loss of test set: 0.37560445070266724 at epoch: 23 and batch_num: 39\n",
      "Loss of test set: 0.4457703232765198 at epoch: 23 and batch_num: 40\n",
      "Loss of test set: 0.23678822815418243 at epoch: 23 and batch_num: 41\n",
      "Loss of test set: 0.28191083669662476 at epoch: 23 and batch_num: 42\n",
      "Loss of test set: 0.33594924211502075 at epoch: 23 and batch_num: 43\n",
      "Loss of test set: 0.45923858880996704 at epoch: 23 and batch_num: 44\n",
      "Loss of test set: 0.4922676086425781 at epoch: 23 and batch_num: 45\n",
      "Loss of test set: 0.38486987352371216 at epoch: 23 and batch_num: 46\n",
      "Loss of test set: 0.34267640113830566 at epoch: 23 and batch_num: 47\n",
      "Loss of test set: 0.523681640625 at epoch: 23 and batch_num: 48\n",
      "Loss of test set: 0.38863953948020935 at epoch: 23 and batch_num: 49\n",
      "Loss of test set: 0.35374799370765686 at epoch: 23 and batch_num: 50\n",
      "Loss of test set: 0.3307141661643982 at epoch: 23 and batch_num: 51\n",
      "Loss of test set: 0.35828858613967896 at epoch: 23 and batch_num: 52\n",
      "Loss of test set: 0.41936901211738586 at epoch: 23 and batch_num: 53\n",
      "Loss of test set: 0.41146010160446167 at epoch: 23 and batch_num: 54\n",
      "Loss of test set: 0.45349350571632385 at epoch: 23 and batch_num: 55\n",
      "Loss of test set: 0.4884055256843567 at epoch: 23 and batch_num: 56\n",
      "Loss of test set: 0.45714741945266724 at epoch: 23 and batch_num: 57\n",
      "Loss of test set: 0.3057791590690613 at epoch: 23 and batch_num: 58\n",
      "Loss of test set: 0.4189579486846924 at epoch: 23 and batch_num: 59\n",
      "Loss of test set: 0.2254611700773239 at epoch: 23 and batch_num: 60\n",
      "Loss of test set: 0.27211272716522217 at epoch: 23 and batch_num: 61\n",
      "Loss of test set: 0.46949678659439087 at epoch: 23 and batch_num: 62\n",
      "Loss of test set: 0.31977418065071106 at epoch: 23 and batch_num: 63\n",
      "Loss of test set: 0.5076276659965515 at epoch: 23 and batch_num: 64\n",
      "Loss of test set: 0.3667777180671692 at epoch: 23 and batch_num: 65\n",
      "Loss of test set: 0.324728786945343 at epoch: 23 and batch_num: 66\n",
      "Loss of test set: 0.265164852142334 at epoch: 23 and batch_num: 67\n",
      "Loss of test set: 0.24642130732536316 at epoch: 23 and batch_num: 68\n",
      "Loss of test set: 0.35046660900115967 at epoch: 23 and batch_num: 69\n",
      "Loss of test set: 0.31708553433418274 at epoch: 23 and batch_num: 70\n",
      "Loss of test set: 0.49248796701431274 at epoch: 23 and batch_num: 71\n",
      "Loss of test set: 0.26682063937187195 at epoch: 23 and batch_num: 72\n",
      "Loss of test set: 0.38160762190818787 at epoch: 23 and batch_num: 73\n",
      "Loss of test set: 0.5330225229263306 at epoch: 23 and batch_num: 74\n",
      "Loss of test set: 0.2949208617210388 at epoch: 23 and batch_num: 75\n",
      "Loss of test set: 0.22302451729774475 at epoch: 23 and batch_num: 76\n",
      "Loss of test set: 0.46670103073120117 at epoch: 23 and batch_num: 77\n",
      "Loss of test set: 0.3612551689147949 at epoch: 23 and batch_num: 78\n",
      "Loss of test set: 0.3240790367126465 at epoch: 23 and batch_num: 79\n",
      "Loss of test set: 0.07007656246423721 at epoch: 23 and batch_num: 80\n",
      "Loss of test set: 0.5278142094612122 at epoch: 23 and batch_num: 81\n",
      "Loss of test set: 0.367605984210968 at epoch: 23 and batch_num: 82\n",
      "Loss of test set: 0.2642057240009308 at epoch: 23 and batch_num: 83\n",
      "Loss of test set: 0.5121766924858093 at epoch: 23 and batch_num: 84\n",
      "Loss of test set: 0.5817744731903076 at epoch: 23 and batch_num: 85\n",
      "Loss of test set: 0.46536731719970703 at epoch: 23 and batch_num: 86\n",
      "Loss of test set: 0.20707502961158752 at epoch: 23 and batch_num: 87\n",
      "Loss of test set: 0.2943706512451172 at epoch: 23 and batch_num: 88\n",
      "Loss of test set: 0.2849571704864502 at epoch: 23 and batch_num: 89\n",
      "Loss of test set: 0.5496681928634644 at epoch: 23 and batch_num: 90\n",
      "Loss of test set: 0.632365345954895 at epoch: 23 and batch_num: 91\n",
      "Loss of test set: 0.35762181878089905 at epoch: 23 and batch_num: 92\n",
      "Loss of test set: 0.4795043468475342 at epoch: 23 and batch_num: 93\n",
      "Loss of test set: 0.3130483329296112 at epoch: 23 and batch_num: 94\n",
      "Loss of test set: 0.22917276620864868 at epoch: 23 and batch_num: 95\n",
      "Loss of test set: 0.2751751244068146 at epoch: 23 and batch_num: 96\n",
      "Loss of test set: 0.47676074504852295 at epoch: 23 and batch_num: 97\n",
      "Loss of test set: 0.3973676264286041 at epoch: 23 and batch_num: 98\n",
      "Loss of test set: 0.3158131241798401 at epoch: 23 and batch_num: 99\n",
      "Loss of test set: 0.24276626110076904 at epoch: 23 and batch_num: 100\n",
      "Loss of test set: 0.44808879494667053 at epoch: 23 and batch_num: 101\n",
      "Loss of test set: 0.14912952482700348 at epoch: 23 and batch_num: 102\n",
      "Loss of test set: 0.3500591218471527 at epoch: 23 and batch_num: 103\n",
      "Loss of test set: 0.4197746515274048 at epoch: 23 and batch_num: 104\n",
      "Loss of test set: 0.20255926251411438 at epoch: 23 and batch_num: 105\n",
      "Loss of test set: 0.3513798117637634 at epoch: 23 and batch_num: 106\n",
      "Loss of test set: 0.39629486203193665 at epoch: 23 and batch_num: 107\n",
      "Loss of test set: 0.2698173522949219 at epoch: 23 and batch_num: 108\n",
      "Loss of test set: 0.375280499458313 at epoch: 23 and batch_num: 109\n",
      "Loss of test set: 0.2541024088859558 at epoch: 23 and batch_num: 110\n",
      "Loss of test set: 0.3171460032463074 at epoch: 23 and batch_num: 111\n",
      "Loss of test set: 0.4465060234069824 at epoch: 23 and batch_num: 112\n",
      "Loss of test set: 0.6006622314453125 at epoch: 23 and batch_num: 113\n",
      "Loss of test set: 0.5147415399551392 at epoch: 23 and batch_num: 114\n",
      "Loss of test set: 0.3619856834411621 at epoch: 23 and batch_num: 115\n",
      "Loss of test set: 0.5150742530822754 at epoch: 23 and batch_num: 116\n",
      "Loss of test set: 0.2119545340538025 at epoch: 23 and batch_num: 117\n",
      "Loss of test set: 0.6150357723236084 at epoch: 23 and batch_num: 118\n",
      "Loss of test set: 0.2707744538784027 at epoch: 23 and batch_num: 119\n",
      "Loss of test set: 0.30115455389022827 at epoch: 23 and batch_num: 120\n",
      "Loss of test set: 0.3533508777618408 at epoch: 23 and batch_num: 121\n",
      "Loss of test set: 0.41025328636169434 at epoch: 23 and batch_num: 122\n",
      "Loss of test set: 0.4381096363067627 at epoch: 23 and batch_num: 123\n",
      "Loss of test set: 0.3487389087677002 at epoch: 23 and batch_num: 124\n",
      "Loss of test set: 0.3883151113986969 at epoch: 23 and batch_num: 125\n",
      "Loss of test set: 0.33413273096084595 at epoch: 23 and batch_num: 126\n",
      "Loss of test set: 0.40012118220329285 at epoch: 23 and batch_num: 127\n",
      "Loss of test set: 0.21731901168823242 at epoch: 23 and batch_num: 128\n",
      "Loss of test set: 0.4997562766075134 at epoch: 23 and batch_num: 129\n",
      "Loss of test set: 0.4640279710292816 at epoch: 23 and batch_num: 130\n",
      "Loss of test set: 0.38294774293899536 at epoch: 23 and batch_num: 131\n",
      "Loss of test set: 0.36402249336242676 at epoch: 23 and batch_num: 132\n",
      "Loss of test set: 0.5646073818206787 at epoch: 23 and batch_num: 133\n",
      "Loss of test set: 0.4095836877822876 at epoch: 23 and batch_num: 134\n",
      "Loss of test set: 0.42496904730796814 at epoch: 23 and batch_num: 135\n",
      "Loss of test set: 0.3221225440502167 at epoch: 23 and batch_num: 136\n",
      "Loss of test set: 0.4212617874145508 at epoch: 23 and batch_num: 137\n",
      "Loss of test set: 0.48189446330070496 at epoch: 23 and batch_num: 138\n",
      "Loss of test set: 0.370198130607605 at epoch: 23 and batch_num: 139\n",
      "Loss of test set: 0.26261621713638306 at epoch: 23 and batch_num: 140\n",
      "Loss of test set: 0.39396294951438904 at epoch: 23 and batch_num: 141\n",
      "Loss of test set: 0.5423921346664429 at epoch: 23 and batch_num: 142\n",
      "Loss of test set: 0.22413282096385956 at epoch: 23 and batch_num: 143\n",
      "Loss of test set: 0.36008232831954956 at epoch: 23 and batch_num: 144\n",
      "Loss of test set: 0.3262444734573364 at epoch: 23 and batch_num: 145\n",
      "Loss of test set: 0.37343496084213257 at epoch: 23 and batch_num: 146\n",
      "Loss of test set: 0.18753719329833984 at epoch: 23 and batch_num: 147\n",
      "Loss of test set: 0.3114655017852783 at epoch: 23 and batch_num: 148\n",
      "Loss of test set: 0.3125303387641907 at epoch: 23 and batch_num: 149\n",
      "Loss of test set: 0.35394731163978577 at epoch: 23 and batch_num: 150\n",
      "Loss of test set: 0.3419424891471863 at epoch: 23 and batch_num: 151\n",
      "Loss of test set: 0.4321534335613251 at epoch: 23 and batch_num: 152\n",
      "Loss of test set: 0.2971765100955963 at epoch: 23 and batch_num: 153\n",
      "Loss of test set: 0.3362431526184082 at epoch: 23 and batch_num: 154\n",
      "Loss of test set: 0.1231231689453125 at epoch: 23 and batch_num: 155\n",
      "Loss of test set: 0.5421718955039978 at epoch: 23 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8693\n",
      "Loss of train set: 0.25693827867507935 at epoch: 24 and batch_num: 0\n",
      "Loss of train set: 0.15264472365379333 at epoch: 24 and batch_num: 1\n",
      "Loss of train set: 0.26255226135253906 at epoch: 24 and batch_num: 2\n",
      "Loss of train set: 0.25383511185646057 at epoch: 24 and batch_num: 3\n",
      "Loss of train set: 0.27386710047721863 at epoch: 24 and batch_num: 4\n",
      "Loss of train set: 0.3810737133026123 at epoch: 24 and batch_num: 5\n",
      "Loss of train set: 0.32285934686660767 at epoch: 24 and batch_num: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.5568482279777527 at epoch: 24 and batch_num: 7\n",
      "Loss of train set: 0.21820908784866333 at epoch: 24 and batch_num: 8\n",
      "Loss of train set: 0.2888792157173157 at epoch: 24 and batch_num: 9\n",
      "Loss of train set: 0.1925114095211029 at epoch: 24 and batch_num: 10\n",
      "Loss of train set: 0.24146568775177002 at epoch: 24 and batch_num: 11\n",
      "Loss of train set: 0.16390711069107056 at epoch: 24 and batch_num: 12\n",
      "Loss of train set: 0.264740526676178 at epoch: 24 and batch_num: 13\n",
      "Loss of train set: 0.3373311161994934 at epoch: 24 and batch_num: 14\n",
      "Loss of train set: 0.3758942484855652 at epoch: 24 and batch_num: 15\n",
      "Loss of train set: 0.25212162733078003 at epoch: 24 and batch_num: 16\n",
      "Loss of train set: 0.3987950086593628 at epoch: 24 and batch_num: 17\n",
      "Loss of train set: 0.35079607367515564 at epoch: 24 and batch_num: 18\n",
      "Loss of train set: 0.16392426192760468 at epoch: 24 and batch_num: 19\n",
      "Loss of train set: 0.3238576650619507 at epoch: 24 and batch_num: 20\n",
      "Loss of train set: 0.39757633209228516 at epoch: 24 and batch_num: 21\n",
      "Loss of train set: 0.5540264248847961 at epoch: 24 and batch_num: 22\n",
      "Loss of train set: 0.3673524260520935 at epoch: 24 and batch_num: 23\n",
      "Loss of train set: 0.188641756772995 at epoch: 24 and batch_num: 24\n",
      "Loss of train set: 0.2042645663022995 at epoch: 24 and batch_num: 25\n",
      "Loss of train set: 0.34605997800827026 at epoch: 24 and batch_num: 26\n",
      "Loss of train set: 0.3421303629875183 at epoch: 24 and batch_num: 27\n",
      "Loss of train set: 0.1861182004213333 at epoch: 24 and batch_num: 28\n",
      "Loss of train set: 0.379164457321167 at epoch: 24 and batch_num: 29\n",
      "Loss of train set: 0.16369864344596863 at epoch: 24 and batch_num: 30\n",
      "Loss of train set: 0.2285856455564499 at epoch: 24 and batch_num: 31\n",
      "Loss of train set: 0.26663780212402344 at epoch: 24 and batch_num: 32\n",
      "Loss of train set: 0.22118590772151947 at epoch: 24 and batch_num: 33\n",
      "Loss of train set: 0.21020293235778809 at epoch: 24 and batch_num: 34\n",
      "Loss of train set: 0.16252687573432922 at epoch: 24 and batch_num: 35\n",
      "Loss of train set: 0.2584189176559448 at epoch: 24 and batch_num: 36\n",
      "Loss of train set: 0.23154473304748535 at epoch: 24 and batch_num: 37\n",
      "Loss of train set: 0.2568308711051941 at epoch: 24 and batch_num: 38\n",
      "Loss of train set: 0.1778264343738556 at epoch: 24 and batch_num: 39\n",
      "Loss of train set: 0.3034623861312866 at epoch: 24 and batch_num: 40\n",
      "Loss of train set: 0.35569867491722107 at epoch: 24 and batch_num: 41\n",
      "Loss of train set: 0.13781678676605225 at epoch: 24 and batch_num: 42\n",
      "Loss of train set: 0.3186129033565521 at epoch: 24 and batch_num: 43\n",
      "Loss of train set: 0.34219980239868164 at epoch: 24 and batch_num: 44\n",
      "Loss of train set: 0.3713366389274597 at epoch: 24 and batch_num: 45\n",
      "Loss of train set: 0.22610265016555786 at epoch: 24 and batch_num: 46\n",
      "Loss of train set: 0.2875252664089203 at epoch: 24 and batch_num: 47\n",
      "Loss of train set: 0.2853015661239624 at epoch: 24 and batch_num: 48\n",
      "Loss of train set: 0.2678985297679901 at epoch: 24 and batch_num: 49\n",
      "Loss of train set: 0.16897548735141754 at epoch: 24 and batch_num: 50\n",
      "Loss of train set: 0.23837682604789734 at epoch: 24 and batch_num: 51\n",
      "Loss of train set: 0.4165858328342438 at epoch: 24 and batch_num: 52\n",
      "Loss of train set: 0.30604881048202515 at epoch: 24 and batch_num: 53\n",
      "Loss of train set: 0.24616524577140808 at epoch: 24 and batch_num: 54\n",
      "Loss of train set: 0.1567075252532959 at epoch: 24 and batch_num: 55\n",
      "Loss of train set: 0.254081130027771 at epoch: 24 and batch_num: 56\n",
      "Loss of train set: 0.2564947009086609 at epoch: 24 and batch_num: 57\n",
      "Loss of train set: 0.21980763971805573 at epoch: 24 and batch_num: 58\n",
      "Loss of train set: 0.14283324778079987 at epoch: 24 and batch_num: 59\n",
      "Loss of train set: 0.14472849667072296 at epoch: 24 and batch_num: 60\n",
      "Loss of train set: 0.25179198384284973 at epoch: 24 and batch_num: 61\n",
      "Loss of train set: 0.16855087876319885 at epoch: 24 and batch_num: 62\n",
      "Loss of train set: 0.27810806035995483 at epoch: 24 and batch_num: 63\n",
      "Loss of train set: 0.16875773668289185 at epoch: 24 and batch_num: 64\n",
      "Loss of train set: 0.3171883821487427 at epoch: 24 and batch_num: 65\n",
      "Loss of train set: 0.252437949180603 at epoch: 24 and batch_num: 66\n",
      "Loss of train set: 0.22146190702915192 at epoch: 24 and batch_num: 67\n",
      "Loss of train set: 0.2770569920539856 at epoch: 24 and batch_num: 68\n",
      "Loss of train set: 0.24947673082351685 at epoch: 24 and batch_num: 69\n",
      "Loss of train set: 0.28288042545318604 at epoch: 24 and batch_num: 70\n",
      "Loss of train set: 0.3416435122489929 at epoch: 24 and batch_num: 71\n",
      "Loss of train set: 0.1535552591085434 at epoch: 24 and batch_num: 72\n",
      "Loss of train set: 0.29333803057670593 at epoch: 24 and batch_num: 73\n",
      "Loss of train set: 0.19877541065216064 at epoch: 24 and batch_num: 74\n",
      "Loss of train set: 0.1296153962612152 at epoch: 24 and batch_num: 75\n",
      "Loss of train set: 0.23701918125152588 at epoch: 24 and batch_num: 76\n",
      "Loss of train set: 0.37681102752685547 at epoch: 24 and batch_num: 77\n",
      "Loss of train set: 0.3214372992515564 at epoch: 24 and batch_num: 78\n",
      "Loss of train set: 0.2637677788734436 at epoch: 24 and batch_num: 79\n",
      "Loss of train set: 0.19823938608169556 at epoch: 24 and batch_num: 80\n",
      "Loss of train set: 0.15134838223457336 at epoch: 24 and batch_num: 81\n",
      "Loss of train set: 0.2757209539413452 at epoch: 24 and batch_num: 82\n",
      "Loss of train set: 0.24282017350196838 at epoch: 24 and batch_num: 83\n",
      "Loss of train set: 0.28043097257614136 at epoch: 24 and batch_num: 84\n",
      "Loss of train set: 0.1857086569070816 at epoch: 24 and batch_num: 85\n",
      "Loss of train set: 0.35664182901382446 at epoch: 24 and batch_num: 86\n",
      "Loss of train set: 0.250315397977829 at epoch: 24 and batch_num: 87\n",
      "Loss of train set: 0.20508138835430145 at epoch: 24 and batch_num: 88\n",
      "Loss of train set: 0.16051693260669708 at epoch: 24 and batch_num: 89\n",
      "Loss of train set: 0.3393556475639343 at epoch: 24 and batch_num: 90\n",
      "Loss of train set: 0.327762246131897 at epoch: 24 and batch_num: 91\n",
      "Loss of train set: 0.20206058025360107 at epoch: 24 and batch_num: 92\n",
      "Loss of train set: 0.25612857937812805 at epoch: 24 and batch_num: 93\n",
      "Loss of train set: 0.32111838459968567 at epoch: 24 and batch_num: 94\n",
      "Loss of train set: 0.32785770297050476 at epoch: 24 and batch_num: 95\n",
      "Loss of train set: 0.25965234637260437 at epoch: 24 and batch_num: 96\n",
      "Loss of train set: 0.361793577671051 at epoch: 24 and batch_num: 97\n",
      "Loss of train set: 0.2405313402414322 at epoch: 24 and batch_num: 98\n",
      "Loss of train set: 0.1402978003025055 at epoch: 24 and batch_num: 99\n",
      "Loss of train set: 0.31639644503593445 at epoch: 24 and batch_num: 100\n",
      "Loss of train set: 0.3066076338291168 at epoch: 24 and batch_num: 101\n",
      "Loss of train set: 0.34113609790802 at epoch: 24 and batch_num: 102\n",
      "Loss of train set: 0.3123069405555725 at epoch: 24 and batch_num: 103\n",
      "Loss of train set: 0.2714728116989136 at epoch: 24 and batch_num: 104\n",
      "Loss of train set: 0.3097130358219147 at epoch: 24 and batch_num: 105\n",
      "Loss of train set: 0.37120985984802246 at epoch: 24 and batch_num: 106\n",
      "Loss of train set: 0.18016116321086884 at epoch: 24 and batch_num: 107\n",
      "Loss of train set: 0.22712039947509766 at epoch: 24 and batch_num: 108\n",
      "Loss of train set: 0.3402292728424072 at epoch: 24 and batch_num: 109\n",
      "Loss of train set: 0.25287967920303345 at epoch: 24 and batch_num: 110\n",
      "Loss of train set: 0.310715913772583 at epoch: 24 and batch_num: 111\n",
      "Loss of train set: 0.3067364990711212 at epoch: 24 and batch_num: 112\n",
      "Loss of train set: 0.29321956634521484 at epoch: 24 and batch_num: 113\n",
      "Loss of train set: 0.31070441007614136 at epoch: 24 and batch_num: 114\n",
      "Loss of train set: 0.19573375582695007 at epoch: 24 and batch_num: 115\n",
      "Loss of train set: 0.19207651913166046 at epoch: 24 and batch_num: 116\n",
      "Loss of train set: 0.2221820056438446 at epoch: 24 and batch_num: 117\n",
      "Loss of train set: 0.1838330328464508 at epoch: 24 and batch_num: 118\n",
      "Loss of train set: 0.28420597314834595 at epoch: 24 and batch_num: 119\n",
      "Loss of train set: 0.21831569075584412 at epoch: 24 and batch_num: 120\n",
      "Loss of train set: 0.30461806058883667 at epoch: 24 and batch_num: 121\n",
      "Loss of train set: 0.2655053436756134 at epoch: 24 and batch_num: 122\n",
      "Loss of train set: 0.249192476272583 at epoch: 24 and batch_num: 123\n",
      "Loss of train set: 0.17131271958351135 at epoch: 24 and batch_num: 124\n",
      "Loss of train set: 0.1667557954788208 at epoch: 24 and batch_num: 125\n",
      "Loss of train set: 0.35481521487236023 at epoch: 24 and batch_num: 126\n",
      "Loss of train set: 0.25842511653900146 at epoch: 24 and batch_num: 127\n",
      "Loss of train set: 0.26637598872184753 at epoch: 24 and batch_num: 128\n",
      "Loss of train set: 0.15399006009101868 at epoch: 24 and batch_num: 129\n",
      "Loss of train set: 0.2994011640548706 at epoch: 24 and batch_num: 130\n",
      "Loss of train set: 0.19152946770191193 at epoch: 24 and batch_num: 131\n",
      "Loss of train set: 0.11567025631666183 at epoch: 24 and batch_num: 132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.17192336916923523 at epoch: 24 and batch_num: 133\n",
      "Loss of train set: 0.21214570105075836 at epoch: 24 and batch_num: 134\n",
      "Loss of train set: 0.2146594524383545 at epoch: 24 and batch_num: 135\n",
      "Loss of train set: 0.23484796285629272 at epoch: 24 and batch_num: 136\n",
      "Loss of train set: 0.3437808156013489 at epoch: 24 and batch_num: 137\n",
      "Loss of train set: 0.25324541330337524 at epoch: 24 and batch_num: 138\n",
      "Loss of train set: 0.30531394481658936 at epoch: 24 and batch_num: 139\n",
      "Loss of train set: 0.13074278831481934 at epoch: 24 and batch_num: 140\n",
      "Loss of train set: 0.18708956241607666 at epoch: 24 and batch_num: 141\n",
      "Loss of train set: 0.2505936622619629 at epoch: 24 and batch_num: 142\n",
      "Loss of train set: 0.29136133193969727 at epoch: 24 and batch_num: 143\n",
      "Loss of train set: 0.23143567144870758 at epoch: 24 and batch_num: 144\n",
      "Loss of train set: 0.18787410855293274 at epoch: 24 and batch_num: 145\n",
      "Loss of train set: 0.2730490565299988 at epoch: 24 and batch_num: 146\n",
      "Loss of train set: 0.30362415313720703 at epoch: 24 and batch_num: 147\n",
      "Loss of train set: 0.26489198207855225 at epoch: 24 and batch_num: 148\n",
      "Loss of train set: 0.2454746514558792 at epoch: 24 and batch_num: 149\n",
      "Loss of train set: 0.3550505042076111 at epoch: 24 and batch_num: 150\n",
      "Loss of train set: 0.2649545669555664 at epoch: 24 and batch_num: 151\n",
      "Loss of train set: 0.2729282081127167 at epoch: 24 and batch_num: 152\n",
      "Loss of train set: 0.31012406945228577 at epoch: 24 and batch_num: 153\n",
      "Loss of train set: 0.13401372730731964 at epoch: 24 and batch_num: 154\n",
      "Loss of train set: 0.17672905325889587 at epoch: 24 and batch_num: 155\n",
      "Loss of train set: 0.1813831627368927 at epoch: 24 and batch_num: 156\n",
      "Loss of train set: 0.3292587995529175 at epoch: 24 and batch_num: 157\n",
      "Loss of train set: 0.2518247663974762 at epoch: 24 and batch_num: 158\n",
      "Loss of train set: 0.46997690200805664 at epoch: 24 and batch_num: 159\n",
      "Loss of train set: 0.2783118486404419 at epoch: 24 and batch_num: 160\n",
      "Loss of train set: 0.2721320688724518 at epoch: 24 and batch_num: 161\n",
      "Loss of train set: 0.14316879212856293 at epoch: 24 and batch_num: 162\n",
      "Loss of train set: 0.24609705805778503 at epoch: 24 and batch_num: 163\n",
      "Loss of train set: 0.20505939424037933 at epoch: 24 and batch_num: 164\n",
      "Loss of train set: 0.2134101688861847 at epoch: 24 and batch_num: 165\n",
      "Loss of train set: 0.24139754474163055 at epoch: 24 and batch_num: 166\n",
      "Loss of train set: 0.27340269088745117 at epoch: 24 and batch_num: 167\n",
      "Loss of train set: 0.18474692106246948 at epoch: 24 and batch_num: 168\n",
      "Loss of train set: 0.39652708172798157 at epoch: 24 and batch_num: 169\n",
      "Loss of train set: 0.11420024931430817 at epoch: 24 and batch_num: 170\n",
      "Loss of train set: 0.279898464679718 at epoch: 24 and batch_num: 171\n",
      "Loss of train set: 0.20470641553401947 at epoch: 24 and batch_num: 172\n",
      "Loss of train set: 0.4770854711532593 at epoch: 24 and batch_num: 173\n",
      "Loss of train set: 0.2101663053035736 at epoch: 24 and batch_num: 174\n",
      "Loss of train set: 0.3702033758163452 at epoch: 24 and batch_num: 175\n",
      "Loss of train set: 0.25154218077659607 at epoch: 24 and batch_num: 176\n",
      "Loss of train set: 0.15661688148975372 at epoch: 24 and batch_num: 177\n",
      "Loss of train set: 0.2009652704000473 at epoch: 24 and batch_num: 178\n",
      "Loss of train set: 0.2505452334880829 at epoch: 24 and batch_num: 179\n",
      "Loss of train set: 0.2226237952709198 at epoch: 24 and batch_num: 180\n",
      "Loss of train set: 0.37173181772232056 at epoch: 24 and batch_num: 181\n",
      "Loss of train set: 0.2887043356895447 at epoch: 24 and batch_num: 182\n",
      "Loss of train set: 0.31359806656837463 at epoch: 24 and batch_num: 183\n",
      "Loss of train set: 0.5385873317718506 at epoch: 24 and batch_num: 184\n",
      "Loss of train set: 0.394599974155426 at epoch: 24 and batch_num: 185\n",
      "Loss of train set: 0.1984325349330902 at epoch: 24 and batch_num: 186\n",
      "Loss of train set: 0.4473639130592346 at epoch: 24 and batch_num: 187\n",
      "Loss of train set: 0.19197186827659607 at epoch: 24 and batch_num: 188\n",
      "Loss of train set: 0.3270706534385681 at epoch: 24 and batch_num: 189\n",
      "Loss of train set: 0.3202831745147705 at epoch: 24 and batch_num: 190\n",
      "Loss of train set: 0.40554171800613403 at epoch: 24 and batch_num: 191\n",
      "Loss of train set: 0.3993193805217743 at epoch: 24 and batch_num: 192\n",
      "Loss of train set: 0.25696831941604614 at epoch: 24 and batch_num: 193\n",
      "Loss of train set: 0.20042791962623596 at epoch: 24 and batch_num: 194\n",
      "Loss of train set: 0.17102253437042236 at epoch: 24 and batch_num: 195\n",
      "Loss of train set: 0.23034000396728516 at epoch: 24 and batch_num: 196\n",
      "Loss of train set: 0.27414217591285706 at epoch: 24 and batch_num: 197\n",
      "Loss of train set: 0.23287463188171387 at epoch: 24 and batch_num: 198\n",
      "Loss of train set: 0.314592182636261 at epoch: 24 and batch_num: 199\n",
      "Loss of train set: 0.18522562086582184 at epoch: 24 and batch_num: 200\n",
      "Loss of train set: 0.17592152953147888 at epoch: 24 and batch_num: 201\n",
      "Loss of train set: 0.22166267037391663 at epoch: 24 and batch_num: 202\n",
      "Loss of train set: 0.212248295545578 at epoch: 24 and batch_num: 203\n",
      "Loss of train set: 0.15626183152198792 at epoch: 24 and batch_num: 204\n",
      "Loss of train set: 0.27464163303375244 at epoch: 24 and batch_num: 205\n",
      "Loss of train set: 0.3207991123199463 at epoch: 24 and batch_num: 206\n",
      "Loss of train set: 0.3157275915145874 at epoch: 24 and batch_num: 207\n",
      "Loss of train set: 0.43752992153167725 at epoch: 24 and batch_num: 208\n",
      "Loss of train set: 0.19225850701332092 at epoch: 24 and batch_num: 209\n",
      "Loss of train set: 0.30867335200309753 at epoch: 24 and batch_num: 210\n",
      "Loss of train set: 0.2449498027563095 at epoch: 24 and batch_num: 211\n",
      "Loss of train set: 0.46820658445358276 at epoch: 24 and batch_num: 212\n",
      "Loss of train set: 0.18805307149887085 at epoch: 24 and batch_num: 213\n",
      "Loss of train set: 0.2618175745010376 at epoch: 24 and batch_num: 214\n",
      "Loss of train set: 0.4149731397628784 at epoch: 24 and batch_num: 215\n",
      "Loss of train set: 0.3561544120311737 at epoch: 24 and batch_num: 216\n",
      "Loss of train set: 0.18115317821502686 at epoch: 24 and batch_num: 217\n",
      "Loss of train set: 0.1579795777797699 at epoch: 24 and batch_num: 218\n",
      "Loss of train set: 0.2250453531742096 at epoch: 24 and batch_num: 219\n",
      "Loss of train set: 0.20585456490516663 at epoch: 24 and batch_num: 220\n",
      "Loss of train set: 0.3573416471481323 at epoch: 24 and batch_num: 221\n",
      "Loss of train set: 0.2353658527135849 at epoch: 24 and batch_num: 222\n",
      "Loss of train set: 0.24034050107002258 at epoch: 24 and batch_num: 223\n",
      "Loss of train set: 0.28360405564308167 at epoch: 24 and batch_num: 224\n",
      "Loss of train set: 0.16785544157028198 at epoch: 24 and batch_num: 225\n",
      "Loss of train set: 0.4081948399543762 at epoch: 24 and batch_num: 226\n",
      "Loss of train set: 0.34803980588912964 at epoch: 24 and batch_num: 227\n",
      "Loss of train set: 0.15807995200157166 at epoch: 24 and batch_num: 228\n",
      "Loss of train set: 0.41808271408081055 at epoch: 24 and batch_num: 229\n",
      "Loss of train set: 0.17805996537208557 at epoch: 24 and batch_num: 230\n",
      "Loss of train set: 0.33135926723480225 at epoch: 24 and batch_num: 231\n",
      "Loss of train set: 0.35874927043914795 at epoch: 24 and batch_num: 232\n",
      "Loss of train set: 0.3285624086856842 at epoch: 24 and batch_num: 233\n",
      "Loss of train set: 0.20397837460041046 at epoch: 24 and batch_num: 234\n",
      "Loss of train set: 0.2532098591327667 at epoch: 24 and batch_num: 235\n",
      "Loss of train set: 0.17575624585151672 at epoch: 24 and batch_num: 236\n",
      "Loss of train set: 0.30097532272338867 at epoch: 24 and batch_num: 237\n",
      "Loss of train set: 0.37576016783714294 at epoch: 24 and batch_num: 238\n",
      "Loss of train set: 0.2990427315235138 at epoch: 24 and batch_num: 239\n",
      "Loss of train set: 0.32250988483428955 at epoch: 24 and batch_num: 240\n",
      "Loss of train set: 0.35560888051986694 at epoch: 24 and batch_num: 241\n",
      "Loss of train set: 0.4264947175979614 at epoch: 24 and batch_num: 242\n",
      "Loss of train set: 0.26053041219711304 at epoch: 24 and batch_num: 243\n",
      "Loss of train set: 0.18643072247505188 at epoch: 24 and batch_num: 244\n",
      "Loss of train set: 0.22441540658473969 at epoch: 24 and batch_num: 245\n",
      "Loss of train set: 0.18776589632034302 at epoch: 24 and batch_num: 246\n",
      "Loss of train set: 0.3895818293094635 at epoch: 24 and batch_num: 247\n",
      "Loss of train set: 0.39643076062202454 at epoch: 24 and batch_num: 248\n",
      "Loss of train set: 0.21120141446590424 at epoch: 24 and batch_num: 249\n",
      "Loss of train set: 0.3257661759853363 at epoch: 24 and batch_num: 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.3952502906322479 at epoch: 24 and batch_num: 251\n",
      "Loss of train set: 0.2378169149160385 at epoch: 24 and batch_num: 252\n",
      "Loss of train set: 0.1148608922958374 at epoch: 24 and batch_num: 253\n",
      "Loss of train set: 0.15921297669410706 at epoch: 24 and batch_num: 254\n",
      "Loss of train set: 0.36960750818252563 at epoch: 24 and batch_num: 255\n",
      "Loss of train set: 0.1612585484981537 at epoch: 24 and batch_num: 256\n",
      "Loss of train set: 0.13326159119606018 at epoch: 24 and batch_num: 257\n",
      "Loss of train set: 0.21426481008529663 at epoch: 24 and batch_num: 258\n",
      "Loss of train set: 0.15550699830055237 at epoch: 24 and batch_num: 259\n",
      "Loss of train set: 0.4102473258972168 at epoch: 24 and batch_num: 260\n",
      "Loss of train set: 0.26758891344070435 at epoch: 24 and batch_num: 261\n",
      "Loss of train set: 0.1810796558856964 at epoch: 24 and batch_num: 262\n",
      "Loss of train set: 0.26528865098953247 at epoch: 24 and batch_num: 263\n",
      "Loss of train set: 0.24831801652908325 at epoch: 24 and batch_num: 264\n",
      "Loss of train set: 0.2484278827905655 at epoch: 24 and batch_num: 265\n",
      "Loss of train set: 0.22749684751033783 at epoch: 24 and batch_num: 266\n",
      "Loss of train set: 0.2407705932855606 at epoch: 24 and batch_num: 267\n",
      "Loss of train set: 0.23895837366580963 at epoch: 24 and batch_num: 268\n",
      "Loss of train set: 0.27272725105285645 at epoch: 24 and batch_num: 269\n",
      "Loss of train set: 0.17124061286449432 at epoch: 24 and batch_num: 270\n",
      "Loss of train set: 0.3380068838596344 at epoch: 24 and batch_num: 271\n",
      "Loss of train set: 0.2814275026321411 at epoch: 24 and batch_num: 272\n",
      "Loss of train set: 0.20747166872024536 at epoch: 24 and batch_num: 273\n",
      "Loss of train set: 0.27693110704421997 at epoch: 24 and batch_num: 274\n",
      "Loss of train set: 0.3297640383243561 at epoch: 24 and batch_num: 275\n",
      "Loss of train set: 0.16180476546287537 at epoch: 24 and batch_num: 276\n",
      "Loss of train set: 0.4514961242675781 at epoch: 24 and batch_num: 277\n",
      "Loss of train set: 0.14104625582695007 at epoch: 24 and batch_num: 278\n",
      "Loss of train set: 0.29044288396835327 at epoch: 24 and batch_num: 279\n",
      "Loss of train set: 0.4800899922847748 at epoch: 24 and batch_num: 280\n",
      "Loss of train set: 0.33307474851608276 at epoch: 24 and batch_num: 281\n",
      "Loss of train set: 0.2704945206642151 at epoch: 24 and batch_num: 282\n",
      "Loss of train set: 0.14946772158145905 at epoch: 24 and batch_num: 283\n",
      "Loss of train set: 0.2528766095638275 at epoch: 24 and batch_num: 284\n",
      "Loss of train set: 0.34042930603027344 at epoch: 24 and batch_num: 285\n",
      "Loss of train set: 0.19474798440933228 at epoch: 24 and batch_num: 286\n",
      "Loss of train set: 0.17053870856761932 at epoch: 24 and batch_num: 287\n",
      "Loss of train set: 0.24875174462795258 at epoch: 24 and batch_num: 288\n",
      "Loss of train set: 0.27243074774742126 at epoch: 24 and batch_num: 289\n",
      "Loss of train set: 0.35332822799682617 at epoch: 24 and batch_num: 290\n",
      "Loss of train set: 0.21816670894622803 at epoch: 24 and batch_num: 291\n",
      "Loss of train set: 0.23541374504566193 at epoch: 24 and batch_num: 292\n",
      "Loss of train set: 0.3614327311515808 at epoch: 24 and batch_num: 293\n",
      "Loss of train set: 0.2756125330924988 at epoch: 24 and batch_num: 294\n",
      "Loss of train set: 0.1380029022693634 at epoch: 24 and batch_num: 295\n",
      "Loss of train set: 0.2825562357902527 at epoch: 24 and batch_num: 296\n",
      "Loss of train set: 0.1803995966911316 at epoch: 24 and batch_num: 297\n",
      "Loss of train set: 0.29402515292167664 at epoch: 24 and batch_num: 298\n",
      "Loss of train set: 0.32305657863616943 at epoch: 24 and batch_num: 299\n",
      "Loss of train set: 0.29766491055488586 at epoch: 24 and batch_num: 300\n",
      "Loss of train set: 0.3372379243373871 at epoch: 24 and batch_num: 301\n",
      "Loss of train set: 0.2853992283344269 at epoch: 24 and batch_num: 302\n",
      "Loss of train set: 0.3745327889919281 at epoch: 24 and batch_num: 303\n",
      "Loss of train set: 0.41093888878822327 at epoch: 24 and batch_num: 304\n",
      "Loss of train set: 0.2843095660209656 at epoch: 24 and batch_num: 305\n",
      "Loss of train set: 0.21448369324207306 at epoch: 24 and batch_num: 306\n",
      "Loss of train set: 0.3374461531639099 at epoch: 24 and batch_num: 307\n",
      "Loss of train set: 0.23845039308071136 at epoch: 24 and batch_num: 308\n",
      "Loss of train set: 0.29215705394744873 at epoch: 24 and batch_num: 309\n",
      "Loss of train set: 0.31331896781921387 at epoch: 24 and batch_num: 310\n",
      "Loss of train set: 0.195443257689476 at epoch: 24 and batch_num: 311\n",
      "Loss of train set: 0.2540329396724701 at epoch: 24 and batch_num: 312\n",
      "Loss of train set: 0.27080637216567993 at epoch: 24 and batch_num: 313\n",
      "Loss of train set: 0.3295349180698395 at epoch: 24 and batch_num: 314\n",
      "Loss of train set: 0.1919981986284256 at epoch: 24 and batch_num: 315\n",
      "Loss of train set: 0.29157689213752747 at epoch: 24 and batch_num: 316\n",
      "Loss of train set: 0.19487890601158142 at epoch: 24 and batch_num: 317\n",
      "Loss of train set: 0.2836308479309082 at epoch: 24 and batch_num: 318\n",
      "Loss of train set: 0.2257038950920105 at epoch: 24 and batch_num: 319\n",
      "Loss of train set: 0.2946488857269287 at epoch: 24 and batch_num: 320\n",
      "Loss of train set: 0.21917521953582764 at epoch: 24 and batch_num: 321\n",
      "Loss of train set: 0.27046722173690796 at epoch: 24 and batch_num: 322\n",
      "Loss of train set: 0.31374627351760864 at epoch: 24 and batch_num: 323\n",
      "Loss of train set: 0.19392497837543488 at epoch: 24 and batch_num: 324\n",
      "Loss of train set: 0.27617183327674866 at epoch: 24 and batch_num: 325\n",
      "Loss of train set: 0.2052551805973053 at epoch: 24 and batch_num: 326\n",
      "Loss of train set: 0.3608984351158142 at epoch: 24 and batch_num: 327\n",
      "Loss of train set: 0.31928807497024536 at epoch: 24 and batch_num: 328\n",
      "Loss of train set: 0.308025598526001 at epoch: 24 and batch_num: 329\n",
      "Loss of train set: 0.26852017641067505 at epoch: 24 and batch_num: 330\n",
      "Loss of train set: 0.2800070345401764 at epoch: 24 and batch_num: 331\n",
      "Loss of train set: 0.13094142079353333 at epoch: 24 and batch_num: 332\n",
      "Loss of train set: 0.21158528327941895 at epoch: 24 and batch_num: 333\n",
      "Loss of train set: 0.42891737818717957 at epoch: 24 and batch_num: 334\n",
      "Loss of train set: 0.18220552802085876 at epoch: 24 and batch_num: 335\n",
      "Loss of train set: 0.2714921236038208 at epoch: 24 and batch_num: 336\n",
      "Loss of train set: 0.16545359790325165 at epoch: 24 and batch_num: 337\n",
      "Loss of train set: 0.26079443097114563 at epoch: 24 and batch_num: 338\n",
      "Loss of train set: 0.22894136607646942 at epoch: 24 and batch_num: 339\n",
      "Loss of train set: 0.24891488254070282 at epoch: 24 and batch_num: 340\n",
      "Loss of train set: 0.18675020337104797 at epoch: 24 and batch_num: 341\n",
      "Loss of train set: 0.32642868161201477 at epoch: 24 and batch_num: 342\n",
      "Loss of train set: 0.274158775806427 at epoch: 24 and batch_num: 343\n",
      "Loss of train set: 0.19364769756793976 at epoch: 24 and batch_num: 344\n",
      "Loss of train set: 0.23046071827411652 at epoch: 24 and batch_num: 345\n",
      "Loss of train set: 0.1690296232700348 at epoch: 24 and batch_num: 346\n",
      "Loss of train set: 0.2943153381347656 at epoch: 24 and batch_num: 347\n",
      "Loss of train set: 0.33142951130867004 at epoch: 24 and batch_num: 348\n",
      "Loss of train set: 0.23329295217990875 at epoch: 24 and batch_num: 349\n",
      "Loss of train set: 0.1221776157617569 at epoch: 24 and batch_num: 350\n",
      "Loss of train set: 0.2360547035932541 at epoch: 24 and batch_num: 351\n",
      "Loss of train set: 0.16962289810180664 at epoch: 24 and batch_num: 352\n",
      "Loss of train set: 0.1331840455532074 at epoch: 24 and batch_num: 353\n",
      "Loss of train set: 0.21522873640060425 at epoch: 24 and batch_num: 354\n",
      "Loss of train set: 0.1657164841890335 at epoch: 24 and batch_num: 355\n",
      "Loss of train set: 0.29320868849754333 at epoch: 24 and batch_num: 356\n",
      "Loss of train set: 0.17060548067092896 at epoch: 24 and batch_num: 357\n",
      "Loss of train set: 0.23716171085834503 at epoch: 24 and batch_num: 358\n",
      "Loss of train set: 0.16070342063903809 at epoch: 24 and batch_num: 359\n",
      "Loss of train set: 0.3756331205368042 at epoch: 24 and batch_num: 360\n",
      "Loss of train set: 0.2712547779083252 at epoch: 24 and batch_num: 361\n",
      "Loss of train set: 0.29427629709243774 at epoch: 24 and batch_num: 362\n",
      "Loss of train set: 0.22969874739646912 at epoch: 24 and batch_num: 363\n",
      "Loss of train set: 0.3789949417114258 at epoch: 24 and batch_num: 364\n",
      "Loss of train set: 0.25753864645957947 at epoch: 24 and batch_num: 365\n",
      "Loss of train set: 0.34811270236968994 at epoch: 24 and batch_num: 366\n",
      "Loss of train set: 0.2314155548810959 at epoch: 24 and batch_num: 367\n",
      "Loss of train set: 0.17854094505310059 at epoch: 24 and batch_num: 368\n",
      "Loss of train set: 0.2857063412666321 at epoch: 24 and batch_num: 369\n",
      "Loss of train set: 0.270004004240036 at epoch: 24 and batch_num: 370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.21526440978050232 at epoch: 24 and batch_num: 371\n",
      "Loss of train set: 0.21448823809623718 at epoch: 24 and batch_num: 372\n",
      "Loss of train set: 0.08544184267520905 at epoch: 24 and batch_num: 373\n",
      "Loss of train set: 0.20733857154846191 at epoch: 24 and batch_num: 374\n",
      "Loss of train set: 0.1856246143579483 at epoch: 24 and batch_num: 375\n",
      "Loss of train set: 0.15533222258090973 at epoch: 24 and batch_num: 376\n",
      "Loss of train set: 0.22748664021492004 at epoch: 24 and batch_num: 377\n",
      "Loss of train set: 0.39207765460014343 at epoch: 24 and batch_num: 378\n",
      "Loss of train set: 0.2876471281051636 at epoch: 24 and batch_num: 379\n",
      "Loss of train set: 0.2223314642906189 at epoch: 24 and batch_num: 380\n",
      "Loss of train set: 0.24511051177978516 at epoch: 24 and batch_num: 381\n",
      "Loss of train set: 0.16461172699928284 at epoch: 24 and batch_num: 382\n",
      "Loss of train set: 0.2135077863931656 at epoch: 24 and batch_num: 383\n",
      "Loss of train set: 0.18073010444641113 at epoch: 24 and batch_num: 384\n",
      "Loss of train set: 0.3233184218406677 at epoch: 24 and batch_num: 385\n",
      "Loss of train set: 0.32583075761795044 at epoch: 24 and batch_num: 386\n",
      "Loss of train set: 0.2951905429363251 at epoch: 24 and batch_num: 387\n",
      "Loss of train set: 0.18346968293190002 at epoch: 24 and batch_num: 388\n",
      "Loss of train set: 0.15468868613243103 at epoch: 24 and batch_num: 389\n",
      "Loss of train set: 0.3162834346294403 at epoch: 24 and batch_num: 390\n",
      "Loss of train set: 0.1635304093360901 at epoch: 24 and batch_num: 391\n",
      "Loss of train set: 0.3333483338356018 at epoch: 24 and batch_num: 392\n",
      "Loss of train set: 0.21564863622188568 at epoch: 24 and batch_num: 393\n",
      "Loss of train set: 0.3262677490711212 at epoch: 24 and batch_num: 394\n",
      "Loss of train set: 0.16185125708580017 at epoch: 24 and batch_num: 395\n",
      "Loss of train set: 0.21224915981292725 at epoch: 24 and batch_num: 396\n",
      "Loss of train set: 0.3432658612728119 at epoch: 24 and batch_num: 397\n",
      "Loss of train set: 0.1773357093334198 at epoch: 24 and batch_num: 398\n",
      "Loss of train set: 0.3213098645210266 at epoch: 24 and batch_num: 399\n",
      "Loss of train set: 0.3634923994541168 at epoch: 24 and batch_num: 400\n",
      "Loss of train set: 0.32848888635635376 at epoch: 24 and batch_num: 401\n",
      "Loss of train set: 0.3315700888633728 at epoch: 24 and batch_num: 402\n",
      "Loss of train set: 0.31899064779281616 at epoch: 24 and batch_num: 403\n",
      "Loss of train set: 0.19807371497154236 at epoch: 24 and batch_num: 404\n",
      "Loss of train set: 0.16314157843589783 at epoch: 24 and batch_num: 405\n",
      "Loss of train set: 0.20779147744178772 at epoch: 24 and batch_num: 406\n",
      "Loss of train set: 0.16777676343917847 at epoch: 24 and batch_num: 407\n",
      "Loss of train set: 0.2573300004005432 at epoch: 24 and batch_num: 408\n",
      "Loss of train set: 0.3974641263484955 at epoch: 24 and batch_num: 409\n",
      "Loss of train set: 0.3805176615715027 at epoch: 24 and batch_num: 410\n",
      "Loss of train set: 0.3035281300544739 at epoch: 24 and batch_num: 411\n",
      "Loss of train set: 0.40534859895706177 at epoch: 24 and batch_num: 412\n",
      "Loss of train set: 0.3486880660057068 at epoch: 24 and batch_num: 413\n",
      "Loss of train set: 0.24323789775371552 at epoch: 24 and batch_num: 414\n",
      "Loss of train set: 0.4365784823894501 at epoch: 24 and batch_num: 415\n",
      "Loss of train set: 0.26755088567733765 at epoch: 24 and batch_num: 416\n",
      "Loss of train set: 0.3348522484302521 at epoch: 24 and batch_num: 417\n",
      "Loss of train set: 0.15116271376609802 at epoch: 24 and batch_num: 418\n",
      "Loss of train set: 0.23990276455879211 at epoch: 24 and batch_num: 419\n",
      "Loss of train set: 0.15121345221996307 at epoch: 24 and batch_num: 420\n",
      "Loss of train set: 0.3079465925693512 at epoch: 24 and batch_num: 421\n",
      "Loss of train set: 0.1788249909877777 at epoch: 24 and batch_num: 422\n",
      "Loss of train set: 0.22774440050125122 at epoch: 24 and batch_num: 423\n",
      "Loss of train set: 0.225168839097023 at epoch: 24 and batch_num: 424\n",
      "Loss of train set: 0.3034287691116333 at epoch: 24 and batch_num: 425\n",
      "Loss of train set: 0.2875519096851349 at epoch: 24 and batch_num: 426\n",
      "Loss of train set: 0.19270944595336914 at epoch: 24 and batch_num: 427\n",
      "Loss of train set: 0.34573137760162354 at epoch: 24 and batch_num: 428\n",
      "Loss of train set: 0.241647869348526 at epoch: 24 and batch_num: 429\n",
      "Loss of train set: 0.39957737922668457 at epoch: 24 and batch_num: 430\n",
      "Loss of train set: 0.3848862648010254 at epoch: 24 and batch_num: 431\n",
      "Loss of train set: 0.4046476483345032 at epoch: 24 and batch_num: 432\n",
      "Loss of train set: 0.2840558588504791 at epoch: 24 and batch_num: 433\n",
      "Loss of train set: 0.23968838155269623 at epoch: 24 and batch_num: 434\n",
      "Loss of train set: 0.2962540090084076 at epoch: 24 and batch_num: 435\n",
      "Loss of train set: 0.3188437223434448 at epoch: 24 and batch_num: 436\n",
      "Loss of train set: 0.23774874210357666 at epoch: 24 and batch_num: 437\n",
      "Loss of train set: 0.21978062391281128 at epoch: 24 and batch_num: 438\n",
      "Loss of train set: 0.20124441385269165 at epoch: 24 and batch_num: 439\n",
      "Loss of train set: 0.2545467019081116 at epoch: 24 and batch_num: 440\n",
      "Loss of train set: 0.2192019373178482 at epoch: 24 and batch_num: 441\n",
      "Loss of train set: 0.27052825689315796 at epoch: 24 and batch_num: 442\n",
      "Loss of train set: 0.19897395372390747 at epoch: 24 and batch_num: 443\n",
      "Loss of train set: 0.37084293365478516 at epoch: 24 and batch_num: 444\n",
      "Loss of train set: 0.2982562780380249 at epoch: 24 and batch_num: 445\n",
      "Loss of train set: 0.45313936471939087 at epoch: 24 and batch_num: 446\n",
      "Loss of train set: 0.26739218831062317 at epoch: 24 and batch_num: 447\n",
      "Loss of train set: 0.15453791618347168 at epoch: 24 and batch_num: 448\n",
      "Loss of train set: 0.19112828373908997 at epoch: 24 and batch_num: 449\n",
      "Loss of train set: 0.21583512425422668 at epoch: 24 and batch_num: 450\n",
      "Loss of train set: 0.22817173600196838 at epoch: 24 and batch_num: 451\n",
      "Loss of train set: 0.23671039938926697 at epoch: 24 and batch_num: 452\n",
      "Loss of train set: 0.227471262216568 at epoch: 24 and batch_num: 453\n",
      "Loss of train set: 0.27512070536613464 at epoch: 24 and batch_num: 454\n",
      "Loss of train set: 0.16741812229156494 at epoch: 24 and batch_num: 455\n",
      "Loss of train set: 0.2626854479312897 at epoch: 24 and batch_num: 456\n",
      "Loss of train set: 0.22834362089633942 at epoch: 24 and batch_num: 457\n",
      "Loss of train set: 0.28031790256500244 at epoch: 24 and batch_num: 458\n",
      "Loss of train set: 0.22508522868156433 at epoch: 24 and batch_num: 459\n",
      "Loss of train set: 0.32238152623176575 at epoch: 24 and batch_num: 460\n",
      "Loss of train set: 0.4320937395095825 at epoch: 24 and batch_num: 461\n",
      "Loss of train set: 0.23241853713989258 at epoch: 24 and batch_num: 462\n",
      "Loss of train set: 0.18242141604423523 at epoch: 24 and batch_num: 463\n",
      "Loss of train set: 0.2571704089641571 at epoch: 24 and batch_num: 464\n",
      "Loss of train set: 0.3633384108543396 at epoch: 24 and batch_num: 465\n",
      "Loss of train set: 0.21069937944412231 at epoch: 24 and batch_num: 466\n",
      "Loss of train set: 0.2467210590839386 at epoch: 24 and batch_num: 467\n",
      "Loss of train set: 0.3041442036628723 at epoch: 24 and batch_num: 468\n",
      "Loss of train set: 0.26850074529647827 at epoch: 24 and batch_num: 469\n",
      "Loss of train set: 0.3475845158100128 at epoch: 24 and batch_num: 470\n",
      "Loss of train set: 0.21745169162750244 at epoch: 24 and batch_num: 471\n",
      "Loss of train set: 0.5411021709442139 at epoch: 24 and batch_num: 472\n",
      "Loss of train set: 0.40125948190689087 at epoch: 24 and batch_num: 473\n",
      "Loss of train set: 0.23693513870239258 at epoch: 24 and batch_num: 474\n",
      "Loss of train set: 0.2855648696422577 at epoch: 24 and batch_num: 475\n",
      "Loss of train set: 0.1975378692150116 at epoch: 24 and batch_num: 476\n",
      "Loss of train set: 0.19153980910778046 at epoch: 24 and batch_num: 477\n",
      "Loss of train set: 0.2855163514614105 at epoch: 24 and batch_num: 478\n",
      "Loss of train set: 0.3659915626049042 at epoch: 24 and batch_num: 479\n",
      "Loss of train set: 0.1421816647052765 at epoch: 24 and batch_num: 480\n",
      "Loss of train set: 0.24079295992851257 at epoch: 24 and batch_num: 481\n",
      "Loss of train set: 0.3145580291748047 at epoch: 24 and batch_num: 482\n",
      "Loss of train set: 0.22288969159126282 at epoch: 24 and batch_num: 483\n",
      "Loss of train set: 0.24270834028720856 at epoch: 24 and batch_num: 484\n",
      "Loss of train set: 0.39220964908599854 at epoch: 24 and batch_num: 485\n",
      "Loss of train set: 0.48080670833587646 at epoch: 24 and batch_num: 486\n",
      "Loss of train set: 0.29679322242736816 at epoch: 24 and batch_num: 487\n",
      "Loss of train set: 0.2571656107902527 at epoch: 24 and batch_num: 488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.18785616755485535 at epoch: 24 and batch_num: 489\n",
      "Loss of train set: 0.32412874698638916 at epoch: 24 and batch_num: 490\n",
      "Loss of train set: 0.17770902812480927 at epoch: 24 and batch_num: 491\n",
      "Loss of train set: 0.17962846159934998 at epoch: 24 and batch_num: 492\n",
      "Loss of train set: 0.35220611095428467 at epoch: 24 and batch_num: 493\n",
      "Loss of train set: 0.18426182866096497 at epoch: 24 and batch_num: 494\n",
      "Loss of train set: 0.2728743553161621 at epoch: 24 and batch_num: 495\n",
      "Loss of train set: 0.18309995532035828 at epoch: 24 and batch_num: 496\n",
      "Loss of train set: 0.32081636786460876 at epoch: 24 and batch_num: 497\n",
      "Loss of train set: 0.20628194510936737 at epoch: 24 and batch_num: 498\n",
      "Loss of train set: 0.24588829278945923 at epoch: 24 and batch_num: 499\n",
      "Loss of train set: 0.26811036467552185 at epoch: 24 and batch_num: 500\n",
      "Loss of train set: 0.17098039388656616 at epoch: 24 and batch_num: 501\n",
      "Loss of train set: 0.3102990984916687 at epoch: 24 and batch_num: 502\n",
      "Loss of train set: 0.320865273475647 at epoch: 24 and batch_num: 503\n",
      "Loss of train set: 0.38421863317489624 at epoch: 24 and batch_num: 504\n",
      "Loss of train set: 0.14706268906593323 at epoch: 24 and batch_num: 505\n",
      "Loss of train set: 0.21805597841739655 at epoch: 24 and batch_num: 506\n",
      "Loss of train set: 0.1965683102607727 at epoch: 24 and batch_num: 507\n",
      "Loss of train set: 0.14001379907131195 at epoch: 24 and batch_num: 508\n",
      "Loss of train set: 0.3977038860321045 at epoch: 24 and batch_num: 509\n",
      "Loss of train set: 0.4547141194343567 at epoch: 24 and batch_num: 510\n",
      "Loss of train set: 0.2809469699859619 at epoch: 24 and batch_num: 511\n",
      "Loss of train set: 0.3278914988040924 at epoch: 24 and batch_num: 512\n",
      "Loss of train set: 0.3697932958602905 at epoch: 24 and batch_num: 513\n",
      "Loss of train set: 0.12996645271778107 at epoch: 24 and batch_num: 514\n",
      "Loss of train set: 0.28945139050483704 at epoch: 24 and batch_num: 515\n",
      "Loss of train set: 0.23176884651184082 at epoch: 24 and batch_num: 516\n",
      "Loss of train set: 0.3711802363395691 at epoch: 24 and batch_num: 517\n",
      "Loss of train set: 0.20671780407428741 at epoch: 24 and batch_num: 518\n",
      "Loss of train set: 0.3171004354953766 at epoch: 24 and batch_num: 519\n",
      "Loss of train set: 0.35217148065567017 at epoch: 24 and batch_num: 520\n",
      "Loss of train set: 0.28567010164260864 at epoch: 24 and batch_num: 521\n",
      "Loss of train set: 0.26059994101524353 at epoch: 24 and batch_num: 522\n",
      "Loss of train set: 0.1567314863204956 at epoch: 24 and batch_num: 523\n",
      "Loss of train set: 0.2795668840408325 at epoch: 24 and batch_num: 524\n",
      "Loss of train set: 0.35464179515838623 at epoch: 24 and batch_num: 525\n",
      "Loss of train set: 0.2962895333766937 at epoch: 24 and batch_num: 526\n",
      "Loss of train set: 0.37090998888015747 at epoch: 24 and batch_num: 527\n",
      "Loss of train set: 0.14463931322097778 at epoch: 24 and batch_num: 528\n",
      "Loss of train set: 0.4016408324241638 at epoch: 24 and batch_num: 529\n",
      "Loss of train set: 0.2951663136482239 at epoch: 24 and batch_num: 530\n",
      "Loss of train set: 0.22332137823104858 at epoch: 24 and batch_num: 531\n",
      "Loss of train set: 0.31550297141075134 at epoch: 24 and batch_num: 532\n",
      "Loss of train set: 0.3167233467102051 at epoch: 24 and batch_num: 533\n",
      "Loss of train set: 0.1629340797662735 at epoch: 24 and batch_num: 534\n",
      "Loss of train set: 0.2758309841156006 at epoch: 24 and batch_num: 535\n",
      "Loss of train set: 0.29977184534072876 at epoch: 24 and batch_num: 536\n",
      "Loss of train set: 0.20898525416851044 at epoch: 24 and batch_num: 537\n",
      "Loss of train set: 0.22143220901489258 at epoch: 24 and batch_num: 538\n",
      "Loss of train set: 0.19776952266693115 at epoch: 24 and batch_num: 539\n",
      "Loss of train set: 0.24257510900497437 at epoch: 24 and batch_num: 540\n",
      "Loss of train set: 0.28054988384246826 at epoch: 24 and batch_num: 541\n",
      "Loss of train set: 0.30958694219589233 at epoch: 24 and batch_num: 542\n",
      "Loss of train set: 0.29850566387176514 at epoch: 24 and batch_num: 543\n",
      "Loss of train set: 0.19176551699638367 at epoch: 24 and batch_num: 544\n",
      "Loss of train set: 0.1733439564704895 at epoch: 24 and batch_num: 545\n",
      "Loss of train set: 0.18912369012832642 at epoch: 24 and batch_num: 546\n",
      "Loss of train set: 0.22860945761203766 at epoch: 24 and batch_num: 547\n",
      "Loss of train set: 0.22416730225086212 at epoch: 24 and batch_num: 548\n",
      "Loss of train set: 0.3069124221801758 at epoch: 24 and batch_num: 549\n",
      "Loss of train set: 0.26253581047058105 at epoch: 24 and batch_num: 550\n",
      "Loss of train set: 0.24442841112613678 at epoch: 24 and batch_num: 551\n",
      "Loss of train set: 0.22989806532859802 at epoch: 24 and batch_num: 552\n",
      "Loss of train set: 0.3620683550834656 at epoch: 24 and batch_num: 553\n",
      "Loss of train set: 0.33378922939300537 at epoch: 24 and batch_num: 554\n",
      "Loss of train set: 0.3596353530883789 at epoch: 24 and batch_num: 555\n",
      "Loss of train set: 0.2521790862083435 at epoch: 24 and batch_num: 556\n",
      "Loss of train set: 0.20791496336460114 at epoch: 24 and batch_num: 557\n",
      "Loss of train set: 0.20380310714244843 at epoch: 24 and batch_num: 558\n",
      "Loss of train set: 0.21936865150928497 at epoch: 24 and batch_num: 559\n",
      "Loss of train set: 0.27391141653060913 at epoch: 24 and batch_num: 560\n",
      "Loss of train set: 0.16288483142852783 at epoch: 24 and batch_num: 561\n",
      "Loss of train set: 0.3070312440395355 at epoch: 24 and batch_num: 562\n",
      "Loss of train set: 0.1766025424003601 at epoch: 24 and batch_num: 563\n",
      "Loss of train set: 0.1376565396785736 at epoch: 24 and batch_num: 564\n",
      "Loss of train set: 0.31059330701828003 at epoch: 24 and batch_num: 565\n",
      "Loss of train set: 0.23376114666461945 at epoch: 24 and batch_num: 566\n",
      "Loss of train set: 0.23822644352912903 at epoch: 24 and batch_num: 567\n",
      "Loss of train set: 0.2821802496910095 at epoch: 24 and batch_num: 568\n",
      "Loss of train set: 0.2511609196662903 at epoch: 24 and batch_num: 569\n",
      "Loss of train set: 0.23230841755867004 at epoch: 24 and batch_num: 570\n",
      "Loss of train set: 0.16789510846138 at epoch: 24 and batch_num: 571\n",
      "Loss of train set: 0.282557874917984 at epoch: 24 and batch_num: 572\n",
      "Loss of train set: 0.44419533014297485 at epoch: 24 and batch_num: 573\n",
      "Loss of train set: 0.3575935363769531 at epoch: 24 and batch_num: 574\n",
      "Loss of train set: 0.22378888726234436 at epoch: 24 and batch_num: 575\n",
      "Loss of train set: 0.17371831834316254 at epoch: 24 and batch_num: 576\n",
      "Loss of train set: 0.3472653925418854 at epoch: 24 and batch_num: 577\n",
      "Loss of train set: 0.4068278670310974 at epoch: 24 and batch_num: 578\n",
      "Loss of train set: 0.1988048553466797 at epoch: 24 and batch_num: 579\n",
      "Loss of train set: 0.5224933624267578 at epoch: 24 and batch_num: 580\n",
      "Loss of train set: 0.2797259986400604 at epoch: 24 and batch_num: 581\n",
      "Loss of train set: 0.1425904929637909 at epoch: 24 and batch_num: 582\n",
      "Loss of train set: 0.4470043182373047 at epoch: 24 and batch_num: 583\n",
      "Loss of train set: 0.23552140593528748 at epoch: 24 and batch_num: 584\n",
      "Loss of train set: 0.2905595004558563 at epoch: 24 and batch_num: 585\n",
      "Loss of train set: 0.1795317381620407 at epoch: 24 and batch_num: 586\n",
      "Loss of train set: 0.32117727398872375 at epoch: 24 and batch_num: 587\n",
      "Loss of train set: 0.19399484992027283 at epoch: 24 and batch_num: 588\n",
      "Loss of train set: 0.2447391301393509 at epoch: 24 and batch_num: 589\n",
      "Loss of train set: 0.2518022656440735 at epoch: 24 and batch_num: 590\n",
      "Loss of train set: 0.1490948498249054 at epoch: 24 and batch_num: 591\n",
      "Loss of train set: 0.15928518772125244 at epoch: 24 and batch_num: 592\n",
      "Loss of train set: 0.34466540813446045 at epoch: 24 and batch_num: 593\n",
      "Loss of train set: 0.42259323596954346 at epoch: 24 and batch_num: 594\n",
      "Loss of train set: 0.32010459899902344 at epoch: 24 and batch_num: 595\n",
      "Loss of train set: 0.23373869061470032 at epoch: 24 and batch_num: 596\n",
      "Loss of train set: 0.28805774450302124 at epoch: 24 and batch_num: 597\n",
      "Loss of train set: 0.22502392530441284 at epoch: 24 and batch_num: 598\n",
      "Loss of train set: 0.2082977294921875 at epoch: 24 and batch_num: 599\n",
      "Loss of train set: 0.1752595156431198 at epoch: 24 and batch_num: 600\n",
      "Loss of train set: 0.3137187659740448 at epoch: 24 and batch_num: 601\n",
      "Loss of train set: 0.2998451590538025 at epoch: 24 and batch_num: 602\n",
      "Loss of train set: 0.35601940751075745 at epoch: 24 and batch_num: 603\n",
      "Loss of train set: 0.26289838552474976 at epoch: 24 and batch_num: 604\n",
      "Loss of train set: 0.29754993319511414 at epoch: 24 and batch_num: 605\n",
      "Loss of train set: 0.18338437378406525 at epoch: 24 and batch_num: 606\n",
      "Loss of train set: 0.14128929376602173 at epoch: 24 and batch_num: 607\n",
      "Loss of train set: 0.33869630098342896 at epoch: 24 and batch_num: 608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.28718316555023193 at epoch: 24 and batch_num: 609\n",
      "Loss of train set: 0.36332762241363525 at epoch: 24 and batch_num: 610\n",
      "Loss of train set: 0.2785358428955078 at epoch: 24 and batch_num: 611\n",
      "Loss of train set: 0.4496784806251526 at epoch: 24 and batch_num: 612\n",
      "Loss of train set: 0.25429025292396545 at epoch: 24 and batch_num: 613\n",
      "Loss of train set: 0.38254326581954956 at epoch: 24 and batch_num: 614\n",
      "Loss of train set: 0.17219218611717224 at epoch: 24 and batch_num: 615\n",
      "Loss of train set: 0.355221152305603 at epoch: 24 and batch_num: 616\n",
      "Loss of train set: 0.25055357813835144 at epoch: 24 and batch_num: 617\n",
      "Loss of train set: 0.24636344611644745 at epoch: 24 and batch_num: 618\n",
      "Loss of train set: 0.16691693663597107 at epoch: 24 and batch_num: 619\n",
      "Loss of train set: 0.24363428354263306 at epoch: 24 and batch_num: 620\n",
      "Loss of train set: 0.2991737127304077 at epoch: 24 and batch_num: 621\n",
      "Loss of train set: 0.3790901303291321 at epoch: 24 and batch_num: 622\n",
      "Loss of train set: 0.4072462320327759 at epoch: 24 and batch_num: 623\n",
      "Loss of train set: 0.2644631862640381 at epoch: 24 and batch_num: 624\n",
      "Loss of train set: 0.2616724967956543 at epoch: 24 and batch_num: 625\n",
      "Loss of train set: 0.315909206867218 at epoch: 24 and batch_num: 626\n",
      "Loss of train set: 0.23447836935520172 at epoch: 24 and batch_num: 627\n",
      "Loss of train set: 0.27564728260040283 at epoch: 24 and batch_num: 628\n",
      "Loss of train set: 0.18367727100849152 at epoch: 24 and batch_num: 629\n",
      "Loss of train set: 0.24548399448394775 at epoch: 24 and batch_num: 630\n",
      "Loss of train set: 0.24175101518630981 at epoch: 24 and batch_num: 631\n",
      "Loss of train set: 0.3206620216369629 at epoch: 24 and batch_num: 632\n",
      "Loss of train set: 0.2951284945011139 at epoch: 24 and batch_num: 633\n",
      "Loss of train set: 0.32300418615341187 at epoch: 24 and batch_num: 634\n",
      "Loss of train set: 0.1739654242992401 at epoch: 24 and batch_num: 635\n",
      "Loss of train set: 0.38748785853385925 at epoch: 24 and batch_num: 636\n",
      "Loss of train set: 0.3671491742134094 at epoch: 24 and batch_num: 637\n",
      "Loss of train set: 0.17193391919136047 at epoch: 24 and batch_num: 638\n",
      "Loss of train set: 0.32347241044044495 at epoch: 24 and batch_num: 639\n",
      "Loss of train set: 0.19600534439086914 at epoch: 24 and batch_num: 640\n",
      "Loss of train set: 0.49383655190467834 at epoch: 24 and batch_num: 641\n",
      "Loss of train set: 0.2461819350719452 at epoch: 24 and batch_num: 642\n",
      "Loss of train set: 0.12091600149869919 at epoch: 24 and batch_num: 643\n",
      "Loss of train set: 0.2908174991607666 at epoch: 24 and batch_num: 644\n",
      "Loss of train set: 0.193945050239563 at epoch: 24 and batch_num: 645\n",
      "Loss of train set: 0.39907151460647583 at epoch: 24 and batch_num: 646\n",
      "Loss of train set: 0.30700480937957764 at epoch: 24 and batch_num: 647\n",
      "Loss of train set: 0.4192395806312561 at epoch: 24 and batch_num: 648\n",
      "Loss of train set: 0.29051291942596436 at epoch: 24 and batch_num: 649\n",
      "Loss of train set: 0.2175607532262802 at epoch: 24 and batch_num: 650\n",
      "Loss of train set: 0.14500780403614044 at epoch: 24 and batch_num: 651\n",
      "Loss of train set: 0.46109551191329956 at epoch: 24 and batch_num: 652\n",
      "Loss of train set: 0.3510569632053375 at epoch: 24 and batch_num: 653\n",
      "Loss of train set: 0.2220933735370636 at epoch: 24 and batch_num: 654\n",
      "Loss of train set: 0.32250481843948364 at epoch: 24 and batch_num: 655\n",
      "Loss of train set: 0.3803202211856842 at epoch: 24 and batch_num: 656\n",
      "Loss of train set: 0.29569149017333984 at epoch: 24 and batch_num: 657\n",
      "Loss of train set: 0.27137547731399536 at epoch: 24 and batch_num: 658\n",
      "Loss of train set: 0.2911912798881531 at epoch: 24 and batch_num: 659\n",
      "Loss of train set: 0.15514209866523743 at epoch: 24 and batch_num: 660\n",
      "Loss of train set: 0.2819015383720398 at epoch: 24 and batch_num: 661\n",
      "Loss of train set: 0.33091723918914795 at epoch: 24 and batch_num: 662\n",
      "Loss of train set: 0.2575034499168396 at epoch: 24 and batch_num: 663\n",
      "Loss of train set: 0.23498201370239258 at epoch: 24 and batch_num: 664\n",
      "Loss of train set: 0.2585884928703308 at epoch: 24 and batch_num: 665\n",
      "Loss of train set: 0.1846516877412796 at epoch: 24 and batch_num: 666\n",
      "Loss of train set: 0.5124709606170654 at epoch: 24 and batch_num: 667\n",
      "Loss of train set: 0.3702237010002136 at epoch: 24 and batch_num: 668\n",
      "Loss of train set: 0.26486778259277344 at epoch: 24 and batch_num: 669\n",
      "Loss of train set: 0.23599976301193237 at epoch: 24 and batch_num: 670\n",
      "Loss of train set: 0.31800174713134766 at epoch: 24 and batch_num: 671\n",
      "Loss of train set: 0.3222230076789856 at epoch: 24 and batch_num: 672\n",
      "Loss of train set: 0.3944630026817322 at epoch: 24 and batch_num: 673\n",
      "Loss of train set: 0.24537697434425354 at epoch: 24 and batch_num: 674\n",
      "Loss of train set: 0.17350147664546967 at epoch: 24 and batch_num: 675\n",
      "Loss of train set: 0.3518868088722229 at epoch: 24 and batch_num: 676\n",
      "Loss of train set: 0.22190162539482117 at epoch: 24 and batch_num: 677\n",
      "Loss of train set: 0.29594725370407104 at epoch: 24 and batch_num: 678\n",
      "Loss of train set: 0.3191463351249695 at epoch: 24 and batch_num: 679\n",
      "Loss of train set: 0.18693029880523682 at epoch: 24 and batch_num: 680\n",
      "Loss of train set: 0.17954055964946747 at epoch: 24 and batch_num: 681\n",
      "Loss of train set: 0.2553550601005554 at epoch: 24 and batch_num: 682\n",
      "Loss of train set: 0.26002880930900574 at epoch: 24 and batch_num: 683\n",
      "Loss of train set: 0.2861144542694092 at epoch: 24 and batch_num: 684\n",
      "Loss of train set: 0.29569217562675476 at epoch: 24 and batch_num: 685\n",
      "Loss of train set: 0.2880064845085144 at epoch: 24 and batch_num: 686\n",
      "Loss of train set: 0.2936640679836273 at epoch: 24 and batch_num: 687\n",
      "Loss of train set: 0.3456093966960907 at epoch: 24 and batch_num: 688\n",
      "Loss of train set: 0.47104889154434204 at epoch: 24 and batch_num: 689\n",
      "Loss of train set: 0.3168962299823761 at epoch: 24 and batch_num: 690\n",
      "Loss of train set: 0.25417184829711914 at epoch: 24 and batch_num: 691\n",
      "Loss of train set: 0.276417076587677 at epoch: 24 and batch_num: 692\n",
      "Loss of train set: 0.42487287521362305 at epoch: 24 and batch_num: 693\n",
      "Loss of train set: 0.2652082145214081 at epoch: 24 and batch_num: 694\n",
      "Loss of train set: 0.27496612071990967 at epoch: 24 and batch_num: 695\n",
      "Loss of train set: 0.5430504679679871 at epoch: 24 and batch_num: 696\n",
      "Loss of train set: 0.20744192600250244 at epoch: 24 and batch_num: 697\n",
      "Loss of train set: 0.32348185777664185 at epoch: 24 and batch_num: 698\n",
      "Loss of train set: 0.29034796357154846 at epoch: 24 and batch_num: 699\n",
      "Loss of train set: 0.3251059651374817 at epoch: 24 and batch_num: 700\n",
      "Loss of train set: 0.26929399371147156 at epoch: 24 and batch_num: 701\n",
      "Loss of train set: 0.2657930850982666 at epoch: 24 and batch_num: 702\n",
      "Loss of train set: 0.3237934708595276 at epoch: 24 and batch_num: 703\n",
      "Loss of train set: 0.18643689155578613 at epoch: 24 and batch_num: 704\n",
      "Loss of train set: 0.28276002407073975 at epoch: 24 and batch_num: 705\n",
      "Loss of train set: 0.3169604539871216 at epoch: 24 and batch_num: 706\n",
      "Loss of train set: 0.35540327429771423 at epoch: 24 and batch_num: 707\n",
      "Loss of train set: 0.35372239351272583 at epoch: 24 and batch_num: 708\n",
      "Loss of train set: 0.1687859296798706 at epoch: 24 and batch_num: 709\n",
      "Loss of train set: 0.2537861764431 at epoch: 24 and batch_num: 710\n",
      "Loss of train set: 0.3149667978286743 at epoch: 24 and batch_num: 711\n",
      "Loss of train set: 0.18868578970432281 at epoch: 24 and batch_num: 712\n",
      "Loss of train set: 0.20332331955432892 at epoch: 24 and batch_num: 713\n",
      "Loss of train set: 0.2760889232158661 at epoch: 24 and batch_num: 714\n",
      "Loss of train set: 0.3814900517463684 at epoch: 24 and batch_num: 715\n",
      "Loss of train set: 0.40751540660858154 at epoch: 24 and batch_num: 716\n",
      "Loss of train set: 0.29977452754974365 at epoch: 24 and batch_num: 717\n",
      "Loss of train set: 0.2657460570335388 at epoch: 24 and batch_num: 718\n",
      "Loss of train set: 0.2636595070362091 at epoch: 24 and batch_num: 719\n",
      "Loss of train set: 0.25672364234924316 at epoch: 24 and batch_num: 720\n",
      "Loss of train set: 0.35532841086387634 at epoch: 24 and batch_num: 721\n",
      "Loss of train set: 0.33540403842926025 at epoch: 24 and batch_num: 722\n",
      "Loss of train set: 0.26573121547698975 at epoch: 24 and batch_num: 723\n",
      "Loss of train set: 0.30425482988357544 at epoch: 24 and batch_num: 724\n",
      "Loss of train set: 0.38623929023742676 at epoch: 24 and batch_num: 725\n",
      "Loss of train set: 0.17718461155891418 at epoch: 24 and batch_num: 726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.34853965044021606 at epoch: 24 and batch_num: 727\n",
      "Loss of train set: 0.2890855669975281 at epoch: 24 and batch_num: 728\n",
      "Loss of train set: 0.3217896819114685 at epoch: 24 and batch_num: 729\n",
      "Loss of train set: 0.37010669708251953 at epoch: 24 and batch_num: 730\n",
      "Loss of train set: 0.22128687798976898 at epoch: 24 and batch_num: 731\n",
      "Loss of train set: 0.26358675956726074 at epoch: 24 and batch_num: 732\n",
      "Loss of train set: 0.37288516759872437 at epoch: 24 and batch_num: 733\n",
      "Loss of train set: 0.19327306747436523 at epoch: 24 and batch_num: 734\n",
      "Loss of train set: 0.23932892084121704 at epoch: 24 and batch_num: 735\n",
      "Loss of train set: 0.40406152606010437 at epoch: 24 and batch_num: 736\n",
      "Loss of train set: 0.26316848397254944 at epoch: 24 and batch_num: 737\n",
      "Loss of train set: 0.2721771001815796 at epoch: 24 and batch_num: 738\n",
      "Loss of train set: 0.2834438681602478 at epoch: 24 and batch_num: 739\n",
      "Loss of train set: 0.2752763032913208 at epoch: 24 and batch_num: 740\n",
      "Loss of train set: 0.15616929531097412 at epoch: 24 and batch_num: 741\n",
      "Loss of train set: 0.22022481262683868 at epoch: 24 and batch_num: 742\n",
      "Loss of train set: 0.16713128983974457 at epoch: 24 and batch_num: 743\n",
      "Loss of train set: 0.2746056020259857 at epoch: 24 and batch_num: 744\n",
      "Loss of train set: 0.22049647569656372 at epoch: 24 and batch_num: 745\n",
      "Loss of train set: 0.19972488284111023 at epoch: 24 and batch_num: 746\n",
      "Loss of train set: 0.3948066830635071 at epoch: 24 and batch_num: 747\n",
      "Loss of train set: 0.22189269959926605 at epoch: 24 and batch_num: 748\n",
      "Loss of train set: 0.48223739862442017 at epoch: 24 and batch_num: 749\n",
      "Loss of train set: 0.19574542343616486 at epoch: 24 and batch_num: 750\n",
      "Loss of train set: 0.29853498935699463 at epoch: 24 and batch_num: 751\n",
      "Loss of train set: 0.3506019711494446 at epoch: 24 and batch_num: 752\n",
      "Loss of train set: 0.3637835681438446 at epoch: 24 and batch_num: 753\n",
      "Loss of train set: 0.3732509911060333 at epoch: 24 and batch_num: 754\n",
      "Loss of train set: 0.2910912036895752 at epoch: 24 and batch_num: 755\n",
      "Loss of train set: 0.3132651448249817 at epoch: 24 and batch_num: 756\n",
      "Loss of train set: 0.1787097007036209 at epoch: 24 and batch_num: 757\n",
      "Loss of train set: 0.347812294960022 at epoch: 24 and batch_num: 758\n",
      "Loss of train set: 0.37092697620391846 at epoch: 24 and batch_num: 759\n",
      "Loss of train set: 0.25462743639945984 at epoch: 24 and batch_num: 760\n",
      "Loss of train set: 0.23560093343257904 at epoch: 24 and batch_num: 761\n",
      "Loss of train set: 0.28537389636039734 at epoch: 24 and batch_num: 762\n",
      "Loss of train set: 0.27570128440856934 at epoch: 24 and batch_num: 763\n",
      "Loss of train set: 0.4032667875289917 at epoch: 24 and batch_num: 764\n",
      "Loss of train set: 0.21901413798332214 at epoch: 24 and batch_num: 765\n",
      "Loss of train set: 0.2743414044380188 at epoch: 24 and batch_num: 766\n",
      "Loss of train set: 0.19598433375358582 at epoch: 24 and batch_num: 767\n",
      "Loss of train set: 0.24514466524124146 at epoch: 24 and batch_num: 768\n",
      "Loss of train set: 0.2635570168495178 at epoch: 24 and batch_num: 769\n",
      "Loss of train set: 0.34262216091156006 at epoch: 24 and batch_num: 770\n",
      "Loss of train set: 0.17950966954231262 at epoch: 24 and batch_num: 771\n",
      "Loss of train set: 0.27434059977531433 at epoch: 24 and batch_num: 772\n",
      "Loss of train set: 0.3264908194541931 at epoch: 24 and batch_num: 773\n",
      "Loss of train set: 0.3578801155090332 at epoch: 24 and batch_num: 774\n",
      "Loss of train set: 0.4077063202857971 at epoch: 24 and batch_num: 775\n",
      "Loss of train set: 0.23207485675811768 at epoch: 24 and batch_num: 776\n",
      "Loss of train set: 0.19385161995887756 at epoch: 24 and batch_num: 777\n",
      "Loss of train set: 0.23911842703819275 at epoch: 24 and batch_num: 778\n",
      "Loss of train set: 0.35488277673721313 at epoch: 24 and batch_num: 779\n",
      "Loss of train set: 0.3296940326690674 at epoch: 24 and batch_num: 780\n",
      "Loss of train set: 0.21244296431541443 at epoch: 24 and batch_num: 781\n",
      "Loss of train set: 0.23021574318408966 at epoch: 24 and batch_num: 782\n",
      "Loss of train set: 0.33750343322753906 at epoch: 24 and batch_num: 783\n",
      "Loss of train set: 0.2498214840888977 at epoch: 24 and batch_num: 784\n",
      "Loss of train set: 0.2929999828338623 at epoch: 24 and batch_num: 785\n",
      "Loss of train set: 0.25299370288848877 at epoch: 24 and batch_num: 786\n",
      "Loss of train set: 0.23245614767074585 at epoch: 24 and batch_num: 787\n",
      "Loss of train set: 0.3577919602394104 at epoch: 24 and batch_num: 788\n",
      "Loss of train set: 0.24262049794197083 at epoch: 24 and batch_num: 789\n",
      "Loss of train set: 0.20622096955776215 at epoch: 24 and batch_num: 790\n",
      "Loss of train set: 0.321738600730896 at epoch: 24 and batch_num: 791\n",
      "Loss of train set: 0.344599187374115 at epoch: 24 and batch_num: 792\n",
      "Loss of train set: 0.20830100774765015 at epoch: 24 and batch_num: 793\n",
      "Loss of train set: 0.250171959400177 at epoch: 24 and batch_num: 794\n",
      "Loss of train set: 0.2027914822101593 at epoch: 24 and batch_num: 795\n",
      "Loss of train set: 0.2881065011024475 at epoch: 24 and batch_num: 796\n",
      "Loss of train set: 0.17388930916786194 at epoch: 24 and batch_num: 797\n",
      "Loss of train set: 0.23930597305297852 at epoch: 24 and batch_num: 798\n",
      "Loss of train set: 0.3548315763473511 at epoch: 24 and batch_num: 799\n",
      "Loss of train set: 0.13617345690727234 at epoch: 24 and batch_num: 800\n",
      "Loss of train set: 0.5657438635826111 at epoch: 24 and batch_num: 801\n",
      "Loss of train set: 0.2673334777355194 at epoch: 24 and batch_num: 802\n",
      "Loss of train set: 0.3901458978652954 at epoch: 24 and batch_num: 803\n",
      "Loss of train set: 0.2501521110534668 at epoch: 24 and batch_num: 804\n",
      "Loss of train set: 0.17818230390548706 at epoch: 24 and batch_num: 805\n",
      "Loss of train set: 0.25237715244293213 at epoch: 24 and batch_num: 806\n",
      "Loss of train set: 0.24751724302768707 at epoch: 24 and batch_num: 807\n",
      "Loss of train set: 0.20442748069763184 at epoch: 24 and batch_num: 808\n",
      "Loss of train set: 0.2035466432571411 at epoch: 24 and batch_num: 809\n",
      "Loss of train set: 0.3129937946796417 at epoch: 24 and batch_num: 810\n",
      "Loss of train set: 0.4146672487258911 at epoch: 24 and batch_num: 811\n",
      "Loss of train set: 0.3748544454574585 at epoch: 24 and batch_num: 812\n",
      "Loss of train set: 0.3105456233024597 at epoch: 24 and batch_num: 813\n",
      "Loss of train set: 0.19230309128761292 at epoch: 24 and batch_num: 814\n",
      "Loss of train set: 0.2298721969127655 at epoch: 24 and batch_num: 815\n",
      "Loss of train set: 0.3101026117801666 at epoch: 24 and batch_num: 816\n",
      "Loss of train set: 0.22885364294052124 at epoch: 24 and batch_num: 817\n",
      "Loss of train set: 0.18097902834415436 at epoch: 24 and batch_num: 818\n",
      "Loss of train set: 0.2714617848396301 at epoch: 24 and batch_num: 819\n",
      "Loss of train set: 0.23538358509540558 at epoch: 24 and batch_num: 820\n",
      "Loss of train set: 0.22799339890480042 at epoch: 24 and batch_num: 821\n",
      "Loss of train set: 0.2331053465604782 at epoch: 24 and batch_num: 822\n",
      "Loss of train set: 0.2841516435146332 at epoch: 24 and batch_num: 823\n",
      "Loss of train set: 0.24365665018558502 at epoch: 24 and batch_num: 824\n",
      "Loss of train set: 0.3784744143486023 at epoch: 24 and batch_num: 825\n",
      "Loss of train set: 0.2315688580274582 at epoch: 24 and batch_num: 826\n",
      "Loss of train set: 0.5155307054519653 at epoch: 24 and batch_num: 827\n",
      "Loss of train set: 0.2576662600040436 at epoch: 24 and batch_num: 828\n",
      "Loss of train set: 0.3536829948425293 at epoch: 24 and batch_num: 829\n",
      "Loss of train set: 0.3381395936012268 at epoch: 24 and batch_num: 830\n",
      "Loss of train set: 0.1279289424419403 at epoch: 24 and batch_num: 831\n",
      "Loss of train set: 0.13989755511283875 at epoch: 24 and batch_num: 832\n",
      "Loss of train set: 0.32842332124710083 at epoch: 24 and batch_num: 833\n",
      "Loss of train set: 0.30800527334213257 at epoch: 24 and batch_num: 834\n",
      "Loss of train set: 0.24328278005123138 at epoch: 24 and batch_num: 835\n",
      "Loss of train set: 0.30573585629463196 at epoch: 24 and batch_num: 836\n",
      "Loss of train set: 0.6206523180007935 at epoch: 24 and batch_num: 837\n",
      "Loss of train set: 0.2572736442089081 at epoch: 24 and batch_num: 838\n",
      "Loss of train set: 0.27640554308891296 at epoch: 24 and batch_num: 839\n",
      "Loss of train set: 0.29606324434280396 at epoch: 24 and batch_num: 840\n",
      "Loss of train set: 0.35190969705581665 at epoch: 24 and batch_num: 841\n",
      "Loss of train set: 0.2332853376865387 at epoch: 24 and batch_num: 842\n",
      "Loss of train set: 0.2576533257961273 at epoch: 24 and batch_num: 843\n",
      "Loss of train set: 0.2907671630382538 at epoch: 24 and batch_num: 844\n",
      "Loss of train set: 0.31756365299224854 at epoch: 24 and batch_num: 845\n",
      "Loss of train set: 0.4603550434112549 at epoch: 24 and batch_num: 846\n",
      "Loss of train set: 0.1519625335931778 at epoch: 24 and batch_num: 847\n",
      "Loss of train set: 0.2166081964969635 at epoch: 24 and batch_num: 848\n",
      "Loss of train set: 0.23710770905017853 at epoch: 24 and batch_num: 849\n",
      "Loss of train set: 0.5460911393165588 at epoch: 24 and batch_num: 850\n",
      "Loss of train set: 0.282036691904068 at epoch: 24 and batch_num: 851\n",
      "Loss of train set: 0.33043891191482544 at epoch: 24 and batch_num: 852\n",
      "Loss of train set: 0.2791216969490051 at epoch: 24 and batch_num: 853\n",
      "Loss of train set: 0.24581950902938843 at epoch: 24 and batch_num: 854\n",
      "Loss of train set: 0.16996490955352783 at epoch: 24 and batch_num: 855\n",
      "Loss of train set: 0.24154698848724365 at epoch: 24 and batch_num: 856\n",
      "Loss of train set: 0.22177350521087646 at epoch: 24 and batch_num: 857\n",
      "Loss of train set: 0.1828947514295578 at epoch: 24 and batch_num: 858\n",
      "Loss of train set: 0.2183534950017929 at epoch: 24 and batch_num: 859\n",
      "Loss of train set: 0.23478934168815613 at epoch: 24 and batch_num: 860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of train set: 0.40435105562210083 at epoch: 24 and batch_num: 861\n",
      "Loss of train set: 0.18923525512218475 at epoch: 24 and batch_num: 862\n",
      "Loss of train set: 0.27916666865348816 at epoch: 24 and batch_num: 863\n",
      "Loss of train set: 0.28553855419158936 at epoch: 24 and batch_num: 864\n",
      "Loss of train set: 0.3437321186065674 at epoch: 24 and batch_num: 865\n",
      "Loss of train set: 0.3317660689353943 at epoch: 24 and batch_num: 866\n",
      "Loss of train set: 0.2734929323196411 at epoch: 24 and batch_num: 867\n",
      "Loss of train set: 0.2885282635688782 at epoch: 24 and batch_num: 868\n",
      "Loss of train set: 0.41108566522598267 at epoch: 24 and batch_num: 869\n",
      "Loss of train set: 0.24612955749034882 at epoch: 24 and batch_num: 870\n",
      "Loss of train set: 0.27389878034591675 at epoch: 24 and batch_num: 871\n",
      "Loss of train set: 0.40916743874549866 at epoch: 24 and batch_num: 872\n",
      "Loss of train set: 0.32674646377563477 at epoch: 24 and batch_num: 873\n",
      "Loss of train set: 0.4057670831680298 at epoch: 24 and batch_num: 874\n",
      "Loss of train set: 0.2866353392601013 at epoch: 24 and batch_num: 875\n",
      "Loss of train set: 0.29017311334609985 at epoch: 24 and batch_num: 876\n",
      "Loss of train set: 0.24579551815986633 at epoch: 24 and batch_num: 877\n",
      "Loss of train set: 0.26494520902633667 at epoch: 24 and batch_num: 878\n",
      "Loss of train set: 0.48075830936431885 at epoch: 24 and batch_num: 879\n",
      "Loss of train set: 0.2956898510456085 at epoch: 24 and batch_num: 880\n",
      "Loss of train set: 0.3035643994808197 at epoch: 24 and batch_num: 881\n",
      "Loss of train set: 0.1895359754562378 at epoch: 24 and batch_num: 882\n",
      "Loss of train set: 0.2884475588798523 at epoch: 24 and batch_num: 883\n",
      "Loss of train set: 0.270977646112442 at epoch: 24 and batch_num: 884\n",
      "Loss of train set: 0.2747262120246887 at epoch: 24 and batch_num: 885\n",
      "Loss of train set: 0.25889092683792114 at epoch: 24 and batch_num: 886\n",
      "Loss of train set: 0.3043365180492401 at epoch: 24 and batch_num: 887\n",
      "Loss of train set: 0.18644699454307556 at epoch: 24 and batch_num: 888\n",
      "Loss of train set: 0.14030370116233826 at epoch: 24 and batch_num: 889\n",
      "Loss of train set: 0.2566321790218353 at epoch: 24 and batch_num: 890\n",
      "Loss of train set: 0.2516511082649231 at epoch: 24 and batch_num: 891\n",
      "Loss of train set: 0.2664198577404022 at epoch: 24 and batch_num: 892\n",
      "Loss of train set: 0.2622602880001068 at epoch: 24 and batch_num: 893\n",
      "Loss of train set: 0.15498939156532288 at epoch: 24 and batch_num: 894\n",
      "Loss of train set: 0.21511538326740265 at epoch: 24 and batch_num: 895\n",
      "Loss of train set: 0.2462819367647171 at epoch: 24 and batch_num: 896\n",
      "Loss of train set: 0.2948570251464844 at epoch: 24 and batch_num: 897\n",
      "Loss of train set: 0.2959408760070801 at epoch: 24 and batch_num: 898\n",
      "Loss of train set: 0.5866814255714417 at epoch: 24 and batch_num: 899\n",
      "Loss of train set: 0.3468173146247864 at epoch: 24 and batch_num: 900\n",
      "Loss of train set: 0.23014825582504272 at epoch: 24 and batch_num: 901\n",
      "Loss of train set: 0.2478076070547104 at epoch: 24 and batch_num: 902\n",
      "Loss of train set: 0.2255810648202896 at epoch: 24 and batch_num: 903\n",
      "Loss of train set: 0.10852726548910141 at epoch: 24 and batch_num: 904\n",
      "Loss of train set: 0.27960774302482605 at epoch: 24 and batch_num: 905\n",
      "Loss of train set: 0.2000197470188141 at epoch: 24 and batch_num: 906\n",
      "Loss of train set: 0.21567663550376892 at epoch: 24 and batch_num: 907\n",
      "Loss of train set: 0.26254361867904663 at epoch: 24 and batch_num: 908\n",
      "Loss of train set: 0.39404720067977905 at epoch: 24 and batch_num: 909\n",
      "Loss of train set: 0.2054109275341034 at epoch: 24 and batch_num: 910\n",
      "Loss of train set: 0.1230664774775505 at epoch: 24 and batch_num: 911\n",
      "Loss of train set: 0.17134687304496765 at epoch: 24 and batch_num: 912\n",
      "Loss of train set: 0.4039655327796936 at epoch: 24 and batch_num: 913\n",
      "Loss of train set: 0.23799319565296173 at epoch: 24 and batch_num: 914\n",
      "Loss of train set: 0.33605074882507324 at epoch: 24 and batch_num: 915\n",
      "Loss of train set: 0.12855342030525208 at epoch: 24 and batch_num: 916\n",
      "Loss of train set: 0.22183898091316223 at epoch: 24 and batch_num: 917\n",
      "Loss of train set: 0.18112105131149292 at epoch: 24 and batch_num: 918\n",
      "Loss of train set: 0.35326963663101196 at epoch: 24 and batch_num: 919\n",
      "Loss of train set: 0.26274052262306213 at epoch: 24 and batch_num: 920\n",
      "Loss of train set: 0.22580799460411072 at epoch: 24 and batch_num: 921\n",
      "Loss of train set: 0.314228355884552 at epoch: 24 and batch_num: 922\n",
      "Loss of train set: 0.2927555441856384 at epoch: 24 and batch_num: 923\n",
      "Loss of train set: 0.24782438576221466 at epoch: 24 and batch_num: 924\n",
      "Loss of train set: 0.1552739143371582 at epoch: 24 and batch_num: 925\n",
      "Loss of train set: 0.32199233770370483 at epoch: 24 and batch_num: 926\n",
      "Loss of train set: 0.30471184849739075 at epoch: 24 and batch_num: 927\n",
      "Loss of train set: 0.24688005447387695 at epoch: 24 and batch_num: 928\n",
      "Loss of train set: 0.25463488698005676 at epoch: 24 and batch_num: 929\n",
      "Loss of train set: 0.2865663170814514 at epoch: 24 and batch_num: 930\n",
      "Loss of train set: 0.07454025000333786 at epoch: 24 and batch_num: 931\n",
      "Loss of train set: 0.32301077246665955 at epoch: 24 and batch_num: 932\n",
      "Loss of train set: 0.32087165117263794 at epoch: 24 and batch_num: 933\n",
      "Loss of train set: 0.21007686853408813 at epoch: 24 and batch_num: 934\n",
      "Loss of train set: 0.1947055608034134 at epoch: 24 and batch_num: 935\n",
      "Loss of train set: 0.22634819149971008 at epoch: 24 and batch_num: 936\n",
      "Loss of train set: 0.11734139919281006 at epoch: 24 and batch_num: 937\n",
      "Accuracy of train set: 0.9022166666666667\n",
      "Loss of test set: 0.3164420425891876 at epoch: 24 and batch_num: 0\n",
      "Loss of test set: 0.34374481439590454 at epoch: 24 and batch_num: 1\n",
      "Loss of test set: 0.18246498703956604 at epoch: 24 and batch_num: 2\n",
      "Loss of test set: 0.314092218875885 at epoch: 24 and batch_num: 3\n",
      "Loss of test set: 0.2823163568973541 at epoch: 24 and batch_num: 4\n",
      "Loss of test set: 0.34012311697006226 at epoch: 24 and batch_num: 5\n",
      "Loss of test set: 0.3572644591331482 at epoch: 24 and batch_num: 6\n",
      "Loss of test set: 0.5898854732513428 at epoch: 24 and batch_num: 7\n",
      "Loss of test set: 0.36687594652175903 at epoch: 24 and batch_num: 8\n",
      "Loss of test set: 0.3198363184928894 at epoch: 24 and batch_num: 9\n",
      "Loss of test set: 0.16961127519607544 at epoch: 24 and batch_num: 10\n",
      "Loss of test set: 0.2979285418987274 at epoch: 24 and batch_num: 11\n",
      "Loss of test set: 0.39696720242500305 at epoch: 24 and batch_num: 12\n",
      "Loss of test set: 0.3506157398223877 at epoch: 24 and batch_num: 13\n",
      "Loss of test set: 0.3235739469528198 at epoch: 24 and batch_num: 14\n",
      "Loss of test set: 0.29765784740448 at epoch: 24 and batch_num: 15\n",
      "Loss of test set: 0.30524909496307373 at epoch: 24 and batch_num: 16\n",
      "Loss of test set: 0.28779083490371704 at epoch: 24 and batch_num: 17\n",
      "Loss of test set: 0.32587510347366333 at epoch: 24 and batch_num: 18\n",
      "Loss of test set: 0.3139903247356415 at epoch: 24 and batch_num: 19\n",
      "Loss of test set: 0.3392888903617859 at epoch: 24 and batch_num: 20\n",
      "Loss of test set: 0.32836127281188965 at epoch: 24 and batch_num: 21\n",
      "Loss of test set: 0.4174637198448181 at epoch: 24 and batch_num: 22\n",
      "Loss of test set: 0.6178944706916809 at epoch: 24 and batch_num: 23\n",
      "Loss of test set: 0.2743879556655884 at epoch: 24 and batch_num: 24\n",
      "Loss of test set: 0.6259353160858154 at epoch: 24 and batch_num: 25\n",
      "Loss of test set: 0.30152881145477295 at epoch: 24 and batch_num: 26\n",
      "Loss of test set: 0.3310462534427643 at epoch: 24 and batch_num: 27\n",
      "Loss of test set: 0.3295038044452667 at epoch: 24 and batch_num: 28\n",
      "Loss of test set: 0.3630838096141815 at epoch: 24 and batch_num: 29\n",
      "Loss of test set: 0.2097761034965515 at epoch: 24 and batch_num: 30\n",
      "Loss of test set: 0.31493979692459106 at epoch: 24 and batch_num: 31\n",
      "Loss of test set: 0.3362446129322052 at epoch: 24 and batch_num: 32\n",
      "Loss of test set: 0.3369670510292053 at epoch: 24 and batch_num: 33\n",
      "Loss of test set: 0.3191685080528259 at epoch: 24 and batch_num: 34\n",
      "Loss of test set: 0.30348271131515503 at epoch: 24 and batch_num: 35\n",
      "Loss of test set: 0.39398324489593506 at epoch: 24 and batch_num: 36\n",
      "Loss of test set: 0.2453436404466629 at epoch: 24 and batch_num: 37\n",
      "Loss of test set: 0.2863417863845825 at epoch: 24 and batch_num: 38\n",
      "Loss of test set: 0.2900829017162323 at epoch: 24 and batch_num: 39\n",
      "Loss of test set: 0.44784456491470337 at epoch: 24 and batch_num: 40\n",
      "Loss of test set: 0.3867419958114624 at epoch: 24 and batch_num: 41\n",
      "Loss of test set: 0.6731253862380981 at epoch: 24 and batch_num: 42\n",
      "Loss of test set: 0.3981338143348694 at epoch: 24 and batch_num: 43\n",
      "Loss of test set: 0.46305447816848755 at epoch: 24 and batch_num: 44\n",
      "Loss of test set: 0.2586289048194885 at epoch: 24 and batch_num: 45\n",
      "Loss of test set: 0.33881378173828125 at epoch: 24 and batch_num: 46\n",
      "Loss of test set: 0.3196694850921631 at epoch: 24 and batch_num: 47\n",
      "Loss of test set: 0.23178136348724365 at epoch: 24 and batch_num: 48\n",
      "Loss of test set: 0.3126017451286316 at epoch: 24 and batch_num: 49\n",
      "Loss of test set: 0.2518707513809204 at epoch: 24 and batch_num: 50\n",
      "Loss of test set: 0.5523248910903931 at epoch: 24 and batch_num: 51\n",
      "Loss of test set: 0.3701712489128113 at epoch: 24 and batch_num: 52\n",
      "Loss of test set: 0.41027408838272095 at epoch: 24 and batch_num: 53\n",
      "Loss of test set: 0.5377906560897827 at epoch: 24 and batch_num: 54\n",
      "Loss of test set: 0.30836907029151917 at epoch: 24 and batch_num: 55\n",
      "Loss of test set: 0.4138222932815552 at epoch: 24 and batch_num: 56\n",
      "Loss of test set: 0.27363139390945435 at epoch: 24 and batch_num: 57\n",
      "Loss of test set: 0.6413397789001465 at epoch: 24 and batch_num: 58\n",
      "Loss of test set: 0.36704039573669434 at epoch: 24 and batch_num: 59\n",
      "Loss of test set: 0.13560789823532104 at epoch: 24 and batch_num: 60\n",
      "Loss of test set: 0.323760449886322 at epoch: 24 and batch_num: 61\n",
      "Loss of test set: 0.26008903980255127 at epoch: 24 and batch_num: 62\n",
      "Loss of test set: 0.24619480967521667 at epoch: 24 and batch_num: 63\n",
      "Loss of test set: 0.3282679617404938 at epoch: 24 and batch_num: 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set: 0.564826250076294 at epoch: 24 and batch_num: 65\n",
      "Loss of test set: 0.1980418860912323 at epoch: 24 and batch_num: 66\n",
      "Loss of test set: 0.17230337858200073 at epoch: 24 and batch_num: 67\n",
      "Loss of test set: 0.3130890727043152 at epoch: 24 and batch_num: 68\n",
      "Loss of test set: 0.3547990918159485 at epoch: 24 and batch_num: 69\n",
      "Loss of test set: 0.3757740259170532 at epoch: 24 and batch_num: 70\n",
      "Loss of test set: 0.45331043004989624 at epoch: 24 and batch_num: 71\n",
      "Loss of test set: 0.1850908249616623 at epoch: 24 and batch_num: 72\n",
      "Loss of test set: 0.37194520235061646 at epoch: 24 and batch_num: 73\n",
      "Loss of test set: 0.38116514682769775 at epoch: 24 and batch_num: 74\n",
      "Loss of test set: 0.22003820538520813 at epoch: 24 and batch_num: 75\n",
      "Loss of test set: 0.475101113319397 at epoch: 24 and batch_num: 76\n",
      "Loss of test set: 0.1773070991039276 at epoch: 24 and batch_num: 77\n",
      "Loss of test set: 0.5092042684555054 at epoch: 24 and batch_num: 78\n",
      "Loss of test set: 0.4548003673553467 at epoch: 24 and batch_num: 79\n",
      "Loss of test set: 0.3244704306125641 at epoch: 24 and batch_num: 80\n",
      "Loss of test set: 0.2990588843822479 at epoch: 24 and batch_num: 81\n",
      "Loss of test set: 0.4509287476539612 at epoch: 24 and batch_num: 82\n",
      "Loss of test set: 0.25046205520629883 at epoch: 24 and batch_num: 83\n",
      "Loss of test set: 0.23994308710098267 at epoch: 24 and batch_num: 84\n",
      "Loss of test set: 0.3533429503440857 at epoch: 24 and batch_num: 85\n",
      "Loss of test set: 0.6108273863792419 at epoch: 24 and batch_num: 86\n",
      "Loss of test set: 0.42382296919822693 at epoch: 24 and batch_num: 87\n",
      "Loss of test set: 0.3079049587249756 at epoch: 24 and batch_num: 88\n",
      "Loss of test set: 0.22436708211898804 at epoch: 24 and batch_num: 89\n",
      "Loss of test set: 0.40113186836242676 at epoch: 24 and batch_num: 90\n",
      "Loss of test set: 0.178762286901474 at epoch: 24 and batch_num: 91\n",
      "Loss of test set: 0.4009740352630615 at epoch: 24 and batch_num: 92\n",
      "Loss of test set: 0.2718994617462158 at epoch: 24 and batch_num: 93\n",
      "Loss of test set: 0.24583013355731964 at epoch: 24 and batch_num: 94\n",
      "Loss of test set: 0.47275564074516296 at epoch: 24 and batch_num: 95\n",
      "Loss of test set: 0.5120782852172852 at epoch: 24 and batch_num: 96\n",
      "Loss of test set: 0.3554149866104126 at epoch: 24 and batch_num: 97\n",
      "Loss of test set: 0.271328866481781 at epoch: 24 and batch_num: 98\n",
      "Loss of test set: 0.5607993602752686 at epoch: 24 and batch_num: 99\n",
      "Loss of test set: 0.4470529556274414 at epoch: 24 and batch_num: 100\n",
      "Loss of test set: 0.44751712679862976 at epoch: 24 and batch_num: 101\n",
      "Loss of test set: 0.25534915924072266 at epoch: 24 and batch_num: 102\n",
      "Loss of test set: 0.4598911702632904 at epoch: 24 and batch_num: 103\n",
      "Loss of test set: 0.34892401099205017 at epoch: 24 and batch_num: 104\n",
      "Loss of test set: 0.2371954321861267 at epoch: 24 and batch_num: 105\n",
      "Loss of test set: 0.2084389328956604 at epoch: 24 and batch_num: 106\n",
      "Loss of test set: 0.18331530690193176 at epoch: 24 and batch_num: 107\n",
      "Loss of test set: 0.245791494846344 at epoch: 24 and batch_num: 108\n",
      "Loss of test set: 0.26260867714881897 at epoch: 24 and batch_num: 109\n",
      "Loss of test set: 0.38777005672454834 at epoch: 24 and batch_num: 110\n",
      "Loss of test set: 0.4123223125934601 at epoch: 24 and batch_num: 111\n",
      "Loss of test set: 0.31399106979370117 at epoch: 24 and batch_num: 112\n",
      "Loss of test set: 0.35895758867263794 at epoch: 24 and batch_num: 113\n",
      "Loss of test set: 0.3610811233520508 at epoch: 24 and batch_num: 114\n",
      "Loss of test set: 0.36225181818008423 at epoch: 24 and batch_num: 115\n",
      "Loss of test set: 0.3043680787086487 at epoch: 24 and batch_num: 116\n",
      "Loss of test set: 0.2301979660987854 at epoch: 24 and batch_num: 117\n",
      "Loss of test set: 0.32821160554885864 at epoch: 24 and batch_num: 118\n",
      "Loss of test set: 0.3740066587924957 at epoch: 24 and batch_num: 119\n",
      "Loss of test set: 0.33134952187538147 at epoch: 24 and batch_num: 120\n",
      "Loss of test set: 0.33931541442871094 at epoch: 24 and batch_num: 121\n",
      "Loss of test set: 0.3654758334159851 at epoch: 24 and batch_num: 122\n",
      "Loss of test set: 0.49506980180740356 at epoch: 24 and batch_num: 123\n",
      "Loss of test set: 0.49957039952278137 at epoch: 24 and batch_num: 124\n",
      "Loss of test set: 0.5012022256851196 at epoch: 24 and batch_num: 125\n",
      "Loss of test set: 0.41577598452568054 at epoch: 24 and batch_num: 126\n",
      "Loss of test set: 0.37858647108078003 at epoch: 24 and batch_num: 127\n",
      "Loss of test set: 0.4122835397720337 at epoch: 24 and batch_num: 128\n",
      "Loss of test set: 0.2187076210975647 at epoch: 24 and batch_num: 129\n",
      "Loss of test set: 0.39678454399108887 at epoch: 24 and batch_num: 130\n",
      "Loss of test set: 0.27895110845565796 at epoch: 24 and batch_num: 131\n",
      "Loss of test set: 0.3295667767524719 at epoch: 24 and batch_num: 132\n",
      "Loss of test set: 0.21876969933509827 at epoch: 24 and batch_num: 133\n",
      "Loss of test set: 0.38562488555908203 at epoch: 24 and batch_num: 134\n",
      "Loss of test set: 0.42873135209083557 at epoch: 24 and batch_num: 135\n",
      "Loss of test set: 0.2786284387111664 at epoch: 24 and batch_num: 136\n",
      "Loss of test set: 0.2223013937473297 at epoch: 24 and batch_num: 137\n",
      "Loss of test set: 0.20530669391155243 at epoch: 24 and batch_num: 138\n",
      "Loss of test set: 0.254447877407074 at epoch: 24 and batch_num: 139\n",
      "Loss of test set: 0.4801793396472931 at epoch: 24 and batch_num: 140\n",
      "Loss of test set: 0.4361669421195984 at epoch: 24 and batch_num: 141\n",
      "Loss of test set: 0.49351343512535095 at epoch: 24 and batch_num: 142\n",
      "Loss of test set: 0.27182918787002563 at epoch: 24 and batch_num: 143\n",
      "Loss of test set: 0.49149712920188904 at epoch: 24 and batch_num: 144\n",
      "Loss of test set: 0.3393283188343048 at epoch: 24 and batch_num: 145\n",
      "Loss of test set: 0.2124253511428833 at epoch: 24 and batch_num: 146\n",
      "Loss of test set: 0.36490654945373535 at epoch: 24 and batch_num: 147\n",
      "Loss of test set: 0.25901275873184204 at epoch: 24 and batch_num: 148\n",
      "Loss of test set: 0.2891054153442383 at epoch: 24 and batch_num: 149\n",
      "Loss of test set: 0.37044018507003784 at epoch: 24 and batch_num: 150\n",
      "Loss of test set: 0.26074549555778503 at epoch: 24 and batch_num: 151\n",
      "Loss of test set: 0.6041727662086487 at epoch: 24 and batch_num: 152\n",
      "Loss of test set: 0.3217942714691162 at epoch: 24 and batch_num: 153\n",
      "Loss of test set: 0.3909366726875305 at epoch: 24 and batch_num: 154\n",
      "Loss of test set: 0.36730265617370605 at epoch: 24 and batch_num: 155\n",
      "Loss of test set: 0.08943486213684082 at epoch: 24 and batch_num: 156\n",
      "Accuracy of FashionMNIST set: 0.8748\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkUAAAHFCAYAAAD8Jo2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XlY1OXawPHvMAz7puwgCuIG4r7v+0JqZnnU7JiWWuabZZodS8O0jmaLeU6mbe6aWpoecykpzSX3JddUXEEWQVT2gWHm9/4xMomACwIzwP25Li5mnnnmN/c8InPzrCpFURSEEEIIISo5K3MHIIQQQghhCSQpEkIIIYRAkiIhhBBCCECSIiGEEEIIQJIiIYQQQghAkiIhhBBCCECSIiGEEEIIQJIiIYQQQghAkiIhhBBCCECSIiEs2n//+19UKhVhYWHmDqXcOH78OCqVismTJxdZJyoqCpVKxWuvvWYq++WXX+jZsyd+fn7Y2tri5+dH586d+fDDDx/4miNGjMDJySlf2fz581myZEmx30dJKSqOK1euoFKpLCJGISyFJEVCWLBFixYBcPr0aQ4cOGDmaMqHRo0a0axZM5YtW4Zery+0zuLFiwEYOXIkAF9++SW9e/fGxcWFefPm8csvvzB79mxCQkJYu3ZtseKw9KTI19eXffv20adPn7IPSggLZW3uAIQQhTt8+DDHjx+nT58+bN68mYULF9KqVStzh1WorKws7OzsUKlU5g4FMCY7Y8eOZevWrfTt2zffY3q9nmXLltGsWTMaNWoEwKxZs+jYsWOBBGjYsGEYDIYyi/tBFEVBq9Vib2//2NeytbWldevWJRCVEBWH9BQJYaEWLlwIwIcffkjbtm1ZvXo1mZmZBeplZ2czY8YMQkJCsLOzw93dnS5durB3715THYPBwOeff07jxo2xt7fHzc2N1q1bs3HjRlMdlUrFe++9V+D6gYGBjBgxwnR/yZIlqFQqtm3bxosvvoinpycODg5kZ2dz4cIFXnjhBWrXro2DgwP+/v7069ePkydPFrju7du3mThxIjVr1sTW1hYvLy+eeOIJzp49i6Io1K5dm169ehV4Xnp6Oq6urvzf//1fkW03dOhQ7O3tTT1Cd9u2bRuxsbG8+OKLprLk5GR8fX0LvZaV1aP/mgwMDOT06dPs3LkTlUqFSqUiMDDQ9HhqaipvvvkmQUFB2NjY4O/vz/jx48nIyMh3HZVKxauvvsqXX35JSEgItra2LF26FIDp06fTqlUrqlatiouLC02bNmXhwoXcfcb3/eIoavhsz549dOvWDWdnZxwcHGjbti2bN2/OVyfvZ2DHjh288soreHh44O7uztNPP01cXNwjt5cQlkJ6ioSwQFlZWaxatYoWLVoQFhbGiy++yKhRo/jhhx8YPny4qV5ubi7h4eHs3r2b8ePH07VrV3Jzc9m/fz/R0dG0bdsWMM55WbFiBSNHjmTGjBnY2Nhw9OhRrly5UuwYX3zxRfr06cPy5cvJyMhAo9EQFxeHu7s7H374IZ6enty8eZOlS5fSqlUrjh07Rt26dQFIS0ujffv2XLlyhX/961+0atWK9PR0du3aRXx8PPXq1WPcuHGMHz+eqKgoateubXrdZcuWkZqaet+kyNXVlWeeeYY1a9aQlJSEp6en6bHFixdjZ2fH0KFDTWVt2rRh3bp1vPfeewwYMICwsDDUanWx22b9+vUMHDgQV1dX5s+fDxh7ZgAyMzPp1KkT165d45133qFhw4acPn2aiIgITp48ya+//pqvx23Dhg3s3r2biIgIfHx88PLyAoxJzcsvv0z16tUB2L9/P+PGjSM2NpaIiIgHxlGYnTt30qNHDxo2bMjChQuxtbVl/vz59OvXj1WrVjF48OB89UeNGkWfPn347rvviImJYdKkSfzzn/9k+/btxW47IcxKEUJYnGXLlimA8uWXXyqKoihpaWmKk5OT0qFDh0LrffPNN0Vea9euXQqgTJky5b6vCSjTpk0rUF6jRg1l+PDhpvuLFy9WAOX5559/4PvIzc1VcnJylNq1aytvvPGGqXzGjBkKoERGRhb53NTUVMXZ2Vl5/fXX85WHhoYqXbp0eeBr79ixQwGUOXPmmMqSk5MVW1tb5bnnnstX98KFC0pYWJgCKIBib2+vdOvWTZk3b56Sk5PzwNcaPny44ujomK+sfv36SqdOnQrUnTVrlmJlZaUcOnQoX/natWsVQNmyZYupDFBcXV2Vmzdv3vf19Xq9otPplBkzZiju7u6KwWB4YByXL19WAGXx4sWmstatWyteXl5KWlqaqSw3N1cJCwtTqlWrZrpu3s/A2LFj813zo48+UgAlPj7+vvEKYalk+EwIC7Rw4ULs7e0ZMmQIAE5OTvzjH/9g9+7dREVFmept3boVOzu7fENB99q6dSvAfXtWiuOZZ54pUJabm8vMmTMJDQ3FxsYGa2trbGxsiIqK4q+//soXU506dejevXuR13d2duaFF15gyZIlpmGl7du3c+bMGV599dUHxtepUyeCg4PzDaGtXLmS7OzsAu0VHBzM8ePH2blzJ9OnT6d79+4cOnSIV199lTZt2qDVah/4eg9r06ZNhIWF0bhxY3Jzc01fvXr1QqVS8fvvv+er37VrV6pUqVLgOtu3b6d79+64urqiVqvRaDRERESQnJxMYmLiI8eVkZHBgQMHGDhwYL6VdGq1mmHDhnHt2jXOnTuX7zlPPvlkvvsNGzYE4OrVq4/8+kJYAkmKhLAwFy5cYNeuXfTp0wdFUbh9+za3b99m4MCBwN8r0gCSkpLw8/O777yXpKQk1Go1Pj4+JRpnYXNwJkyYwLvvvstTTz3FTz/9xIEDBzh06BCNGjUiKysrX0zVqlV74GuMGzeOtLQ0Vq5cCcC8efOoVq0a/fv3f+BzVSoVL774IidPnuTw4cOAcegsKCiILl26FKhvZWVFx44diYiIYOPGjcTFxTF48GCOHDmSr80f1/Xr1zlx4gQajSbfl7OzM4qicOPGjXz1C2vngwcP0rNnTwC++eYb/vjjDw4dOsSUKVMA8rX1w7p16xaKohT6en5+foBx7tXd3N3d893PG5orzusLYQlkTpEQFmbRokUoisLatWsLXQ6+dOlSPvjgA9RqNZ6enuzZsweDwVBkYuTp6YlerychIaHIycRg/EDLzs4uUH7vB2GewlaarVixgueff56ZM2fmK79x4wZubm75Yrp27VqRseSpVasW4eHhfPHFF4SHh7Nx40amT5/+0PN9RowYQUREBIsWLUKj0XDs2DHef//9h1ol5+joyNtvv82aNWs4derUQ73ew/Dw8MDe3r7IRMvDwyPf/cJiXb16NRqNhk2bNmFnZ2cq37BhQ7HjqlKlClZWVsTHxxd4LG/y9L2xCVHRSE+REBZEr9ezdOlSgoOD2bFjR4GviRMnEh8fbxoSCw8PR6vV3nc/nPDwcAAWLFhw39cODAzkxIkT+cq2b99Oenr6Q8evUqkKTOTdvHkzsbGxBWI6f/78Q03Iff311zlx4gTDhw9HrVYzevToh47Hz8+P3r17s2rVKr744gusrKzyTVTPU1giAJiG/PJ6Sh6Fra1toT0mffv25eLFi7i7u9O8efMCX3evUiuKSqXC2to6X3KYlZXF8uXLHzqOezk6OtKqVSt+/PHHfPUNBgMrVqygWrVq1KlT54HXEaI8k54iISzI1q1biYuLY/bs2XTu3LnA42FhYcybN4+FCxfSt29fnn32WRYvXsyYMWM4d+4cXbp0wWAwcODAAUJCQhgyZAgdOnRg2LBhfPDBB1y/fp2+fftia2vLsWPHcHBwYNy4cYBxT553332XiIgIOnXqxJkzZ5g3bx6urq4PHX/fvn1ZsmQJ9erVo2HDhhw5coSPP/64wFDZ+PHjWbNmDf3792fy5Mm0bNmSrKwsdu7cSd++ffMNb/Xo0YPQ0FB27NjBP//5T9Pqq4c1cuRINm/ezLfffkuvXr0ICAgoUKd+/fp069aN8PBwgoOD0Wq1HDhwgE8//RRvb2/TJo+PokGDBqxevZo1a9ZQs2ZN7OzsaNCgAePHj2fdunV07NiRN954g4YNG2IwGIiOjmbbtm1MnDjxgftR9enThzlz5jB06FBeeuklkpOT+eSTTwpdWVZUHIWZNWsWPXr0oEuXLrz55pvY2Ngwf/58Tp06xapVqyxmHyohSo1553kLIe721FNPKTY2NkpiYmKRdYYMGaJYW1srCQkJiqIoSlZWlhIREaHUrl1bsbGxUdzd3ZWuXbsqe/fuNT1Hr9crn332mRIWFqbY2Ngorq6uSps2bZSffvrJVCc7O1t56623lICAAMXe3l7p1KmT8ueffxa5+uze1VOKoii3bt1SRo4cqXh5eSkODg5K+/btld27dyudOnUqsALq1q1byuuvv65Ur15d0Wg0ipeXl9KnTx/l7NmzBa773nvvKYCyf//+h21Kk5ycHMXb21sBlO+//77QOl999ZXy9NNPKzVr1lQcHBwUGxsbJTg4WBkzZowSExPzwNcobPXZlStXlJ49eyrOzs4KoNSoUcP0WHp6ujJ16lSlbt26pn+PBg0aKG+88Ybp31VRjKvP/u///q/Q11y0aJFSt25dxdbWVqlZs6Yya9YsZeHChQqgXL58+YFxFLb6TFEUZffu3UrXrl0VR0dHxd7eXmndunW+nxNFKfpnIG/F344dOx7YZkJYIpWi3LXTlxBCWKDmzZujUqk4dOiQuUMRQlRgMnwmhLBIqampnDp1ik2bNnHkyBHWr19v7pCEEBWcJEVCCIt09OhRunTpgru7O9OmTeOpp54yd0hCiApOhs+EEEIIIZAl+UIIIYQQgCRFQgghhBCAJEVCCCGEEIBMtC6UwWAgLi4OZ2dn2axMCCGEKCcURSEtLe2BZ0IWRZKiQsTFxRW6660QQgghLF9MTMxDHTp9L0mKCuHs7AwYG9XFxaVEr63T6di2bRs9e/ZEo9GU6LVF0aTdzUPa3Tyk3c1D2t087m73rKwsAgICTJ/jj0qSokLkDZm5uLiUSlLk4OCAi4uL/KcpQ9Lu5iHtbh7S7uYh7W4ehbV7cae+yERrIYQQQggkKRJCCCGEACQpEkIIIYQAZE7RY9Hr9eh0ukd6jk6nw9raGq1Wi16vL6XIyheNRoNarTZ3GEIIISo5SYqKQVEUEhISuH37drGe6+PjQ0xMjOyBdBc3Nzd8fHykTYQQQpiNJEXFkJcQeXl54eDg8Egf5AaDgfT0dJycnIq1sVRFoygKmZmZJCYmAuDr62vmiIQQQlRWkhQ9Ir1eb0qI3N3dH/n5BoOBnJwc7OzsJCm6w97eHoDExES8vLxkKE0IIYRZyKfyI8qbQ+Tg4GDmSCqWvPZ81DlaQgghREmRpKiYZO5LyZL2FEIIYW6SFAkhhBBCIEmRKCGtW7dm8uTJ5g5DCCGEKDaZaF1JPGh4avjw4SxZsqTY19+yZQs2NjbFfr4QQghhbpIUVRLx8fGm22vWrCEiIoJz586ZyvJWgN1Lp9M91MGGVatWffwghRBCVArXU7WkaXOp5eVk7lDykeGzSsLHx8f05erqikqlKlB29uxZVCoVP/74Ix06dMDW1pa1a9dy/fp1Bg0ahL+/Pw4ODjRq1Ih169blu/69w2c+Pj588sknPP/88zg5OREYGPhYPVFCCCHKL61Oz+6oJP69+Qy95+6i1czf+HDrX+YOqwDpKSoBiqKQpXu4IzsMBgNZOXqsc3JLZJ8ie426xFdu/etf/+KTTz6hYcOG2Nvbk5WVRdu2bXnnnXdwdnbmf//7H4MHD+bw4cM0bty4yOvMnj2bmTNnEhERwXfffcfo0aPp1KkTQUFBJRqvEEIIy6IoClGJ6ew6n8SuqBscuJRMdq7B9LhKBZk5ehRFsajVx5IUlYAsnZ7QiF/M8tpnZvTCwaZk/xnffPNN+vfvn69s/PjxptsTJkxg8+bNrF279r5J0VNPPcXo0aMBmDp1KnPmzGHnzp2SFAkhRAV0KyOHPRdusOt8ErujbpCQqs33uI+LHR1qe9Cxjifta3lQxdHy5qFKUiQKaN68eb77ubm5zJw5kx9++IHY2FhycnLIzs7G39//vtdp2LCh6baVlRXe3t6m4zyEEEKUbzq9gWPRt+8kQUmciE1BUf5+3NbailY13el4JxGq7eVkUb1ChZGkqATYa9ScmdHroeoaDAbSUtNwdnEuseGzkubo6Jjv/syZM/niiy+YO3cuoaGhODo68sorr5CTk3Pf69w7QVulUmEwGIqoLYQQwtJdTc5gV5SxN2jfxWTSs3PzPV7Px9nUG9QisCp2pfAZVZokKSoBKpXqoYewDAYDuTZqHGysy83ZZ7t372bgwIE8++yzgLHnKCoqqlhnvwkhhChfFEXh178S+XTbOc4mpOV7rKqjDe1reZgSIW8XOzNFWTIkKRIPVKtWLX7++WcOHDiAs7Mzs2fP5tatW+YOSwghRCk7cvUmH249y6Erxt/51lYqmtWoQsc6nnSs7Ul9PxesrCx7SOxRSFIkHmjGjBnExMTQrVs3nJ2dGTt2LOHh4eYOSwghRCm5kJjOx7+c5ZfT1wHj/KAX2wcxpmMwrg4P3ruuvJKkqBIaMWIEI0aMKFBer149lLtnyd3h6enJpk2b7nvN/fv357ufkJBQoM7Zs2cfLVAhhBBlKjFVy2e/RvH94Rj0BgUrFfyjWQBv9KiDj2v5Hhp7GGaf1DJ//nyCgoKws7OjWbNm7N69+771v/jiC0JCQrC3t6du3bosW7asQJ1169YRGhqKra0toaGhrF+/vrTCF0IIIcq9NK2OT7edo9PHv7PqYDR6g0L3EG9+Gd+R2QMbVoqECMzcU7RmzRrGjx/P/PnzadeuHV999RXh4eGcOXOG6tWrF6i/YMEC3n77bb755htatGjBwYMHGT16NFWqVKFfv34A7Nu3j8GDB/P+++8zYMAA1q9fz6BBg9izZw+tWrUq67cohBBCWKycXAMrD1zl8+0XuJlhXFHctLobk8NDaBlU+Y5vMmtSNGfOHEaOHMmoUaMAmDt3Lr/88gsLFixg1qxZBeovX76cl19+mcGDBwNQs2ZN9u/fz+zZs01J0dy5c+nRowdvv/02AG+//TY7d+5k7ty5rFq1qozemRBCCGG5DAaFTSfj+eSXc0TfzASgpqcjb/WqR6/63ha/n1BpMVtSlJOTw5EjR/KdlwXQs2dP9u7dW+hzsrOzsbPL34Vnb2/PwYMHTQeX7tu3jzfeeCNfnV69ejF37twiY8nOziY7O9t0PzU1FTAehqrT6fLV1el0KIqCwWAo1p47eXN28q4hjAwGA4qioNPpUKtLfl+LvH/He/89RemSdjcPaXfzKC/tvvdiMh9vi+JUnPGzztPJhnFdg/lHU3+s1Vbk5uY+4AqW5e52f9y2N1tSdOPGDfR6Pd7e3vnKvb29C52kC8bk5ttvv+Wpp56iadOmHDlyhEWLFqHT6bhx4wa+vr4kJCQ80jUBZs2axfTp0wuUb9u2DQcHh3xl1tbW+Pj4kJ6e/sDNC+8nLS3twZUqkZycHLKysti1a1ep/oeMjIwstWuLokm7m4e0u3lYartfy4CfrlpxNsU4ndhWrdDNz0Bn30xsk06y7ZeTZo7w8URGRpKZmflY1zD76rN7u+judzjcu+++S0JCAq1bt0ZRFLy9vRkxYgQfffRRvt6FR7kmGIfYJkyYYLqfmppKQEAAPXv2xMXFJV9drVZLTEwMTk5OBXqtHoaiKKSlpeHs7FxpuycLo9Vqsbe3p2PHjsVq1wfR6XRERkbSo0ePAjtti9Ij7W4e0u7mYantfu1WFnN/u8DGk/EoCmjUKp5tEcDYzjVxt8Dzxx7V3e2elZX1WNcyW1Lk4eGBWq0u0IOTmJhYoKcnj729PYsWLeKrr77i+vXr+Pr68vXXX+Ps7IyHhwcAPj4+j3RNAFtbW2xtbQuUazSaAj/Yer0elUqFlZVVsXakzhsyy7uGMLKyskKlUhXa5iWptK8vCiftbh7S7uZR1u2uKAq3M3UkpGpJSNWSmKolISXbeD8liz8uJJOjN372PNnIj4k961DD3fEBVy1/NBrNY480mC0psrGxoVmzZkRGRjJgwABTeWRkZIET2u+l0WioVq0aAKtXr6Zv376mBKNNmzZERkbmm1e0bds22rZtWwrvQgghhCg9Wp2e66larqcak5zrKcbE5/qdr4Q7j+Xk3n+Oarta7kzuHUKDaq5lFHn5ZNbhswkTJjBs2DCaN29OmzZt+Prrr4mOjmbMmDGAcVgrNjbWtBfR+fPnOXjwIK1ateLWrVvMmTOHU6dOsXTpUtM1X3/9dTp27Mjs2bPp378///vf//j111/Zs2ePWd6jEEII8bC0Oj2L/7jCxuNxxKdkcTvz4ScOV3W0wdvFDm8XW3xc7PB2scPH1Y463s40re4mUzYeglmTosGDB5OcnMyMGTOIj48nLCyMLVu2UKNGDQDi4+OJjo421dfr9Xz66aecO3cOjUZDly5d2Lt3L4GBgaY6bdu2ZfXq1UydOpV3332X4OBg1qxZI3sUCSGEsFgGg8LG43F8/Ms5Ym/nnxdja22Fj+udJOdO0pOX8OQlP14utthal68T6S2R2Sdajx07lrFjxxb62JIlS/LdDwkJ4dixYw+85sCBAxk4cGBJhFdhPOgvhOHDhxdo70c1efJkfv/99wJHfgghhCjagUvJ/HvLX5y4lgKAn6sd47vXoVGAGz4udrjYW0svTxkxe1IkykZ8fLzp9po1a4iIiODcuXOmMnt7e3OEJYQQldalpHRmbT1L5BnjoatOtta80jmYke2DsNNIr485yPKnSsLHx8f05erqikqlKlAGcPXqVQYOHIirqyseHh48/fTTxMTEmK4TGRlJ8+bNcXBwoEqVKnTo0IG4uDi+/PJLZs+ezYEDB1CpVKhUKlavXm2utyuEEBYrOT2baf87Rc/PdhF55jpqKxXDWtfg90md+b8utSQhMiPpKSoJigK6h9wwymAw1s1RQ0ksydc4QAl1q6alpdG5c2d69+7NH3/8gUqlYvr06fTp04ejR4+Sm5vLgAEDGD9+PN9//z1ardY0VDZ8+HBOnz7N3r172bx5MwBubm4lEpcQQlQEWp2eJXuv8MX2C6RlG5eOdw/xYnJ4PWp5OZs5OgGSFJUMXSbM9HuoqlZAiaYK78SBTcnsN7F8+XJcXV1ZsGCBqWzp0qW4urqyd+9eatWqRUZGBv369aNmzZoAhIaGmuo6Ojqi0Wjw8fEpkXiEEKIiMBgUfjoRx0c//z2Jur6fC1OeCKFtLQ8zRyfuJkmRMDly5AinT5/GyckpX3lubi4XL16kY8eODBkyhC5dutCjRw+6d+/OoEGD7rsxphBCVGYHLiUzc8tfHL8zidrX1Y5JveryVGN/rKxk8rSlkaSoJGgcjD02D8FgMJCaloaLs3PJ7GitcXhwnYdkMBho06YNixYtKvCYl5cXAKtWreLIkSP8/PPPrFixgqlTp7Jjxw6aNm1aYnEIIUR5dykpnQ+3nmXbnUnUjjZqxnapxYvtgrC3kTlDlkqSopKgUj38EJbBABq9sb6FHfPRtGlTtmzZgq+vL46ORb+fZs2a0axZM6ZMmUKTJk1YvXo1TZs2xcbGBr1eX4YRCyGEZUnXwYzNZ1l1MIZcg4LaSsWQFgGM714HT+eCx0kJyyJJkTAZPnw4n332GQMGDGDatGn4+flx5coV1q1bR0REBLdu3WL58uX07dsXPz8/Tp8+zeXLlwkJCQEgMDCQqKgoTp48ia+vLy4uLtjYlP/DBoUQ4l46vYGULF2+rxPRt1hwTI1Wb9x0uFs94yTq2t4yibq8kKRImLi4uLB7927+9a9/0b9/f9LT06lWrRo9evTA0dERnU7HiRMnWLhwITdv3sTf358333yTESNGAMYdyjdu3EiHDh1ISUlh1apVDBkyxLxvSggh7iM9O5frqVpTYpOal+Rk5k94bt/9WJaOzJyiesVVhPg4M7VvKO1kEnW5I0lRJTRixAhTInMvf39/VqxYUehjjo6ObNy4scjrOjo6smHDhpIIUQghSo2iKOy7mMzKA9FsO5OATq8U+1rOtta42GtwtddQxUFDIIlEPN8aW1vpJS+PJCkSQghRKdzMyGHdkWt8dzCayzcyTOXOdta43kls7v1yKaI87zH1XSvIdDodW7ZskVVl5ZgkRUIIISosRVE4dOUWKw9cZevJBHL0BsB4pMZTTfwY2rIGoX4uZo5SWApJioQQQlQ4KZk61h019gpdSEw3lTfwd+W5VtXp18gPR1v5CBT5yU+EEEKICkFRFI7F3Gbl/mg2nYgjO9fYK2SvUdO/sR9DW1WnYTU5fkgUTZKiYlKU4k/MEwVJewohiitNq2PDn3Gs3H+VswlppvJ6Ps4816o6/Zv442KnMWOEoryQpOgRaTTG/1iZmZnY29ubOZqKIzPTeKBuXvsKIcSDnLyWwsoDV9l4PM60RN7W2oq+Df14rnV1mgS4oSqhA7NF5SBJ0SNSq9W4ubmRmJgIgIODwyP9pzMYDOTk5KDVakvmmI9yTlEUMjMzSUxMxM3NDbVatr8XQhQtTatj04l4vjsQzcnYFFN5LS8nhraszjNNq+HqIH9cieKRpKgY8k6Bz0uMHoWiKGRlZWFvby9/wdzFzc3N1K5CCHG3vBVkaw7FsOVkPFk6Y6+QjdqK8AY+DG1ZnZZBVeV3qnhskhQVg0qlwtfXFy8vL3Q63SM9V6fTsWvXLjp27ChDRXdoNBrpIRJCFHA9Vcu6o9f44fC1fPsKBXs6MrhFAAObBVDVUTZJFCVHkqLHoFarH/nDXK1Wk5ubi52dnSRFQghxD53ewPaziXx/KIbfzyehNxgXYTjaqOnb0I9BLarRtHoV6RUSpUKSIiGEEGZ3ITGN7w9f48ej17iRnmMqb16jCoNaBNCnga/sKyRKnfyECSGEMIv07Fw2HY/j+8MxHI2+bSr3cLLlmWb+DGoeQLCnkxkjFJWNJEVCCCHKjKIoHL5qnDS9+cTfk6bVViq61vNiUPMAOtf1RKOW1bmi7ElSJIQQotQlpmpZdzSWHw7HcOmuSdM1PR04sC5QAAAgAElEQVQZ1DyAp5v64+VsZ8YIhZCkSAghRCnJzMll2+nr/Hgslj1RSdyZM42DjZq+DX0Z1DyAZjVk0rSwHJIUCSGEKDEGg8L+S8n8eCyWrSfjybiz0zTcmTTdPIAnGvriJJOmhQWSn0ohhBCP7UJiGj8ejWXDsVjiUrSm8upVHXi6qT8DmvhTw93RjBEK8WCSFAkhhCiW5PRsfjoex4/HYjlx7e8jN1zsrOnT0I9nmvrL8JgoVyQpEkII8dC0Oj3bzyby49Fr/H4uidw7E4WsrVR0ruvJ002r0bWeF3Ya2aVelD+SFAkhhLivvGX0Px6NZdOJONK0uabHGlZz5ekm/vRr5Ie7k60ZoxTi8UlSJIQQolBJadms2H+V9cdiib6ZaSr3dbVjQBN/nm7qTy0vZzNGKETJkqRICCFEPjm5BpbuvcJ/f4siLdvYK+Rooya8gS9PN/GndU13rKxknpCoeCQpEkIIYbL97HXe3/SX6VT6Bv6ujGwfRM/63jjYyEeGqNjkJ1wIIQQXEtP5YPMZfj+XBBjPH3urd10GNq0mvUKi0pCkSAghKrFUrY7//hrFkr1XyDUoaNQqXmwXxKtda+FspzF3eEKUKUmKhBCiEtIbFH44HMPHv5wjOSMHgG71vJjaN5QgD9lkUVROkhQJIUQlc/DyTab/dJrTcakABHs68m7fUDrX9TJzZEKYlyRFQghRScTdzmLW1rP8dDwOAGc7a8Z3r8PzbWqgUVuZOTohzE+SIiGEMDO9QWHb6QQyc/QEezlR09MRlxKcz5OVo+frXZdYsPMCWp0BlQqGtKjOmz3ryIaLQtxFkiIhhDCjI1dv8u6G05yJT81X7uVsS7CnE7W8nAj2dCTYy4lgTyd8Xe0e+iwxRVHYdCKOWVvOEns7C4CWgVWJ6BdKmL9rib8XIco7SYqEEMIMktKy+XDrWdYdvQYYD1Gt7+fKpRvpXE/NJjHN+LXvUnK+5znYqKnp6Uiwp5Ppq5aXEzXcHfKdN3YtA55bdJhDV24B4Odqxzt9QujTwFcOaBWiCJIUCSFEGcrVG1ix/yqfRp43nSE2uHkAb/WuaxrKStPquJiUwcXEdC4m5X1lcOVGBpk5ek7FpnIqNn/PkpUKAqo6EOzphK1axc+n1Sjcwk5jxZhOwbzcMRh7GzmkVYj7MXtSNH/+fD7++GPi4+OpX78+c+fOpUOHDkXWX7lyJR999BFRUVG4urrSu3dvPvnkE9zd3QHQ6XTMmjWLpUuXEhsbS926dZk9eza9e/cuq7ckhBCFOnTlJu9uOMXZhDTAuFv0jP71aVK9Sr56znYaGge40TjALV+5Tm8g+mbmnWQpg4tJ6Vy4kzilaXO5mpzJ1eS8M8pU9Anz4Z2+ofi72ZfF2xOi3DNrUrRmzRrGjx/P/PnzadeuHV999RXh4eGcOXOG6tWrF6i/Z88enn/+eT777DP69etHbGwsY8aMYdSoUaxfvx6AqVOnsmLFCr755hvq1avHL7/8woABA9i7dy9NmjQp67cohBAkpWUza+tf/Hg0FgBXew1v9a7LkBbVUT/CbtEatZVpyOxuiqKQlJ7NxURjohR7MwNNchTjBjdEo5ENGIV4WGZdgzlnzhxGjhzJqFGjCAkJYe7cuQQEBLBgwYJC6+/fv5/AwEBee+01goKCaN++PS+//DKHDx821Vm+fDnvvPMOTzzxBDVr1uSVV16hV69efPrpp2X1toQQAjAOlS3ac5mun/zOj0djUang2ZYB7HizM8+1qvFICdH9qFQqvJztaBPszj9b12BCj9oEu5TIpYWoVMzWU5STk8ORI0eYPHlyvvKePXuyd+/eQp/Ttm1bpkyZwpYtWwgPDycxMZG1a9fSp08fU53s7Gzs7OzyPc/e3p49e/YUGUt2djbZ2dmm+6mpxrF6nU6HTqd75Pd2P3nXK+nrivuTdjePytzuh67cYvqmvzh3PR2ABv4uTOsbQqNqxlVfpdkmlbndzUna3TzubvfHbXuVoihKSQT1qOLi4vD39+ePP/6gbdu2pvKZM2eydOlSzp07V+jz1q5dywsvvIBWqyU3N5cnn3yStWvXmrqIhw4dyvHjx9mwYQPBwcH89ttv9O/fH71eny/xudt7773H9OnTC5R/9913ODg4lMC7FUJUFik5sPGqFYdvGDviHawV+lU30NpLQc5VFaJ0ZWZmMnToUFJSUnBxefTuUrNPtL53aaiiKEUuFz1z5gyvvfYaERER9OrVi/j4eCZNmsSYMWNYuHAhAP/5z38YPXo09erVQ6VSERwczAsvvMDixYuLjOHtt99mwoQJpvupqakEBATQs2fPYjXq/eh0OiIjI+nRo4eM9ZchaXfzqEztnqs3sPxADP/ZfoGMbD0qFQxqVo2JPWpRxcGmTGOpTO1uSaTdzePuds/Kynqsa5ktKfLw8ECtVpOQkJCvPDExEW9v70KfM2vWLNq1a8ekSZMAaNiwIY6OjnTo0IEPPvgAX19fPD092bBhA1qtluTkZPz8/Jg8eTJBQUFFxmJra4utbcFdXTUaTan9YJfmtUXRpN3No6K3+4FLyUT87zTnrhtXlTWq5sqM/mE0umf1WFmr6O1uqaTdzUOj0ZCbm/tY1zBbUmRjY0OzZs2IjIxkwIABpvLIyEj69+9f6HMyMzOxts4fslpt3Hfj3lFAOzs7/P390el0rFu3jkGDBpXwOxBCVHbXU7XM2vIXG/40niVWxUHDW73rMbh5AFYyViZEuWPW4bMJEyYwbNgwmjdvTps2bfj666+Jjo5mzJgxgHFYKzY2lmXLlgHQr18/Ro8ezYIFC0zDZ+PHj6dly5b4+fkBcODAAWJjY2ncuDGxsbG89957GAwG3nrrLbO9TyFExRKdnMnXuy/y/eFr5OQazxIb2rI6b/asSxXHsh0qE0KUHLMmRYMHDyY5OZkZM2YQHx9PWFgYW7ZsoUaNGgDEx8cTHR1tqj9ixAjS0tKYN28eEydOxM3Nja5duzJ79mxTHa1Wy9SpU7l06RJOTk488cQTLF++HDc383ZjCyHKv1OxKXy58yJbTsZjuNM53axGFab1C6VhNfkdI0R5Z/aJ1mPHjmXs2LGFPrZkyZICZePGjWPcuHFFXq9Tp06cOXOmpMITQlRyiqKw72IyC3ZeZHfUDVN5pzqevNI5mFZBVeUsMSEqCLMnRUIIYYn0BoVfTifw5c6LnLiWAoDaSkXfhr683DGYUD/ZHVGIikaSIiGEuItWp2f9sVi+3nWJyzcyALC1tmJwiwBGd6hJQFXZu0yIikqSIiGEAFK1Olbuj2bRH5dJSjNu9Opqr2F4mxoMbxtoOsFeCFFxSVIkhKjUElO1LPrjCiv3XyUt27jHia+rHSPbB/Fsy+o42sqvSSEqC/nfLoSolC7fyODrXRdZdySWHL0BgNpeTrzcKZgnG/lhY23W87KFEGYgSZEQolI5HnObr3ZdZOupBJS7ltWP6RRMt3pesumiEJWYJEVCiApPb1CIPHOdhXsucejKLVN5t3pejOkcTIvAqmaMTghhKSQpEkJUWBnZuaw9co1Ff1zmanImABq1in6N/Hi5YzB1fZzNHKEQwpJIUiSEqHASUrQs3XeF7w5Ek5KlA4wryf7ZujrPtwnE28XOvAEKISySJEVCiArjdFwKC3dfZuPxOHLvnMMR6O7AyPZBPNOsGg428itPCFE0+Q0hhCjXDAaF388n8s2uy+y7lGwqbxlUlVHtg+gW4o1aJk8LIR6CJEVCiHJJq9Oz7ug1Fu65zKUk487TecdwjGwfJAe0CiEemSRFQohyJSktm+X7rrDiQDQ3M3IAcLazZmjL6gxvG4ifm715AxRClFuSFAkhyoVzCWks3HOJDcfiTJstVqtiz4vtghjUIgAn2XlaCPGY5LeIEMKi3czI4YPNZ/jxaKyprGl1N0Z1qEnPUG+s1bLztBCiZEhSJISwSIqi8L8/45ix6Qw3M3JQqSA8zIeR7WvSrEYVc4cnhKiAJCkSQlicmJuZTNlwil3nkwCo6+3MrGca0LS6JENCiNIjSZEQwmLk6g0s2XuFT7edJ0unx8baite61uKljsFyQKsQotRJUiSEsAin41KYvO4kJ2NTAGgVVJVZTzegpqeTmSMTQlQWkhQJIcwqK0fP3N/O8+3uy+gNCi521rzzRAiDmgfIifVCiDIlSZEQwmz+uHCDd9afNB3W2qeBL9OeDMXLWc4mE0KUPUmKhBBl7lZGDh9s/ot1R68B4Otqx/v9w+ge6m3myIQQlZkkRUKIMqMoChuPxzHjpzMk31lm/3zrGrzZqy7OdhpzhyeEqOQkKRJClIlrtzKZuuEUv58zLrOv4+3ErKcbyp5DQgiLIUmREKJUGRRYvPcqc3+7QGaOHhu1FeO61uLlTrLMXghhWSQpEkKY6A0KBy4lcz1Ni9rKCmsrFVYqFdZWKtRqFeo7t62s7pTd9WVtZYXaCtRWVqhVxvqxN9P57KSa6IxzALQMrMrMpxtQy0uW2QshLI8kRUIIrqdq+f5QDKsPxRB7O6uEr67C2c6at8NDGNJCltkLISyXJEVCVFJ6g8KuqCRWHYjmt7OJ6A0KAC521jSs5obeoKBXFPQGhVyDguHOd73BYHysQPmd+vo79xUFRVEIc9Mzb2Q7/KtK75AQwrJJUiREJZOQouX7wzGsuadXqEVgFYa2qk54mC92GnWJvJZOp2PLli14OduWyPWEEKI0SVIkRCWgNyjsOp/Edwej2X5Xr5CrvYZnmlbj2ZYB1PZ2NnOUQghhXpIUCVGBxadk8f2ha6w5FE1citZU3jKoKkNbVqd3mE+J9QoJIUR5J0mREBWM3qCw83wi3x0w9grd6RTCzeHvXqFaXtIrJIQQ95KkSIgKIj4lizWHYvj+UEyBXqHnWlWnV33pFRJCiPuRpEiIcu789TTm/nqen08lmHqFqtzpFRrSsrrsCSSEEA9JkiIhyqkrNzL4z29RbPgzFuVOMtS6ZlWebSm9QkIIURySFAlRzsTdzuLz7VF8f/iaaRVZeJgP47vXoa6PzBUSQojikqRIiHIiKS2bL3Zc4LsD0eToDQB0qevJxJ51CfN3NXN0QghR/klSJISFu52Zw5c7L7F07xWydHrAOEz2Zs+6NA+saubohBCi4pCkSAgLlabVsXDPZRbuvkxadi4AjQPcmNSrLm2D3VGp5AwxIYQoSZIUCWFhsnL0LNt3hS93XuRWpg6AEF8XJvaoQ7cQL0mGhBCilEhSJISFyM7Vs/pgDPN2XCApLRuAmp6OTOhRhyfCfOV0eSGEKGWSFAlhZrl6A+uOXuO/v10wHdBarYo947vX4anGflirrcwcoRBCVA6SFAlhJgaDwk8n4vgs8jxXkjMB8Hax5dWutRncPAAba0mGhBCiLJn9t+78+fMJCgrCzs6OZs2asXv37vvWX7lyJY0aNcLBwQFfX19eeOEFkpOT89WZO3cudevWxd7enoCAAN544w20Wm0RVxSi7Gl1ekYvO8zrq//kSnImVR1tmNonhJ2TujCsdQ1JiIQQwgzM+pt3zZo1jB8/nilTpnDs2DE6dOhAeHg40dHRhdbfs2cPzz//PCNHjuT06dP88MMPHDp0iFGjRpnqrFy5ksmTJzNt2jT++usvFi5cyJo1a3j77bfL6m0JcV9ZOXpGLT3Mb2cTsdNY8WbPOux6qwujOtSUXaiFEMKMzJoUzZkzh5EjRzJq1ChCQkKYO3cuAQEBLFiwoND6+/fvJzAwkNdee42goCDat2/Pyy+/zOHDh0119u3bR7t27Rg6dCiBgYH07NmTZ599Nl8dIcwlPTuX4YsPsufCDRxs1Cx5oSWvdq2Nk62MZAshhLmZ7TdxTk4OR44cYfLkyfnKe/bsyd69ewt9Ttu2bZkyZQpbtmwhPDycxMRE1q5dS58+fUx12rdvz4oVKzh48CAtW7bk0qVLbNmyheHDhxcZS3Z2NtnZ2ab7qampAOh0OnQ63eO8zQLyrlfS1xX3ZwntnqbVMXLZUY7FpOBka83C55vSNMClQv8sWEK7V0bS7uYh7W4ed7f747a9SlHyjpIsW3Fxcfj7+/PHH3/Qtm1bU/nMmTNZunQp586dK/R5a9eu5YUXXkCr1ZKbm8uTTz7J2rVr0Wg0pjqff/45EydORFEUcnNzeeWVV5g/f36Rsbz33ntMnz69QPl3332Hg4PDY7xLIYwydLDgLzUxGSoc1AqvhOqpLofXCyFEicrMzGTo0KGkpKTg4uLyyM83e5/9vRvRKYpS5OZ0Z86c4bXXXiMiIoJevXoRHx/PpEmTGDNmDAsXLgTg999/59///jfz58+nVatWXLhwgddffx1fX1/efffdQq/79ttvM2HCBNP91NRUAgIC6NmzZ7Ea9X50Oh2RkZH06NEjXyInSpc52z05I4cRS44Qk5FGFQcNS0Y0I9S3ZH+uLJX8vJuHtLt5SLubx93tnpWV9VjXMltS5OHhgVqtJiEhIV95YmIi3t7ehT5n1qxZtGvXjkmTJgHQsGFDHB0d6dChAx988IEp8Rk2bJhp8nWDBg3IyMjgpZdeYsqUKVhZFZxGZWtri62tbYFyjUZTaj/YpXltUbSybvfENC3DFh0mKjEdDydbvhvdijrele8ke/l5Nw9pd/OQdjcPjUZDbm7uY13DbBOtbWxsaNasGZGRkfnKIyMj8w2n3S0zM7NAUqNWG1fr5I0CFlVHURTMNFIoKqn4lCyGfLWfqMR0fFzs+P7l1pUyIRJCiPLCrMNnEyZMYNiwYTRv3pw2bdrw9ddfEx0dzZgxYwDjsFZsbCzLli0DoF+/fowePZoFCxaYhs/Gjx9Py5Yt8fPzM9WZM2cOTZo0MQ2fvfvuuzz55JOmBEqI0hZzM5Oh3+4n5mYW/m72rBrdmuruMj9NCCEsmVmTosGDB5OcnMyMGTOIj48nLCyMLVu2UKNGDQDi4+Pz7Vk0YsQI0tLSmDdvHhMnTsTNzY2uXbsye/ZsU52pU6eiUqmYOnUqsbGxeHp60q9fP/7973+X+fsTldOVGxk89+0BYm9nUcPdgZWjWlGtiiREQghh6cw+0Xrs2LGMHTu20MeWLFlSoGzcuHGMGzeuyOtZW1szbdo0pk2bVlIhCvHQLiSm89y3+7memk1NT0e+G9UaH1c7c4clhBDiIZg9KRKiojiXkMZz3+7nRnoOdb2dWTGqFZ7OBSfwCyGEsEySFAlRAk7FpjBs4QFuZeoI9XVhxahWVHW0MXdYQgghHoGcOinEY/oz5jZDv9nPrUwdjaq5smp0a0mIhBCWYfu/YVl/0KaaO5JyQZIiIR7D4Ss3+ee3B0jV5tK8RhVWjGqFq4PsTyKEsAB6HfzxH7j0OxxfZe5oygVJioQopr0Xb/D8ooOkZ+fSumZVlr7YEmc7SYiEEBYi6Rzo75zreWQJyF59DyRJkRDFsPN8Ei8sPkRmjp4OtT1YPKIljnLSvRDCksQf//t24hm4dsh8sZQTkhQJ8Yh+++s6o5ceJjvXQLd6XnzzfHPsbWRjUCGEhclLilR3PuqPLDFbKOWF/GkrxF0UReFmRg4JqVqup2q5nppNQkrebS0JqdlEXU8j16DQu74P/322CTbW8reFEMIC5SVFzV+EQ9/CqR+h10ywdzNvXBZMkiJRaeTo4WpyJjcyc40JToox6TEmO8b7SWnZ5OgND7zWU439+OQfjbBWS0IkhLBABj0knDTebjEKru41DqGd/AFajjZvbBZMkiJR4R2Puc1rq45x9aY1HNzzUM/xcLLB28UOHxc7vF3t8Ha2w8fVFm8XO6pVcaCWl1MpRy2EEI8h+SLoMsDaHjzqQLMRsPUtOLzYmCSpVOaO0CJJUiQqtP2Xkhm55BAZOXoA7DVW+Lja4+1ie0/CY2dMglzt8HSylSExIUT5ljd05tMArNTQcBBERkDiaYg9AtWamzc+CyVJkaiwdpxNZMyKI2TnGmgdVIV+7kn848lwbGxkY0UhRAUX/6fxu28j43f7KlB/gHG/oiOLJSkqgvw5LCqkzSfieWn53yvEvh3WFCcNqKTLWAhRGeT1FOUlRWAcQgPjhGttSpmHVB5IUiQqnO8PxzBu1VF0eoW+DX35clgzbDWyZF4IUUkYDH8nRX6N/y4PaAWe9UCXaZxwLQqQpEhUKIv/uMxba09gUGBIiwD+M6QJGlkhJoSoTG5fgexUUNsYk6A8KtXfvUWHl8gO14WQTwtRISiKwrztUUz/6QwAI9sHMevpBqitZLhMCFHJ5PUSedcH9T1HDzUcDGpbuH4S4o6WfWwWTpIiUe4pisKHP5/lk23nAXi9W22m9gmR+UNCiMqpsPlEeRyqQv2njLdlh+sCJCkS5ZrBoDB1wym+2nkJgClPhPBGjzqSEAkhKq+4e1ae3StvCO3kOtCmlklI5YUkRaLcytUbmPjDcVYeiEalgpkDGjC6Y01zhyWEEOajKPfvKQKo3sa4oaMuA06tLbvYygFJikS5lJ2r5/++O8r6Y7GorVTMHdyYoa2qmzssIYQwr5RrkHUTrKzBq37hde6ecG2uIbQjSy2yl0qSIlHuZObkMmrpYX45fR0bayu+/Gcz+jf2N3dYQghhfnm9RJ4hoLErul6jZ42r0+KPQ9yxsoktz1+b4KfX4Mt2oMsq29d+AEmKRLmSqtXx/MKD7I66gb1GzeIRLegR6m3usIQQwjI8aOgsj0NVCO1vvF2WvUXaFNjypvF22DOgsS+7134IkhSJciM5PZuh3+zn8NVbONtZs2JUS9rV8jB3WEIIYTkeNimCuyZcr4XstFILKZ9fp0NaPFStCZ3+VTav+QgkKRLlQkKKlsFf7+dUbCrujjasfqk1zWpUNXdYorKLOQgXfjV3FEL87VGSohrtwL0W5KQbE6PSFr0fDi803u73H4vrJQJJikQ5EHMzk398tZcLien4uNix5uU21PdzNXdYorLTpsKyp2DFMxBzyNzRCAFpCZCeACor8Al7cP2ynHCdmw0bxxlvN/knBHUs3dcrJkmKhEW7kJjGwC/3EnMzi+pVHfhhTBtqeTmZOywh4NwW45JmgN+my5EJwvzyeok86oCN48M9p9HQOxOu/yzdCde758CN8+DoCT3eL73XeUySFAmLdfJaCoO+2s/11Gxqeznxw5g2BFR1MHdYQhidWvf37Su74eJv5otFCHi0obM8ju4Q0s94+8jSko8JIPEs7P7UeDt8tnGSt4WSpEhYDL1B4cjVW3y67Rx9P99Nv3l7uJmRQwN/V9a83AZvl/ssLxWiLGXehIvbjbfrhBu//zrdeDq5EOZSnKQI7ppw/QNkp5doSBgMxuX3Bh3U6Q31ny7Z65cwa3MHICq3Wxk57IpKYsfZRHaeT+JWpi7f493qefHZkMa42GmKuIIQZvDXRjDkgk8D6P8F/KcRJJyAM+uNy4yFMIfiJkWBHYyrwW5eMvaANhtecjEdXggxB8DGCfp8apzHZMEkKRJlSlEUTsel8vu5RHacS+JY9C0Md03FcLazpmMdT7rW9aJjHU88nW3NF6wQRckbOgt7xjj80O412PFv2P4BhDxZ8GRyIUpbRjKkxBhv+zR4tOfmTbiOjDBOuC6ppCgl1tiDCtAtAlyrlcx1S5EkRaLUpWl1/HHhBtvPJvL7uSQS07LzPV7Px5ku9bzoUteLptXdsFbLqK6wYGkJcHm38XbeUEDrsXDgK+Nf2seWQ/MXzRefqJwS7vQSVa0JdsVYndtoKPz2PsQdNfY4PWpv070UBbZMgpw08G8OLUY93vXKiCRFosQpisLFpHS2n01kx9kkDl25Se5d3UEONmra1fKgS10vOtf1xM/N8vaqEKJIZ/4HKFCtBVSpYSyzdYJOb8HWt+D32dBwCNjIogBRhuL+NH73bVy85zt5QkhfOL3eOOG675zHi+evjXBus/EMtic/Byv1412vjBQrKVq6dCkeHh706dMHgLfeeouvv/6a0NBQVq1aRY0aNUo0SFE+6A0K3+y+xIr9V7l2K/95NjU9HOlc14uu9bxoEVQFW+vy8R9EiALuHjq7W7MRsG8e3I6Gg19B+zfKPDRRiRV3PtHdmo0wJkUnvoee7z/8sv57Zd0y9hKB8f+Bd2jxYypjxRqnmDlzJvb2xr/u9+3bx7x58/joo4/w8PDgjTfkF0FlFJ+SxdBv9vPh1rNcu5WFjbUVnep48l6/UH5/szPb3+xMRL9Q2tf2kIRIlF+3o42TRlFB6FP5H7O2hS5TjLf3fGb8YBCirJREUhTYEaoEGYe8Tv1Y/OtEToP06+BeGzq8WfzrmEGxeopiYmKoVasWABs2bGDgwIG89NJLtGvXjs6dO5dkfKIc2HY6gbfWneB2pg5HGzXv9g3lycZ+ONjI6KyoYE6vN34PbA8uvgUfb/AP+OM/kHjG+L37e2UZnaissm7DrcvG24+TFFlZGSdZ//qeccJ102GPfo0re+Donf2O+v0HNOVrK5Vi9RQ5OTmRnJwMwLZt2+jevTsAdnZ2ZGVl3e+pogLR6vRM+98pXlp+hNuZOhr4u7LptQ4MaVldEiJRMZmGzorYa8VKbVxlA7D/S0iNL5u4SlPmTXNHUD7odcaDTs2xs3nCSeN31+qPvzFi4+eM84BiD/993Yel08JPrxtvNx0Oge0eLxYzKNYnV48ePRg1ahRNmjTh/PnzprlFp0+fJjAwsCTjExYq6noa41Yd42yC8WTllzrW5M2edbGxlpVjooK6ccE4RGFlDSH9i65XpzcEtDIOs+36CPp+VnYxlrS9n8O2qdB2nPFoBgvfY6ZMGPRw6wok/mX8Srrz/UYUGoOOOr4DgT5lG5Np6Kzh41/LyQvq9TEuKDiyFPp88vDP3f0JJF8AJ2/oMePxYzGDYiVFX3zxBVOnTiUmJoZ169bh7u4OwJEjR3j22WdLNEBhWRRFYdXBGGZsOo1WZ8DDyYZPBzWmUx1Pc4cmROk6fYYlmBwAACAASURBVGeORc0uxr2JiqJSGYfNFocbP1TavAruwWURYcm7us/4fe/noE2BvnPLzSqix6YokBp7J/k5c1cSdA5yix4RCUjeVfa9RfGPufLsXs1GGJOiE2uMyc3DrKS8fto4lw7giY/B3q1kYiljxUqK3NzcmDdvXoHy6dOnP3ZAwnKlZOp4e/0JtpxMAKBDbQ8+HdQIL+fyNWYsxCNTFDi51nj7YXasrtEWaveEqG3GTR0HLird+EpLauzft48uMx4BMeArsLYxX0ylIT3prsTnzveks5CdWnh9azvwrAteoeBZz/i9Sg2UL9vjlJOILjkKfOuXXfx5PUV+JZQUBXUGtxpw+6pxHl2T5+5f36CHja8Zd3mv28e4gWk5Vayk6Oeff8bJyYn27dsDxp6jb775htDQUL744guqVKlSokEK8zt85Savr/6T2NtZWFupeKt3XUa1r4mVlXSni0og8QzcOAdqW6j3xMM9p1uEMSk6tQ7avf74m+GZQ2qc8XuHifDHf429ZTnpMGgZaCrA/mK3rsB3g40JUGGsrI0rqLxCjImPV14CFFhoj5lSowOqS79hFfVz2SVF2elwI8p4u6R+xvImXP82wzjh+kFJ0aFvjXOQbJyNw23leJi1WBNAJk2aRGqqMYM+efIkEydO5IknnuDSpUtMmDChRAMU5qU3KPz3tygGfbWP2NtZ1HB3YN0rbXmpY7AkRKLyyJtgXbvHw+8W7NPAuBoNjB8u5U1uDmQkGW+3HgvPrgJre2Oit2IgaIvoRSkv9DpYO/JOQqQy7gRdry90nGTs2XtlH7wTD/+3H/6xGP6fvTuPi6reHz/+mhkGEAVcUEBFQMt9qTBTlDJLzUqt7u1a3TJNK9M0s/y2eG+ZLda9v8zUtFtp1s02zWyRSrqauWSpue87LuCCCyACs5zfHx9mABlggJk5A7yfj4ePOQxnzvnM8Tjz9vN5f96fGyaq1eQbtSp1CFG7sj8Ahr0/+e59nNwOaBAarfKBPOWq+1VQeOwPNTRWmvNHC+/vvpMhrKnn2qCDSgVFhw4don17VYzpq6++4vbbb+e1115j9uzZ/PDDDx5toNCPo/bQtJS92DW46+pmLB2XRJeY6jlWLESlaFrpBRvLc+Pz6otl/8+FS4NUF9npgAamQAhppALCBxar3oAjq+HjwdV7ZtqKV1XvRnA4PLEZxm2CexZAn3+ov+fI9hUeJrS3LgiKjq9Xa5H5gifqE7kSGgltBqjtjR+53kfTYOlTqvcwpjskVP/lbSoVFAUGBpKTkwPAzz//TL9+/QBo2LChswfJXbNnzyY+Pp7g4GASEhJYtarsD44FCxbQpUsXQkJCiI6OZvjw4c7yAAC9e/fGYDCU+OOYISfcs2xHOgPeXsXvh85SN9DEtL91YdqQq6gXJFPtRS1z4k81zGKuCwVfem5r2FIlrQL87yV9pmtXlmPoLKxp4XBIbCIM+w7qNFTX5cNb1Vpw1c2BFbB6utoeNFMNh3lCWDPO12mBQbOrHjVf8FZQBIX37tbPIT+n5O93LIZ9P4HRrGoSGav/7ONKvYNevXoxYcIEXn75Zf744w9nwLF3716aN3d/FdwvvviC8ePHM2nSJDZt2kRSUhIDBgwgNTXV5f6rV69m6NChjBgxgh07drBw4ULWr1/PyJGFC80tXryYtLQ055/t27djMpm4++67K/NWa51ci41/LimsPdS5eThLxyVx1zX+v7qxEF7hqOzbZkDllj24fiKYQ+DYetiT7Nm2eZMjyTqsWfHnm14Nw39QwzWnd8G8/iporC6yT8PXjwIaJAyH9mWUV6iEk+FXqw1f/V17Myhq2Qfqt1AzD3d+U/x3OWfhh2fUdtJTKt+qBqhUUDRr1iwCAgJYtGgRc+bMoVkz9Y/mhx9+4JZbbnH7ONOmTWPEiBGMHDmSdu3aMX36dGJiYpgzZ47L/detW0dcXBzjxo0jPj6eXr168eijj7JhwwbnPg0bNiQqKsr5JyUlhZCQEAmK3LD3ZBaDZ63hv+uOAKr20KJRicRFVHL9GyGqO7u9MCiq6NCZQ2gUdH9Mbf9vipqpUx04eopCXVTubtIWHvpR9bCcOwzzBqip6v7Oboclo9QSFI3bwS1TPX6KdEdQdGA5WPM8fvxiLJfUTDnw3HT8ooxGVYQRVMJ1USn/VDlnEW0gqebkElcqKGrRogXff/89W7ZsYcSIEc7n33rrLWbMmOHWMfLz89m4caNz6M2hX79+rF271uVrEhMTOXbsGMnJyWiaxsmTJ1m0aFGZQ2Nz587lnnvuoW5d+WIvjaZpfPp7KoNmrWbPySwi6gXy0UPdeP7WdlKMUdRuR9dB1gkICocrbqr8cRLHQXB9ldS79QvPtc+bHNW4S0ucbRAHw39UU9KzTqi6TI6V2v3VundUfldAsEqe9sIMuvN14tDqRao8m8NeziM7uRM0G4REeC/B+er7wWBS/xYcAdjBlbDpE7U9aIZa96+GqHSCiM1mY8mSJezatQuDwUC7du0YPHgwJpN7hb3OnDmDzWYjMjKy2PORkZGkp7seo05MTGTBggUMGTKE3NxcrFYrgwYNYubMmS73/+OPP9i+fTtz584tsy15eXnk5RVG9I68KIvFgsVicev9uMtxPE8ftzKsNjspu04xb+0RNh+9AEDSFY341186ElEvyC/a6Cn+dN1rk+p+3Y1bF2IC7G1uw6YZobLvI6AuxsQnMC1/CW35q1jbDPLqF4knrrvpwjGMgK1eFPbSjlMnAu7/BtPnQzCmbUb76HZsQz5Di+le6fN6i+HEn5h+nowBsPV9BXuDKyr/91kKi8UCBiO2ljcTsHUBtl3J2GNv8Og5ijIe26juz6jO2KxW75wkuBGm1rdg3LMU2/oPsd/4DwK+e0Jdx2uGY49O8Ph1rKii93tVP2sqFRTt37+fW2+9lePHj9OmTRs0TWPv3r3ExMSwdOlSWrVyv3qr4bJ6BpqmlXjOYefOnYwbN44XXniB/v37k5aWxsSJExk1apTLwGfu3Ll07NiRbt26ldmGqVOnuiw8uWzZMkJC3KjkWQkpKSleOa47cm2w7pSBlWlGzuapax1g0LithZ3eESf549eTurXN2/S87rVZdbzuBs1G/+0qKFp3sRmnk6uWI2K0x3CzuQF1Mo+x+5P/42CTCiZtV0JVrntS6g4aAhv3pZF2uuz3HtD4Ma7LmkZE9h74712sbzmO02EeWHLCQwJsl+i9+5/UtVs5Uf9a1qc1hir+fZZlQ3YTugN5W5eQYrvea3V7uqQuJQ7Yf7Euu7z4fppY29GDpdj+/ISjhw/T6twhLpkbsNx6HVYvnreiUlJSnJPAKsugaRWfDnHrrbeiaRoLFiygYUO1+FxGRgb3338/RqORpUuXlnuM/Px8QkJCWLhwIXfeeafz+SeeeILNmzezcuXKEq954IEHyM3NZeHChc7nVq9eTVJSEidOnCA6unDsOycnh+joaKZMmcITTzxRZltc9RTFxMRw5swZwsLCyn0vFWGxWEhJSaFv376YzWaPHrs8aRdy+ei3I3yx4TjZeep/FQ1CzNzXLYb7r4shol7N6QK9nJ7XvTarztfdcPAXAj77K1pII6xP7FBT66t6zE0fE5A8QR1z9AYICvVAS0vyxHUPmNkFQ+ZxrMN+QmuW4MZJL2H6ajjGAz+jGc3Y7nwPre3ASp3bozQN0zejMO74Ci2sOdaRv3htCQrnde/dizozO2CwXsIy8heI7OiV85nm3oQxfQvWu+ahebOKtN1GwOyuGC4cdT5l/evHaG3cLGTqZUXv90uXLhEREcGFCxcq9f1dqX/lK1euZN26dc6ACKBRo0a8/vrr9Ozp3qq4gYGBJCQkkJKSUiwoSklJYfBg17MBcnJyCAgo3mTHcN3lsd2XX35JXl4e999/f7ltCQoKIiioZEBgNpu99kHuzWNfbuux83yw6hBLt6Vhs6vr1LJxXUb2asld1zQj2FxL1jLCt9ddFKqW1333EgAM7e/AHOSh3JOEB+H32Rgy9mPe8B70ftYzxy1Fpa+73eacah/QsAW4cwyzWRV4/PoRDDu+JmDxCBg0q/xqyN62aQHs+AoMJgx/nYc5zPvrNJpDwjC0uhH2JGM+8DM0v9rzJ7Hmq9l/QEDza9z7O6o0s0q4XvGK+rHdIAI6enbWnieYzWasVRxGrFQWbVBQEFlZWSWez87OJjDQ/WJXEyZM4IMPPmDevHns2rWLJ598ktTUVEaNGgXAc889x9ChQ537Dxw4kMWLFzNnzhwOHjzImjVrGDduHN26daNp0+JJZnPnzuWOO+5wLlZb29jtGst2pPO3//zGoFlr+HbLCWx2jR4tGzFvWFd+fvIG7ruuRa0KiIRwmzUPdn2ntis768wVU4AqDghqkdWLZzx3bE+6eFol8BpMasVzdwUEwl/mwjVDQbPDN6Nh3bvea2d5zuyD5KfV9o3PQYvrfHfu1gUzsb01Nf/0LrDlq+KTnqqzVJar71e1uoLrqwVfa6hK9RTdfvvtPPLII8ydO9eZr/P7778zatQoBg1yvwtvyJAhZGRkMGXKFNLS0ujYsSPJycnExsYCkJaWVqxm0bBhw8jKymLWrFk89dRT1K9fnz59+vDGG28UO+7evXtZvXo1y5b5qHiWH8nJt/LVxmPMXX2IwxlqbDXAaGBQl6Y81Cuejs3cXKJAiNrswHJVmyU0Glr08Oyx2w1W06fTNsOqN70yLbzKHDWKQqNKXdKiVEYTDJwBQWHw2yz48Rm1sOr1E327JpY1DxYNB0sOxCVBLx9PG3cU+jzxp+p1C43y7PGL1ifyxXUNi4ZRq9TMPU+/Fz9SqaBoxowZPPjgg/To0cPZNWuxWBg8eDDTp0+v0LFGjx7N6NGjXf5u/vz5JZ4bO3YsY8eOLfOYrVu3LjGcVtOdzMzl498Os+D3VM7nqOz7sOAA/t49lgd7xBEVLivZC+E2x7IeHe7yfJVeoxFufhH+e6daSLP7Y6pAnj8pWs26MgwG6PeKCox+eU0tqZF7QT3nq8Ao5QVI36aWKLnr/YoHd1UVGgXNEuD4Rtj7kxo69SRvFm0sTSP3J1FVV5UKiurXr88333zD/v372bVrF5qm0b59e6644gpPt0+UY+eJTD5YfZDvtpzAYlOBYGyjEB7qGc9fE5pTV5blEKJi8nNgd8GQhyeHzopqeSPEXw+HfoVfXoc7ZnvnPJVVVuFGdxkM0PsZCA6DH59VvUZ5mTDg32D28n/S9vwAvxcM290xR/Vy6KH1ABUU7fnBi0GRF4o21mJuf2NOmFB21+Mvv/zi3J42bVqlGyTcs/dkFlO+28nq/YU5CdfGNWBEr5b0bR+JSVawF6Jy9v0ElotQPxaaXeOdcxgMcNNk+KAPbPkMEsdCk3beOVdllLbER2V0fwwC68F34+DPj9XCuAPeqPg6cu7KPAFLCkYfuo/23nnc0eYWlZx88BcVbAd6qMSLzQrp29W2L3uKagG3g6JNmza5tV9pNYaEZ9jtGvPXHub1H3eTb7VjMhoY0DGKkUktuUpWrxei6hxDZx3/4t2hnuYJ0G6gSuhe/opaod1flFfNuqKueQDqNobvx8O5Q/Dp36DNrSqfypNJwnYbfPUwXDoLUZ3h5smeO3ZlRHaE8Bi4cBQOrSxcdb6qMvaB9ZIKNhvW/CEtX3I7KFqxYoU32yHccDIzl6cXbmHVPtU71LtNY14e3JGYht4pMClErZObCXsLJmh4a+isqD4vwO6lsPt7OPoHxJRdaNZnqppT5EqbWyBuPaz8F6ybrWZlHVgOvZ6Enk94ZsmNVW/CkdVqltRfP9R/+QmDQc1CW/++GkLzVFDkGDqL6lQjVqb3J3I1q4kft6fRf/qvrNp3hqAAIy8P7sCHw66VgEgIT9qTDLY8tchlZAfvn69xa7iqoI7Pz5PBXyaIeHL4rKigUOj3Mjy2FuJvAGsu/DIVZneHPT9W7dhHflPHArjtTYjwkxxXRyC09ye1IK0nONaYk3wij5OgyM9l51mZuHALoz75k/M5Fjo2C2PpuF480CNOhiqF8DRfDZ0V1ftZMAXBkTWw/3++OWdZNK1IT5GXEpQbt4Gh38Dd8yG0KZw7DJ8NgU+HwNlDFT9ezln4aqSqjdR5CFx1r6dbXHlxvdQwV3Y6pLmXhlIuPWae1RISFPmxjUfOcuvbq1i48RgGA4zu3YrFj/XkiibeWRpAiFot56wazgHoeJfvzhveHLo9rLZXvlH2vr6Qc1b1lkHVZp+Vx2CADnfC4+uh53gwmmHvj/DOdbBiKlguuXccTYNvx0LmMWjYUvUS+ZOAIGjVR21XtTcMVG9T+la1LUGRx0lQ5IcsNjvTlu3h7nd/I/VsDs3q1+GLR3rwf7e0JTBA/sqE8Ipd34LdqhJ0I6707bmvHaEe07Z4boilsrIKeonqNvZNTk5QPej7khpSa9lbBWQrX1fB0e7k8ocUN8xVOVlGM/x1ntfWk6sSxxphe3+o+rHOHoT8bFVEMaJ11Y8nipFvWD9z8HQ2f52zlhnL92PX4K6rm/HD+CS6xTcs/8VCiMorOnTma+Et1JIatjzIPun78xfljSRrdzRuDQ8sgbs/UrlM54/A5/eqmWpnD7p+zckd8OPzavvmydDUC2uMecKV/cBgVMUkzx8tf/+ypBXkE0V2VMvGCI+SoMhPaJrGp7+nctuM1Ww5doGw4ABm3ns104ZcRVhwNVtIU1RfllxYcLcajvCXpF9fyEpX9XNADen4mimgMKn5fGrZ+3qbt5Ks3WEwQIc71JBarydV78++ZfBOd1j+qqr145CfAwuHq0Dyir6qJpG/qtsIYgrWXdtbxSE0ySfyKgmK/MCZ7Dwe/ngDz3+9jUsWG4mtGvHTk9czsIuP/6cmxI6v1ZfQnx+r2TK1xc5vAA2ad4MGsfq0wbHUh+5BkQeqWVdVYF3V8zP6N5WPY8uDX/9VMKS2VAXsPz4LZ/ZAvShVtdrfp6Y7F4it4hCaIyhqKjPPvMHP76Kab/nuk9wy/Vd+3nWKQJORf9zWjk9GXEd0uAdqdghRURvmFW7/PFkVw6sN9Bw6c3AGRUf0awPoN3zmSsSVcP9i+Nt/Iaw5XEiFz++DD26CPz8CDHDXe1Cvsd4tLZ9jav7hVZCXVbljaJr0FHmZBEU6uZRv4x9LtvHQ/A2cyc6nTWQo3zzek5FJLTHKEh1CD+nb4NgfYAyA4HA4vUstQVHTnU+Fo78DBUM3evG3niI9hs9cMRig/SB4/A9IegpMgWo9MYCkCdDyBn3b566I1mp2nC0fDlSyGPL5I5B7Xg0rNvajZWFqEAmKdHA0G+6Y8xufrFMffiN6xfPN4z1pFx2mc8tEreboJWp7OyQ9rbZXvOb+1OjqasfX6jGul1rZXC9+FxT5QU9RUYF14aYX4LHfVN7XVfdD7+f0bpX7DAa1QCxUfgjN0UsU2R4CAj3TLlGMpK77kM2u8e7Kg7y13YRdyyEyLIg3776KXldG6N00UdvlZcHWL9V214dUUujv/1G1X/54Ty3DUFP5w9AZ+GFQ5Cc9RZeLuEIVfayO2twC695Riw7bbWA0Vez1MnTmddJT5ENfrD/Kmz/vx64ZuKVDJD+Nv14CIuEftn6pap80ugLirwdzMPSZpH636k24dE7f9nnLmf3qi8YYAO0G6dsWR4L3haP61SrKzYT8gnwXb1Wzrs1a9FBD0zkZcGx9xV8vQZHXSVDkQ3d3bU5iq4b8/QobM4Z0pn6IdH8KP6BpsOFDtd31ocLlLToPgSbtIfcCrH5Lv/Z5047F6rHljWratJ5CmxbUKsrXr1aRo5coOFwNVwnPMplV+QCo+BCaphVZ88xP6zHVABIU+ZDZZGT+gwl0a6zJumXCfxzbACe3qQq5XYqsGWU0qWnRAOvehQvH9Gid92gabFuktvUeOgNVqyhc51pFWX4+dFYTOBeIrWC9oswTkHNGBc6R7T3fLgFIUORzEgwJv7NhrnrscBeEXFY5/cp+ENtT1YlxrEBeU5zaqercmIKg7a16t0apXzCEpldQ5K9J1jXJFTer4drTu0uv1O2KY+iscVswS8kWb5GgSIjaLOcsbC8YQur6UMnfGwxw80tqe/OncGqX79rmbY4E6yv7quEif6B3rSIJiryvTn2VWwQVWyBW8ol8QoIiIWqzLZ+pXqCoTtC8q+t9Yq6FdgNBs8P/pvi2fd6iaf4z66wovWegOZb4CJWgyKscC8TuSXb/NRIU+YQERULUVppWWJuoaIK1Kze9qHIZ9iTDkd980z5vOvEnnDsM5rrQur/erSmke1AkPUU+0aZgyY/U3+DSefdeI0GRT0hQJERtdehXyNgPgfWg091l7xtxJVwzVG2nvFD9F4vdVtBL1GaAf82y0j0oSlOPkmjtXQ1bqtwguxX2/1z+/tmnCpLgDapXV3iNBEVC1FaOBOvOQyAotPz9ez8L5hC1FMjupd5tmzfZ7YVT8f1p6AwKgyK9ahU5hs+kp8j7KrJAbNpW9RhxJQTV816bhARFQtRKWemFgY2rBGtXQqOg+2i1/b+XwGb1Ttu8LfU3yEqDoHC44ia9W1OcnrWKLJfg0lm1LUGR9zmm5u9PAZul7H3TNqlHGTrzOgmKhKiNNv1Xdd3HXAdRHd1/Xc9xUKchnNkLmxd4r33e5EiwbjcQAoL0bcvlitUq8vEMNEc+kTnEf2bj1WTNr4WQRqo4amo5eXqST+QzEhQJUdvYbbDxI7Xtbi+RQ3A4XD9Rbf8yFfJzPNs2b7PmFy4A2/EufdtSGr1qFRVNspZ6at5nNMGVBUn+5U3Nl6DIZyQoEkJPuZm+P+e+FJWzUqcBtL+j4q+/dgSEt1BDUL+/6/n2edP+FDVEVC8KWvbWuzWu6VWrKMuRZC1DZz7jmIW2J7n0yQs5ZwsD5KjOvmlXLSZBkRB6sNth4XD4dyvY+Y1vz+2Yhn/V39XCrxUVEAR9/qG2V09XH9rVxZbP1WPnuyu+Qrmv6DUDzZlkLTPPfKZVHzAFwrlDakjalfSCJOsGcarwo/AqCYqE0MOKV9UMKFs+fDuucCq0t507AvuWqe2KDp0V1eluiOwEeRdg1ZueaZu3XTpXuN5U53v0bUtZdAuKpEaRzwWFQlyS2i5tFppz6Owq37SplpOgSAhf274YVv0/tR3aFHLPw7eP+6b2z58fAZoaOmrUqvLHMRqh72S1/cd7+tXVqYgdX6sgNLJTxZLLfU3vnKLQaN+et7Yrb4HYE5vVo+QT+YQERUL4UtpW+GaM2k4cC0OXqAVJ9/8MGz/07rmt+fDnx2q7Kr1EDq1ugvjrVaCx4rWqH8/bHENnXYbo247yOHuKfFyryNlTJMNnPuWoV3T0d7iYUfL3kmTtUxIUCeEr2afh8/vAkqNWyr75JWjcBm6erH7/0yTIOOC98+/+Hi6eVknGbTywKrzBUNj2LZ9D+vaqH9NbMg6oLx2Dsfzq3XoLjVarqNstkJ3uu/PK8Jk+6seo3kvNXji07ZCbCWcLPhMkKPIJCYqE8AVrPnw5VM36atgK/jK3MNH3ulEqr8CSA0seU1PmvcGRYH3NUDCZPXPMZgnQ4U5AUwUd/dXWL9VjyxtVEUp/Zgoo7K3x1RCazVJYLFJ6inzPMYR2+QKx6dvUY1hzqBvh2zbVUhIUCeELP/wfpK6FoDC49/Pis0iMRrhjNgSGqt6MtTM8f/7Te+HwKtVTkvCgZ4/d55+qZ2PfMji0yrPH9gRNg62OobN79W2Lu3ydbJ2VDmhqJlRII9+cUxRyTM0/sByseYXPy9CZz0lQJIS3rZ9bkC9kUD1EjVuX3Kd+Cxjwhtpe/qrnh6Ic+Uqtb4Hw5p49dqNWkDBMbf/8ov8tFnv0dzh3WC182/Y2vVvjHmeytY9qFTmTrKNUkC58K/pqNaydnw2HVxc+7wiKmsrMM1+Ru18Ibzq8WvUSAdz8IrTuV/q+V90HbW5TuSRfP1r8f4xVkZ9TuCSHJxKsXbn+/8BcF45vhF3feucclbXlM/XYfjAEhujbFnf5vKdIkqx1ZTRCa0d16yJT86WnyOckKBLCW84dUXlEdit0/Cv0HF/2/gYDDHwbQiLg5Ha1jIYn7Phara9Uv4UqFucNoZGQ+Lja/t+U8he49BVLbuGyHp39fNZZUb4OiiTJWn9Fp+ZrGuRfhDN71HMSFPmMBEVCeEP+RTXTLCdDfaANmuneelL1GsPA6Wp7zduQuq7qbXEkWCcM924V58SxKqDL2K8WnPUHe39UAWFY88IiedWBBEW1T/wNEFBHTcY4uR1O7lAz0upF+v/kgBpEgiIhPE3T1Cyyk9uhbhO459OKDdu0Gwhd7lMfiF+PgrzsyrclbQsc3wBGM1z9QOWP446gULjhGbX9y+sqMNTb1i/UY+e7q1eujK9rFTmW+AiVoEg3gSGF6/Ht+VGGznRSjT4lhKgmfv23Ws/MaIYh/61cYvOA11XvxrlDkPLPyrfF0UvUfpDqhfK2hGFqjabsk/DbbO+frywXzxTWffHnZT1c8XWtIukp8g+OWWh7f4A0qWStBwmKhPCk3UvVumYAt0+DFt0rd5zgcDVNH1Rgsy+l4sfIzYStC9W2txKsLxcQqKbogxr+u3jGN+d1Zftilc8VfRU0aatfOyqjaK2icz6YgeZYe08SrfXlqG59fCMc+EVty5pnPiVBkRCecnInLH5EbXd7RBVJrIqWN8B1j6ntbx6v+Gr0W78Ay0WIaAOxPavWlorocJf6321+FsY1b/nuvJdzzDqrLrWJLuervCK7vcjsM+kp0lVoFDS9Rm1nHlOP0lPkU7oHRbNnzyY+Pp7g4GASEhJYtars4m8LFiygS5cuhISEEB0dzfDhw8nIKL5ezPnz5xkzZgzR0dEEBwfTrl07kpOTSzmiEB6QcxY+v1fVGYlLgv4eWgvs5hchorUaQkl+2v3XaRpsKKhN1PUh95K8JHi5YQAAIABJREFUPcVoVEuYAMaN8wjJO+27czuc3gsn/gSDCTr+xffn9wRfLQx78bTqUTMYVVKv0FfRJXjqNPR8XTFRJl2Doi+++ILx48czadIkNm3aRFJSEgMGDCA11fWHwOrVqxk6dCgjRoxgx44dLFy4kPXr1zNy5EjnPvn5+fTt25fDhw+zaNEi9uzZw/vvv0+zZtItLLzEZoWFw1SBwPqx8LePPbeMhrkO3Pmu+nLf/hVsW+Te647+Dqd2qNksXXTIp2l1I7S8EYPdQru0hb4/v6OC9ZV9fZNL5Q3OniIvD585kqzrRaphO6EvR14RqF4iX/6HRugbFE2bNo0RI0YwcuRI2rVrx/Tp04mJiWHOnDku91+3bh1xcXGMGzeO+Ph4evXqxaOPPsqGDRuc+8ybN4+zZ8+yZMkSevbsSWxsLL169aJLF+mCFF6ybBIcWqmKF977GYQ09OzxmyXA9RPV9tKnCvM/yuJIsO70l+JLivhS35fQMND83DoMx9b77rx2e+FaZ9WpNtHlfDV8JknW/iWyo5pkATJ0pgPd/luQn5/Pxo0befbZZ4s9369fP9auXevyNYmJiUyaNInk5GQGDBjAqVOnWLRoEbfdVli6/9tvv6VHjx6MGTOGb775hsaNG3PffffxzDPPYDK5rtGSl5dHXl5h9eDMzEwALBYLFotni9A5jufp44qyeeu6GzYvIOD3dwGwDpqN1rA1eOPvtscTmPb8gDF9C/YlY7Dd83np/4PMySBgxxIMgPWqoWh63WsR7TF0upeAbZ9i+Ok5LA8tU0M0XmY4spqAC0fRgsKwtrzZO38fPmAIbUYAoJ1PxVrB91CR+914/hgmwF4vGls1vVb+wlOfM8Zuj2L89Q2sbQZW2/vXl4pe96pee92CojNnzmCz2YiMLD6GHRkZSXq66ymoiYmJLFiwgCFDhpCbm4vVamXQoEHMnDnTuc/BgwdZvnw5f//730lOTmbfvn2MGTMGq9XKCy+84PK4U6dO5aWXSq7wvWzZMkJCvLMsQEpKJWYTiSrz5HVvcHEfPfepqtO7o+5kz0EDHPRe7lq9BvfQ++ROTAf/x9b/TuRIhOvq1K1OJtPRlsf5OnGs3JQGm/XLpwvSenCTcTHm9M1s+eQfHG3Uy+vnvOrIB8QCR+pdzZaUFV4/n7cE52fQHxUUJS/9vlIBpTv3e7sTq2gNHDqbz3bJvfSIqn/OxEKH2bDpuPoj3JKSkkJOTk6VjmHQNH1Wbzxx4gTNmjVj7dq19OjRw/n8q6++yn//+192795d4jU7d+7k5ptv5sknn6R///6kpaUxceJErr32WubOnQtA69atyc3N5dChQ86eoWnTpvHvf/+btDTXww6ueopiYmI4c+YMYWFhnnzbWCwWUlJS6Nu3L2azh/JORLk8ft0zTxAw72YMF09hb3M7tr/M80kviPGPdzGl/APNHIJ15C/QsGXxHTQ7AXOuw3DuENZb30LzdsHGclgsFg59Mp4OJ75AqxeJ9bHf1cKsXjthDgHT22PIz8b6wHdoLXqU/xp/ZbcR8EYzDHYrlrFbKzS8VZH73fTNYxi3L8TW50XsPcZWtdW1mny+66Podb906RIRERFcuHChUt/fuvUURUREYDKZSvQKnTp1qkTvkcPUqVPp2bMnEyeq/IrOnTtTt25dkpKSeOWVV4iOjiY6Ohqz2VxsqKxdu3akp6eTn59PYGBgieMGBQURFBRU4nmz2ey1G9ubxxal88h1t1yCRUPh4ilo0gHjXf/BGFjy/vGKHmNg308YDq/C/P1YGP5D8aU7DixXBR+Dwgi4agj4wT12sHE/2l/6A8O5Q5jXzYSbXPfYesTuFDUDsH4LAuJ7Va8q1iWY1cyjc4cxZ5+ARrEVP4I793tBcUhT/RhMfnC/1ATy+a4Ps9mM1Wqt0jF0+8QIDAwkISGhRDdjSkoKiYmJLl+Tk5OD8bIPOUfw4+jw6tmzJ/v378depDT+3r17iY6OdhkQCVEhmgbfjlXVZus0hHs/hSAv9nxczmiEO+ZAUJiaYbZ2RvHfOxKsu9wDgXV9164y2I1mbDcVDE+vnaVm6XnLloJZZ53vqeYBUQFfJFs7Zp9JorUQ+s4+mzBhAh988AHz5s1j165dPPnkk6SmpjJq1CgAnnvuOYYOLSyAN3DgQBYvXsycOXM4ePAga9asYdy4cXTr1o2mTdU/6Mcee4yMjAyeeOIJ9u7dy9KlS3nttdcYM2aMLu9R1DDbFsG2hWqK/N8+Ukta+Fr9GBjwhtpe/iqkb1fbmWmwuyAnJGG479tVBq31AIi/Hmx5kOKlnqKsk6qnDPQpQ+AN3g6KNK1INWsJioTQtSjFkCFDyMjIYMqUKaSlpdGxY0eSk5OJjVXdxGlpacVqFg0bNoysrCxmzZrFU089Rf369enTpw9vvPGGc5+YmBiWLVvGk08+SefOnWnWrBlPPPEEzzzzjM/fn6iB9v6oHhMfV1/yeulyr1pSZPf38PWj8PBy+PNj0GzQogdEttevba4YDHDL6/BuL7Uu3OHVEOfhpOvti9T7b34tNGrl2WPrxVnA0Uu1ii6dA+sltR0a7Z1zCFGN6F6pa/To0YwePdrl7+bPn1/iubFjxzJ2bNnJgD169GDdunWeaJ4QxR37Qz22vFHfdhgMcPt0SF0HJ7fD8pfVWl8AXUfo27bSRHZQPVgb5sKPz8IjK4vnQ1WVY1mP6lyb6HLe7ily1CgKiQBzsHfOIUQ1UgMG3YXwkayTBV9OBlVQUW/1GsOggpyitTNVbkhII2g/SN92leXGSWqx2/RtsOkTzx335A51TKO5+i7r4YqvgqIw6SUSAiQoEsJ9jqrMTdpBsGdLNVRa29vgqr8X/nz1/RDgo5lwlVG3EdxQULD1f1Mg94JnjutIsG7d3/MVxfXkCIouHAO7zfPHdyZZyzJIQoAERUK4zzF01vxafdtxuVumqoRvc4jfJVi71O1haHQl5JyBX/9d9ePZbSr5HWpOgrVDaDQYA8BugSzXRW2rJEuSrIUoSoIiIdx1tKCnKKabvu24XHC4ys8ZuxEaxuvdmvKZzND/NbW97l3IOFC14x1aqb7cg+vDlf2q3j5/YjQVrpLujWRrmY4vRDESFAnhDpsFTmxS2839LCgCtehrdfpia90PrrhZ9YAs+0fVjrXlC/XY8S/+PXRYWd7MK3LmFMnwmRAgQZEQ7knfpqYuB4dDoyv0bk3N0P81Ve9pTzIcqOQaZXnZsOtbtV3Ths4cfBEUyXR8IQAJioQ/2/czHF6jdyuUYxvUY/Nra0alZH/QuA10e0Rt//gc2CpRnn/392DJUWvA+Vuul6d4s1aR9BQJUYx8ugvY/Cm8c11hNWR/sHYWLPgLfHIX5GXp3ZoiSdZ+OHRWnfV+Ri2XcnoXbPyw4q931ia6R9VuqomcQZGHe4rysiAvU23LlHwhAAmKxOm98N14OL0bvnygsACgntbOhGWT1LY1F1J/17c9AEcLgqKYGtoboZc6DeDG59X2ilch56z7r808AQdXqu3Of/N82/yFt4bPHMt7BIVDUKhnjy1ENSVBUW1mt8E3Y9R6VMHhYLfCVyMKE1f1sHZmYeJtvSj1eHiVfu0ByD5VMHRhgGZd9W1LTZQwHJq0V0tOrHyj/P0dtn4JaGpZk+ow666yvFWrSGaeCVGCBEW12bo5algoMBQeXaUK/2l2tZbWnx/7vj1rZhQGRDc8C30LVlY/vNr3bSnK0UvkT0UbaxJTQOEU/T/eh9N7yn+NphUWbKypCdYOoVGqUrfdWlhXyBOkmrUQJUhQVFtlHFDrZQH0fwUaxMLAmQXrZmnw7Vj1BeUra2ZAyj/V9g3Pwo3PQWxP9fOJTfrmFTnziaSXyGta3QhtblULuv70fPn7p29VeUimIGh/h/fbp6ditYo8OITmDIqkp0gIBwmKaiO7XQ2bWXOhZW+45kH1vNEIt70J3QsW6E1+Gn57x/vtWfN2yYAIoH6MqtSs2dTCp3pxzjyTJGuv6veK6hHZ/zPsXVb2vo4h3jYDVI2mms4beUVZMvNMiMtJUFQb/fEepP4GgfVg4Izis3YMBjWU0etJ9fNPz8OqN73XljVvQ8oLarv3c4UBkUNcL/WoV16RzQLH/1Tb/lbJuqZp1Aq6P6a2f3oerPmu97NZa+6yHqXxRlAkPUVClCBBUW1z9iD8ryBXp+9LatjscgYD3PSiClJALdy54jWVx+FJq6cXD4h6P1tyn7gk9ahXXtHJ7UWKNl6pTxtqk+snQt3GkLEP1pcyfHtwBVw8BSGNVFXs2sAbtYpkMVghSpCgqDax2+GbsarYXVwSJDxU+r4GgwpSbnpR/bzyDfh5sucCo9XT4eeCY/d+3nVABEXyijZDbqZnzl0RjvXOpGijbwSHQZ+CodRf3oCLZ0ru46hN1OlutY5abeDNniKpZi2Ek3zK1yYb5sKR1Wo19UEz3fuST5oA/aeq7TXTVeXhqgZGq9+6LCB6pvR9i+YVHdWhXpEUbfS9q++HqE6Qd0HVLioqNxN2L1XbnYf4vm168XRQZMmFnAy1LcNnQjhJUFRbnDsCKQWByM2TK1bXpcdolYAN8PscWDpB9TpVxuq3VI8TwI2Tyg6IHPTMKzrm6CmSmWc+YzTBLQX1ijbOh/Tthb/b+Y2aIBDRBpperUvzdOHpWkWOqf0BdVQBTSEEIEFR7aAVTLG3XIQWiXDtwxU/xrUjYdAswAAb5qnjVfTDedW04gHRDf/n3uvirlePvs4ryj4N5w4DBgmKfC2uJ7QfrOpm/VSkd3JrwayzLkNq7rIerni6VlHRJOvadB2FKIcERbXBxvlwaKX6X+HgWZXPjbnmAbjrPbWy+eZPVJFHdxfxXDWtMMH7xn+4HxCB+oIE3+cVOYbOGrdVidbCt/q+rOoQHfpVDZmdTy3oLTRApxq8rIcrnq5VJDPPhHBJgqKa7vxRWFaQuHrTC2rac1V0/hv8dR4YA9S06EXDS5867bDqzcsCookVO2d4c2gQ7/t6RbLemb4axELiWLW9bBL8+V+1HddL5ZrVNo4htHMemIEmS3wI4ZIERTWZpsF34yA/C2Kug+se9cxxO9wBQz4BUyDs+ha+HArWPNf7rnpTTekH6FOJgMhBj7wiZz6RJFnrpteTag28c4dh1f9Tz3W5V9cm6caTydbSUySESxIU1WSbPoEDyyEgGAa/o7rgPaXNALjnM3XsvT/AZ/eC5VLxfX79f8UDousrGRCB7+sVFS3a2Fx6inQTVE9NDACVXxRQB9oP0rNF+nHUFPNEUCTVrIVwSYKimurC8cI1pG6cBBFeKDx45c1w35dqiv+B/8GCuyH/ovrdr/+vcG21Pv+sWkAEhXlFaT7KKzq5o7BoY0Rr759PlK7zEGiWoLbb3gZBofq2Ry+eLOAoPUVCuCRBUU2kafD9eMjLhGZdoccY752r5Q1w/2IIDFVDW5/8RVW/LhYQPV318zjziuy+yStyDJ016ypFG/VmNMJd76vFim+erHdr9OON4TMp3ChEMfJpXxNt+Qz2LVMzd+6Y7dlhM1die8DQJRAUrtZUW1lQY+amFzwTEDnEO4bQfJBX5Eyylnwiv9CoFdw+rXYmWDs4gqLM4+7P+nTFZoGsdLUtw2dCFCNBUU2TmQY/FiyZ0ftZaNzGN+dt3hUe/LawENxNL0DSU549hy/zipyVrCWfSPiJeh6qVZR9EtDUDNK6jT3WPCFqggC9GyA8SNPg+ych94Kq9ps4zrfnb3oVjP4dMo8V5oB4UmzRvKIL3qsdJEUbhT8yGlVP2dmDagitsr1mmQUBVWhTGRoW4jLyL6Im2bZQzQQzmmHwbDDpEPOGRnonIAIIbwYNW3o/r8iRT9S4jRRtFP7FE3lFUqNIiFJJUFRTZJ2EHwqqRN/wDES217c93uKLekUydCb8lUeCIsfMM0myFuJyEhTVBJqmFmm9dA6iOkOv8Xq3yHt8kVd0tKCnSJKshb/xaE+RJFkLcTkJimqCHYth9/cqcfKO2WAy690i73HmFW1ReUWeZrPCCUfRRgmKhJ/xRK0iqVEkRKkkKKrusk9DckFhxKSnIaqTvu3xNm/nFZ3cDpYcVV5AijYKf+OJniLHzDUJioQoQYKi6i75acjJgMiOnp8C76/ivFivyLneWYLMzBH+xxO1imT4TIhSyad+dbZjCexcAgaTWtssIFDvFvmGIyg65M2gSIbOhB+qaq0iu73IlHxJtBbichIUVVcXM2BpQc9Q0gRVI6i2cKyDlr4VLp337LGdlaxl5pnwQ45aRVC5IbScM2C3AAYIjfJo04SoCSQoqq7Wf6A+4Bq3q/piq9VNWFNo2MrzeUXZp+HcIbXdTIo2Cj/lzCuqRLK1Y+isXmTNnpAhRCVJUFRd7VyiHnuOg4AgfduiB2/UK3IWbWwLdep77rhCeJJzBloleooyJclaiLJIUFQdnd4Lp3aq3II2A/RujT68Ua9IijaK6qAqM9CkmrUQZZKgqDra+Y16bNm7cAHW2sbRU+TJvCJH0UYJioQ/q1JPkdQoEqIsEhRVR46hsw536NsOPYVFQ6MrPJdXVLRoo1SyFv6sSjlFEhQJURYJiqqbM/tVgUFjALS5Ve/W6MuTeUWndhQp2tim6scTwlscQdGFStQqkhpFQpRJ96Bo9uzZxMfHExwcTEJCAqtWlf0Ft2DBArp06UJISAjR0dEMHz6cjIwM5+/nz5+PwWAo8Sc3N9fbb8U3HL1E8TdASEN926I3TxZxdEzFl6KNwt/ViwRTIGg2yDpRsddKNWshyqTrp/8XX3zB+PHjmTRpEps2bSIpKYkBAwaQmup6rHz16tUMHTqUESNGsGPHDhYuXMj69esZOXJksf3CwsJIS0sr9ic4ONgXb8n7ZOiskHMdNA/kFUnRRlFdGI0QXolaRZomw2dClEPXoGjatGmMGDGCkSNH0q5dO6ZPn05MTAxz5sxxuf+6deuIi4tj3LhxxMfH06tXLx599FE2bNhQbD+DwUBUVFSxPzVCxgFI36YqWLe9Xe/W6M+RV4QGqb9V7VhHZeaZqEYqMwMt97waIgapZi1EKXQLivLz89m4cSP9+vUr9ny/fv1Yu3aty9ckJiZy7NgxkpOT0TSNkydPsmjRIm677bZi+2VnZxMbG0vz5s25/fbb2bRpk9feh085Zp3FXy9DZw7OvKIqTM2/eKawaGPzhKq3SQhvq0xQ5OglqtMQzHU83yYhaoAAvU585swZbDYbkZGRxZ6PjIwkPT3d5WsSExNZsGABQ4YMITc3F6vVyqBBg5g5c6Zzn7Zt2zJ//nw6depEZmYmb7/9Nj179mTLli1ceeWVLo+bl5dHXl6e8+fMzEwALBYLFoulqm+1GMfxKnPcgB1fYwCsbQeiebhd1ZUhpgcBG+ejHfoVaxnXpKzrbjj8GwGAFtEaa0A9kGvrMVW530XpjGHNMQH2s4exubi2rq674Wyqus9Dm5b5b0VUntzv+ih63at67XULihwMBkOxnzVNK/Gcw86dOxk3bhwvvPAC/fv3Jy0tjYkTJzJq1Cjmzp0LQPfu3enevbvzNT179uSaa65h5syZzJgxw+Vxp06dyksvvVTi+WXLlhESElLZt1amlJSUCu0fkneKvulbsWMk5WgQ+WnJXmlXdRNkyeMWgPRtLPt2IdaAumXu7+q6tzuxkNZAqj2KzclyXb2hove7KFuzsxl0BTIObmZtGfds0eve4swvXA2czDXxu9znXiX3uz5SUlLIycmp0jF0C4oiIiIwmUwleoVOnTpVovfIYerUqfTs2ZOJE9VaX507d6Zu3bokJSXxyiuvEB1dcpzcaDRy7bXXsm/fvlLb8txzzzFhwgTnz5mZmcTExNCvXz/CwsIq8/ZKZbFYSElJoW/fvpjN7q89ZFw7A3YCcb24edAQj7aputPSZmDI2E//tqForW9xuU9Z1930yX8AaNb9TppeXcvLHHhYZe93UTbDsQj46F0iTBe59daS96yr6278dRschcatrnL5GlF1cr/ro+h1v3TpUpWOpVtQFBgYSEJCAikpKdx5553O51NSUhg8eLDL1+Tk5BAQULzJJpMJUD1MrmiaxubNm+nUqVOpbQkKCiIoqOT6YWaz2Ws3doWPvec7AIwd78Qo/9iKi0uCjP0EHP0NOgwsc9cS191mhRMq5ywgrgfItfUKb/5bqpUatQTAkHkcs9EAJtcf5cWue7b6D6ipfnNM8nfhVXK/68NsNmO1VrB212V0nX02YcIEPvjgA+bNm8euXbt48sknSU1NZdSoUYDqwRk6dKhz/4EDB7J48WLmzJnDwYMHWbNmDePGjaNbt240baqmmL700kv89NNPHDx4kM2bNzNixAg2b97sPGa1dO6w+uI2GKFt2V/6tVJVijg6izaGSdFGUX1UplaRTMcXoly65hQNGTKEjIwMpkyZQlpaGh07diQ5OZnYWLW2T1paWrGaRcOGDSMrK4tZs2bx1FNPUb9+ffr06cMbb7zh3Of8+fM88sgjpKenEx4eztVXX82vv/5Kt27VuP6MY9ZZbE+o11jftvgj5zpo2+DSuYqtB+eoT9RMijaKasRRq+jsATUDzTEbrSwSFAlRLt0TrUePHs3o0aNd/m7+/Pklnhs7dixjx44t9XhvvfUWb731lqea5x8cQZEUbHQtNAoaXQkZ++DIb9C2AvkSjkVgZb0zUd00iC0Mitzh6FGSJT6EKJX819jfnU+F4xvV0Fm7QXq3xn9Vtl7RMUfRRgmKRDXj6B0658bCsHnZkHtBbUtPkRClkqDI3xUbOmuib1v8WWXyii6egbMH1bYUbRTVTUUKODrWPAsMhaBQ77VJiGpOgiJ/5wiK2ruekScKOBaHdeQVucORTxTRpmJ5SEL4g/oq99KtoCjzuHqUXiIhyiRBkT+7cKzgi9sgQ2flCY2EiNaABkdcLxNTgnMRWFnvTFRDFekpyizoKZKgSIgySVDkz5xDZ4nqS1+UraJ5RY5FYGMkKBLVkCMoyjyu6m2VxdlTJEnWQpRFgiJ/JkNnFVORvCKbFY7/qbYlyVpUR3WbgClI1SpyBD2lken4QrhFgiJ/deE4HP0dGTqrgFhHvaLtkHO27H1P7QTLRVW0sXFb77dNCE8zGqF+jNoubwjNGRSVXApJCFFIgiJ/tetb9diiu3yQuatoXlHqb2Xv65iKL0UbRXXmbl6RDJ8J4Rb5NvBXMnRWOe7mFR2VJGtRA7gbFGVJorUQ7pCgyB9lpkHqOrUtQ2cV425e0TGpZC1qAHeCImseXDyttqWnSIgySVDkj3Z9C2gQcx2Ey4dYhTjrFZWRV3QxQy2PANC8q2/aJYQ3uFOryNFLFBAs9biEKIcERf5oxxL1KENnFVevScFq92XUK3IWbWwtXxKienOnp8iRZB0aDQaD99skRDUmQZG/yUovTBKWoKhyyssrkvXORE3hTq2iTFkIVgh3SVDkb3Z9B2gqATi8ud6tqZ7KC4qkaKOoKdypVSQ1ioRwmwRF/sY5dHaHvu2ozhxB0UkXeUX2okUbJSgS1Zw7tYokKBLCbRIU+ZPsU3BkjdpuL7POKq2svKLTu1XRxsBQKdooaobykq1lMVgh3CZBkT9xzDprllCYKyAqp5QhNKNzEdgEMJp83CghvMCZbH3E9e+lp0gIt0lQ5E9k6Mxz4gum5l9Wr8hwfIPakCRrUVOUNwNNgiIh3CZBkb/IPi1DZ54U6zqvyHBcijaKGqasoMhuheyTaltmnwlRLgmK/MXu70CzQ9OroUGc3q2p/uo1LswZKgg2A61ZGM4eVM81S9CpYUJ4WFk5Rdmn1cw0YwDUbezbdglRDUlQ5C9k6MzzLssranCxoIp1oyshpKFOjRLCw4rVKrIU+5Uhq2DorF6U5NAJ4QYJivzBxTOFCcFSsNFzSgRF+9XPMnQmapJ6TdQSHpq9ZK2iLMknEqIiJCjyB7u/V13c0V2gYbzerak5LssraugIiqQ+kahJDAYId12ryCBJ1kJUiARF/kCGzryjSF6R4chqGuQU5BNJT5GoaUpLtnYsBitJ1kK4RYIiveWchUO/qm0ZOvO8giE044YPCLDnogXWk6KNouYpJSgyyPCZEBUiQZHeHENnUZ2gUSu9W1PzxKl6RcZUVdlaa3qNJJyKmqe0niLn8Fm0b9sjRDUlQZHeZOjMu2J7FvtRayb5RKIGKrWnSIbPhKgICYr0lHMWDq1U2xIUeUe9xtC4nfNHrXlXHRsjhJe4qlWkaUVyimT4TAh3SFCkpz3JquJsZEeIuELv1tRcjqn5gNZMgiJRA7moVRRozcJgywcMqk6REKJcEhTpSYbOfCP+egAyg5tBnQY6N0YIL3BRq6iO5WyR3wXq2Dghqg8JivRy6Twc/EVtd5CgyKva3o7t5pfZ1GKk3i0RwjsMhhJ5RcGWc+rnUEmyFsJdEhTpxLDvR7BboEl7iLhS7+bUbEYj9use43xdmd0narDLgqI6+QU9RZJkLYTbJCjSiXHXN2pDhs6EEJ7gCIrOHQGKDJ9JkrUQbpOgSAcB1osYZOhMCOFJpQ2fSVAkhNskKNJBVOZmDHaLqqzcuI3ezRFC1AQyfCZElUlQpIOm5/5QGzJ0JoTwlMtqFRX2FEmitRDukqDI13IzaZK1TW3L0JkQwlMcPUVZJ8CWXySnSHqKhHCXBEU+Ztj/EybNitboSlmYVAjhOXUbF9YqOrWLAHueel6m5AvhNgmKfMy461sA7O0GqdoiQgjhCUVqFRmPqSF6rU4DCAzRs1VCVCsSFPlSbiaGA8sBsLcbrHNjhBA1TkFQZDi6Tv0cKjPPhKgICYp8ad8yDLY8soKiiy1SKoQQHuEMin4HQJOhMyEqJEDvBtQq7QdjDQxn57pVXCNDZ0IIT3MERdnp6mcJioSoEAmKfMlkRmvZm/TdOXq3RAhREzlmoBXQpHCjEBWi+/DZ7NmziY8pRHDfAAAQwklEQVSPJzg4mISEBFatWlXm/gsWLKBLly6EhIQQHR3N8OHDycjIcLnv559/jsFg4I47ZOq7EKIWcNQqKqBJTpEQFaJrUPTFF18wfvx4Jk2axKZNm0hKSmLAgAGkpqa63H/16tUMHTqUESNGsGPHDhYuXMj69esZObLk6udHjhzh6aefJikpydtvQwgh/MNlPUWyxIcQFaNrUDRt2jRGjBjByJEjadeuHdOnTycmJoY5c+a43H/dunXExcUxbtw44uPj6dWrF48++igbNmwotp/NZuPvf/87L730Ei1btvTFWxFCCP3VbQwBdZw/SqK1EBWjW05Rfn4+Gzdu5Nlnny32fL9+/Vi7dq3L1yQmJjJp0iSSk5MZMGAAp06dYtGiRdx2223F9psyZQqNGzdmxIgR5Q7HAeTl5ZGXl+f8OTMzEwCLxYLFYqnoWyuT43iePq4om1x3fch1972A+jEYzuwFwFKnMci19xm53/VR9LpX9drrFhSdOXMGm81GZGRksecjIyNJT093+ZrExEQWLFjAkCFDyM3NxWq1MmjQIGbOnOncZ82aNcydO5fNmze73ZapU6fy0ksvlXh+2bJlhIR4p/BZSkqKV44ryibXXR9y3X2ne34wkYDVGEzKynVSJFYHcr/rIyUlhZycqk1k0n32meGyf7CappV4zmHnzp2MGzeOF154gf79+5OWlsbEiRMZNWoUc+fOJSsri/vvv5/333+fiIgIt9vw3HPPMWHCBOfPmZmZxMTE0K9fP8LCwir3xkphsVhISUmhb9++mM1mjx5blE6uuz7kuvue8YcV8OdWLpkb0LdfP7nuPiT3uz6KXvdLly5V6Vi6BUURERGYTKYSvUKnTp0q0XvkMHXqVHr27MnEiRMB6Ny5M3Xr1iUpKYlXXnmFkydPcvjwYQYOHOh8jd1uByAgIIA9e/bQqlWrEscNCgoiKCioxPNms9lrN7Y3jy1KJ9ddH3LdfahhHACXAhvSQK67LuR+14fZbMZqtVbpGLolWgcGBpKQkFCimzElJYXExESXr8nJycFoLN5kk8kEqB6mtm3bsm3bNjZv3uz8M2jQIG688UY2b95MTEyMd96MEEL4i9a3oNWP43iD7nq3RIhqR9fhswkTJvDAAw/QtWtXevTowXvvvUdqaiqjRo0C1LDW8ePH+fjjjwEYOHAgDz/8MHPmzHEOn40fP55u3brRtKmaetqxY8di56hfv77L54UQokZq0hbrmA2kJicjn3pCVIyuQdGQIUPIyMhgypQppKWl0bFjR5KTk4mNVQXI0tLSitUsGjZsGFlZWcyaNYunnnqK+vXr06dPH9544w293oIQQgghagjdE61Hjx7N6NGjXf5u/vz5JZ4bO3YsY8eOdfv4ro4hhBBCCHE53Zf5EEIIIYTwBxIUCSGEEEIgQZEQQgghBCBBkRBCCCEEIEGREEIIIQQgQZEQQgghBCBBkRBCCCEEIEGREEIIIQQgQZEQQgghBCBBkRBCCCEEIEGREEIIIQQgQZEQQgghBOAHC8L6I03TAMjMzPT4sS0WCzk5OWRmZmI2mz1+fOGaXHd9yHXXh1x3fch110fR637p0iWg8Hu8oiQociErKwuAmJgYnVsihBBCiIrKysoiPDy8wq8zaJUNp2owu93OiRMnCA0NxWAwePTYmZmZxMTEcPToUcLCwjx6bFE6ue76kOuuD7nu+pDrro+i1z00NJSsrCyaNm2K0VjxDCHpKXLBaDTSvHlzr54jLCxM/tHoQK67PuS660Ouuz7kuuvDcd0r00PkIInWQgghhBBIUCSEEEIIAYBp8uTJk/VuRG1jMpno3bs3AQEyeulLct31IdddH3Ld9SHXXR+euu6SaC2EEEIIgQyfCSGEEEIAEhQJIYQQQgASFAkhhBBCABIUCSGEEEIAEhT51OzZs4mPjyc4OJiEhARWrVqld5NqtMmTJ2MwGIr9iYqK0rtZNc6vv/7KwIEDadq0KQaDgSVLlhT7vaZpTJ48maZNm1KnTh169+7Njh07dGptzVHedR82bFiJ+7979+46tbbmmDp1Ktdeey2hoaE0adKEO+64gz179hTbR+55z3PnunvinpegyEe++OILxo8fz6RJk9i0aRNJSUkMGDCA1NRUvZtWo3Xo0IG0tDTnn23btundpBrn4sWLdOnShVmzZrn8/b/+9S+mTZvGrFmzWL9+PVFRUfTt29e5xqConPKuO8Att9xS7P5PTk72YQtrppUrVzJmzBjWrVtHSkoKVquVfv36cfHiRec+cs97njvXHTxwz2vCJ7p166aNGjWq2HNt27bVnn32WZ1aVPO9+OKLWpcuXfRuRq0CaF9//bXzZ7vdrkVFRWmvv/6687nc3FwtPDxce/fdd/VoYo10+XXXNE178MEHtcGDB+vUotrj1KlTGqCtXLlS0zS5533l8uuuaZ6556WnyAfy8/PZuHEj/fr1K/Z8v379WLt2rU6tqh327dtH06ZNiY+P55577uHgwYN6N6lWOXToEOnp6cXu/aCgIG644Qa5933gl19+oUmTJrRu3ZqHH36YU6dO6d2kGufChQsANGzYEJB73lcuv+4OVb3nJSjygTNnzmCz2YiMjCz2fGRkJOnp6Tq1qua77rrr+Pjjj/npp594//33SU9PJzExkYyMDL2bVms47m+5931vwIABLFiwgOXLl/Pmm2+yfv16+vTpQ15ent5NqzE0TWPChAn06tWLjh07AnLP+4Kr6w6eueelDrkPGQyGYj9rmlbiOeE5AwYMcG536tSJHj160KpVKz766CMmTJigY8tqH7n3fW/IkCHO7Y4dO9K1a1diY2NZunQpd911l44tqzkef/xxtm7dyurVq0v8Tu557yntunvinpeeIh+IiIjAZDKV+F/CqVOnSvxvQnhP3bp16dSpE/v27dO7KbWGY7af3Pv6i46OJjY2Vu5/Dxk7dizffvstK1asoHnz5s7n5Z73rtKuuyuVueclKPKBwMBAEhISSElJKfZ8SkoKiYmJOrWq9snLy2PXrl1ER0fr3ZRaIz4+nqioqGL3fn5+PitXrpR738cyMjI4evSo3P9VpGkajz/+OIsXL2b58uXEx8cX+73c895R3nV3pTL3vGny5MmTq9BO4aawsDD++c9/0qxZM4KDg3nttddYsWIFH374IfXr19e7eTXS008/TVBQEJqmsXfvXh5//HH27t3Lf/7zH7nmHpSdnc3OnTtJT0/nP//5D9dddx116tQhPz+f+vXrY7PZmDp1Km3atMFms/HUU09x/Phx3nvvPYKCgvRufrVV1nU3mUw8//zzhIaGYrPZ2Lx5MyNHjsRisTBr1iy57lUwZswYFixYwKJFi2jatCnZ2dlkZ2djMpkwm80YDAa5572gvOuenZ3tmXu+SnPXRIW88847WmxsrBYYGKhdc801xaYSCs8bMmSIFh0drZnNZq1p06baXXfdpe3YsUPvZtU4K1as0IASfx588EFN09QU5RdffFGLiorSgoKCtOuvv17btm2bvo2uAcq67jk5OVq/fv20xo0ba2azWWvRooX24IMPaqmpqXo3u9pzdc0B7cMPP3TuI/e855V33T11zxsKTiaEEEIIUatJTpEQQgghBBIUCSGEEEIAEhQJIYQQQgASFAkhhBBCABIUCSGEEEIAEhQJIYQQQgASFAkhhBBCABIUCSH8lMFgYMmSJXo3o5hffvkFg8HA+fPn9W6KEMILJCgSQuji9OnTmM1mcnJysFqt1K1bl9TUVOfv09LSGDBgAACHDx/GYDCwefNmn7Wvd+/ejB8/vthziYmJpKWlER4e7rN2CCF8R4IiIYQufvvtN6666ipCQkLYuHEjDRs2pEWLFs7fR0VFeWWdKIvFUunXBgYGEhUVhcFg8GCLhBD+QoIiIYQu1q5dS8+ePQFYvXq1c9uh6PCZY0Xsq6++GoPBQO/evZ37ffjhh7Rr147g4GDatm3L7Nmznb9z9DB9+eWX9O7dm+DgYD755BMyMjK49957ad68OSEhIXTq1InPPvvM+bphw4axcuVK3n77bQwGAwaDgcOHD7scPvvqq6/o0KEDQUFBxMXF8eabbxZ7H3Fxcbz22ms89NBDhIaG0qJFC9577z3PXEQhhGd5fNU2IYQoxZEjR7Tw8HAtPDxcM5vNWnBwsBYeHq4FBgZqQUFBWnh4uPbYY49pmqYWgPz66681TdO0P/74QwO0n3/+WUtLS9MyMjI0TdO09957T4uOjta++uor7eDBg9pXX32lNWzYUJs/f76maZp26NAhDdDi4uKc+xw/flw7duyY9u9//1vbtGmTduDAAW3GjBmayWTS1q1bp2mapp0/f17r0aOH9vDDD2tpaWlaWlqaZrVanYuwnjt3TtM0TduwYYNmNBq1KVOmaHv27NE+/PBDrU6dOsUWB42NjdUaNmyovfPOO9q+ffu0qVOnakajUdu1a5evLrsQwk0SFAkhfMZisWiHDh3StmzZopnNZm3z5s3a/v37tXr16mkrV67UDh06pJ0+fVrTtOJBkSO42bRpU7HjxcTEaJ9++mmx515++WWtR48exV43ffr0ctt26623ak899ZTz5xtuuEF74okniu1zeVB03333aX379i22z8SJE7X27ds7f46NjdXuv/9+5892u11r0qSJNmfOnHLbJITwLRk+E0L4TEBAAHFxcezevZtrr72WLl26kJ6eTmRkJNdffz1xcXFERES4dazTp09z9OhRRowYQb169Zx/XnnlFQ4cOFBs365duxb72Waz8eqrr9K5c2caNWpEvXr1WLZsWbFEb3fs2rWrxLBfz5492bdvHzabzfnc/2/f/mGZWwMAjD/Vj1TaEBI0KqlBJOrPUGmMJAZMFQsJQwezCN9KhE3iT8QiDB1FYhWjhJAmVhaJsJXFTEXv8OXKbXBzDdd18z2/qe/p6Xnfdnpy+p6urq7X14FAgGg0yv39/afmkvTv+/FfL0DS76O9vZ3b21sKhQIvLy9EIhGen595fn4mEokQj8e5uLj4R9d6eXkBYHt7m56enpL3gsFgyTgcDpeMV1ZWWFtbY319nc7OTsLhMNPT0zw9PX3q+xSLxTebrovF4pvzysvLS8aBQOB1/ZK+D6NI0pc5ODigUCjQ39/P8vIy3d3djI2NkclkGBwcfBMPf6qoqAAoufvS0NBALBbj+vqa8fHxT63j+PiYdDrNxMQE8Cuwrq6uaGtrK5nzr/O9J5FIcHJyUnLs9PSU1tbWN2Em6fsziiR9mXg8Tj6f5+7ujnQ6TVlZGZeXl4yMjNDY2Pjh5+rr66msrOTw8JCmpiZCoRDV1dUsLCwwNTVFVVUVQ0NDPD4+cn5+zsPDAzMzMx9er6Wlhf39fU5PT6mpqWF1dZV8Pl8SRc3NzeRyOW5ubohEItTW1r65zuzsLKlUiqWlJUZHRzk7O2Nzc7PkCThJ/x/uKZL0pY6OjkilUoRCIXK5HLFY7G+DCH7tRdrY2GBra4vGxkbS6TQAk5OT7OzskM1m6ezspLe3l2w2+/oI/0fm5uZIJpMMDAzQ19dHNBpleHi45JyfP38SDAZJJBLU1dW9u98omUyyt7fH7u4uHR0dzM/Ps7i4SCaT+dyPIulbCBTf+wNckiTpN+OdIkmSJIwiSZIkwCiSJEkCjCJJkiTAKJIkSQKMIkmSJMAokiRJAowiSZIkwCiSJEkCjCJJkiTAKJIkSQKMIkmSJAD+AGssa9IOgzO3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model([train_loader, test_loader], num_epochs=25, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ceb5783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.8748\n"
     ]
    }
   ],
   "source": [
    "print(f'Final test accuracy: {test_accuracies[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e128ed",
   "metadata": {},
   "source": [
    "## Visualization of the labels and predictions\n",
    "\n",
    "In this section, you should visual one image from each class and show both the actual label and the predicted label for that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6c0b79fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x2000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## FILL HERE\n",
    "data = test_set.data\n",
    "targets = test_set.targets\n",
    "plt.figure(figsize=(15,20))\n",
    "for i in range(num_classes):\n",
    "    class_data = data[targets == i,:,:]\n",
    "    random_idx = np.random.choice(class_data.shape[0],1)\n",
    "    x = class_data[random_idx,:,:]\n",
    "    x = x.type(torch.FloatTensor)\n",
    "    x = x.to(device)\n",
    "    \n",
    "    p = model(x, parameters)\n",
    "    y_pred = p.argmax(dim=-1)\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.imshow(class_data[random_idx,:,:].reshape(class_data.shape[1],class_data.shape[2]))\n",
    "    title = \"actual class: \"+str(i) + \"   &  predicted class: \"+ str(y_pred.item())\n",
    "    plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a8934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
